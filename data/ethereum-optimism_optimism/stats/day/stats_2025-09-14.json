{
  "interval": {
    "intervalStart": "2025-09-14T00:00:00.000Z",
    "intervalEnd": "2025-09-15T00:00:00.000Z",
    "intervalType": "day"
  },
  "repository": "ethereum-optimism/optimism",
  "overview": "From 2025-09-14 to 2025-09-15, ethereum-optimism/optimism had 5 new PRs (0 merged), 1 new issues, and 8 active contributors.",
  "topIssues": [
    {
      "id": "I_kwDODjvEJM6mxmwB",
      "title": "op-node: Improve archive node synchronization by adding configurable timeout flag",
      "author": "pengin7384",
      "number": 13852,
      "repository": "ethereum-optimism/optimism",
      "body": "**Is your feature request related to a problem? Please describe.**\n I'm operating multiple Base and Optimism archive nodes. During this process, I am encountering persistent issues where block synchronization slows down significantly.\n\n**Describe the solution you'd like**\nI would like to improve the synchronization speed of both the Base Mainnet and Optimism Mainnet archive nodes.\n\n**Describe alternatives you've considered**\nWhile observing the logs of the op-node during periods of slow synchronization, I noticed the following warning message appearing multiple times:\n\n```\nlvl=warn msg=\"Engine temporary error\" err=\"temporarily cannot insert new safe block: failed to create new block via forkchoice: context deadline exceeded\"\n```\n\nThis error occurs when the `engine_forkchoiceUpdatedV3` RPC call exceeds the configured timeout (5 seconds) and results in a request timeout. This behavior is intentional and expected, but when the request times out, the op-node retries the same RPC request, which introduces additional delay.\n\nTo address this, I experimented by increasing the timeout from 5 seconds to 30 seconds while synchronizing the Base Mainnet archive node. This change proved effective. The experiment involved three Base Mainnet archive nodes:\n\nBlue Node: The node that was synchronizing well (no special settings were applied to it other than displaying the latest block number).\nYellow Node: The node with synchronization issues (timeout increased from 5 seconds to 30 seconds).\nGreen Node: The node using the official image without any modifications.\nAfter one week of monitoring synchronization, as shown in the attached graph:\n\n<img width=\"824\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/93620912-2206-4514-8e5e-5781343715a4\" />\n\nThe Yellow Node completed synchronization up to the latest block.\nThe Green Node still had not synchronized because its synchronization speed matched the block generation rate.\nFurther investigation through trace logs revealed that for blocks containing transactions with hundreds of logs, the `engine_forkchoiceUpdatedV3` RPC call could take more than 10 seconds. (`DEBUG[01-19|05:13:53.027] Served engine_forkchoiceUpdatedV3        reqid=4941 duration=20.337986076s`)\n\nBased on these findings, I suggest the following improvements:\nAdd a flag to make the timeout adjustable, allowing users to set it according to their needs.\nFor full node operators, this change may not have a significant impact since they are already synchronizing well. \nHowever, for archive node operators experiencing slow synchronization, this flexible timeout could be beneficial. \nImportantly, this change should not negatively impact any operators.\n\n**Additional context**\nThe versions used for testing were op-node v1.10.2 and op-geth v1.101411.4, with all other specs and flags remaining the same except for the timeout. The experiment was conducted on an in-house server.\n\nI am running nodes both on my own server and in an AWS EC2 environment, and both environments are experiencing similar synchronization speed degradation. However, one unusual observation is that once the node catches up to the latest block, synchronization degradation does not occur frequently. But when the node is restarted, the blocks start to fall behind again. The proposed changes were helpful in this scenario.\n\nAdditionally, I also attempted to run archive nodes with reth, but due to its behavior of syncing multiple blocks at once, I observed that the gap would grow by about 10 blocks, then close again, repeating this cycle. As a result, I'm currently using op-geth.\n\n\n",
      "createdAt": "2025-01-20T02:24:45Z",
      "closedAt": "2025-09-14T07:25:28Z",
      "state": "CLOSED",
      "commentCount": 1
    },
    {
      "id": "I_kwDODjvEJM7LjzER",
      "title": "op-node invalid block signature unrecognized signer",
      "author": "wluisw",
      "number": 17448,
      "repository": "ethereum-optimism/optimism",
      "body": "<img width=\"2548\" height=\"767\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e1149b4e-97b2-46e1-b66a-bd70d95bf9c4\" />\n\nI encountered the above problems while deploying on the testnet. What could be the reason",
      "createdAt": "2025-09-14T15:06:16Z",
      "closedAt": "2025-09-24T09:59:36Z",
      "state": "CLOSED",
      "commentCount": 0
    }
  ],
  "topPRs": [
    {
      "id": "PR_kwDODjvEJM6hLsA1",
      "title": "op-node, op-devstack: initial execution multiplexing",
      "author": "nonsense",
      "number": 16873,
      "body": "**Description**\r\n\r\nThis PR is adding support for op-node to run multiple execution engines for a given chain.\r\n\r\nDesign document at: https://www.notion.so/oplabs/Execution-Multiplexing-Specific-Implementation-and-Plan-23af153ee1628051a7e4ece97fff689e\r\n\r\nFixes: https://github.com/ethereum-optimism/optimism/issues/16822",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-07-29T13:45:25Z",
      "mergedAt": null,
      "additions": 253,
      "deletions": 29
    },
    {
      "id": "PR_kwDODjvEJM6oeOok",
      "title": "Dynamic cancellation recursive timelock",
      "author": "alcueca",
      "number": 17444,
      "body": "**Description**\r\n\r\nImplementation of a dynamic cancellation threshold timelock using a recursive cancellation algorithm.\r\n\r\n\r\n",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-09-14T09:00:53Z",
      "mergedAt": null,
      "additions": 65,
      "deletions": 2
    },
    {
      "id": "PR_kwDODjvEJM6oePNw",
      "title": "Dynamic cancellation forward timelock",
      "author": "alcueca",
      "number": 17445,
      "body": "Description\r\n\r\nImplementation of a dynamic cancellation threshold timelock using a forward cancellation algorithm.",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-09-14T09:04:16Z",
      "mergedAt": null,
      "additions": 59,
      "deletions": 8
    },
    {
      "id": "PR_kwDODjvEJM6oePfX",
      "title": "Static cancellation recursive timelock",
      "author": "alcueca",
      "number": 17446,
      "body": "Description\r\n\r\nImplementation of a static cancellation threshold timelock using a recursive cancellation algorithm.",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-09-14T09:05:38Z",
      "mergedAt": null,
      "additions": 48,
      "deletions": 6
    },
    {
      "id": "PR_kwDODjvEJM6oePrG",
      "title": "Static cancellation forward timelock",
      "author": "alcueca",
      "number": 17447,
      "body": "Description\r\n\r\nImplementation of a static cancellation threshold timelock using a forward cancellation algorithm.",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-09-14T09:06:40Z",
      "mergedAt": null,
      "additions": 37,
      "deletions": 9
    }
  ],
  "codeChanges": {
    "additions": 0,
    "deletions": 0,
    "files": 0,
    "commitCount": 6
  },
  "completedItems": [],
  "topContributors": [
    {
      "username": "wiz-inc-a178a98b5d",
      "avatarUrl": "https://avatars.githubusercontent.com/u/58791460?v=4",
      "totalScore": 45.2,
      "prScore": 0,
      "issueScore": 0,
      "reviewScore": 45,
      "commentScore": 0.2,
      "summary": null
    },
    {
      "username": "ajit2903",
      "avatarUrl": "https://avatars.githubusercontent.com/u/97722590?v=4",
      "totalScore": 39.8887738965761,
      "prScore": 35.3887738965761,
      "issueScore": 0,
      "reviewScore": 4.5,
      "commentScore": 0,
      "summary": null
    },
    {
      "username": "fakedev9999",
      "avatarUrl": "https://avatars.githubusercontent.com/u/71915289?u=f73e198cb79fc2db8b768a87e3fafff378f5d6cd&v=4",
      "totalScore": 36.18832709429617,
      "prScore": 35.98832709429617,
      "issueScore": 0,
      "reviewScore": 0,
      "commentScore": 0.2,
      "summary": null
    },
    {
      "username": "alcueca",
      "avatarUrl": "https://avatars.githubusercontent.com/u/38806121?u=05446d587fca577d19ea91499b0ae9ce4d66c3ae&v=4",
      "totalScore": 25.55224809864737,
      "prScore": 25.55224809864737,
      "issueScore": 0,
      "reviewScore": 0,
      "commentScore": 0,
      "summary": null
    },
    {
      "username": "wlawt",
      "avatarUrl": "https://avatars.githubusercontent.com/u/22751307?u=1c1135cc903766b3c5fb1f140e565e967e78e890&v=4",
      "totalScore": 4.5,
      "prScore": 0,
      "issueScore": 0,
      "reviewScore": 4.5,
      "commentScore": 0,
      "summary": null
    },
    {
      "username": "joshklop",
      "avatarUrl": "https://avatars.githubusercontent.com/u/31332481?v=4",
      "totalScore": 4.5,
      "prScore": 0,
      "issueScore": 0,
      "reviewScore": 4.5,
      "commentScore": 0,
      "summary": null
    },
    {
      "username": "wluisw",
      "avatarUrl": "https://avatars.githubusercontent.com/u/145568895?u=036a41317a4f321c9f30f3e99589dd95e37b4c4b&v=4",
      "totalScore": 4,
      "prScore": 0,
      "issueScore": 4,
      "reviewScore": 0,
      "commentScore": 0,
      "summary": null
    }
  ],
  "newPRs": 5,
  "mergedPRs": 0,
  "newIssues": 1,
  "closedIssues": 1,
  "activeContributors": 8
}