{
  "interval": {
    "intervalStart": "2025-12-29T00:00:00.000Z",
    "intervalEnd": "2025-12-30T00:00:00.000Z",
    "intervalType": "day"
  },
  "repository": "ethereum-optimism/optimism",
  "overview": "From 2025-12-29 to 2025-12-30, ethereum-optimism/optimism had 6 new PRs (0 merged), 1 new issues, and 7 active contributors.",
  "topIssues": [
    {
      "id": "I_kwDODjvEJM7giGQs",
      "title": "[Question]  When a new chain's  op-batcher  run  in  flag of  `--data-availability-type=callData` ,  Is there have some potential problems?",
      "author": "lyh169",
      "number": 18698,
      "repository": "ethereum-optimism/optimism",
      "body": "If my op-batcher set the flag of  `--data-availability-type=callData ` , that means  the op-batcher does not put the batch data into the blob, but put  in the callData.  It no need to configure the L1 beacon in the op-node and op-challenger. Is there have some potential problems , about op-node  and op-challenger?",
      "createdAt": "2025-12-29T10:29:00Z",
      "closedAt": null,
      "state": "OPEN",
      "commentCount": 0
    }
  ],
  "topPRs": [
    {
      "id": "PR_kwDODjvEJM66zNzD",
      "title": "feat(conductor): fix leader loop switch by adding round-robin leadership transfer",
      "author": "KyrinCode",
      "number": 18697,
      "body": "**Description**\r\n\r\nThis PR adds an optional round-robin leader transfer mechanism to op-conductor, configurable via the `--raft.round-robin-leader-transfer` flag.\r\n\r\n**Problem**\r\n\r\nThe current leader transfer in op-conductor relies on Raft's default `LeadershipTransfer()`, which selects the next leader based on log completeness (the follower with the most up-to-date log). However, this selection mechanism is unaware of application-level health status (op-node & execution layer).\r\nThis can cause a problematic scenario where unhealthy nodes keep being elected as leader in a loop:\r\n1. Node A becomes leader but is unhealthy → transfers to Node B (most complete log)\r\n2. Node B becomes leader but is also unhealthy → transfers back to Node A (now has most complete log)\r\n3. Healthy Node C never gets elected because its log is slightly behind\r\n\r\n**Solution**\r\n\r\nIntroduce a deterministic round-robin leader transfer that cycles through all voters in sorted order (by ServerID):\r\n- Node A → Node B → Node C → Node A → ...\r\nThis ensures that even if only one node in the cluster is healthy, it will eventually become the leader after at most N-1 transfers.\r\n\r\n**Changes**\r\n\r\n- Added `--raft.round-robin-leader-transfer` flag (default: false, backward compatible)\r\n- Added transferLeaderRoundRobin() function that:\r\n  - Sorts all voters by ServerID for consistent ordering across nodes\r\n  - Transfers to the next node in sorted order\r\n  - If transfer fails (e.g., target has stale logs), tries the next node\r\n  - Handles \"leadership transfer in progress\" gracefully\r\n\r\n**Tests**\r\n\r\nManual testing was performed with a 4-node conductor cluster where 2 nodes were intentionally made unhealthy. With round-robin enabled, leadership eventually transferred to a healthy node. Without round-robin, leadership kept bouncing between the unhealthy nodes.\r\n\r\n**Additional context**\r\n\r\nThis feature is opt-in and disabled by default to maintain backward compatibility. When disabled, the behavior is identical to the current implementation.\r\nThe round-robin approach trades off Raft's \"most up-to-date log\" optimization for guaranteed leader rotation. In practice, this is acceptable because:\r\nAll voters in a healthy cluster should have similar log states\r\nThe primary goal of leader transfer in op-conductor is to find a healthy sequencer, not to optimize for log completeness\r\nIf a target node's log is too stale, Raft will reject the transfer and we try the next node\r\n\r\n**Metadata**\r\n\r\n- Related to sequencer high-availability and failover reliability\r\n",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-12-29T09:35:38Z",
      "mergedAt": null,
      "additions": 113,
      "deletions": 1
    },
    {
      "id": "PR_kwDODjvEJM66yRTr",
      "title": "test(contracts): improve OptimismMintableERC20 test coverage",
      "author": "devin-ai-integration",
      "number": 18696,
      "body": "## Summary\n\nEnhances the `OptimismMintableERC20` test file by converting focused tests to fuzz tests for broader coverage and adding tests for previously uncovered code paths.\n\n**Changes:**\n- Convert 6 focused tests to fuzz tests (mint, burn, permit2 transferFrom, allowance)\n- Add coverage for `allowance()` non-PERMIT2 branch\n- Add coverage for `supportsInterface()` false return case\n- Add tests for `decimals()` and `version()` functions\n- Fix assertion style: `assert()` → `assertTrue()` (Forge convention)\n- Improve test naming to follow conventions\n\n## Review & Testing Checklist for Human\n\n- [ ] Verify the `test_decimals_succeeds` assertion of 18 decimals matches how `L2Token` is created in `CommonTest.bridgeInitializerSetUp()` via `createStandardL2Token()`\n- [ ] Confirm fuzz test constraints are appropriate (e.g., `vm.assume(_owner != _to)` in permit2 test prevents self-transfer edge case - is this intentional?)\n- [ ] Run `just test-dev --match-path test/universal/OptimismMintableERC20.t.sol -v` to verify all 16 tests pass\n\n**Recommended test plan:** Run the test suite locally and verify the fuzz tests exercise meaningful value ranges.\n\n### Notes\n\nLink to Devin run: https://app.devin.ai/sessions/84c09a1ef33944368652d4d50888283a\nRequested by: Ariel Diaz (@aliersh)\n\nLast update: 2025-12-29 03:00 ET",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-12-29T07:13:34Z",
      "mergedAt": null,
      "additions": 80,
      "deletions": 27
    },
    {
      "id": "PR_kwDODjvEJM660guH",
      "title": "feat(conductor): add raft config to allow no shutdown on remove",
      "author": "KyrinCode",
      "number": 18699,
      "body": "**Description**\r\n\r\nThis PR adds a new flag `--raft.no-shutdown-on-remove` to `op-conductor` that prevents Raft from shutting down when the node is removed from the cluster.\r\n\r\n**Problem**\r\n\r\nWhen a node is removed from the cluster via `conductor_removeServer`, HashiCorp Raft's default behavior (`ShutdownOnRemove=true`) causes the Raft instance to completely shut down. This means:\r\n- The node cannot be re-added to the cluster without restarting the entire process\r\n- This creates operational complexity when managing cluster membership dynamically\r\n- In scenarios where nodes need to be temporarily removed and re-added (e.g., during maintenance or role changes), a full restart is required\r\n\r\n**Solution**\r\n\r\nAdd a configurable flag that sets `ShutdownOnRemove=false` in Raft configuration. When enabled:\r\n- The node transitions to follower state instead of shutting down when removed\r\n- The node can be immediately re-added to the cluster via `conductor_addServerAsVoter` or `conductor_addServerAsNonvoter`\r\n- No process restart is required\r\n\r\n**Changes**\r\n\r\n- Added `--raft.no-shutdown-on-remove` flag (default: `false`, backward compatible)\r\n- Added `NoShutdownOnRemove` field to `RaftConsensusConfig`\r\n- Configured Raft's `ShutdownOnRemove` based on the flag value\r\n\r\n**Tests**\r\n\r\nManual testing was performed:\r\n1. Started a 3-node conductor cluster with `--raft.no-shutdown-on-remove` enabled\r\n2. Removed a node via `conductor_removeServer`\r\n3. Verified the node transitioned to follower state (instead of Raft shutting down)\r\n4. Re-added the node via `conductor_addServerAsVoter`\r\n5. Verified the node successfully rejoined the cluster without restarting (by checking the `conductor_clusterMembership` of this node)\r\n\r\n**Additional context**\r\n\r\nThis feature is opt-in and disabled by default to maintain backward compatibility with existing deployments. The default Raft behavior (shutdown on remove) is preserved unless explicitly configured otherwise.\r\nUse cases for this flag:\r\n- Dynamic cluster membership management without downtime\r\n- Graceful role transitions (voter ↔ nonvoter)\r\n- Easier disaster recovery scenarios\r\n\r\n**Metadata**\r\n\r\n- Related to cluster membership management and operational flexibility\r\n",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-12-29T12:36:47Z",
      "mergedAt": null,
      "additions": 27,
      "deletions": 0
    },
    {
      "id": "PR_kwDODjvEJM661ySn",
      "title": "op-batcher: Nil Pointer Dereference in `getReadyChannel`",
      "author": "blockchaindevsh",
      "number": 18701,
      "body": "When `forcePublish` is true and `currentChannel` is nil, the batcher panics due to a nil pointer dereference, causing the entire batcher process to crash.\r\n\r\n\r\n",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-12-29T15:30:17Z",
      "mergedAt": null,
      "additions": 20,
      "deletions": 1
    },
    {
      "id": "PR_kwDODjvEJM660ll4",
      "title": "feat(conductor): add demote-voter to conductor rpc",
      "author": "KyrinCode",
      "number": 18700,
      "body": "**Description**\r\n\r\nThis PR exposes the `DemoteVoter` method via the conductor RPC API, allowing operators to demote a voting member to a non-voting member without removing and re-adding the node.\r\n\r\n**Problem**\r\n\r\nThe consensus layer already has a `DemoteVoter` method implemented, but it was not exposed through the RPC API. Without this RPC endpoint, operators who wanted to demote a voter to nonvoter had to:\r\n1. Remove the server via `conductor_removeServer`\r\n2. Re-add it via `conductor_addServerAsNonvoter`\r\nThis workaround has a significant drawback: when a node is removed, Raft's default behavior causes it to shut down, requiring a process restart before re-adding.\r\n\r\n**Solution**\r\n\r\nExpose the existing `DemoteVoter` consensus method through the RPC API as `conductor_demoteVoter`. This allows:\r\n- Direct demotion of a voter to nonvoter in a single RPC call\r\n- No Raft shutdown or process restart required\r\n- Seamless role transitions within the cluster\r\n\r\n**Changes**\r\n\r\n- Added `DemoteVoter` method to `rpc/api.go` interface\r\n- Added `DemoteVoter` implementation in `rpc/backend.go`\r\n- Added `DemoteVoter` client method in `rpc/client.go`\r\n- Added `DemoteVoter` handler in `conductor/service.go`\r\n\r\n**Usage**\r\n\r\n```\r\ncurl -X POST http://<conductor>:8547 \\\r\n  -d '{\"jsonrpc\":\"2.0\",\"method\":\"conductor_demoteVoter\",\"params\":[\"<server-id>\", 0],\"id\":1}'\r\n```\r\n\r\n**Tests**\r\n\r\nManual testing was performed:\r\n1. Started a 3-node conductor cluster with all nodes as voters\r\n2. Called `conductor_demoteVoter` on one node\r\n3. Verified the node transitioned from voter to nonvoter\r\n4. Verified cluster membership reflected the change\r\n5. Verified the demoted node continued to receive log replication\r\nThis is a straightforward RPC exposure of an existing consensus method. The underlying `DemoteVoter` functionality is already tested as part of HashiCorp Raft.\r\n\r\n**Additional context**\r\n\r\nNote: If the leader is demoted, it will trigger a new leader election. This is expected Raft behavior.\r\n\r\n**Metadata**\r\n\r\nComplements existing cluster management APIs (`addServerAsVoter`, `addServerAsNonvoter`, `removeServer`)\r\n",
      "repository": "ethereum-optimism/optimism",
      "createdAt": "2025-12-29T12:48:34Z",
      "mergedAt": null,
      "additions": 19,
      "deletions": 0
    }
  ],
  "codeChanges": {
    "additions": 0,
    "deletions": 0,
    "files": 0,
    "commitCount": 7
  },
  "completedItems": [],
  "topContributors": [
    {
      "username": "KyrinCode",
      "avatarUrl": "https://avatars.githubusercontent.com/u/30864546?v=4",
      "totalScore": 57.455271760003264,
      "prScore": 57.455271760003264,
      "issueScore": 0,
      "reviewScore": 0,
      "commentScore": 0,
      "summary": null
    },
    {
      "username": "MSilb7",
      "avatarUrl": "https://avatars.githubusercontent.com/u/4006780?u=ec77ee3977c8adca2b0c7d1e2bd64d8624f03381&v=4",
      "totalScore": 22.06775477931522,
      "prScore": 22.06775477931522,
      "issueScore": 0,
      "reviewScore": 0,
      "commentScore": 0,
      "summary": null
    },
    {
      "username": "devin-ai-integration",
      "avatarUrl": "https://avatars.githubusercontent.com/in/811515?v=4",
      "totalScore": 15.49806561356211,
      "prScore": 15.29806561356211,
      "issueScore": 0,
      "reviewScore": 0,
      "commentScore": 0.2,
      "summary": null
    },
    {
      "username": "blockchaindevsh",
      "avatarUrl": "https://avatars.githubusercontent.com/u/100516036?v=4",
      "totalScore": 13.086654742026425,
      "prScore": 13.086654742026425,
      "issueScore": 0,
      "reviewScore": 0,
      "commentScore": 0,
      "summary": null
    },
    {
      "username": "lyh169",
      "avatarUrl": "https://avatars.githubusercontent.com/u/44960676?v=4",
      "totalScore": 2,
      "prScore": 0,
      "issueScore": 2,
      "reviewScore": 0,
      "commentScore": 0,
      "summary": null
    }
  ],
  "newPRs": 6,
  "mergedPRs": 0,
  "newIssues": 1,
  "closedIssues": 0,
  "activeContributors": 7
}