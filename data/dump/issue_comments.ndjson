["IC_kwDOKIwiaM68kvZ5", "I_kwDOKIwiaM694diW", "0xaed4b681fb202fed67f47e8580303f871db37ada", "2025-08-07T11:39:27Z", "2025-08-07T11:39:27Z", "Saad4343", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM68kvaT", "I_kwDOKIwiaM694diW", "0xaed4b681fb202fed67f47e8580303f871db37ada", "2025-08-07T11:39:27Z", "2025-08-07T11:39:27Z", "Saad4343", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM68k6cG", "I_kwDOKIwiaM694diW", "0xaed4b681fb202fed67f47e8580303f871db37ada", "2025-08-07T11:44:36Z", "2025-08-07T11:44:36Z", "Saad4343", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM68k6d0", "I_kwDOKIwiaM694diW", "0xaed4b681fb202fed67f47e8580303f871db37ada", "2025-08-07T11:44:37Z", "2025-08-07T11:44:37Z", "Saad4343", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM6j3LgR", "I_kwDOKIwiaM6vfRhv", "@opfocus This is such a nice initiative, i'll go through this and let you know my experience.\nThanks for putting this together.", "2025-03-24T19:10:05Z", "2025-03-24T19:10:05Z", "krofax", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM6kJ0Qq", "I_kwDOKIwiaM6vfRhv", "https://github.com/ethereum-optimism/docs/issues/1530", "2025-03-26T11:08:11Z", "2025-03-26T11:08:11Z", "Hakkmihakim", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM6lJG5g", "I_kwDOKIwiaM6vfRhv", "@opfocus op-deployer only deploys the permissioned dispute game, game type 1. It also does it with a placeholder for the absolute prestate hash, which goes in each game type's contract. It's a circular dependency because to use the preimage and absolute prestate hash, you need the chains configuration included in there, but you can't do that until the chain has been deployed. \n\n@krofax is working on a a document that goes into these details here: https://github.com/ethereum-optimism/docs/pull/1539", "2025-04-01T20:38:29Z", "2025-04-01T20:38:29Z", "sbvegan", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM6ma-2Z", "I_kwDOKIwiaM6vfRhv", "Suppose I want to have one chain deployed with permissioned fault proofs. Is there any issue that could arise from this configuration discrepancy?", "2025-04-10T09:12:32Z", "2025-04-10T09:15:45Z", "seromenho", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM6nsfij", "I_kwDOKIwiaM6vfRhv", "Currently, docs is mixed up between an old `tutorials/chain` manual deployment way and the new `op-deployer` way make it impossible for external chain operator to follow and launch their chain.\n\n<img width=\"857\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f8f0b582-3b33-4ccf-bbc0-59c59671e08f\" />\n\n<img width=\"870\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c5f51bbf-d804-496f-a049-8a22aa70773d\" />\n\n<img width=\"889\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/69abb7a4-8a72-455c-8fd7-4a80f6f9f7a3\" />\n\nAlso I think op-deployer now default to FaultDisputeGame not L2OO?", "2025-04-17T16:12:56Z", "2025-04-17T16:12:56Z", "Chomtana", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM6nxaZt", "I_kwDOKIwiaM6vfRhv", "We've observed a significantly high number of RPC calls on Alchemy when using the permissioned fault-proof system. The majority of these are eth_call requests targeting the Permissioned Dispute Game contracts \u2014 totaling approximately 13-15 million requests per day.\n\nIt appears the challenger is making these calls very frequently, which may be contributing heavily to the RPC load.\n\nChallenger version: v1.4.0\n", "2025-04-18T07:14:45Z", "2025-04-18T07:14:45Z", "Visheshgpt", "2025-08-30 03:26:25"]
["IC_kwDOKIwiaM7Aw_6R", "I_kwDOKIwiaM6vfRhv", "Hi @opfocus I have a [rich PR](https://github.com/ethereum-optimism/docs/pull/1705/), that overhauls the old \"Create an op stack Rollup testnet with op-deployer\", do you mind taking a look?", "2025-08-28T15:56:40Z", "2025-08-28T15:56:47Z", "krofax", "2025-08-30 03:26:25"]
["IC_kwDOH2Qg5s7Azr7z", "I_kwDOH2Qg5s7IfUtn", "Option C would be to have a periodic job in the monorepo that checks if it is using the latest op-geth commit from optimism branch. It would notify in slack if it fails. This doesn't prevent creating the problem but does alert us to it pretty quickly and we can fix it either by updating the monorepo if it's easy or reverting the op-geth commit if not. Downside is that you will get some false alarms if the job runs between the op-geth PR merging and the corresponding monorepo PR merging.\n\nNo strong opinion on the best option here but thought I'd throw this out there.", "2025-08-28T20:01:03Z", "2025-08-28T20:01:03Z", "ajsutton", "2025-08-30 03:26:26"]
["IC_kwDOLB-lzc67gUh4", "I_kwDOLB-lzc7C6RMU", "We would like to compute a rough estimate for the state growth implied by this feature. It can be included in the motivation section of the specs, so adding it to this ticket. ", "2025-08-01T21:26:38Z", "2025-08-01T21:26:38Z", "geoknee", "2025-08-30 03:26:30"]
["IC_kwDOLB-lzc69QecW", "I_kwDOLB-lzc7C6RMU", "> We would like to compute a rough estimate for the state growth implied by this feature. It can be included in the motivation section of the specs, so adding it to this ticket.\n\nDecided this was better placed in the FMA, tracking that here https://github.com/ethereum-optimism/design-docs/issues/316", "2025-08-11T14:51:01Z", "2025-08-11T14:51:01Z", "geoknee", "2025-08-30 03:26:30"]
["IC_kwDOLB-lzc7A6fX7", "I_kwDOLB-lzc7C6RMU", "Via https://github.com/ethereum-optimism/specs/pull/747", "2025-08-29T10:29:35Z", "2025-08-29T10:29:35Z", "geoknee", "2025-08-30 03:26:30"]
["IC_kwDODjvEJM7AzQGe", "I_kwDODjvEJM7IWExg", "> Flashblocks tests should be enabled against persistent devnets. The gate is there and pretty much the only work needed is to ensure it does work correctly. Task size XS-S.\n\n", "2025-08-28T19:17:07Z", "2025-08-28T19:17:07Z", "Stevmark11", "2025-08-30 03:27:07"]
["IC_kwDODjvEJM7AU9le", "I_kwDODjvEJM7H4bXl", "Can I take this one\uff1f@mslipper", "2025-08-27T04:45:32Z", "2025-08-27T04:45:32Z", "tooshiNoko", "2025-08-30 03:27:07"]
["IC_kwDODjvEJM7AU9oR", "I_kwDODjvEJM7H4bMO", "Can I take this one\uff1f@mslipper", "2025-08-27T04:45:40Z", "2025-08-27T04:45:40Z", "tooshiNoko", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7AU9qn", "I_kwDODjvEJM7H4bFv", "Can I take this one\uff1f@mslipper", "2025-08-27T04:45:48Z", "2025-08-27T04:45:48Z", "tooshiNoko", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7AU9sB", "I_kwDODjvEJM7H4bBy", "Can I take this one\uff1f@mslipper", "2025-08-27T04:45:52Z", "2025-08-27T04:45:52Z", "tooshiNoko", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7AU9ww", "I_kwDODjvEJM7H4a3c", "Can I take this one\uff1f@mslipper", "2025-08-27T04:46:06Z", "2025-08-27T04:46:06Z", "tooshiNoko", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7AZKOi", "I_kwDODjvEJM7H4aXm", "Hi, @bitwiseguy  I just noticed you self-assigned this. I just submitted #17235 \u2014maybe we can sync up on the progress to avoid overlapping work? BTW I would love to take all of them if possible.", "2025-08-27T11:29:34Z", "2025-08-27T11:29:34Z", "tooshiNoko", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM6-4uim", "I_kwDODjvEJM7Gyo49", "This workflow job (\"go-tests-short\") isn't meant for acceptance tests.\nIt does have a few sysgo acceptance tests, but they aren't run via `op-acceptor` which would nicely capture the logs into files for us.\n\nI'm currently working on moving them across, which should resolve this issue:\nhttps://github.com/ethereum-optimism/optimism/pull/16755", "2025-08-19T22:41:48Z", "2025-08-19T22:41:48Z", "scharissis", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM6__trR", "I_kwDODjvEJM7Gyo49", "The logs have been found in one of the root logs", "2025-08-25T17:37:46Z", "2025-08-25T17:37:46Z", "janjakubnanista", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM6-PFcE", "I_kwDODjvEJM7GNdTW", "https://github.com/ethereum-optimism/specs/pull/748", "2025-08-15T14:23:26Z", "2025-08-15T14:23:26Z", "mslipper", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7ABXGJ", "I_kwDODjvEJM7FXwvl", "This was done at retro last week.", "2025-08-25T20:06:36Z", "2025-08-25T20:06:36Z", "mslipper", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM6-PD4d", "I_kwDODjvEJM7FVBIC", "Design doc is here: https://github.com/ethereum-optimism/design-docs/pull/317/", "2025-08-15T14:20:40Z", "2025-08-15T14:20:40Z", "mslipper", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7ACkZD", "I_kwDODjvEJM7FUE2V", "Without op-geth eliminating all use of the global logger, this is likely not worthwhile.", "2025-08-25T22:12:04Z", "2025-08-25T22:12:04Z", "joshklop", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7ADXlO", "I_kwDODjvEJM7FUAhI", "Distributing the challenger in `op-up` for use in `sysgo` is likely more of a lift that it's worth. If anything, it's better to focus on other backends like Docker that don't require us to distribute binaries.", "2025-08-26T00:09:58Z", "2025-08-26T00:09:58Z", "joshklop", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM6-C262", "I_kwDODjvEJM7DIXUs", "@pcw109550 I have added PR, i will be grateful if you look at, when you have free time", "2025-08-14T13:15:45Z", "2025-08-14T13:15:45Z", "sashass1315", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM6-OlHo", "I_kwDODjvEJM7DIXUs", "@sashass1315 Thanks for your contribution. However we are leaning towards removing the entire event system, so this shall be not merged. We may still use the code to follow the control flow while removing the event system.", "2025-08-15T13:24:08Z", "2025-08-15T13:24:31Z", "pcw109550", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7ABWuV", "I_kwDODjvEJM7DIXUs", "We have something like this in https://github.com/ethereum-optimism/optimism/pull/16793, closing.", "2025-08-25T20:05:58Z", "2025-08-25T20:05:58Z", "mslipper", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7ARHsq", "I_kwDODjvEJM7C_jKk", "Hmm theres already peer count alerting for op-node sequencer peering  <2 [link](https://github.com/ethereum-optimism/k8s/blob/2510a58115ef8cc3dc4d39e6e5ca30f150a9b1e6/tf/grafana/parametrized-rules/peer-class-parameterized.yaml?plain=1#L115)\n\nWill probably just add reth/op-rbuilder peer count for this one\n", "2025-08-26T20:59:59Z", "2025-08-26T21:00:08Z", "jelias2", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7ARIBN", "I_kwDODjvEJM7C_hmk", "https://github.com/flashbots/rollup-boost/pull/400\n\nOnce this lands will create the alert", "2025-08-26T21:00:34Z", "2025-08-26T21:00:34Z", "jelias2", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM66caTv", "I_kwDODjvEJM7CV4hm", "Other in-flight work to keep track of:\n- https://github.com/alloy-rs/op-alloy/pull/580\n- https://github.com/ethereum-optimism/optimism/pull/16852 ", "2025-07-28T16:19:48Z", "2025-07-28T16:19:48Z", "wlawt", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM67dRb5", "I_kwDODjvEJM7CV4hm", "Adding this early reth draft so that we can call out any decisions earlier and have more eyes on it:\nhttps://github.com/paradigmxyz/reth/pull/17704 ", "2025-08-01T15:46:47Z", "2025-08-01T15:46:47Z", "wlawt", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7AHtlZ", "I_kwDODjvEJM7BouYk", "@yashvardhan-kukreja @zhwrd can you link to a doc that summarizes the outcome - what load test scenarios we ran through, on which network the results, etc.?", "2025-08-26T08:55:44Z", "2025-08-26T08:55:44Z", "alfonso-op", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7ALQWN", "I_kwDODjvEJM7BouYk", "oh oops, moved this to done by accident. yeah working on the report", "2025-08-26T13:26:04Z", "2025-08-26T13:26:04Z", "zhwrd", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7AI7--", "I_kwDODjvEJM7Apbxc", "Test case Ideas(ordered by priority)\n\n1. Check every OP Stack HF transitions, not only Isthmus, but all HFs for real data.\n2. Multiple CLs, Single Sync Tester: Each CLs will initialize their own mock EL endpoints given by the sync tester. This validates that multiple CLs can sync together.\n3. User Tx with Sync Tester: Current tests only sync blocks which only contain deposit tx. Create user tx inside L2 blocks and check the L2CLs are synced also.\n\nItem 2, 3 are captured at different issue: https://github.com/ethereum-optimism/optimism/issues/17313", "2025-08-26T10:24:34Z", "2025-09-03T15:04:06Z", "pcw109550", "2025-08-30 03:27:08"]
["IC_kwDODjvEJM7AiOyN", "I_kwDODjvEJM68XM2y", "This task is deprioritized right now and will not be implemented. A summary so far:\n- Tests have pretty much been implemented, there are two outstanding (linked) PRs that need to be cleaned up and merged.\n- Estimate (at the time of this writing) - S size.", "2025-08-27T23:57:07Z", "2025-08-27T23:57:07Z", "serpixel", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM65qB3d", "I_kwDODjvEJM67Cx_F", "Very rough notes after a call b/w me and Jacob:\n \n- peer count - min. 4\n- SLAs around block progression and divergence\n\n- pending transactions - if they go beyond ~60 or spike  (be very verbose with the runbook)\n\n- flashblocks related SLA-like metric (p99 over <some time interval>) - reth_op_rbuilder_total_block_built_duration{namespace=\"$namespace\"}\n\nattach unique label to them for the ease of silencing all.\n\n\nflashblocks-websocket-proxy connections - \n- related to connection failures\n- errors related would be helpful as well\n- the absence of these metrics should be an alert.\n\n\nflashblocks-rpc node-reth metrics related to failing to listening to the flashblocks-websocket-proxy despite the latter being healthy:\n- upstream errors (count below a number)\n- upstream messages (rate below a number)\n\n\nop-node-rbuilder\n- inherit most of the op-node alerts and metrics.\n(look for any metrics exposed)\n\n------\n\nLinks\n\nhttps://github.com/base/node-reth/blob/329e1ad3707e023a748a4b60019aeeeb9a4ace6f/crates/flashblocks-rpc/src/metrics.rs?plain=1#L5", "2025-07-24T20:18:14Z", "2025-07-24T20:29:02Z", "yashvardhan-kukreja", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6_1Dyk", "I_kwDODjvEJM67Cx_F", "Associated PRs:\n- https://github.com/ethereum-optimism/k8s/pull/7162", "2025-08-24T20:26:50Z", "2025-08-24T20:26:50Z", "yashvardhan-kukreja", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6waG7e", "I_kwDODjvEJM61wUco", "https://github.com/ethereum-optimism/optimism/pull/16340", "2025-06-10T15:09:23Z", "2025-06-10T15:09:23Z", "stevennevins", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM62B1_o", "I_kwDODjvEJM61wUco", "- [x] Move WETH to the implArgs\n- [x] Move L2ChainId to the implArgs\n- [x] \u2028Fix StandardValidator assertions that are running twice\nside\n- [ ] \u2028StandardValidator assertion update fixes\n- [ ] Remove anchorStateRegistry from StandardValidatorOverrides\n- [ ] Unskip and Update two skipped tests in DeployOPChain.t.sol\n- [ ] Review comment responses/mitigations\n- [ ] Address TODO comments to come back to that Adrian flagged\n- [x] Rebase some updates from EVM safety in their refactoring effort that caused conflicts\n- [ ] \u2028Fix the Go updates on the Go test/integration ", "2025-07-09T20:37:50Z", "2025-07-28T20:27:00Z", "stevennevins", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6lfjsB", "I_kwDODjvEJM6xCDU7", "Valid. The spec should also be fixed to indicate that the maximum number of chains is 126 rather than 127.", "2025-04-03T17:43:50Z", "2025-04-03T17:43:50Z", "Inphi", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6-SYpP", "I_kwDODjvEJM6tVetd", "Reopening this tracker issue until it's shipped. (Note for the Proofs team: I set it as an \"epic\" so it won't show up on the main view of the project board, only under the epics tab).\n\nI'll also add a sub-issue for the OPCM changes.", "2025-08-15T19:14:21Z", "2025-08-15T19:14:21Z", "pauldowman", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM7A_bXj", "I_kwDODjvEJM6okMy9", "Some additional context / ideas from @emilianobonassi, who has prototyped an alternate unsafe block sync mechanism. It has the following benefits over req-resp over p2p:\n\n* cheaper and simpler to run\n  * less components, instead of (1+1)xN + 2 components we would run (1+1)xN + 1 (no boot node for opnode) - consider Nitro is just N+1\n  * simpler networking\n    * p2p uses UDP which is well recognized to be a mess on cloud environments and in general (eg NAT punching)\n    * p2p only supports static ips no support for domain names (eg public boot nodes) which make complex HA on cloud environments\n* faster scale out and recovery\n  * req/resp is notoriously slow, you can sync small ranges of unsafe blocks (like 5 every second) so if you have medium/low tps chains batching every 1hr scale out takes a bit \n  * req/resp suffers of race conditions, if you have multiple nodes on the same channel behind they interfere with the sync process (which is why an external contributor added req/resp sync only via static ips) making it unstable and slow convergence (the topic we discussed, where someone ask for ranges, others are behind and answer before who is in sync saying they don\u2019t have, node retrying, same issue again => deadlock (especially when you scale more than one in parallel during burst) - this is also why we implemented that script + an API to force a batch (so u sync from l1), but ofc have costs impacts (l1 + manual ops)\n\nMore details on his proposed alternative:\n\n* it's complementary and doesn't replace p2p but its an additional way\n* the idea is that you can plug multiple \"drivers\" to gossip blocks and not just p2p\n* one of the driver, the centralized one, works this way:\n   * for normal syncs, seq leader pub via redis of the latest unsafe block - other nodes sub and injecting via the [admin_postUnsafePayload](https://github.com/ethereum-optimism/optimism/blob/develop/op-node/node/api.go#L80-L93) on the CL (same api being used by p2p at the very end)\n   * on restarts, the sidecar check the sync status against the sequencer leader, and query the EL to fetch the blocks (mapping to payloads) injecting via the same API\n\n\nAll bullets copy/pasted directly from Emiliano, who I know also has a code snippet for the prototype. Thank you @emilianobonassi! \n", "2025-08-29T18:23:03Z", "2025-08-29T18:23:03Z", "tessr", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM7ABVDS", "I_kwDODjvEJM6kplqE", "Closing this out for now - I don't think we'll be implementing this for the time being. Will reopen if I receive guidance otherwise.", "2025-08-25T20:03:22Z", "2025-08-25T20:03:22Z", "mslipper", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6bebL5", "I_kwDODjvEJM6kDRKd", "Hello! Did you ever figure out the solution to this? I am getting the same error.\n\n`WARN [01-22|16:54:05.103] Failed to create a transaction, will retry service=proposer err=\"failed to estimate gas: execution reverted, reason: 0x031c6de40000000000000000000000000000000000000000000000000000000000000000\"\nERROR[01-22|16:54:05.104] Failed to send proposal transaction      err=\"failed to create the tx: operation failed permanently after 30 attempts: failed to estimate gas: execution reverted, reason: 0x031c6de40000000000000000000000000000000000000000000000000000000000000000\" l1blocknum=7,550,015 l1blockhash=17b134..ac44bb l1head=7,550,019`\n\nThis is how I am starting it \n\n go run ./op-proposer/cmd \\\n  --l1-eth-rpc $L1_RPC_URL \\\n  --rollup-rpc http://localhost:9545 \\\n  --game-factory-address=0x3b945943f4cd29a25739e1653f66d022ad39bfc8 \\\n  --game-type=0 \\\n  --poll-interval=12s \\\n  --proposal-interval=12s \\\n  --private-key=$GS_PROPOSER_PRIVATE_KEY \\\n  --rpc.port=8560", "2025-01-22T23:03:37Z", "2025-01-22T23:03:37Z", "Red-Pandaz", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6dATFa", "I_kwDODjvEJM6kDRKd", "The error there matches `NoImplementation`, you are likely hitting `if (address(impl) == address(0)) revert NoImplementation(_gameType);`\nThe contracts are misconfigured or the op-proposer is misconfigured, with the wrong game-type.\n", "2025-02-04T14:17:08Z", "2025-02-04T14:17:08Z", "protolambda", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6dnEQU", "I_kwDODjvEJM6kDRKd", "I was running into this issue as well, here's what i did to fix it:\n\n- Upgraded the implementation for the game factory address proxy (through proxy admin)\n- Checked my game type. It's set to FaultDisputeGame [0] by default, but if you deploy with op-deployer, you want the game type to be PermissionedFaultDisputeGame [1].\n\nFor setting game type, just pass in the flag `--game-type=<game type>` to op-proposer", "2025-02-07T22:43:16Z", "2025-02-07T22:43:16Z", "mw2000", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6f7oX2", "I_kwDODjvEJM6kDRKd", "@mw2000 why did you need to upgrade the implementation for the factory proxy? And what did you change it to?\n\nIn my case the issue is that the [`gameImpls` mapping](\nhttps://github.com/ethereum-optimism/optimism/tree/3aa46be949f7b998204fe18857aaf7c832f817b4/packages/contracts-bedrock/src/dispute/DisputeGameFactory.sol#L57) for hasn't been initialized for `0` and `1` games, and I'm not sure why.\n\nEDIT: I was querying the impl, rather than the Proxy.", "2025-02-25T20:27:25Z", "2025-03-05T12:50:38Z", "asymmetric", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM69HB1m", "I_kwDODjvEJM6kDRKd", "\u6709\u89e3\u51b3\u7684\u65b9\u6848\u4e86\u4e48\uff0c\u6211\u4e5f\u9047\u5230\u8fd9\u4e2a\u9519\u8bef\u4e86\u8fd9\u4e2a\u662f\u6211\u7684\u542f\u52a8\u547d\u4ee4./bin/op-proposer \\\n  --poll-interval=12s \\\n  --rpc.port=8560 \\\n  --rollup-rpc=http://localhost:9545 \\\n  --private-key=\"\" \\\n  --l1-eth-rpc=\"4\" \\\n  --game-factory-address=\"\" \\\n  --proposal-interval=3600s \\\n  --num-confirmations=1 \\\n  --resubmission-timeout=30s \\\n  --wait-node-sync=true \\\n  --log.level=info", "2025-08-10T16:02:55Z", "2025-08-10T16:02:55Z", "17718597202", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM7AG3pu", "I_kwDODjvEJM6kDRKd", "> I was running into this issue as well, here's what i did to fix it:\n> \n>     * Upgraded the implementation for the game factory address proxy (through proxy admin)\n> \n>     * Checked my game type. It's set to FaultDisputeGame [0] by default, but if you deploy with op-deployer, you want the game type to be PermissionedFaultDisputeGame [1].\n> \n> \n> For setting game type, just pass in the flag `--game-type=<game type>` to op-proposer\n\nthanks, setting `--game-type=1`  fixed it in my case", "2025-08-26T07:50:10Z", "2025-08-26T07:50:10Z", "dariusL3", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6qrJgc", "I_kwDODjvEJM6hrIZD", "closing since we are tracking all issue in the main parent issue, as sub-issues: https://github.com/ethereum-optimism/optimism/issues/12421", "2025-05-08T15:20:17Z", "2025-05-08T15:20:17Z", "BlocksOnAChain", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6VnyCD", "I_kwDODjvEJM6ZyU6G", "oops, missed the tag add it automatically to close, just meant to link", "2024-12-01T20:13:47Z", "2024-12-01T20:13:47Z", "ControlCplusControlV", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6VnyUQ", "I_kwDODjvEJM6ZyU6G", "Yeah the word `fixes` automatically closes.  You can just mention the PR without that to link (e.g. https://github.com/ethereum-optimism/design-docs/pull/153)", "2024-12-01T20:17:22Z", "2024-12-01T20:17:22Z", "ajsutton", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6aMv1j", "I_kwDODjvEJM6ZyU6G", "@ControlCplusControlV ok to close this one now?\n", "2025-01-13T12:55:25Z", "2025-01-13T12:55:25Z", "BlocksOnAChain", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6aN1g8", "I_kwDODjvEJM6ZyU6G", "Still on pause, Drippy got moved in front of it in priority", "2025-01-13T14:52:32Z", "2025-01-13T14:52:32Z", "ControlCplusControlV", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6eB9N6", "I_kwDODjvEJM6ZyU6G", "I believe the AnchorStateRegistry no longer has immutables so going forward we can reuse those implementations, but the dispute game contracts do still need to be refactored to remove immutables so the implementations can be shared between all chains. \n\nCurrently, changing the prestate requires deploying new, full dispute game implementations for each chain. These contracts are large (~24kb, right at the size limit), therefore they cost 3-5M gas to deploy. Say it's 4M gas, this means to update prestates we can only update the prestate for 3 chains in one transaction (30M gas limit / 4M gas per game / 2 games per chain = 3.75). This makes the dispute game contracts the bottleneck for scaling our upgrade processes, as we will have to perform multiple signing ceremonies and multiple transactions for any upgrade that requires new dispute game contracts. Therefore we should bump the priority of this issue", "2025-02-11T16:18:53Z", "2025-02-11T16:18:53Z", "mds1", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6hATHq", "I_kwDODjvEJM6ZyU6G", "@ControlCplusControlV can you share more details on this one, since I see that related PR (https://github.com/ethereum-optimism/design-docs/pull/153) for it is merged?\nAny other work remaining for you to close this out?", "2025-03-05T15:06:14Z", "2025-03-05T15:06:14Z", "BlocksOnAChain", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6hAuP5", "I_kwDODjvEJM6ZyU6G", "That was just the design doc, but now the actual changes + specs need to go in", "2025-03-05T15:39:33Z", "2025-03-05T15:39:33Z", "ControlCplusControlV", "2025-08-30 03:27:09"]
["IC_kwDODjvEJM6pDpHl", "I_kwDODjvEJM6ZyU6G", "Updates to the design doc based on latest direction:\nhttps://github.com/ethereum-optimism/design-docs/pull/275", "2025-04-28T19:33:17Z", "2025-04-28T19:33:17Z", "stevennevins", "2025-08-30 03:27:09"]
["IC_kwDOKIwiaM6AEcRV", "I_kwDOKIwiaM6LCgIR", "Activated! Thank you", "2024-06-05T00:55:46Z", "2024-06-05T00:55:46Z", "ohbyeongmin", "2025-08-30 07:09:17"]
["IC_kwDOKIwiaM5_7giK", "I_kwDOKIwiaM6KCTNA", "related: https://github.com/ethereum-optimism/docs/issues/655", "2024-06-03T23:42:52Z", "2024-06-03T23:42:52Z", "sbvegan", "2025-08-30 07:10:03"]
["IC_kwDOKIwiaM5_7gpG", "I_kwDOKIwiaM6KCTNA", "Thanks for the feedback, I tend to agree. We'll see what we can do", "2024-06-03T23:43:24Z", "2024-06-03T23:43:24Z", "sbvegan", "2025-08-30 07:10:03"]
["IC_kwDOKIwiaM6BJ9jd", "I_kwDOKIwiaM6KCTNA", "@PaulRBerg we've since updated this page now that Fault Proofs is live on OP Mainnet:\r\n\r\nhttps://docs.optimism.io/stack/protocol/fault-proofs/explainer\r\n\r\nWe'd love your feedback on if this is more what you were looking for.", "2024-06-13T22:05:47Z", "2024-06-13T22:05:47Z", "OPMattie", "2025-08-30 07:10:03"]
["IC_kwDOKIwiaM6BOvbt", "I_kwDOKIwiaM6KCTNA", "Thanks @OPMattie, it looks good now! Closing the issue.", "2024-06-14T14:04:59Z", "2024-06-14T14:04:59Z", "PaulRBerg", "2025-08-30 07:10:03"]
["IC_kwDOKIwiaM6FZQHB", "I_kwDOKIwiaM6O206I", "This exists here: https://docs.optimism.io/builders/chain-operators/configuration/rollup. I'm doing some work to bring it completely up to date", "2024-07-19T03:04:02Z", "2024-07-19T03:04:02Z", "sbvegan", "2025-08-30 07:10:07"]
["IC_kwDOKIwiaM6FZQ3E", "I_kwDOKIwiaM6O206I", "@richardgreg I see that you've started to add new documentation to cover this. A new page might be worth while once the [standard rollup charter](https://gov.optimism.io/t/season-6-draft-standard-rollup-charter/8135) is ratified. This [rollup deployment configuration ](https://docs.optimism.io/builders/chain-operators/configuration/rollup) page has some of the configuration requirements outlined in the [OP Stack Configurability Spec](https://specs.optimism.io/protocol/configurability.html).", "2024-07-19T03:07:49Z", "2024-07-19T03:07:49Z", "sbvegan", "2025-08-30 07:10:07"]
["IC_kwDOKIwiaM6AAGh3", "I_kwDOKIwiaM6LFlaZ", "We may want to de-duplicate the first few steps with the tutorial in https://docs.optimism.io/builders/chain-operators/management/custom-gas-token\r\n\r\nThe `Deploy` and `L2Genesis` scripts are the way to go when working with the very latest `develop` branch (disclaimer: this is for testing, not the ideal target for mainnet chains)", "2024-06-04T13:15:22Z", "2024-06-04T13:15:22Z", "protolambda", "2025-08-30 07:10:16"]
["IC_kwDOKIwiaM6AAb_e", "I_kwDOKIwiaM6LFlaZ", "Specifically: the https://docs.optimism.io/builders/chain-operators/tutorials/create-l2-rollup docs are missing the `L2Genesis.s.sol` step, since the L2-genesis allocs generation was split from the L2 chain spec generation. The custom-gas-token docs do describe the correct way to use the `L2Genesis.s.sol` script.", "2024-06-04T13:53:15Z", "2024-06-04T13:53:15Z", "protolambda", "2025-08-30 07:10:16"]
["IC_kwDOKIwiaM6AB0vW", "I_kwDOKIwiaM6LFlaZ", "Hey proto, I'm going to take a stab at this, but I'm going to approach it as a `Deployment` section of the `Chain Operator` section. It'll have pages for the key pieces of deployment: L1 contract deployment, configuration, offchain components, etc. Each page will have a section for the MCP upgrade and one for the tip of `develop`. ", "2024-06-04T16:35:44Z", "2024-06-04T16:35:44Z", "sbvegan", "2025-08-30 07:10:16"]
["IC_kwDOKSJyfM6LLKyk", "I_kwDOKSJyfM6VOr8p", "The runbook for the faucet can be found here: https://www.notion.so/oplabs/Superchain-Faucet-Runbook-cd1132d4ba034158855cd368b2bd381f?pvs=4", "2024-09-07T00:17:22Z", "2024-09-07T00:17:22Z", "tremarkley", "2025-08-30 07:11:05"]
["IC_kwDOKSJyfM6K5vv1", "I_kwDOKSJyfM6VOqeH", "this ended up not being related to world id and was instead an issue with the admin wallet running low on funds", "2024-09-05T00:54:15Z", "2024-09-05T00:54:15Z", "tremarkley", "2025-08-30 07:11:05"]
["IC_kwDOKSJyfM6IfvkY", "I_kwDOKSJyfM6RfySz", "Created a mixpanel dashboard for tracking usage of faucet on dev console: https://mixpanel.com/s/2Y71kt\n\nBased on the usage I am seeing it doesnt seem like the captcha has had any effect on the number of drips. I still suspect there are people botting this based on the number of users using Privy auth and also the fact that so many users are from one region:\n\n\n![Image](https://github.com/user-attachments/assets/556ea832-7ff5-4cd5-a18e-a9a8b4d29282)\n\n", "2024-08-14T22:24:20Z", "2024-08-15T02:00:49Z", "tremarkley", "2025-08-30 07:11:05"]
["IC_kwDOKSJyfM6IhASc", "I_kwDOKSJyfM6RfySz", "Started working on updating the [grafana dashboard](https://optimistic.grafana.net/d/ebc95cfa-3368-410f-9d72-ca240f4e2831/superchain-faucet-v1-monitoring?orgId=1&from=now-6h&to=now). The \"Count of faucet drip failures\" panel has been updated to be pointed at the faucet failures on dev console.\n\nThe drippie contract balance panel is pointed to the right contract.\n\nThe L1 Sepolia Faucet Contract and L1 Sepolia Faucet Admin balance panels are still pointed to the old faucet, but should automatically update to the new faucet once this merges and the gateway pods are updated: https://github.com/ethereum-optimism/k8s/pull/4283\n\nNext steps would be to add some alerting so that we get alerted when drip failures increase.", "2024-08-15T01:58:01Z", "2024-08-15T01:59:03Z", "tremarkley", "2025-08-30 07:11:05"]
["IC_kwDOKSJyfM6LLB7n", "I_kwDOKSJyfM6RfySz", "Created 4 grafana alerts for the faucet:\n- L1 Sepolia: Faucet Admin Wallet Balance\n  - Triggered when admin wallet drops below 0.1 eth\n- L1 Sepolia: Faucet Contract Balance\n  - Triggered when faucet balance drops below 10 eth\n- L1 Sepolia: Faucet Drippie Contract Balance\n  - Triggered when drippie contract drops below 1000 eth\n- Spike in faucet claim tx failures\n  - Triggered when > 20 faucet drip txs have failed in the last hour\n\nFor now these alerts will ping in the #pod-devx slack channel. Next step is getting the alerts integrated into ops genie so that the alerts get sent to the on call engineer. \n\nIf you're curious, the alerts can be found here: https://optimistic.grafana.net/alerting/list?search=dashboard:ebc95cfa-3368-410f-9d72-ca240f4e2831", "2024-09-06T23:09:54Z", "2024-09-06T23:10:40Z", "tremarkley", "2025-08-30 07:11:05"]
["IC_kwDOKSJyfM5zEfJt", "I_kwDOKSJyfM5-cgaX", "current work remaining:\r\n[xs] - 5\r\n[s] - 17\r\n[m] - 2\r\n=25.5 working days", "2024-02-06T18:48:59Z", "2024-02-06T18:48:59Z", "jvmi7", "2025-08-30 07:11:05"]
["IC_kwDOKSJyfM5zFAJV", "I_kwDOKSJyfM5-cgaX", "Can you please add a section to faucet?", "2024-02-06T20:23:23Z", "2024-02-06T20:23:23Z", "zainbacchus", "2025-08-30 07:11:05"]
["IC_kwDOKSJyfM50iJKa", "I_kwDOKSJyfM5-cgaX", "Would be good to scope and flesh out Faucet & Mixpanel (Tracking) changes. I can help with that \r\n\r\nMaybe we all can sync on this @imranjami @zainbacchus ", "2024-02-20T21:13:46Z", "2024-02-20T21:13:46Z", "tarunkhasnavis", "2025-08-30 07:11:05"]
["IC_kwDOKSJyfM52lU8T", "I_kwDOKSJyfM5-cgaX", "Bugbash Bugs also here: https://www.notion.so/oplabs/Dapp-Console-MS1-Bugbash-de83cf2315bd4fd28afa20b65ad98ef8", "2024-03-11T21:42:00Z", "2024-03-11T21:42:00Z", "tarunkhasnavis", "2025-08-30 07:11:05"]
["IC_kwDOKIwiaM5vfdrX", "I_kwDOKIwiaM53o5Ue", "Can you please assign this task to me?", "2023-12-27T18:04:00Z", "2023-12-27T18:04:00Z", "raebeatrose4", "2025-08-30 07:16:23"]
["IC_kwDOKIwiaM5vuXDP", "I_kwDOKIwiaM53o5Ue", "Yes! Thank you in advance for contributing! :) ", "2024-01-02T19:00:47Z", "2024-01-02T19:00:47Z", "cpengilly", "2025-08-30 07:16:23"]
["IC_kwDOKIwiaM5wAg8t", "I_kwDOKIwiaM53o5Ue", "page recently updated so no longer need this!", "2024-01-05T19:50:29Z", "2024-01-05T19:50:29Z", "cpengilly", "2025-08-30 07:16:23"]
["IC_kwDOKIwiaM5wAiC_", "I_kwDOKIwiaM53owyT", "pages merged and revised here: https://docs.optimism.io/chain/testing/testing-dapps", "2024-01-05T19:55:06Z", "2024-01-05T19:55:06Z", "cpengilly", "2025-08-30 07:16:23"]
["IC_kwDOKIwiaM5wBITT", "I_kwDOKIwiaM53n2Lc", "done.", "2024-01-05T22:41:07Z", "2024-01-05T22:41:07Z", "cpengilly", "2025-08-30 07:16:23"]
["IC_kwDOKIwiaM5wRXSL", "I_kwDOKIwiaM57WkDM", "link fixed! thank you so much for letting us know :) ", "2024-01-09T18:48:52Z", "2024-01-09T18:48:52Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wbyDp", "I_kwDOKIwiaM57WkDM", "Thank you for your reply. I have submitted a launch support request. But no feedback yet. I just want to confirm that if the support team received our request from the Darwinia team.", "2024-01-11T05:48:34Z", "2024-01-11T05:48:46Z", "boundless-forest", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wobhy", "I_kwDOKIwiaM57WkDM", "@boundless-forest this repo only manages the Optimism Developer docs, so unfortunately, I don't have any insight into the launch support process. I will make an inquiry for you \ud83d\udc9f ", "2024-01-12T16:57:31Z", "2024-01-12T16:57:31Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wRXrk", "I_kwDOKIwiaM56nbvy", "empty issue", "2024-01-09T18:50:12Z", "2024-01-09T18:50:12Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wZhGY", "I_kwDOKIwiaM56QkqA", "Available here: https://docs.optimism.io/builders/dapp-developers/transactions/statuses\r\n\r\nWill also go into protocol docs", "2024-01-10T21:09:37Z", "2024-01-10T21:09:37Z", "smartcontracts", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wpG0U", "I_kwDOKIwiaM54-u2i", "notes folder added to repo which documents nextra, actions, and remark\r\n- nextra.md\r\n- actions.md\r\n- remark.md", "2024-01-12T19:17:05Z", "2024-01-12T19:17:05Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wocbR", "I_kwDOKIwiaM52i970", "@opfocus We ask that you please limit issues to one item or page to make reviews and edits easier. This is just best practices for git issues. Thank you! \ud83d\ude4f\ud83c\udffe \ud83d\ude04 ", "2024-01-12T17:00:10Z", "2024-01-12T17:00:10Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5opwGQ", "I_kwDOKIwiaM5zYNtv", "tied to this conversation https://github.com/ethereum-optimism/developer-support/discussions/243#discussioncomment-7202022", "2023-10-10T16:14:23Z", "2023-10-10T16:14:23Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5rV_HI", "I_kwDOKIwiaM5zYNtv", "@smartcontracts if this tutorial is on the list to refresh, should we move these notes to that open issue instead?", "2023-11-08T02:49:32Z", "2023-11-08T02:49:32Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5ss6pO", "I_kwDOKIwiaM5zEp5t", "adding note here and removing duplicate issue https://github.com/ethereum-optimism/docs/issues/58\r\nadd FAQ section to the page\r\n\r\nFAQs\r\n- Q: What is the recommended way to send the transaction with viem? Devs asked if they need to, \"need to specify price/basefee/priority fee? or just send and pray?\"\r\n- A: One reason we went with \u2018\u2019\u2019estimateFees\u2019\u2019\u2019 is because it's an action that will exist on non op chains. On ethereum estimateFees would just do \u2018\u2019\u2019gasPrice * gasUsed\u2019\u2019\u2019 whille on OP chains it's \u2018\u2019\u2019gasPrice * gasUsed + L1GasFee\u2019\u2019\u2019 but from developer/user point of view they don't have to think about that they just call \u2018\u2019\u2019viemClient.estimateFees()\u2019\u2019\u2019\r\n\r\n- Q: wouldn\u2019t devs likely send their transaction directly with an \u2018\u2019\u2019eth_sendRawTransaction\u2019\u2019\u2019? Likely abstracted away with a library like viem? \r\n- A: the sending transaction directly flow works! the libraries just give you an L1 estimate as well, which is automagically handled in the direct flow, but you're deducted more than the estimateGas endpoint since that only accounts for the L2 portion", "2023-11-23T01:11:45Z", "2023-11-23T01:20:30Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5ui34X", "I_kwDOKIwiaM5zEp5t", "@sbvegan dropping your gas FAQ questions here:\n\nThe following are questions that developer support has encountered in the past.\n\n- Does the **`eth_estimateGas` RPC method estimate the total L1 + L2 gas cost?**\n\nNo, `eth_estimateGas` only estimates the gas on the L2.\n\n************Note:************ we also get this general question for `maxFeePerGas` and `eth_gasprice`\n\n- How do I estimate the L1 gas cost?\n\nUtilize the functions in the `GasPriceOracle`\n\nhttps://github.com/ethereum-optimism/optimism/blob/v1.1.4/packages/contracts-bedrock/src/L2/GasPriceOracle.sol\n\n- Can I limit the L1 gas I\u2019m willing to spend? Does setting the `gasLimit` on transactions account for L1?\n\n- How do I make a good gas estimate before submitting my transaction?\n\n******Note:****** this question is usually for backend developers\n\n- How do I provide gas estimates for my frontend?\n\n[sdk.optimism.io](http://sdk.optimism.io/)\n\nhttps://github.com/ethereum-optimism/optimism/tree/develop/packages/fee-estimation", "2023-12-13T20:12:08Z", "2023-12-13T20:12:08Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wZeg4", "I_kwDOKIwiaM5zEp5t", "Resolved now", "2024-01-10T21:02:14Z", "2024-01-10T21:02:14Z", "smartcontracts", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5nIZWC", "I_kwDOKIwiaM5xtVW_", "Notes via @cpengilly \r\n\r\n> these are great questions, and I'll start thinking through possible solutions and link with emmanuel with firmer ideas :)  \r\n> \r\n> two things come to mind right now that we need to solve for: \r\n> \r\n> -  ethereum-optimism/developer-support#1 the error codes themselves: how many do we have, where are they stored, do they use clear/human-readable language, how can we get them all documented, etc.\r\n> - ethereum-optimism/developer-support#2 building out the self service options or troubleshooting guides; we'll def need help to pinpoint which errors/solution combinations to prioritize; this part becomes easier and can even encourage community-made entries to the troubleshooting guide if we tackle ethereum-optimism/developer-support#1 above internally\r\n> *as a side note, this also makes me think of the issue raised by @marine@oplabs.co yesterday in slack about the alchemy bridging issue -- documenting the error codes and solution (i.e. troubleshooting guide)", "2023-08-31T17:45:21Z", "2023-08-31T17:45:21Z", "OPMattie", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5nIZr3", "I_kwDOKIwiaM5xtVuV", "\n\n![Image](https://github.com/ethereum-optimism/developer-support/assets/85043086/776138a1-78f5-4cc5-be63-35621517e05a)\n\n", "2023-08-18T15:11:32Z", "2023-08-18T15:11:32Z", "sbvegan", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wpDC_", "I_kwDOKIwiaM5xtVuV", "callouts added to pages to address this issue", "2024-01-12T19:04:13Z", "2024-01-12T19:04:13Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wpFl1", "I_kwDOKIwiaM5xtWm3", "added to developer community resources https://github.com/ethereum-optimism/developers/blob/main/community/tools/blockchain-indexers.md#covalent", "2024-01-12T19:13:02Z", "2024-01-12T19:13:02Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5sOlNM", "I_kwDOKIwiaM5xtWru", "update: tokenlist now lives in docs @ https://docs.optimism.io/tokenlist", "2023-11-17T05:16:55Z", "2023-11-17T05:16:55Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wRevi", "I_kwDOKIwiaM5xtWw3", "we are now maintaining simple-optimism-node repo inhouse so closing this issue", "2024-01-09T19:11:33Z", "2024-01-09T19:11:33Z", "cpengilly", "2025-08-30 07:16:31"]
["IC_kwDOKIwiaM5wWrr_", "I_kwDOKIwiaM5xtWw3", "Not technically in house at OP Labs, but we have a maintainer committed to the project", "2024-01-10T14:50:07Z", "2024-01-10T14:50:07Z", "sbvegan", "2025-08-30 07:16:31"]
["IC_kwDOI7W0xc5xRuoY", "I_kwDOI7W0xc58DVbi", "Hi, Mission Proposals / Requests should be shared in the governance forum: https://gov.optimism.io/", "2024-01-19T13:58:54Z", "2024-01-19T13:58:54Z", "MSilb7", "2025-08-30 07:16:43"]
["IC_kwDOI7W0xc5xRukD", "I_kwDOI7W0xc58DThB", "Hi, Mission Proposals / Requests should be shared in the governance forum: https://gov.optimism.io/", "2024-01-19T13:58:41Z", "2024-01-19T13:58:41Z", "MSilb7", "2025-08-30 07:16:43"]
["IC_kwDOH2Qg5s5wAzkh", "I_kwDOH2Qg5s57C3Kf", "Relaying information that I have on this subject:\r\n\r\n> The fee-cap = 0 case is being handled correctly in the local op-geth node I believe, since we made no changes from what upstream geth does here, and the short-circuit behavior of the fee-cap looks correct in checkTxFee.\r\nIt likely is an issue with the OP Mainnet sequencer RPC then, which has its own independent fee-cap. I believe this is configured with the geth default, which would be 1 ETH, matching the behavior seen here.\r\nCan I ask in what particular circumstance the tx sender is trying to submit a tx with a fee of more than 1 ETH? Most L2 txs don't cost nearly as much. When using the full 30 million gas, it takes a gas price of 33.3 gwei to consume the full 1 ETH. However, priority-fees and base-fees are generally much lower than this on L2.\r\n\r\n", "2024-01-05T21:09:33Z", "2024-01-05T21:09:33Z", "sbvegan", "2025-08-30 07:16:53"]
["IC_kwDOH2Qg5s5vZyix", "I_kwDOH2Qg5s56dbEp", "The \"401 Unauthorized: signature is invalid\" indicates the JWT is wrong. Is the op-geth node configured with the same JWT? It should be configured like `--authrpc.jwtsecret=/shared/jwt.txt` to match the op-node.", "2023-12-25T15:41:11Z", "2023-12-25T15:41:11Z", "protolambda", "2025-08-30 07:16:53"]
["IC_kwDOH2Qg5s5v77By", "I_kwDOH2Qg5s56dbEp", "@liobah has this been resolved? Can we close this issue?", "2024-01-05T01:15:36Z", "2024-01-05T01:15:36Z", "protolambda", "2025-08-30 07:16:53"]
["IC_kwDOH2Qg5s5r26Y4", "I_kwDOH2Qg5s52qCIH", "And here is the op-node command and log\r\n\r\ndocker run -d --name op-node \\\r\n-v /root/optimism/op-node:/root/optimism/op-node \\\r\n-p 0.0.0.0:38547:38547 \\\r\nus-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:v1.3.0 \\\r\nop-node \\\r\n--l1=http://192.168.30.196:8545 \\\r\n--l1.rpckind=basic \\\r\n--l2=http://192.168.30.196:38551 \\\r\n--l2.jwt-secret=/root/optimism/op-node/jwt.txt \\\r\n--network=mainnet \\\r\n--rpc.addr=0.0.0.0 \\\r\n--rpc.port=38547\r\n\r\n\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Generating next batch\"                  epoch=0x534f83e93d37f981cb73501ebaf816cbd41d166b610ab5b64854e4f7b86b7d1d:18564349 timestamp=1,699,895,041\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"generated attributes in payload queue\"  txs=1  timestamp=1,699,895,041\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"inserted block\"                         hash=0xb893df569594f700a8dbc5e22969bc40fe41ca14c8e011b84fd2cc85ee6387aa number=112,148,132 state_root=0xcd899cad242c9b8591ae0b60135239166b9a6122581db605446224f34fa94864 timestamp=1,699,895,041 parent=0x50bfbbf35943e9d811f90a4c9e75a1e0b36c6da4e000c38dd1c51f68f766af11 prev_randao=0x37118054e312292cb93d2da1c943d70b4fb9d2a44c1bd951ff76f56a57758596 fee_recipient=0x4200000000000000000000000000000000000011 txs=1  update_safe=true\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Sync progress\"                          reason=\"processed safe block derived from L1\" l2_finalized=0xe812d6a1e458f344fc5b74b1629b4c0bb826797243ba62d3e94e90bced596f8e:112147662 l2_safe=0xb893df569594f700a8dbc5e22969bc40fe41ca14c8e011b84fd2cc85ee6387aa:112148132 l2_unsafe=0xb893df569594f700a8dbc5e22969bc40fe41ca14c8e011b84fd2cc85ee6387aa:112148132 l2_engineSyncTarget=0xb893df569594f700a8dbc5e22969bc40fe41ca14c8e011b84fd2cc85ee6387aa:112148132 l2_time=1,699,895,041 l1_derived=0xecf33e57f5efae9a9e17f48f107fcb330f9d5430fab0b38f4c63ca1c5ba0d197:18567949\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"ignoring batch with mismatching parent hash\" batch_index=0 batch_timestamp=1,699,895,043 parent_hash=0x7fcc08b28c9689eb0c36a604efe0073cdf66731fab6676eefae9428b13b88ac8 batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=4  current_safe_head=0xb893df569594f700a8dbc5e22969bc40fe41ca14c8e011b84fd2cc85ee6387aa\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"Dropping batch\"                         batch_timestamp=1,699,895,043 parent_hash=0x7fcc08b28c9689eb0c36a604efe0073cdf66731fab6676eefae9428b13b88ac8 batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=4  l2_safe_head=0xb893df569594f700a8dbc5e22969bc40fe41ca14c8e011b84fd2cc85ee6387aa:112148132 l2_safe_head_time=1,699,895,041\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Generating next batch\"                  epoch=0x534f83e93d37f981cb73501ebaf816cbd41d166b610ab5b64854e4f7b86b7d1d:18564349 timestamp=1,699,895,043\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"generated attributes in payload queue\"  txs=1  timestamp=1,699,895,043\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"inserted block\"                         hash=0x16846338cec01b05778c899bdb3d38153e839f34e29e37ae69a2417e93580b40 number=112,148,133 state_root=0x31c23ca9e92a5af8d9ddb2317b0d3d652891a5ac0a5ccf21f2a21d1ade3ab5a2 timestamp=1,699,895,043 parent=0xb893df569594f700a8dbc5e22969bc40fe41ca14c8e011b84fd2cc85ee6387aa prev_randao=0x37118054e312292cb93d2da1c943d70b4fb9d2a44c1bd951ff76f56a57758596 fee_recipient=0x4200000000000000000000000000000000000011 txs=1  update_safe=true\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Sync progress\"                          reason=\"processed safe block derived from L1\" l2_finalized=0xe812d6a1e458f344fc5b74b1629b4c0bb826797243ba62d3e94e90bced596f8e:112147662 l2_safe=0x16846338cec01b05778c899bdb3d38153e839f34e29e37ae69a2417e93580b40:112148133 l2_unsafe=0x16846338cec01b05778c899bdb3d38153e839f34e29e37ae69a2417e93580b40:112148133 l2_engineSyncTarget=0x16846338cec01b05778c899bdb3d38153e839f34e29e37ae69a2417e93580b40:112148133 l2_time=1,699,895,043 l1_derived=0xecf33e57f5efae9a9e17f48f107fcb330f9d5430fab0b38f4c63ca1c5ba0d197:18567949\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"ignoring batch with mismatching parent hash\" batch_index=0 batch_timestamp=1,699,895,045 parent_hash=0x42774e441826971c2c0997fdcc84e613ff75ff3ffd00ba227cf6df8255271d3d batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=5  current_safe_head=0x16846338cec01b05778c899bdb3d38153e839f34e29e37ae69a2417e93580b40\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"Dropping batch\"                         batch_timestamp=1,699,895,045 parent_hash=0x42774e441826971c2c0997fdcc84e613ff75ff3ffd00ba227cf6df8255271d3d batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=5  l2_safe_head=0x16846338cec01b05778c899bdb3d38153e839f34e29e37ae69a2417e93580b40:112148133 l2_safe_head_time=1,699,895,043\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Generating next batch\"                  epoch=0x534f83e93d37f981cb73501ebaf816cbd41d166b610ab5b64854e4f7b86b7d1d:18564349 timestamp=1,699,895,045\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"generated attributes in payload queue\"  txs=1  timestamp=1,699,895,045\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"inserted block\"                         hash=0x65e0875beccec95f6ba6ead100605bc3fac7a3e881db91230b84bfaf19ada7f9 number=112,148,134 state_root=0x19f877fb047eb64ef9bb226ce2fa2cfeb8a067d99cd8c34f69c0ad93450d6d09 timestamp=1,699,895,045 parent=0x16846338cec01b05778c899bdb3d38153e839f34e29e37ae69a2417e93580b40 prev_randao=0x37118054e312292cb93d2da1c943d70b4fb9d2a44c1bd951ff76f56a57758596 fee_recipient=0x4200000000000000000000000000000000000011 txs=1  update_safe=true\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Sync progress\"                          reason=\"processed safe block derived from L1\" l2_finalized=0xe812d6a1e458f344fc5b74b1629b4c0bb826797243ba62d3e94e90bced596f8e:112147662 l2_safe=0x65e0875beccec95f6ba6ead100605bc3fac7a3e881db91230b84bfaf19ada7f9:112148134 l2_unsafe=0x65e0875beccec95f6ba6ead100605bc3fac7a3e881db91230b84bfaf19ada7f9:112148134 l2_engineSyncTarget=0x65e0875beccec95f6ba6ead100605bc3fac7a3e881db91230b84bfaf19ada7f9:112148134 l2_time=1,699,895,045 l1_derived=0xecf33e57f5efae9a9e17f48f107fcb330f9d5430fab0b38f4c63ca1c5ba0d197:18567949\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"ignoring batch with mismatching parent hash\" batch_index=0 batch_timestamp=1,699,895,047 parent_hash=0x0b3438572ed18f3826955488bb0f813a983221af70da47b6ecc7acc6a359915e batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=5  current_safe_head=0x65e0875beccec95f6ba6ead100605bc3fac7a3e881db91230b84bfaf19ada7f9\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"Dropping batch\"                         batch_timestamp=1,699,895,047 parent_hash=0x0b3438572ed18f3826955488bb0f813a983221af70da47b6ecc7acc6a359915e batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=5  l2_safe_head=0x65e0875beccec95f6ba6ead100605bc3fac7a3e881db91230b84bfaf19ada7f9:112148134 l2_safe_head_time=1,699,895,045\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Generating next batch\"                  epoch=0x534f83e93d37f981cb73501ebaf816cbd41d166b610ab5b64854e4f7b86b7d1d:18564349 timestamp=1,699,895,047\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"generated attributes in payload queue\"  txs=1  timestamp=1,699,895,047\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"inserted block\"                         hash=0xddeb2ea446a0b4b9036b0ef3034d29b636542e0580e163ba386b3e0dbf77fbb0 number=112,148,135 state_root=0x5b4b6b1c4f343d9c31cc6bad7bce1fb7b8dc61f7fe86df825aecc0a9afb9b06c timestamp=1,699,895,047 parent=0x65e0875beccec95f6ba6ead100605bc3fac7a3e881db91230b84bfaf19ada7f9 prev_randao=0x37118054e312292cb93d2da1c943d70b4fb9d2a44c1bd951ff76f56a57758596 fee_recipient=0x4200000000000000000000000000000000000011 txs=1  update_safe=true\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Sync progress\"                          reason=\"processed safe block derived from L1\" l2_finalized=0xe812d6a1e458f344fc5b74b1629b4c0bb826797243ba62d3e94e90bced596f8e:112147662 l2_safe=0xddeb2ea446a0b4b9036b0ef3034d29b636542e0580e163ba386b3e0dbf77fbb0:112148135 l2_unsafe=0xddeb2ea446a0b4b9036b0ef3034d29b636542e0580e163ba386b3e0dbf77fbb0:112148135 l2_engineSyncTarget=0xddeb2ea446a0b4b9036b0ef3034d29b636542e0580e163ba386b3e0dbf77fbb0:112148135 l2_time=1,699,895,047 l1_derived=0xecf33e57f5efae9a9e17f48f107fcb330f9d5430fab0b38f4c63ca1c5ba0d197:18567949\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"ignoring batch with mismatching parent hash\" batch_index=0 batch_timestamp=1,699,895,049 parent_hash=0xba8cc33630c9ec901da523ba674e10ac494fb26629d06969d7ae9ddaa6858d87 batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=7  current_safe_head=0xddeb2ea446a0b4b9036b0ef3034d29b636542e0580e163ba386b3e0dbf77fbb0\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"Dropping batch\"                         batch_timestamp=1,699,895,049 parent_hash=0xba8cc33630c9ec901da523ba674e10ac494fb26629d06969d7ae9ddaa6858d87 batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=7  l2_safe_head=0xddeb2ea446a0b4b9036b0ef3034d29b636542e0580e163ba386b3e0dbf77fbb0:112148135 l2_safe_head_time=1,699,895,047\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Generating next batch\"                  epoch=0x534f83e93d37f981cb73501ebaf816cbd41d166b610ab5b64854e4f7b86b7d1d:18564349 timestamp=1,699,895,049\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"generated attributes in payload queue\"  txs=1  timestamp=1,699,895,049\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"inserted block\"                         hash=0xbfd1f1ab833a85ecb904a2e3f90abe4189402d9d6fb793381c38c3e552d67568 number=112,148,136 state_root=0xdb8cd084f1afc373f5d439d052e23241a5807e0553387ab087019befed2735b5 timestamp=1,699,895,049 parent=0xddeb2ea446a0b4b9036b0ef3034d29b636542e0580e163ba386b3e0dbf77fbb0 prev_randao=0x37118054e312292cb93d2da1c943d70b4fb9d2a44c1bd951ff76f56a57758596 fee_recipient=0x4200000000000000000000000000000000000011 txs=1  update_safe=true\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"Sync progress\"                          reason=\"processed safe block derived from L1\" l2_finalized=0xe812d6a1e458f344fc5b74b1629b4c0bb826797243ba62d3e94e90bced596f8e:112147662 l2_safe=0xbfd1f1ab833a85ecb904a2e3f90abe4189402d9d6fb793381c38c3e552d67568:112148136 l2_unsafe=0xbfd1f1ab833a85ecb904a2e3f90abe4189402d9d6fb793381c38c3e552d67568:112148136 l2_engineSyncTarget=0xbfd1f1ab833a85ecb904a2e3f90abe4189402d9d6fb793381c38c3e552d67568:112148136 l2_time=1,699,895,049 l1_derived=0xecf33e57f5efae9a9e17f48f107fcb330f9d5430fab0b38f4c63ca1c5ba0d197:18567949\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"ignoring batch with mismatching parent hash\" batch_index=0 batch_timestamp=1,699,895,051 parent_hash=0xb2199bb8b41ee8cae9fa870c4290d133af1cbf8e891ce3a613410c5e92c16335 batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=3  current_safe_head=0xbfd1f1ab833a85ecb904a2e3f90abe4189402d9d6fb793381c38c3e552d67568\r\nt=2023-11-14T05:07:26+0000 lvl=warn msg=\"Dropping batch\"                         batch_timestamp=1,699,895,051 parent_hash=0xb2199bb8b41ee8cae9fa870c4290d133af1cbf8e891ce3a613410c5e92c16335 batch_epoch=0xa67056af2fbedac10d7b267e053ed320c4f066ddd985d64928c9bf9c5249de21:18564343 txs=3  l2_safe_head=0xbfd1f1ab833a85ecb904a2e3f90abe4189402d9d6fb793381c38c3e552d67568:112148136 l2_safe_head_time=1,699,895,049\r\nt=2023-11-14T05:07:26+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAm1jrzXkXpcum2ujEJGxj34JnCeE6VDxVbibt7VpeEdZUK\r\nt=2023-11-14T05:07:27+0000 lvl=info msg=\"connected to peer\"                      peer=16Uiu2HAm1jrzXkXpcum2ujEJGxj34JnCeE6VDxVbibt7VpeEdZUK addr=/ip4/148.251.178.51/tcp/9222\r\nt=2023-11-14T05:07:27+0000 lvl=info msg=\"Starting P2P sync client event loop\"    peer=16Uiu2HAm1jrzXkXpcum2ujEJGxj34JnCeE6VDxVbibt7VpeEdZUK\r\nt=2023-11-14T05:07:27+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAm9mZBv2sQpTbA3BWk5TxbzQ1kHgSRZVfA86JMhRCT9aTz", "2023-11-14T05:08:32Z", "2023-11-14T05:08:32Z", "cheneyweb", "2025-08-30 07:16:53"]
["IC_kwDOH2Qg5s5s2qCM", "I_kwDOH2Qg5s52qCIH", "What kind of storage is used for `/mnt/backup2/optimism`? Make sure it has plenty IOPS - 10k+ steady is the usual recommendation for anything blockchain.", "2023-11-25T09:47:51Z", "2023-11-25T09:47:51Z", "yorickdowne", "2025-08-30 07:16:53"]
["IC_kwDOH2Qg5s5v75E9", "I_kwDOH2Qg5s52qCIH", "Sorry about the delay in response here, I hope you found a solution for this issue, or resynced with correct configuration.\r\n\r\nBut to explain the issue, here goes...\r\n\r\nIf you look at block https://optimistic.etherscan.io/block/112120763 then you see the canonical blockhash is `0xe19b0c33a6fb8c1afc5042ecfbdbdde24e281962349ad25737e64321cf10dc79`, not `826fea..989448` as reported by your node.\r\n\r\nThe 12 hours delay, and the non-canonical empty chain, strongly imply that your node likely lost track of the valid chain, and then started forcing deposit-only blocks for liveness: the sequencer-window is 12 hours. When no batches are detected in the inbox after this, it starts deterministically creating L2 blocks from the L1 data without batches, so that users can still move funds without sequencer co-operation. Here it looks like it ended up missing valid batches for some reason, and then got out of sync because of that.\r\n\r\nGetting into this situation likely means there is (or was) something misconfigured. You can check the `optimism_rollupConfig` JSON-RPC of the op-node RPC endpoint (not op-geth!) and the CLI options, and compare against the docs. That should tell you if something is off.\r\n", "2024-01-05T01:02:47Z", "2024-01-05T01:02:47Z", "protolambda", "2025-08-30 07:16:53"]
["IC_kwDOH2Qg5s5w1oaQ", "I_kwDOH2Qg5s58HQwI", "I assume that you are on OP-mainnet, because [block number 105235063](https://optimistic.etherscan.io/block/105235063) is start of the OP-mainnet bedrock genesis. Also, your are getting some sane responses for prebedrock(block number below 105235063), so I again assume that [l2geth](https://github.com/ethereum-optimism/optimism/releases/tag/%40eth-optimism%2Fl2geth%400.5.31) is properly setuped on your side(or you are using public infra).\r\n\r\nOverall node architecture follows below diagram([Ref](https://docs.optimism.io/builders/node-operators/overview#diagram)):\r\n![image](https://github.com/ethereum-optimism/op-geth/assets/29061389/1cdab89b-4f84-4c05-a215-cab6b1ba07cf)\r\n\r\nIf you send your RPC request, it first arrives to op-geth. If op-geth decides that this request is for prebedrock(block number below 105235063), it routes to l2geth, which stores legacy data and has the ability to trace prebedrock transactions.\r\n\r\nBased on your observation, op-geth fetched upstream(geth) and implements `flatCallTracer`. Thats why `flatCallTracer` works in bedrock blocks. But, l2geth does not support `flatCallTracer`, because it is not actively following geth's upstream changes.\r\n\r\nSome sidenote: l2geth and op-geth have different state transition rules. Thats why there are two different binaries, and two different databases.\r\n\r\n\r\n", "2024-01-16T05:47:21Z", "2024-01-16T05:47:21Z", "pcw109550", "2025-08-30 07:16:57"]
["IC_kwDOH2Qg5s5w2L8X", "I_kwDOH2Qg5s58HQwI", "Question if `flatCallTracer` uses `callTracer` as a proxy to collect call stack info and at the end formats the output in Parity style, is there a chance that op-geth would be converting callTracer from l2geth to `flatCallTracer` to keep the standard?", "2024-01-16T08:15:58Z", "2024-01-16T08:15:58Z", "MRabenda", "2025-08-30 07:16:57"]
["IC_kwDOH2Qg5s5w4jVg", "I_kwDOH2Qg5s58HQwI", "That seems possible. I proxied `ots_traceTransaction`(erigon only RPC) by reconstructing response of `debug_traceTransaction` at op-erigon: https://github.com/testinprod-io/op-erigon/pull/30. However I am not 100% sure.", "2024-01-16T14:35:14Z", "2024-01-16T14:35:14Z", "pcw109550", "2025-08-30 07:16:57"]
["IC_kwDOH2Qg5s5w6fSt", "I_kwDOH2Qg5s58HQwI", "@ pcw109550 is this something that Optimism Foundation can consider worth doing?", "2024-01-16T19:29:41Z", "2024-01-16T19:29:41Z", "MRabenda", "2025-08-30 07:16:57"]
["IC_kwDOH2Qg5s5xiaic", "I_kwDOH2Qg5s54AYpm", "I've confirmed that the write path is correct with the prefixed code. Ideally we should be able to revert this change, but L2 Geth did not prefix the code which means that any datadir created with L2 Geth (basically OP mainnet starting datadir + end users syncing pre-bedrock) will not be able to serve snap sync.\r\n\r\nOnce we distribute pebble based database (& most people have migrated), we may be able to go from `ContractCode` to `ContractCodeWithPrefix` to improve performance, but it needs to be shelved for now. \r\n\r\nThe code prefix was introduced in https://github.com/ethereum/go-ethereum/pull/21080\r\n\r\nL2 Geth DB Schema: https://github.com/ethereum-optimism/optimism-legacy/blob/develop/l2geth/core/rawdb/schema.go", "2024-01-22T21:26:18Z", "2024-01-22T21:30:29Z", "trianglesphere", "2025-08-30 07:17:01"]
["IC_kwDODjvEJM5wQtwL", "I_kwDODjvEJM57dMzb", "Opened a PR to fix this", "2024-01-09T16:57:36Z", "2024-01-09T16:57:36Z", "smartcontracts", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5wdJqJ", "I_kwDODjvEJM57dMzb", "@smartcontracts thanks for looking into the issue. Sadly the update does not really solve the issue, now other arguments are required, e.g. `maxFeePerGas`. ", "2024-01-11T09:17:01Z", "2024-01-11T09:17:01Z", "maxklenk", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5vYKRm", "I_kwDODjvEJM56gEO0", "USDC.e circle USD Coin (Bridged from Ethereum) (USDC.e) Token Tracker on OP Mainnet.\r\n\r\nA relay transaction was requesting a deposit. When the L2 signature asked for 350,000,000 USDC.e there was no option to optimize the allocated amount signed from the wallet given access. Even though a bridge was being constructed this configuration confuses the user. This new transaction was applied from the previous transaction (airdrop) not allowing the wallet owner to understand this was a designed bridge that was new; instead it came from the first airdrop with the \u201cwithdrawal button\u201d highlighted. Noticing the 0x Flashbot lead to researching to ensure no signature attack was detected. Is there a tool to help a user identify this was a bridge mission; especially for a new transaction, because this is what raised concerns from the wallet this happened in. Would like to have this documented as a way to help apply to an app. Next time will apply the notes of what was found through OKX that raised questions. Time though finally told the truth. This could be an added feature to a future IDS (Intrusion Detection System), or alert system as a way to implement alerts for security and detection purposes. ", "2023-12-24T22:31:16Z", "2024-01-14T22:17:07Z", "philliprossii", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5uigk5", "I_kwDODjvEJM54AZfI", "@trianglesphere and I just discussed that we probably want to drop this one, as it's not critical to get snap sync to work and adds complexity. Can also add later after Snap Sync is already prod ready.", "2023-12-13T18:58:48Z", "2023-12-13T18:58:48Z", "sebastianst", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5whjKV", "I_kwDODjvEJM54AZfI", "Closing this because we're going to have to finalize whatever block we finish syncing to when done with EL sync & this checkpoint block flag doesn't work.", "2024-01-11T19:37:57Z", "2024-01-11T19:37:57Z", "trianglesphere", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5kw03q", "I_kwDODjvEJM5uFfrL", "Hi @protolambda & @refcell I would like to work on this issue, if its fine?\r\nOne small request though, if you could please explain the issue in a bit detail?\r\nmany thanks!!! ", "2023-08-23T19:28:19Z", "2023-08-23T19:28:19Z", "shubhamshd", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5k2vtb", "I_kwDODjvEJM5uFfrL", "@shubhamshd you can search more info about pprof online, it's a pretty common performance testing library. It can be used via API, or started programmatically. We have the former, but also want the option to use the latter.", "2023-08-24T16:54:10Z", "2023-08-24T16:54:10Z", "protolambda", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5k3mWG", "I_kwDODjvEJM5uFfrL", "ok sure @protolambda I'd research and then add my contribution to the repo, thanks!!!", "2023-08-24T19:35:04Z", "2023-08-24T19:35:04Z", "shubhamshd", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5k9tg5", "I_kwDODjvEJM5uFfrL", "apologies @protolambda but with my limited knowledge of this repo and pprof library, it seems to me that we already have cli driven pprof setup and not the api driven as you mentioned earlier?", "2023-08-25T20:33:34Z", "2023-08-25T20:33:34Z", "shubhamshd", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5k9z7V", "I_kwDODjvEJM5uFfrL", "@shubhamshd the API is configurable through CLI, but profiling is not yet possible to enable at startup through the CLI.", "2023-08-25T21:01:17Z", "2023-08-25T21:01:17Z", "protolambda", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5lHJAQ", "I_kwDODjvEJM5uFfrL", "@protolambda I see the pprof server being started in different modules based on pprof.enabled flag, here is an example\r\nhttps://github.com/ethereum-optimism/optimism/blob/18bcfb0dcc16de4287cb614f0a0b7bb3c276cbd3/op-node/cmd/main.go#L153C1-L153C1 \r\n\r\nThis makes me believe that starting pprof server through cli is already implemented and I am not able to exactly understand as to how is this API enabled and not programatically enabled specially with above example setup in place!!!\r\n\r\nPlease bear my noob questions, I just don't want to give up.\r\nHowever, if this is high priority issue then I am ready to hand this over to any expert if available!!!\r\n\r\nThanks :)", "2023-08-28T20:31:40Z", "2023-08-28T20:31:40Z", "shubhamshd", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5lHJnI", "I_kwDODjvEJM5uFfrL", "Follow up question to the above one, is there any specific file/module, where I need to enable the pprof server to make it run at the program startup?", "2023-08-28T20:33:47Z", "2023-08-28T20:33:47Z", "shubhamshd", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aS5RQ", "I_kwDODjvEJM5jgixy", "I'm working on providing a snapshot to increase sync speed, will keep you updated.", "2023-04-19T15:05:43Z", "2023-04-19T15:05:43Z", "smartcontracts", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aVgL0", "I_kwDODjvEJM5jgixy", "> I'm working on providing a snapshot to increase sync speed, will keep you updated.\r\n\r\nPlease let us know - it's nearly impossible to sync a node right now.", "2023-04-20T01:29:50Z", "2023-04-20T01:29:50Z", "ajb", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aVmy8", "I_kwDODjvEJM5jgixy", "> > I'm working on providing a snapshot to increase sync speed, will keep you updated.\r\n> \r\n> Please let us know - it's nearly impossible to sync a node right now.\r\n\r\nL2 sync from alchemy : half a million per day .  (totally 15 million blocks sync from the current snapshot 2.20 ) more than 30 days\r\nL1 sync from my erigon archive node : one million per day . (from scratch totally 92 million blocks )  more than 3 month . \r\nDid I get it right??\r\nIs there anything wrong with my node ?\r\nBr\r\nLu", "2023-04-20T02:11:09Z", "2023-04-20T02:11:09Z", "luwang920", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aX_fO", "I_kwDODjvEJM5jgixy", "L1 sync is really slow right now. I have some tricks up my sleeve to make it a little faster but unfortunately with the Bedrock upgrade coming so soon it's been de-prioritized.\r\n\r\n@ajb in the meantime you can use these community provided snapshots. Not maintained by OP Labs or anything so use at your own risk: https://archive.org/download/optimism-snapshot", "2023-04-20T12:26:03Z", "2023-04-20T12:26:03Z", "smartcontracts", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aYc74", "I_kwDODjvEJM5jgixy", "> L1 sync is really slow right now. I have some tricks up my sleeve to make it a little faster but unfortunately with the Bedrock upgrade coming so soon it's been de-prioritized.\r\n\r\nL2 sync is also quite slow, even when you already have the data in DTL. (It was just my l2geth database that got borked on a restart.)\r\n\r\n@smartcontracts I am already using these snapshots. \r\n", "2023-04-20T13:46:06Z", "2023-04-20T13:46:06Z", "ajb", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aeIbG", "I_kwDODjvEJM5jgixy", "Yeah it's all slow :-/ running on like 3 year old Geth. Unfortunately the legacy l2geth gets limited support right now because of the push to Bedrock. With Bedrock these sort of issues should go away but it's an awkward middle period at the moment. I can make more recent snapshots.", "2023-04-21T13:37:22Z", "2023-04-21T13:37:22Z", "smartcontracts", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aean_", "I_kwDODjvEJM5jgixy", "> I can make more recent snapshots.\r\n\r\nThis will certainly alleviate the issue.", "2023-04-21T14:29:51Z", "2023-04-21T14:29:51Z", "ajb", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aebLx", "I_kwDODjvEJM5jgixy", "Gotcha. Can get on this end of next week at the earliest.", "2023-04-21T14:31:47Z", "2023-04-21T14:31:47Z", "smartcontracts", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aedWg", "I_kwDODjvEJM5jgixy", "Cool, thanks for the update.\r\n\r\nI don't really know how it works, but my node is almost synced and I could upload it to archive.org as well.", "2023-04-21T14:36:53Z", "2023-04-21T14:36:53Z", "ajb", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aiQFV", "I_kwDODjvEJM5jgixy", "Looking forward to the new snapshots \uff0c thanks ", "2023-04-23T03:03:09Z", "2023-04-23T03:03:09Z", "luwang920", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5aq6sY", "I_kwDODjvEJM5jgixy", "I have a similar issue, where my node got borked just after the DTL finished syncing and l2geth was just about starting to sync.\r\n\r\nI have tried using the archive.org snapshots, but it doesn't look like they are getting used at all, as I see the DTL syncing from block 13596466, like the snapshot files weren't even there.\r\n\r\nIs there any additional config I am missing to use the snapshots? \r\n\r\nI have just copied the snapshot files in an empty /db folder and started the node", "2023-04-25T06:06:26Z", "2023-04-25T11:30:30Z", "bsantoni-bc", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5awnQN", "I_kwDODjvEJM5jgixy", "I got the 'nonce too high' error again at the block 81829227, sync from Alchemy L2\r\nI am afraid I cannot set up the optimism node without the new snapshot .. \r\nI got 'nonce too high' error 4 times , and this time it took me 3 weeks to resync it , still failed  , this is too depressing ...", "2023-04-26T02:50:55Z", "2023-04-26T02:50:55Z", "luwang920", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5awosf", "I_kwDODjvEJM5jgixy", "Not sure why my `simple-optimism-node-l2geth-1` container will being restart randomly.\r\n\r\nWithin this month already resync more than 3 times with 2 nodes. I did make it synced, but suddenly the container will restart.\r\n\r\n```\r\nDEBUG[04-26|02:17:43.505] Applying transaction to tip              index=94277066 hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 origin=sequencer\r\nDEBUG[04-26|02:17:43.505] Attempting to commit rollup transaction  hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471\r\nINFO [04-26|02:17:43.511] New block                                index=94277066 l1-timestamp=1682475447 l1-blocknumber=17127328 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 queue-orign=sequencer gas=70978    fees=8.51736e-08     elapsed=6.335ms\r\nDEBUG[04-26|02:17:43.513] Miner got new head                       height=94277067 block-hash=0xab57c14ef00ac31b1b7ee96d18fc7272ccd0c35cfc11fc2fdc24608e574060d0 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471\r\nDEBUG[04-26|02:17:43.605] Served eth_gasPrice                      conn=18.139.81.231:54273  reqid=671311  t=43.34\u00b5s\r\nDEBUG[04-26|02:17:44.008] Served eth_gasPrice                      conn=18.139.81.231:25839  reqid=676949  t=45.417\u00b5s\r\nDEBUG[04-26|02:17:44.546] Served eth_gasPrice                      conn=18.143.94.160:34176  reqid=440575  t=36.599\u00b5s\r\nDEBUG[04-26|02:17:44.634] Served eth_gasPrice                      conn=18.140.83.19:38924   reqid=609028  t=81.648\u00b5s\r\nDEBUG[04-26|02:17:44.700] Served eth_chainId                       conn=172.30.0.2:59170     reqid=417949  t=41.016\u00b5s\r\nDEBUG[04-26|02:17:44.701] Served eth_getBlockByNumber              conn=172.30.0.2:59180     reqid=417950  t=166.514\u00b5s\r\nDEBUG[04-26|02:17:44.731] Served eth_gasPrice                      conn=18.140.83.19:50821   reqid=593393  t=29.241\u00b5s\r\n+ GETH_DATA_DIR=/geth\r\n+ GETH_CHAINDATA_DIR=/geth/geth/chaindata\r\n+ GETH_KEYSTORE_DIR=/geth/keystore\r\n+ '[' '!' -d /geth/keystore ]\r\n+ '[' '!' -d /geth/geth/chaindata ]\r\n+ echo '/geth/geth/chaindata exists, checking for hardfork.'\r\n+ echo 'Chain config:'\r\n+ geth dump-chain-cfg '--datadir=/geth'\r\n/geth/geth/chaindata exists, checking for hardfork.\r\nChain config:\r\nINFO [04-26|02:17:50.472] Maximum peer count                       ETH=50 LES=0 total=50\r\n```\r\n\r\nI running with 8 CPU 32G ram still able to met this problem.", "2023-04-26T02:59:00Z", "2023-04-26T02:59:00Z", "joslee7410", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5awroS", "I_kwDODjvEJM5jgixy", "> Not sure why my `simple-optimism-node-l2geth-1` container will being restart randomly.\r\n> \r\n> Within this month already resync more than 3 times with 2 nodes. I did make it synced, but suddenly the container will restart.\r\n> \r\n> ```\r\n> DEBUG[04-26|02:17:43.505] Applying transaction to tip              index=94277066 hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 origin=sequencer\r\n> DEBUG[04-26|02:17:43.505] Attempting to commit rollup transaction  hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471\r\n> INFO [04-26|02:17:43.511] New block                                index=94277066 l1-timestamp=1682475447 l1-blocknumber=17127328 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 queue-orign=sequencer gas=70978    fees=8.51736e-08     elapsed=6.335ms\r\n> DEBUG[04-26|02:17:43.513] Miner got new head                       height=94277067 block-hash=0xab57c14ef00ac31b1b7ee96d18fc7272ccd0c35cfc11fc2fdc24608e574060d0 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471\r\n> DEBUG[04-26|02:17:43.605] Served eth_gasPrice                      conn=18.139.81.231:54273  reqid=671311  t=43.34\u00b5s\r\n> DEBUG[04-26|02:17:44.008] Served eth_gasPrice                      conn=18.139.81.231:25839  reqid=676949  t=45.417\u00b5s\r\n> DEBUG[04-26|02:17:44.546] Served eth_gasPrice                      conn=18.143.94.160:34176  reqid=440575  t=36.599\u00b5s\r\n> DEBUG[04-26|02:17:44.634] Served eth_gasPrice                      conn=18.140.83.19:38924   reqid=609028  t=81.648\u00b5s\r\n> DEBUG[04-26|02:17:44.700] Served eth_chainId                       conn=172.30.0.2:59170     reqid=417949  t=41.016\u00b5s\r\n> DEBUG[04-26|02:17:44.701] Served eth_getBlockByNumber              conn=172.30.0.2:59180     reqid=417950  t=166.514\u00b5s\r\n> DEBUG[04-26|02:17:44.731] Served eth_gasPrice                      conn=18.140.83.19:50821   reqid=593393  t=29.241\u00b5s\r\n> + GETH_DATA_DIR=/geth\r\n> + GETH_CHAINDATA_DIR=/geth/geth/chaindata\r\n> + GETH_KEYSTORE_DIR=/geth/keystore\r\n> + '[' '!' -d /geth/keystore ]\r\n> + '[' '!' -d /geth/geth/chaindata ]\r\n> + echo '/geth/geth/chaindata exists, checking for hardfork.'\r\n> + echo 'Chain config:'\r\n> + geth dump-chain-cfg '--datadir=/geth'\r\n> /geth/geth/chaindata exists, checking for hardfork.\r\n> Chain config:\r\n> INFO [04-26|02:17:50.472] Maximum peer count                       ETH=50 LES=0 total=50\r\n> ```\r\n> \r\n> I running with 8 CPU 32G ram still able to met this problem.\r\n\r\ncheck your docker container exit code", "2023-04-26T03:12:57Z", "2023-04-26T03:12:57Z", "luwang920", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5a7eO4", "I_kwDODjvEJM5jgixy", "> > Not sure why my `simple-optimism-node-l2geth-1` container will being restart randomly.\r\n> > Within this month already resync more than 3 times with 2 nodes. I did make it synced, but suddenly the container will restart.\r\n> > ```\r\n> > DEBUG[04-26|02:17:43.505] Applying transaction to tip              index=94277066 hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 origin=sequencer\r\n> > DEBUG[04-26|02:17:43.505] Attempting to commit rollup transaction  hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471\r\n> > INFO [04-26|02:17:43.511] New block                                index=94277066 l1-timestamp=1682475447 l1-blocknumber=17127328 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 queue-orign=sequencer gas=70978    fees=8.51736e-08     elapsed=6.335ms\r\n> > DEBUG[04-26|02:17:43.513] Miner got new head                       height=94277067 block-hash=0xab57c14ef00ac31b1b7ee96d18fc7272ccd0c35cfc11fc2fdc24608e574060d0 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471 tx-hash=0xd9e6b0461318663e84851b8a9886be6168c1e99376e9c2ac512c4c8a93b81471\r\n> > DEBUG[04-26|02:17:43.605] Served eth_gasPrice                      conn=18.139.81.231:54273  reqid=671311  t=43.34\u00b5s\r\n> > DEBUG[04-26|02:17:44.008] Served eth_gasPrice                      conn=18.139.81.231:25839  reqid=676949  t=45.417\u00b5s\r\n> > DEBUG[04-26|02:17:44.546] Served eth_gasPrice                      conn=18.143.94.160:34176  reqid=440575  t=36.599\u00b5s\r\n> > DEBUG[04-26|02:17:44.634] Served eth_gasPrice                      conn=18.140.83.19:38924   reqid=609028  t=81.648\u00b5s\r\n> > DEBUG[04-26|02:17:44.700] Served eth_chainId                       conn=172.30.0.2:59170     reqid=417949  t=41.016\u00b5s\r\n> > DEBUG[04-26|02:17:44.701] Served eth_getBlockByNumber              conn=172.30.0.2:59180     reqid=417950  t=166.514\u00b5s\r\n> > DEBUG[04-26|02:17:44.731] Served eth_gasPrice                      conn=18.140.83.19:50821   reqid=593393  t=29.241\u00b5s\r\n> > + GETH_DATA_DIR=/geth\r\n> > + GETH_CHAINDATA_DIR=/geth/geth/chaindata\r\n> > + GETH_KEYSTORE_DIR=/geth/keystore\r\n> > + '[' '!' -d /geth/keystore ]\r\n> > + '[' '!' -d /geth/geth/chaindata ]\r\n> > + echo '/geth/geth/chaindata exists, checking for hardfork.'\r\n> > + echo 'Chain config:'\r\n> > + geth dump-chain-cfg '--datadir=/geth'\r\n> > /geth/geth/chaindata exists, checking for hardfork.\r\n> > Chain config:\r\n> > INFO [04-26|02:17:50.472] Maximum peer count                       ETH=50 LES=0 total=50\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > I running with 8 CPU 32G ram still able to met this problem.\r\n> \r\n> check your docker container exit code\r\n\r\nIt does not have exit code, cause the container auto restart already.", "2023-04-27T11:38:08Z", "2023-04-27T11:38:08Z", "joslee7410", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5bc5ck", "I_kwDODjvEJM5jgixy", "Hi again\r\nAny news for the new snapshot\uff1f\r\nThanks\r\nBr\r\nLu", "2023-05-04T08:34:10Z", "2023-05-04T08:34:10Z", "luwang920", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5cLFVi", "I_kwDODjvEJM5jgixy", "Do you have the latest news about snapshots? We have encountered similar problems many times, which have lasted for about a month.", "2023-05-12T22:55:33Z", "2023-05-12T22:55:33Z", "git-ljm", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5cQ9Ox", "I_kwDODjvEJM5jgixy", "How can I get latest snapshots of optimism node both (l2geth & DTL) can anyone please share.", "2023-05-15T14:13:10Z", "2023-05-15T14:13:10Z", "smohite03", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5dIzgz", "I_kwDODjvEJM5jgixy", "> How can I get latest snapshots of optimism node both (l2geth & DTL) can anyone please share.\r\n\r\n+1\r\n", "2023-05-25T09:32:21Z", "2023-05-25T09:32:21Z", "pavel-bc", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5djadJ", "I_kwDODjvEJM5jgixy", "I hope to update the snapshot progress quickly.", "2023-05-31T06:21:31Z", "2023-05-31T06:21:31Z", "git-ljm", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5fFJp2", "I_kwDODjvEJM5jgixy", "Closing this because bedrock snapshots are available", "2023-06-16T19:26:08Z", "2023-06-16T19:26:08Z", "tynes", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5vwPhY", "I_kwDODjvEJM5jgixy", "I think this issue is still relevant. The bedrock snapshot is from ages ago, currently a fresh installation (using bedrock snapshot) is syncing at ~800ms per block... at this rate it would take me roughly 2.8 months to finish syncing. \r\n\r\nI have verified that L1 RPC is not the bottleneck as it responds to each `eth_getBlockByHash` within 30ms.", "2024-01-03T06:46:52Z", "2024-01-03T06:46:52Z", "sajal", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5wpvL8", "I_kwDODjvEJM5jgixy", "@sajal facing the same issue.\r\nI just upgraded my optimism node and now i am stuck in 12 hours syncing it's too slow. ", "2024-01-12T21:45:09Z", "2024-01-12T21:45:09Z", "zainirfan13", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5vaY11", "I_kwDOFpg0Ns56AZMb", "This issue has been automatically marked as stale and will be closed in 7 days if no updates", "2023-12-26T01:46:00Z", "2023-12-26T01:46:00Z", "github-actions", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5vrEDu", "I_kwDOFpg0Ns56AZMb", "This issue was closed as stale.  Please reopen if this is a mistake", "2024-01-02T01:46:53Z", "2024-01-02T01:46:53Z", "github-actions", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5vXa1Q", "I_kwDOFpg0Ns55421A", "This issue has been automatically marked as stale and will be closed in 7 days if no updates", "2023-12-24T01:47:13Z", "2023-12-24T01:47:13Z", "github-actions", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5vnk3r", "I_kwDOFpg0Ns55421A", "This issue was closed as stale.  Please reopen if this is a mistake", "2023-12-31T01:47:36Z", "2023-12-31T01:47:36Z", "github-actions", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5gQlw2", "I_kwDOFpg0Ns5psmK8", "Thanks for calling this out! To provide more details, if no updates have been made to a pull request it is marked stale after 7 days. Then after another 7 days, if no additional updates or comments are made on the pr, then it is auto-closed. Which means it is actually a 14 day window before the PR auto-closes. Additionally, an author can comment to remove the stale label. \r\n\r\nLooking over the list of stale issues some of them are legitimate, but there were two recently that appear to be our fault and due to a slow review process for Base tokens. Currently, Base tokens have to go through a manually review process and we are working on moving this into an automated process. Additionally, on our end we make sure to review token list PR's at least once a week. I will bring this to the teams attention and make sure we do not drop the ball in the future and we remove the stale label for Base PR's where the staleness is due to slowness on our end.\r\n\r\nFor now, I am hesitant to increase the staleness as I think it is reasonable to expect an update within 14 days, and if that isn't happening due to slowness on our end, then we need to make an adjustment to our review process and not just extend the staleness period.", "2023-06-30T17:29:41Z", "2023-06-30T17:29:41Z", "tremarkley", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5gUDFe", "I_kwDOFpg0Ns5psmK8", "Copy, thanks @tremarkley! Appreciate the review. Going to close this, then. :+1: ", "2023-07-01T11:49:16Z", "2024-01-03T16:21:31Z", "wbnns", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5hFLOC", "I_kwDOFpg0Ns5psmK8", "> For now, I am hesitant to increase the staleness as I think it is reasonable to expect an update within 14 days, and if that isn't happening due to slowness on our end, then we need to make an adjustment to our review process and not just extend the staleness period.\r\n\r\nHey @tremarkley do you have any updates re this, doesn't look like the review process is efficient tbh. Related issue #385  ", "2023-07-10T11:20:36Z", "2023-07-10T11:20:36Z", "stas", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5vzBrX", "I_kwDOFpg0Ns5psmK8", "Heya! Happy New Year! Just wanted to reopen this if that's ok!\r\n\r\nLooks like there are around 50 PRs this has happened to.\r\nhttps://github.com/ethereum-optimism/ethereum-optimism.github.io/pulls?q=is%3Apr+is%3Aclosed+label%3AStale\r\n\r\nCc: @stas ", "2024-01-03T16:33:06Z", "2024-01-03T16:33:06Z", "wbnns", "2025-08-30 07:17:50"]
["IC_kwDOFpg0Ns5v6WIG", "I_kwDOFpg0Ns5psmK8", "https://github.com/ethereum-optimism/ethereum-optimism.github.io/pull/611/files", "2024-01-04T18:25:10Z", "2024-01-04T18:25:10Z", "roninjin10", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5tWuAU", "I_kwDODjvEJM54AZ1B", "A draft of this exact thing has already been implemented as part of https://github.com/ethereum-optimism/optimism/pull/8106\r\nThe derivation-pipeline is able to operate in a state where there's no \"chain\" on the EL side, and the engine-queue new-payload queue feeds into the engine as it's syncing. The engine handles the new payloads by extending the skeleton header chain, maintaining an accurate sync target, while continuing snap-sync, just like it would on a L1 CL<>EL optimistic beacon-chain sync.\r\nThis has been tested to successfully sync the op-goerli network, without prior datadir on the op-geth side. We may still want to decouple more of the engine-controls from the engine-queue / derivation pipeline, to encapsulate this syncing state better from the derivation work.\r\n\r\nThe remaining feature is the finalization: the op-node needs a way to turn the resulting synced chain into a finalized chain.", "2023-11-30T22:39:05Z", "2023-11-30T22:39:05Z", "protolambda", "2025-08-30 07:17:50"]
["IC_kwDODjvEJM5tW28M", "I_kwDODjvEJM54AZ1B", "> We may still want to decouple more of the engine-controls from the engine-queue / derivation pipeline, to encapsulate this syncing state better from the derivation work.\r\n\r\nYep. This is going to be the bulk of the work on the op-node. The current interactions are already pretty complex & I want to make sure that they stay manageable.", "2023-11-30T23:15:21Z", "2023-11-30T23:15:21Z", "trianglesphere", "2025-08-30 07:17:50"]
["IC_kwDOKIwiaM5xMy1p", "I_kwDOKIwiaM58Aurx", "Hi, this should be fixed now, can you confirm?", "2024-01-18T20:46:28Z", "2024-01-18T20:46:28Z", "smartcontracts", "2025-08-30 07:17:51"]
["IC_kwDOKIwiaM5x7I6R", "I_kwDOKIwiaM58Aurx", "Yes, thanks!", "2024-01-26T02:54:26Z", "2024-01-26T02:55:46Z", "Ezra-Labs", "2025-08-30 07:17:51"]
["IC_kwDOKIwiaM5vMKnv", "I_kwDOKIwiaM53o6Gh", "Please assign this issue to me.\r\n\r\n\r\n\r\n\r\n\r\n", "2023-12-21T04:30:03Z", "2023-12-21T04:30:03Z", "0xgun", "2025-08-30 07:17:51"]
["IC_kwDOKIwiaM5vuhvQ", "I_kwDOKIwiaM53o6Gh", "sorry @0xgun, it looks like this issue already has a pending PR.", "2024-01-02T19:43:48Z", "2024-01-02T19:43:48Z", "cpengilly", "2025-08-30 07:17:51"]
["IC_kwDOKIwiaM5rlPDK", "I_kwDOKIwiaM52aS-1", "@smartcontracts pinging you to see if we can move this up in the priority list. ", "2023-11-10T01:14:51Z", "2023-11-10T01:14:51Z", "cpengilly", "2025-08-30 07:17:51"]
["IC_kwDOKIwiaM5sLlVi", "I_kwDOKIwiaM52aS-1", "adding more feedback from different issue (going to close duplicate issue): \r\nFirst, I wanted to work with the OP devnet. I followed the instructions at https://community.optimism.io/docs/developers/build/dev-node/, but wasn't successful. I discovered via Discord and GitHub issues that there are several missing steps, including git submodule init && git submodule update. The docs page still doesn't reflect that.", "2023-11-16T18:04:41Z", "2023-11-16T18:04:41Z", "cpengilly", "2025-08-30 07:17:51"]
["IC_kwDODjvEJM5vwE98", "I_kwDODjvEJM5258RY", "Closed as part of https://github.com/ethereum-optimism/optimism/pull/8517", "2024-01-03T05:40:24Z", "2024-01-03T05:40:24Z", "tynes", "2025-08-30 07:18:00"]
["IC_kwDOH2Qg5s5xh2L6", "I_kwDOH2Qg5s54AYRS", "If we don't do this, it would require us to spin up a second set of replicas that don't have a tx pool but are peered with our replicas. This would be the initial set of nodes people can snap sync to.", "2024-01-22T19:50:01Z", "2024-01-22T19:50:01Z", "mslipper", "2025-08-30 07:28:49"]
["IC_kwDOH2Qg5s5xh2fT", "I_kwDOH2Qg5s54AYRS", "Josh is leaning towards the netrestrict flag - will leave up to him for a decision.", "2024-01-22T19:50:52Z", "2024-01-22T19:50:52Z", "mslipper", "2025-08-30 07:28:49"]
["IC_kwDOH2Qg5s5zOfej", "I_kwDOH2Qg5s54AYRS", "Cancelling this in favor of having a dedicated node that has the txpool disable flaged on.", "2024-02-08T00:42:33Z", "2024-02-08T00:42:33Z", "trianglesphere", "2025-08-30 07:28:49"]
["IC_kwDODjvEJM5y6sBN", "I_kwDODjvEJM5-TwUs", "@clabby what checks actually need to happen here? It's not super clear to me", "2024-02-05T19:55:22Z", "2024-02-05T19:55:22Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5y6u0X", "I_kwDODjvEJM5-TwUs", "Closing this as a duplicate issue: https://github.com/ethereum-optimism/client-pod/issues/529", "2024-02-05T19:59:23Z", "2024-02-05T19:59:23Z", "clabby", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yy-mN", "I_kwDODjvEJM5-Kvjb", "The OP Stack is built on the assumption of working from a merged Ethereum chain which includes EIP1559.  There's a fair bit of functionality around calculating the L1 fees to charge transactions which depends on the concept of base fee.", "2024-02-04T23:21:18Z", "2024-02-04T23:21:18Z", "ajsutton", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yb9VE", "I_kwDODjvEJM59z-Lv", "Be sure to update to the latest releases of the software that support the ecotone hardfork", "2024-01-31T20:49:06Z", "2024-01-31T20:49:06Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yb_Sb", "I_kwDODjvEJM59z-Lv", "@tynes so should I create new docker image for my op-node by using latest release, and replace it with the current op-node docker image ? \n\nPs- It is my op-stack chain with L1 as sepolia. Using release v1.30", "2024-01-31T20:53:10Z", "2024-01-31T20:57:35Z", "nitantchhajed", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzM1l", "I_kwDODjvEJM59z-Lv", "Yeah, need to use the latest op-node because Sepolia was updated to support 4844", "2024-02-05T00:47:04Z", "2024-02-05T00:47:04Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzM8x", "I_kwDODjvEJM59ww0M", "Closing as unclear, feel free to open a new issue with more detail", "2024-02-05T00:47:26Z", "2024-02-05T00:47:26Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yKLcv", "I_kwDODjvEJM59CprA", "This is a bug with your remote node likely", "2024-01-29T17:56:21Z", "2024-01-29T17:56:21Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yNBqD", "I_kwDODjvEJM59CprA", "> This is a bug with your remote node likely\r\n\r\n@tynes yes\uff0ci have found it, because i use two eth nodes backup, i think when new block comes, op-node query latest block in node 1 while query receipts in node 2, if node 2 has no latest block, getBlockReceipts will return null\uff0cthis may be the root cause for this problem, will you consider this situation ? ", "2024-01-30T03:40:09Z", "2024-01-30T03:40:09Z", "wangjiangw", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yUkQJ", "I_kwDODjvEJM59CprA", "This is not something that we will fix within `op-node` as it adds extra complexity in consensus critical code. `op-node` assumes a consistent RPC", "2024-01-30T22:18:05Z", "2024-01-30T22:18:05Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzOOI", "I_kwDODjvEJM59CprA", "Closing as wontfix for the moment, reducing the critical code within `op-node` is worth making some assumptions about upstream RPC consistency.", "2024-02-05T00:52:54Z", "2024-02-05T00:52:54Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzOXP", "I_kwDODjvEJM58nLGk", "What chain is this for?", "2024-02-05T00:53:32Z", "2024-02-05T00:53:32Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzPRN", "I_kwDODjvEJM58nLGk", "You're disabling gossip with:\r\n```\r\n      --p2p.disable\r\n```\r\nso will not receive unsafe heads and thus only update the chain from data from L1. The batcher has up to 12 hours to submit data to L1 so this is much higher latency. If you remove that option you should start tracking the unsafe head correctly.\r\n\r\nAlso for future reference these kinds of user support queries are better directed to the discussions forum: https://github.com/ethereum-optimism/developers/discussions", "2024-02-05T00:57:39Z", "2024-02-05T00:57:39Z", "ajsutton", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5y1z3e", "I_kwDODjvEJM58nLGk", "thanks!", "2024-02-05T10:53:26Z", "2024-02-05T10:53:26Z", "tmeinlschmidt", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5wroyc", "I_kwDODjvEJM571LUy", "Remove --l1.trustrpc", "2024-01-13T15:22:05Z", "2024-01-13T15:22:05Z", "MrFrogoz", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ws2vi", "I_kwDODjvEJM571LUy", "> Remove --l1.trustrpc\r\n\r\nThis is my op-node config. No --l1.trustrpc\r\n\r\n> ./bin/op-node \\\r\n>     --l1=$L1URL  \\\r\n>     --l1.rpckind=$L1KIND \\\r\n>     --l2=http://localhost:7551 \\\r\n>     --l2.jwt-secret=/local_optimism/op-geth/jwt.txt \\\r\n>     --network=$NET \\\r\n>     --rpc.addr=0.0.0.0 \\\r\n>     --rpc.port=7547 \\\r\n>     --network=op-mainnet \\\r\n>     --rollup.load-protocol-versions=true \\\r\n>     --rollup.halt=major", "2024-01-14T00:31:19Z", "2024-01-14T00:31:19Z", "dandavid3000", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5wtPK7", "I_kwDODjvEJM571LUy", "I have the same issue, any recommendation to solve it?", "2024-01-14T10:09:02Z", "2024-01-14T10:09:02Z", "eldimious", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5xb3nC", "I_kwDODjvEJM571LUy", "has been ok ? i have same issue, any updates ?", "2024-01-22T03:45:34Z", "2024-01-22T03:45:34Z", "wangjiangw", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzSF3", "I_kwDODjvEJM542IRI", "Should be fixed in https://github.com/ethereum-optimism/optimism/pull/9329.", "2024-02-05T01:14:00Z", "2024-02-05T01:14:00Z", "mslipper", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5r_UMs", "I_kwDODjvEJM520oWx", "## Issue Closure\r\n\r\nReport: The key to solving the problem was switching from `make` to `gmake`. Additionally, during the process, I reinstalled Homebrew and encountered some challenges with `direnv`. However, this was addressed by reconfiguring `direnv` in the `.bashrc` file, ensuring its proper functionality.\r\n\r\nThank you for the support.", "2023-11-15T03:39:57Z", "2023-11-15T03:39:57Z", "ZentaChainAdmin", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5siAyR", "I_kwDODjvEJM520oWx", "> ## Issue Closure\r\n> Report: The key to solving the problem was switching from `make` to `gmake`. Additionally, during the process, I reinstalled Homebrew and encountered some challenges with `direnv`. However, this was addressed by reconfiguring `direnv` in the `.bashrc` file, ensuring its proper functionality.\r\n> \r\n> Thank you for the support.\r\n\r\nHow did you solved this issue by switching from `make` to `gmake`, it seems to me was no difference, could you explained with more details, thanks very much. ", "2023-11-21T12:43:29Z", "2023-11-21T12:43:29Z", "evanlikn", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5tgOj7", "I_kwDODjvEJM520oWx", "Following up to this. I ran into this issue. It seems like Forge as of the recent nightly builds are escaping strings passed into constructor (and likely other places) in the broadcast JSON. \r\n\r\nForge v0.2.0 (commit 5b7e4cb)\r\n\r\n```solidity\r\npragma solidity ^0.8.13;\r\n\r\ncontract Test {\r\n\r\n   string public s;\r\n\r\n   constructor(string memory _s) {\r\n    s = _s;\r\n   }\r\n\r\n}\r\n```\r\n\r\n```json\r\n\"arguments\": [\r\n        \"\\\"HelloWorld\\\"\"\r\n ]\r\n```\r\n\r\nThis escaping leads to it being encoded as a string:\r\n\r\n```json\r\n{\r\n \"arguments\": \"[\\\"0x6E5ed6320F977a956fA446EDF43Ab8DaABc274D4\\\",\\\"\\\\OVM_L1CrossDomainMessenger\\\\\\\"]\"\r\n}\r\n```\r\n\r\nWhere as Go expects an `[]any` slice/array.\r\n\r\nA solution I found for this was to change the Deployer.sol script:\r\n\r\nhttps://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/scripts/Deployer.sol#L382\r\n\r\nand use `jq` to parse the escaped string as JSON: \r\n\r\noriginal:\r\n```solidity\r\n     cmd[2] = string.concat(Executables.jq, \" -r '.arguments' <<< '\", _transaction, \"'\");\r\n```\r\n\r\nnew:\r\n\r\n```solidity\r\n     cmd[2] = string.concat(Executables.jq, \" -r 'try(.arguments|map_values(try(fromjson),.))' <<< '\", _transaction, \"'\");\r\n```\r\n", "2023-12-02T14:45:20Z", "2023-12-02T14:45:20Z", "rbrick", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5tshhs", "I_kwDODjvEJM520oWx", "I have the same issue\r\nt=2023-12-05T09:37:45+0000 lvl=info msg=\"Deploy config\" path=/app/deploy-config/142095.json\r\nt=2023-12-05T09:37:45+0000 lvl=info msg=\"Deployment directory\" path=/app/deployments/142095\r\nt=2023-12-05T09:37:45+0000 lvl=crit msg=\"Application failed\"   message=\"json: cannot unmarshal string into Go struct field Deployment.args of type []interface {}\"", "2023-12-05T09:40:28Z", "2023-12-05T09:40:28Z", "Sallery-X", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5tshuW", "I_kwDODjvEJM520oWx", "> Following up to this. I ran into this issue. It seems like Forge as of the recent nightly builds are escaping strings passed into constructor (and likely other places) in the broadcast JSON.\r\n> \r\n> Forge v0.2.0 (commit 5b7e4cb)\r\n> \r\n> ```solidity\r\n> pragma solidity ^0.8.13;\r\n> \r\n> contract Test {\r\n> \r\n>    string public s;\r\n> \r\n>    constructor(string memory _s) {\r\n>     s = _s;\r\n>    }\r\n> \r\n> }\r\n> ```\r\n> \r\n> ```json\r\n> \"arguments\": [\r\n>         \"\\\"HelloWorld\\\"\"\r\n>  ]\r\n> ```\r\n> \r\n> This escaping leads to it being encoded as a string:\r\n> \r\n> ```json\r\n> {\r\n>  \"arguments\": \"[\\\"0x6E5ed6320F977a956fA446EDF43Ab8DaABc274D4\\\",\\\"\\\\OVM_L1CrossDomainMessenger\\\\\\\"]\"\r\n> }\r\n> ```\r\n> \r\n> Where as Go expects an `[]any` slice/array.\r\n> \r\n> A solution I found for this was to change the Deployer.sol script:\r\n> \r\n> https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/scripts/Deployer.sol#L382\r\n> \r\n> and use `jq` to parse the escaped string as JSON:\r\n> \r\n> original:\r\n> \r\n> ```solidity\r\n>      cmd[2] = string.concat(Executables.jq, \" -r '.arguments' <<< '\", _transaction, \"'\");\r\n> ```\r\n> \r\n> new:\r\n> \r\n> ```solidity\r\n>      cmd[2] = string.concat(Executables.jq, \" -r 'try(.arguments|map_values(try(fromjson),.))' <<< '\", _transaction, \"'\");\r\n> ```\r\n\r\nI tried it, but it did not work", "2023-12-05T09:41:03Z", "2023-12-05T09:41:03Z", "Sallery-X", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzUxr", "I_kwDODjvEJM520oWx", "Please make sure that you're running the correct foundry version by running `pnpm update:foundry` in the root of this repo.", "2024-02-05T01:26:56Z", "2024-02-05T01:26:56Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5zO4la", "I_kwDODjvEJM520oWx", "Closing, make sure that you're always using the correct version of foundry by running `pnpm update:foundry`.", "2024-02-08T02:49:40Z", "2024-02-08T02:49:40Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5q1Grx", "I_kwDODjvEJM51xbsp", "Try running the command `pkill geth` - you probably have a zombie geth process running", "2023-11-03T11:53:37Z", "2023-11-03T11:53:37Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5q1kGe", "I_kwDODjvEJM51xbsp", "Thanks. But not solved. The same error.", "2023-11-03T13:20:43Z", "2023-11-03T13:20:43Z", "HE1M", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5q5nHM", "I_kwDODjvEJM51xbsp", "https://github.com/ethereum-optimism/developers/discussions/17#discussioncomment-7399981    \r\nIt is recommended that you delete it and then follow this guide to re-create it", "2023-11-04T16:19:28Z", "2023-11-04T16:19:28Z", "opfocus", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzTpL", "I_kwDODjvEJM5z0Xm4", "Linked doc is out of date, please use the new docs: docs.optimism.io", "2024-02-05T01:22:11Z", "2024-02-05T01:22:11Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ob-eD", "I_kwDODjvEJM5zBXtr", "The derivation rules do not allow the L2 to skip L1 blocks.  Step 5 as described is not restarting the network, but creating an entirely new, alternate network. There isn't an easy way to do that with the current OP Stack.\r\n\r\nIf you stop the sequencer for some period of time, and then start it again, it will resume processing from where it left off on the L1 chain.", "2023-10-08T21:18:04Z", "2023-10-08T21:18:04Z", "ajsutton", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5n9dHN", "I_kwDODjvEJM5yoWgq", "This is because the `bedrock-devnet` [script](https://github.com/ethereum-optimism/optimism/blob/0025fff177d656fc14d64c174eef3b69a03413dc/bedrock-devnet/devnet/__init__.py#L116)  is running a series of `cast` commands. Do you have [Foundry](https://github.com/foundry-rs/foundry) installed?", "2023-10-03T04:22:44Z", "2023-10-03T04:22:44Z", "Sabnock01", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5oEDxX", "I_kwDODjvEJM5yoWgq", "I am a beginner and installing for the first time, what should I do,or what to install first.", "2023-10-03T23:44:22Z", "2023-10-03T23:48:38Z", "sameplacewei", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5oEGoD", "I_kwDODjvEJM5yoWgq", "Since `cast` is part of Foundry so the first thing you'd need to do is install it which you can find instructions for from the above link but specifically can be found [here](https://book.getfoundry.sh/getting-started/installation).", "2023-10-03T23:57:28Z", "2023-10-03T23:59:45Z", "Sabnock01", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5oEIVH", "I_kwDODjvEJM5yoWgq", "Thanks ,I try again.", "2023-10-04T00:05:00Z", "2023-10-04T00:05:00Z", "sameplacewei", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5oHLMy", "I_kwDODjvEJM5yoWgq", "I have installed Foundry,node.when i run make devnet-up,the fellowing err occur:\r\n\r\n> INFO [10-04|19:41:40.796] Maximum peer count                       ETH=50 LES=0 total=50\r\nINFO [10-04|19:41:40.797] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nDEBUG[10-04|19:41:40.797] FS scan times                            list=\"34.487\u00b5s\" set=380ns diff=\"1.233\u00b5s\"\r\nWARN [10-04|19:41:40.798] Disable transaction unindexing for archive node \r\nDEBUG[10-04|19:41:40.799] Sanitizing Go's GC trigger               percent=100\r\nINFO [10-04|19:41:40.799] Enabling recording of key preimages since archive mode is used \r\nINFO [10-04|19:41:40.799] Set global gas cap                       cap=50,000,000\r\nError: \r\n(code: -32000, message: nonce too low, data: None)\r\nTraceback (most recent call last):\r\n  File \"/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism/./bedrock-devnet/main.py\", line 9, in <module>\r\n    main()\r\n  File \"/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism/./bedrock-devnet/main.py\", line 5, in main\r\n    devnet.main()\r\n  File \"/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 94, in main\r\n    devnet_l1_genesis(paths)\r\n  File \"/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 166, in devnet_l1_genesis\r\n    raise Exception(f\"Exception occurred in child process: {err}\")\r\nException: Exception occurred in child process: Command '['cast', 'publish', '--rpc-url', 'http://127.0.0.1:8545', '0xf8a58085174876e800830186a08080b853604580600e600039806000f350fe7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf31ba02222222222222222222222222222222222222222222222222222222222222222a02222222222222222222222222222222222222222222222222222222222222222']' returned non-zero exit status 1.\r\nmake[1]: *** [Makefile:120\uff1adevnet-allocs] error 1\r\nmake[1]: leave path\u201c/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism\u201d\r\nmake: *** [Makefile:91\uff1adevnet-up] error 2\r\n\r\nIs there anything else I need to do? I just want to set up a local test environment first\r\n\r\nI followed this step([Running a local development environment](https://community.optimism.io/docs/developers/build/dev-node/)), but it seems that this is not suitable for beginners, is there any other suitable for beginners guide?", "2023-10-04T11:47:24Z", "2023-10-04T11:52:22Z", "sameplacewei", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5oKhod", "I_kwDODjvEJM5yoWgq", "Is\r\n \r\n>  Exception: Exception occurred in child process: Command '['cast', 'publish', '--rpc-url', 'http://127.0.0.1:8545', '0xf8a58085174876e800830186a08080b853604580600e600039806000f350fe7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf31ba02222222222222222222222222222222222222222222222222222222222222222a02222222222222222222222222222222222222222222222222222222222222222']' returned non-zero exit status 1.\r\n\r\nthe only error you see?", "2023-10-04T20:26:41Z", "2023-10-04T20:26:41Z", "Sabnock01", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5oLY7s", "I_kwDODjvEJM5yoWgq", "This is certainly because you do not have `cast` in your `PATH`", "2023-10-05T00:05:04Z", "2023-10-05T00:05:04Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5oLbnh", "I_kwDODjvEJM5yoWgq", "> Is\r\n> \r\n> > Exception: Exception occurred in child process: Command '['cast', 'publish', '--rpc-url', 'http://127.0.0.1:8545', '0xf8a58085174876e800830186a08080b853604580600e600039806000f350fe7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf31ba02222222222222222222222222222222222222222222222222222222222222222a02222222222222222222222222222222222222222222222222222222222222222']' returned non-zero exit status 1.\r\n> \r\n> the only error you see?\r\n\r\nthis is all the information I get\r\n\r\n> PYTHONPATH=./bedrock-devnet python3 ./bedrock-devnet/main.py --monorepo-dir=. --allocs\r\n[INFO|10-04-2023 07:41:40] Generating L1 genesis state\r\n==================================================\r\n<devnet.Bunch object at 0x7f59fb148f40>\r\n[INFO|10-04-2023 07:41:40] Trying 127.0.0.1:8545\r\n[INFO|10-04-2023 07:41:40] Connected 127.0.0.1:8545\r\n[INFO|10-04-2023 07:41:40] Waiting for RPC server at 127.0.0.1:8545\r\n[INFO|10-04-2023 07:41:40] RPC server at 127.0.0.1:8545 ready\r\n[INFO|10-04-2023 07:41:40] Fetch eth_accounts 127.0.0.1:8545\r\n[INFO|10-04-2023 07:41:40] Deploying with 0xa56222cb89bfc647296ec74331b9b47aaf6b4fea\r\nINFO [10-04|19:41:40.791] Starting Geth in ephemeral dev mode... \r\nWARN [10-04|19:41:40.791] You are running Geth in --dev mode. Please note the following:\r\n\r\n  1. This mode is only intended for fast, iterative development without assumptions on\r\n     security or persistence.\r\n  2. The database is created in memory unless specified otherwise. Therefore, shutting down\r\n     your computer or losing power will wipe your entire block data and chain state for\r\n     your dev environment.\r\n  3. A random, pre-allocated developer account will be available and unlocked as\r\n     eth.coinbase, which can be used for testing. The random dev account is temporary,\r\n     stored on a ramdisk, and will be lost if your machine is restarted.\r\n  4. Mining is enabled by default. However, the client will only seal blocks if transactions\r\n     are pending in the mempool. The miner's minimum accepted gas price is 1.\r\n  5. Networking is disabled; there is no listen-address, the maximum number of peers is set\r\n     to 0, and discovery is disabled.\r\n \r\n\r\nblockHash               0x279c2953f2082e254d5345ea7c48904f6afb1d66fbe062f5b90fe83349b73356\r\nblockNumber             6\r\ncontractAddress         \r\ncumulativeGasUsed       21000\r\neffectiveGasPrice       3449445906\r\ngasUsed                 21000\r\nlogs                    []\r\nlogsBloom               0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\r\nroot                    \r\nstatus                  1\r\ntransactionHash         0x01ea91f03c2308123fb0a0c38438c13fde1e1b0bf1c32517e87dac4bccb64ef1\r\ntransactionIndex        0\r\ntype                    2\r\nINFO [10-04|19:41:40.796] Maximum peer count                       ETH=50 LES=0 total=50\r\nINFO [10-04|19:41:40.797] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nDEBUG[10-04|19:41:40.797] FS scan times                            list=\"34.487\u00b5s\" set=380ns diff=\"1.233\u00b5s\"\r\nWARN [10-04|19:41:40.798] Disable transaction unindexing for archive node \r\nDEBUG[10-04|19:41:40.799] Sanitizing Go's GC trigger               percent=100\r\nINFO [10-04|19:41:40.799] Enabling recording of key preimages since archive mode is used \r\nINFO [10-04|19:41:40.799] Set global gas cap                       cap=50,000,000\r\nError: \r\n(code: -32000, message: nonce too low, data: None)\r\nTraceback (most recent call last):\r\n  File \"/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism/./bedrock-devnet/main.py\", line 9, in <module>\r\n    main()\r\n  File \"/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism/./bedrock-devnet/main.py\", line 5, in main\r\n    devnet.main()\r\n  File \"/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 94, in main\r\n    devnet_l1_genesis(paths)\r\n  File \"/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 166, in devnet_l1_genesis\r\n    raise Exception(f\"Exception occurred in child process: {err}\")\r\nException: Exception occurred in child process: Command '['cast', 'publish', '--rpc-url', 'http://127.0.0.1:8545', '0xf8a58085174876e800830186a08080b853604580600e600039806000f350fe7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf31ba02222222222222222222222222222222222222222222222222222222222222222a02222222222222222222222222222222222222222222222222222222222222222']' returned non-zero exit status 1.\r\nmake[1]: *** [Makefile:120\uff1adevnet-allocs] error 1\r\nmake[1]: leave path\u201c/home/weizp/work/gows/src/github.com/ethereum-optimism/optimism\u201d\r\nmake: *** [Makefile:91\uff1adevnet-up] error 2\r\n", "2023-10-05T00:16:16Z", "2023-10-05T00:49:39Z", "sameplacewei", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5oLdCG", "I_kwDODjvEJM5yoWgq", "> This is certainly because you do not have `cast` in your `PATH`\r\n\r\nI'm sure I've set the environment variables,Path\r\n\r\n> export PATH=\"$PATH:/home/weizp/.foundry/bin\"", "2023-10-05T00:19:25Z", "2023-10-05T00:19:25Z", "sameplacewei", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ozoYd", "I_kwDODjvEJM5yoWgq", "Try restarting your computer, you have a zombie geth process that you need to kill", "2023-10-11T19:12:38Z", "2023-10-11T19:12:38Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5qU1Da", "I_kwDODjvEJM5yoWgq", "well, most possible `foundry` is installed, but `~/.foundry/bin` is not on your classpath.\r\nYou can try using that command to install everything automatically:\r\n\r\n`wget -L https://raw.githubusercontent.com/softsky/optimism/develop/install-ubuntu.sh\" -O - | bash`", "2023-10-28T15:17:06Z", "2023-10-28T15:17:06Z", "aahutsal", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzTPu", "I_kwDODjvEJM5yoWgq", "Closing, need `cast` installed.", "2024-02-05T01:20:21Z", "2024-02-05T01:20:21Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5kYyUV", "I_kwDODjvEJM5upPhm", "We would like to include a fix for this in the next network upgrade/hard fork. The idea would be that the next channel that is considered canonical is the first channel that closes rather than the first channel that is seen. Until then, the solution is plan B", "2023-08-18T17:26:47Z", "2023-08-18T17:26:47Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5lzLtY", "I_kwDODjvEJM5upPhm", "@tynes do you have any consideration why Plan A is not good idea? want to hear more ", "2023-09-06T08:39:07Z", "2023-09-06T08:39:07Z", "huihzhao", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzQOU", "I_kwDODjvEJM5upPhm", "@tynes was this fixed?", "2024-02-05T01:03:42Z", "2024-02-05T01:03:42Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5y3NQq", "I_kwDODjvEJM5upPhm", "This was fixed as part of the canyon network upgrade", "2024-02-05T13:59:54Z", "2024-02-05T13:59:54Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5jNusc", "I_kwDODjvEJM5tUwiE", "It looks like your genesis blocks are different. Please provide a reproducible example to help with the debugging process", "2023-08-03T19:40:42Z", "2023-08-03T19:40:42Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzSOJ", "I_kwDODjvEJM5tUwiE", "Closing for inactivity", "2024-02-05T01:14:45Z", "2024-02-05T01:14:45Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5iidwX", "I_kwDODjvEJM5stvjn", "step: https://stack.optimism.io/docs/build/getting-started/#op-node", "2023-07-27T09:03:56Z", "2023-07-27T09:03:56Z", "minhhoang2404", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ikFOV", "I_kwDODjvEJM5stvjn", "@minhhoang2404  i think the documentation can confuse...\r\n\r\nyou obtain your config from --> https://stack.optimism.io/docs/build/getting-started/#op-node\r\nbut... check this config --------> https://community.optimism.io/docs/developers/bedrock/node-operator-guide/#configuring-op-node\r\n\r\n`A minimal valid configuration for a rollup node on our beta-1 testnet looks like this:`\r\n\r\n```\r\nop-node --l1=<goerli RPC url> \\\r\n        --l2=<op-geth authenticated RPC url> \\\r\n        --network=beta-1\r\n        --rpc.addr=127.0.0.1 \\\r\n        --rpc.port=9545 \\\r\n        --l2.jwt-secret=<path to JWT secret>\r\n```\r\n\r\nu need to add `--l1=<goerli RPC url>` ( in my case im using `http://l1:8545` )", "2023-07-27T13:24:48Z", "2023-07-27T13:31:33Z", "netzulo", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ik5-o", "I_kwDODjvEJM5stvjn", "Are you attempting to sync a public network or are you running a private network?", "2023-07-27T15:22:43Z", "2023-07-27T15:22:43Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5iqlAP", "I_kwDODjvEJM5stvjn", "I newbie, I want to build network layer2 test. Can u help me build it ? love you <3", "2023-07-28T09:05:00Z", "2023-07-28T09:05:00Z", "minhhoang2404", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5i1rIN", "I_kwDODjvEJM5stvjn", "just try to run by entering the command in single line, I also faced this error few days ago but running the command by changing the format of the command like this worked for me :\r\n\r\n./bin/op-node --l2=http://localhost:8551 --l2.jwt-secret=./jwt.txt --sequencer.enabled --sequencer.l1-confs=3 --verifier.l1-confs=3 --rollup.config=./rollup.json --rpc.addr=0.0.0.0 --rpc.port=8547 --p2p.disable --rpc.enable-admin --p2p.sequencer.key=$SEQ_KEY --l1=$L1_RPC --l1.rpckind=$RPC_KIND", "2023-07-31T12:02:37Z", "2023-07-31T12:02:37Z", "nitantchhajed", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5i-Ttf", "I_kwDODjvEJM5stvjn", "Thank you @nitantchhajed, I had the same issue than @minhhoang2404 and it worked using your command I don't know why ... formatting issue ...\r\nAlso, if anyone wants to use Sepolia testnet instead of Goerli, you must change \"l1_chain_id\" in op-node/rollup.json from 5 to 11155111", "2023-08-01T14:58:54Z", "2023-08-01T14:59:57Z", "flbouchut", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5jCpNj", "I_kwDODjvEJM5stvjn", "@flbouchut  Editing the `rollup.json` or `genesis.json` file is not a good practice, instead change the `l1ChainID` in `getting-started.json` file. @minhhoang2404 you can close this Issue if the above command works for you. :)", "2023-08-02T07:14:19Z", "2023-08-02T07:14:19Z", "nitantchhajed", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5jZ2zi", "I_kwDODjvEJM5stvjn", "> I newbie, I want to build network layer2 test. Can u help me build it ? love you <3\r\n\r\nHello @minhhoang2404, you should try deploying your L2 using a Rollup Control Panel. Shards.dev makes it easy and it 's free ", "2023-08-07T11:58:58Z", "2023-08-07T11:58:58Z", "rflihxyz", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzSGc", "I_kwDODjvEJM5stvjn", "Closing for inactivity, please refer to the updated docs if you'd like to run a testnet: docs.optimism.io", "2024-02-05T01:14:03Z", "2024-02-05T01:14:03Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ifmKw", "I_kwDODjvEJM5soCo4", "Are you sure enough time has passed? ", "2023-07-26T20:30:07Z", "2023-07-26T20:30:07Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5i1sL4", "I_kwDODjvEJM5soCo4", "can you follow his tutorial and check the message status of your transaction ? https://github.com/ethereum-optimism/optimism-tutorial/tree/main/sdk-trace-tx\r\n\r\nmake sure your `MESSAGE_STATUS` is 3 before you prove the transaction", "2023-07-31T12:05:41Z", "2023-07-31T12:05:41Z", "nitantchhajed", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5kROIk", "I_kwDODjvEJM5soCo4", "> Are you sure enough time has passed?\r\n\r\nHow can we get the time left for challenge period?", "2023-08-17T12:54:13Z", "2023-08-17T12:54:13Z", "carlbarrdahl", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzSA_", "I_kwDODjvEJM5soCo4", "Closing for inactivity, feel free to open a new issue if this is still a problem", "2024-02-05T01:13:28Z", "2024-02-05T01:13:28Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5is03y", "I_kwDODjvEJM5sGhP6", "Thank you for reporting this, this is definitely an issue that we can fix. The docs now live in this repo: https://github.com/ethereum-optimism/stack-docs\r\n\r\nUpdate: its actually here: https://github.com/ethereum-optimism/community-hub/blob/e57d46489973298826155c6c814820a24ff3fbfc/src/docs/protocol/protocol-2.0.md?plain=1#L6", "2023-07-28T15:52:43Z", "2023-07-28T16:00:49Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5is5To", "I_kwDODjvEJM5sGhP6", "Open PR to fix some of the issue, will need to circle back around to fix more of it: https://github.com/ethereum-optimism/community-hub/pull/850", "2023-07-28T16:01:10Z", "2023-07-28T16:01:10Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5h3IfP", "I_kwDODjvEJM5r-8IH", "it just came to me that when I migrating the sequencer node, I didn't copy the datadir of op-geth because there's no much data there I thought it would be fast to derive from L1. maybe this is the reason why my new sequencer node produces new blocks that irrelevant to the existing blocks on L1. \r\nBut IMO, given the concrete genesis.json and rollup.json, even with an empty chain datadir, we should derive a consistent chain according to L1", "2023-07-19T10:35:51Z", "2023-07-19T10:38:19Z", "cifer76", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5h42ze", "I_kwDODjvEJM5r-8IH", "Are you sure that you have configured everything correctly?", "2023-07-19T15:22:10Z", "2023-07-19T15:22:10Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5h5V5y", "I_kwDODjvEJM5r-8IH", "> Are you sure that you have configured everything correctly?\r\n\r\nyes I am sure about that, the second sequencer node is exactly the same with the first one, except I didn't copy the datadir of op-geth (but I initialized the geth state using the same genesis.json)", "2023-07-19T16:45:48Z", "2023-07-19T16:46:32Z", "cifer76", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzR4s", "I_kwDODjvEJM5r-8IH", "Going to close this for inactivity, if this is still an issue feel free to reopen.", "2024-02-05T01:12:40Z", "2024-02-05T01:12:40Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzRt1", "I_kwDODjvEJM5r4J-U", "stack.optimism.io is deprecated", "2024-02-05T01:11:43Z", "2024-02-05T01:11:43Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5iQWDp", "I_kwDODjvEJM5rX_JQ", "This is a metamax issue regarding the way that they compute the \"max amount\", they must not be taking into account the L1 fee", "2023-07-24T18:58:36Z", "2023-07-24T18:58:36Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ifyWP", "I_kwDODjvEJM5rX_JQ", "We are working on a solution to make accurate fee estimation much easier here - https://github.com/ethereum-optimism/optimism/tree/develop/packages/fee-estimation", "2023-07-26T21:10:19Z", "2023-07-26T21:10:19Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzRiO", "I_kwDODjvEJM5rX_JQ", "Closing as this is a metamask issue", "2024-02-05T01:10:53Z", "2024-02-05T01:10:53Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ifygU", "I_kwDODjvEJM5rSs_R", "The contracts are a WIP, you can find them here - https://github.com/ethereum-optimism/optimism/tree/develop/packages/contracts-bedrock/contracts/cannon", "2023-07-26T21:10:54Z", "2023-07-26T21:10:54Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5iizWX", "I_kwDODjvEJM5rSs_R", "> The contracts are a WIP, you can find them here - https://github.com/ethereum-optimism/optimism/tree/develop/packages/contracts-bedrock/contracts/cannon\r\n\r\nThanks but I got 404 from the link you provide, would like to help confirm? I guess maybe the branch is not develop?", "2023-07-27T09:56:52Z", "2023-07-27T09:56:52Z", "David-Web3", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzRSk", "I_kwDODjvEJM5rSs_R", "For anyone reading this, the contracts can be found in the `dispute` folder in `contracts-bedrock`.", "2024-02-05T01:09:25Z", "2024-02-05T01:09:25Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5hDg97", "I_kwDODjvEJM5q5Ae2", "Good", "2023-07-10T06:20:17Z", "2023-07-10T06:20:17Z", "rjamalijam", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5if1sV", "I_kwDODjvEJM5q5Ae2", "Did you use `forge` or `hardhat` to deploy?", "2023-07-26T21:21:50Z", "2023-07-26T21:21:50Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ijB4J", "I_kwDODjvEJM5q5Ae2", "I have followed the [op stack Doc](https://stack.optimism.io/docs/build/getting-started/#configure-your-network). \r\nwhich uses **forge**", "2023-07-27T10:33:51Z", "2023-07-27T10:33:51Z", "devudilip", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzRnD", "I_kwDODjvEJM5q5Ae2", "Doc referenced in this issue is now very out of date, closing.", "2024-02-05T01:11:20Z", "2024-02-05T01:11:20Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5inmK7", "I_kwDODjvEJM5qw_l2", "Thank you for opening this PR. If there was some way you could repro an error that would be very helpful because the RPC requests seem to be working fine as is and are running in production", "2023-07-27T20:47:46Z", "2023-07-27T20:47:46Z", "tynes", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5yzRYs", "I_kwDODjvEJM5qw_l2", "Closing as stale, no repro was provided", "2024-02-05T01:09:59Z", "2024-02-05T01:09:59Z", "smartcontracts", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5g0SwK", "I_kwDODjvEJM5qrftl", "All the links in https://github.com/ethereum-optimism/community-hub/edit/main/src/docs/protocol/protocol-2.0.md are broken. I tried replacing \"master\" branch with \"v1.0.9\" which worked for all except .../L2/predeploys/iOVM_L1MessageSender.sol. Obviously this needs to be updated to match the current code, but at least pointing it back to the moment in time that it was correct would be better than getting the 404.", "2023-07-06T21:16:44Z", "2023-07-06T21:16:44Z", "brianjohnson5972", "2025-08-30 07:33:19"]
["IC_kwDODjvEJM5ymnxz", "I_kwDODjvEJM59uR1G", "We have encountered this issue in https://github.com/ethereum-optimism/optimism/pull/9313\r\n\r\nIt looks like its related to the foundry version. Likely due to an alloy migration internal to forge", "2024-02-02T03:08:21Z", "2024-02-02T03:08:21Z", "tynes", "2025-08-30 07:33:25"]
["IC_kwDODjvEJM5ymsJ6", "I_kwDODjvEJM59uR1G", "db7f837ed20807444fb7659eb209dfc7b2f29947 is an attempt to fix this", "2024-02-02T03:32:46Z", "2024-02-02T03:32:46Z", "tynes", "2025-08-30 07:33:25"]
["IC_kwDODjvEJM5yMGrp", "I_kwDODjvEJM59jiMg", "The SDK is extremely large! This is a huge issue for those who are cognizant about bundle size.  \r\n\r\nMost of the SDK size comes from contract artifacts that are consumed by the SDK in a way such that treeshaking cannot occur.  It's not because the sdk itself is very large, it's mostly just one single file. So breaking it up into modules actually won't fix the issue unless the underlying contracts artifacts issue is also fixed. And even then treeshaking for ethers v5 and our class based sdk makes it better but still not great.\r\n\r\nFor context we created [op-viem](https://github.com/base-org/op-viem) to address this issue. Op-viem has all the functionality of the SDK in it at a much smaller bundle size hit that gets even better if you only use some of op-viem both viem and op-viem are tree shakable.\r\n\r\nDepending on your use case you might not even need op-viem because [viem supports the op stack natively](https://viem.sh/op-stack)\r\n\r\nWe have no plans to update the `@eth-optimism/sdk` and suggest everybody consider migrating to viem or op-viem", "2024-01-29T23:47:55Z", "2024-01-29T23:47:55Z", "roninjin10", "2025-08-30 07:33:25"]
["IC_kwDODjvEJM5yMIbP", "I_kwDODjvEJM59jiMg", "Thanks for the issue and if you either\r\n- Need the sdk updated and want guidance on how to do it\r\n- Need help migrating to viem\r\n- Feel like this issue shouldn't have been closed\r\n\r\nJust let me know and I'm happy to help :)", "2024-01-29T23:53:39Z", "2024-01-29T23:53:55Z", "roninjin10", "2025-08-30 07:33:25"]
["IC_kwDODjvEJM5yKLQ-", "I_kwDODjvEJM59S4ld", "If there was a way to decouple this tool from the superchain registry then it could run against any networks", "2024-01-29T17:55:53Z", "2024-01-29T17:55:53Z", "tynes", "2025-08-30 07:33:25"]
["IC_kwDODjvEJM5w7NrL", "I_kwDODjvEJM58EBLd", "30 million per block", "2024-01-16T21:49:48Z", "2024-01-16T21:49:48Z", "tynes", "2025-08-30 07:33:25"]
["IC_kwDOH2Qg5s5iXGKb", "I_kwDOH2Qg5s5shrOS", "I did start this conversation here: \r\n\r\nhttps://github.com/ethereum-optimism/optimism/issues/6107#issuecomment-1603313548\r\n\r\nBut from what I read correctly, I was told to bring it up w/ geth so I am bringing this issue here :) ", "2023-07-25T17:10:59Z", "2023-07-25T17:10:59Z", "yaanakbr", "2025-08-30 07:37:13"]
["IC_kwDOH2Qg5s5zxiPS", "I_kwDOH2Qg5s5shrOS", "`chainId` is missing in the response object, this is most likely due to client implementation differing from the execution spec. The latest release of geth, v1.13.12 has fixes to make the RPC conformant to the execution specification, probably fixed now. \r\n\r\n", "2024-02-13T20:11:48Z", "2024-02-13T20:11:48Z", "sambacha", "2025-08-30 07:37:13"]
["IC_kwDOJ_r-bs5zseux", "I_kwDOJ_r-bs5-_iye", "We can hook it into the #notify-ci-failures slack channel, too. ", "2024-02-13T10:07:15Z", "2024-02-13T10:07:15Z", "geoknee", "2025-08-30 07:37:34"]
["IC_kwDOJ_r-bs5zkAsm", "I_kwDOJ_r-bs5--bIN", "I think we should consider running the CI checks nightly, since I only discovered this CI failure on a pull request with unrelated changes. ", "2024-02-12T14:48:17Z", "2024-02-12T14:48:17Z", "geoknee", "2025-08-30 07:37:34"]
["IC_kwDOJ_r-bs5zlXSn", "I_kwDOJ_r-bs5--bIN", "> I think we should consider running the CI checks nightly, since I only discovered this CI failure on a pull request with unrelated changes.\r\n\r\nThis seems like a good idea to me so I created https://github.com/ethereum-optimism/superchain-registry/issues/89", "2024-02-12T17:12:25Z", "2024-02-12T17:12:25Z", "mds1", "2025-08-30 07:37:34"]
["IC_kwDODjvEJM50UIxU", "I_kwDODjvEJM5_fs1v", "Th is is already fixed in v1.6.1. https://github.com/ethereum-optimism/optimism/releases/tag/v1.6.1\r\n\r\nFor the record, it also only occurs when `op-node` is directly attached to a terminal.  `op-node <options> | tee /dev/null` works around the issue while still logging to console.  But I'd recommend just upgraded to 1.6.1.", "2024-02-18T20:25:20Z", "2024-02-18T20:25:20Z", "ajsutton", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5zqY-p", "I_kwDODjvEJM5-_oi2", "why are you putting these findings here ?", "2024-02-13T06:19:35Z", "2024-02-13T06:19:35Z", "t-maker564", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5zt26y", "I_kwDODjvEJM5-_oi2", "@mds1 asked me to since the cantina repo is private and not everyone at optimism has access. also serves as backup in case the cantina repo gets deleted", "2024-02-13T12:26:31Z", "2024-02-13T12:26:31Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z23Ji", "I_kwDODjvEJM5-_oi2", "you do know that others will report these ?", "2024-02-14T13:24:25Z", "2024-02-14T13:24:25Z", "t-maker564", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6M2u", "I_kwDODjvEJM5-_oi2", "moved to https://github.com/ethereum-optimism/client-pod/issues/575", "2024-02-14T21:22:33Z", "2024-02-14T21:22:33Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z7fz8", "I_kwDODjvEJM5-_oi2", "yep but some findings are out of scope only few contracts are in scope ", "2024-02-14T23:18:05Z", "2024-02-14T23:18:05Z", "t-maker564", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6MvS", "I_kwDODjvEJM5-_oW_", "https://github.com/ethereum-optimism/client-pod/issues/576", "2024-02-14T21:22:24Z", "2024-02-14T21:22:24Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6Mpm", "I_kwDODjvEJM5-_oLo", "moved to https://github.com/ethereum-optimism/client-pod/issues/577", "2024-02-14T21:22:16Z", "2024-02-14T21:22:16Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z58LW", "I_kwDODjvEJM5-_n-e", "Chatted with @maurelian and this will not break anything. We don't want to downgrade the semver after a release, so we'll just stick with the major version bump and close this issue", "2024-02-14T20:49:34Z", "2024-02-14T20:49:34Z", "mds1", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6MjR", "I_kwDODjvEJM5-_n4G", "moved to https://github.com/ethereum-optimism/client-pod/issues/578", "2024-02-14T21:22:07Z", "2024-02-14T21:22:07Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6Mb3", "I_kwDODjvEJM5-_nuC", "moved to https://github.com/ethereum-optimism/client-pod/issues/579", "2024-02-14T21:21:58Z", "2024-02-14T21:21:58Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6MWT", "I_kwDODjvEJM5-_ni0", "moved to https://github.com/ethereum-optimism/client-pod/issues/580", "2024-02-14T21:21:51Z", "2024-02-14T21:21:51Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6MP8", "I_kwDODjvEJM5-_naI", "moved to https://github.com/ethereum-optimism/client-pod/issues/581", "2024-02-14T21:21:42Z", "2024-02-14T21:21:42Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6MIZ", "I_kwDODjvEJM5-_nQj", "moved to https://github.com/ethereum-optimism/client-pod/issues/582", "2024-02-14T21:21:32Z", "2024-02-14T21:21:32Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6MBi", "I_kwDODjvEJM5-_nGD", "moved to https://github.com/ethereum-optimism/client-pod/issues/583", "2024-02-14T21:21:23Z", "2024-02-14T21:21:23Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6L41", "I_kwDODjvEJM5-_m8o", "moved to https://github.com/ethereum-optimism/client-pod/issues/584", "2024-02-14T21:21:12Z", "2024-02-14T21:21:12Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6Lum", "I_kwDODjvEJM5-_mwB", "moved to https://github.com/ethereum-optimism/client-pod/issues/585", "2024-02-14T21:20:58Z", "2024-02-14T21:20:58Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6LlV", "I_kwDODjvEJM5-_mnG", "moved to https://github.com/ethereum-optimism/client-pod/issues/586", "2024-02-14T21:20:47Z", "2024-02-14T21:20:47Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5z6Lb4", "I_kwDODjvEJM5-_mV9", "moved to https://github.com/ethereum-optimism/client-pod/issues/587", "2024-02-14T21:20:35Z", "2024-02-14T21:20:35Z", "0xfuturistic", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5l9Vv-", "I_kwDODjvEJM5uhPTh", "In my case I've upgraded from 1.1.1 to 1.1.4 and instead of throwing that `reorg is too deep` and looping it showed `Hit finalized L2 head, returning immediately` (when it reached l1 block 0) and then it resumed working properly", "2023-09-07T18:09:20Z", "2023-09-07T18:09:20Z", "0xalex88", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5l_E8m", "I_kwDODjvEJM5uhPTh", "> In my case I've upgraded from 1.1.1 to 1.1.4 and instead of throwing that `reorg is too deep` and looping it showed `Hit finalized L2 head, returning immediately` (when it reached l1 block 0) and then it resumed working properly\r\n\r\nthanks, it works like a charm", "2023-09-08T03:46:38Z", "2023-09-08T03:46:38Z", "folowing", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5mSLQC", "I_kwDODjvEJM5uhPTh", "Unfortunately my node continued to loop on v1.1.4 back down to zero over and over again. I had previously been using a Nethermind node for the L1, and in bedrock had set the \"nethermind\" node type. I was able to point it at a go-ethererum node and changed the \"type\" parameter to \"basic\" and now it is catching back up again.", "2023-09-12T16:22:20Z", "2023-09-12T16:22:20Z", "DaveWK", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5n_tLN", "I_kwDODjvEJM5uhPTh", "> Unfortunately my node continued to loop on v1.1.4 back down to zero over and over again. I had previously been using a Nethermind node for the L1, and in bedrock had set the \"nethermind\" node type. I was able to point it at a go-ethererum node and changed the \"type\" parameter to \"basic\" and now it is catching back up again.\r\n\r\nI have been using \"nethermind\" stack instead of \"geth\" and the sync just stops from time to time at certain blocks. Is it mandatory to run a Geth node for the l1 URL after the bedrock upgrade?\r\n\r\nThanks. \ud83d\ude4f ", "2023-10-03T11:17:29Z", "2023-10-03T11:17:29Z", "avi-piertwo", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5pIilR", "I_kwDODjvEJM5uhPTh", "I am using a nethermind node, but I am using the \"basic\" type. Something seems broken with the nethermind enhancements", "2023-10-16T06:57:04Z", "2023-10-16T06:57:04Z", "DaveWK", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5pIuNg", "I_kwDODjvEJM5uhPTh", "> I am using a nethermind node, but I am using the \"basic\" type. Something seems broken with the nethermind enhancements\r\n\r\nI have been digging into this for some time, but we are running a _pruned_ Nethermind nodes stack which might have been causing the issue. Need to test this out with a new non-pruned stack to revert with better answers.\r\n\r\nDid you happen to run a non-pruned Neth stack and still see the issue?\r\nI am assuming you might have also tried \"_nethermind_\" as the [type](https://github.com/ethereum-optimism/optimism/blob/0025fff177d656fc14d64c174eef3b69a03413dc/op-node/sources/receipts.go#L281) to test your setup. If you use \"_basic_\" it runs through a list of eth calls to figure out which will suit the stack you are running which to me increases latency.\r\n\r\n", "2023-10-16T07:34:09Z", "2023-10-16T07:34:09Z", "avi-piertwo", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5pJPT9", "I_kwDODjvEJM5uhPTh", "I have tried with a pruned and \"archive\" node with the \"nethermind\" type, wihch run into the same issue. As I understand it, nethemind has some non-standard rpc paths which I believe optimism is attempting to use to retrieve and decode relevant l2 transactions faster. I am able to use the rpc from nethermind with \"basic\" and I am able to keep up with the network on my l2, while not dealing with the awful instability and corruption issues associated with geth's usage of the goleveldb module. I'd be interested in seeing the metrics on how much faster the \"nethermind\" type is supposed to be, but it doesn't seem from my perspective like the integration tests are sufficient to prove reliability or performance improvements.,", "2023-10-16T08:58:21Z", "2023-10-16T08:58:50Z", "DaveWK", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5wd-Sd", "I_kwDODjvEJM5uhPTh", "I have the same problem - my node get stucked and its `Walking back L1Block by hash` for one week already.\r\n\r\nMy case is that space in my disk volume got 100%, so I had to extend volume. I had to turn off op-geth and op-node, and had to delete optimism directory because I could not ran any command in terminal without any space in terminal.\r\n\r\nNow I installed everything again and ran nodes and this is happening for one week already.\r\n\r\nI am worried that it get stacked because few days ago block ID was lesser than today:\r\n```\r\nJan 05 09:02:19 my-server bash[4347]: t=2024-01-05T09:02:19+0000 lvl=info msg=\"Walking back L1Block by hash\"        curr=0x8b00822ccf06c436c8881300e153ab6714fe161b0e158c6337bb5790a05a97b7:18264178 next=0x3bc0825482846f5bdf091de5abfbee1460897ba655e1c6a58fc71e066cea69ba:18264177 l2block=0x6ad96a63861c6415215ac63d552bb618d3a615c93f68186efe5415d266769c3b:110333528\r\nJan 05 09:02:19 my-server bash[4347]: t=2024-01-05T09:02:19+0000 lvl=info msg=\"Walking back L1Block by hash\"        curr=0x3bc0825482846f5bdf091de5abfbee1460897ba655e1c6a58fc71e066cea69ba:18264177 next=0xeddf92494fceffff0fbef9b08fbbf90ef72062301080fcad585ba73649a7d79f:18264176 l2block=0x185672d17a8ace9eda3ecd1dda22b794ee0bd569f2032680b2b7143fd2de0eb4:110333522\r\n```\r\n\r\nAnd today:\r\n```\r\nJan 11 11:02:48 my-server bash[4347]: t=2024-01-11T11:02:48+0000 lvl=info msg=\"Walking back L1Block by hash\"        curr=0x98437d38e000a415b5d80169533ed0e1d046b7f537ca4249e54b862f803cad0e:18548343 next=0xd9d2b47334cfda62089bfa827e213e4266faec695d7129f2f40ce8757e3c4aaf:18548342 l2block=0xf3aee3538ae5a7cc802a84d8fdefa64b75088ffd1e1e5d694ba18d37b365aba0:112051414\r\nJan 11 11:02:48 my-server bash[4347]: t=2024-01-11T11:02:48+0000 lvl=info msg=\"Walking back L1Block by hash\"        curr=0xd9d2b47334cfda62089bfa827e213e4266faec695d7129f2f40ce8757e3c4aaf:18548342 next=0xb41726190ffd3d40529feaab042d82a68beca68f83faf69ce637658e0721e14d:18548341 l2block=0xe9b2f96c8148b91965f5f5eea652cf1ca4c491aa00281d2186b9c54a5f797955:112051403\r\n```\r\n\r\nPlease help!", "2024-01-11T11:10:21Z", "2024-01-11T11:10:21Z", "Rogalek", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5xp5Rn", "I_kwDODjvEJM5uhPTh", "Also faced almost the same issue with [`opbnb`](https://github.com/bnb-chain/opbnb/issues/112)", "2024-01-23T19:45:07Z", "2024-01-23T19:45:07Z", "alexqrid", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5zgbXV", "I_kwDODjvEJM5uhPTh", "Same issue, all I did was stop and start the node, no changes", "2024-02-11T21:42:51Z", "2024-02-11T21:42:51Z", "PeaStew", "2025-08-30 07:37:53"]
["IC_kwDODjvEJM5zlOwC", "I_kwDODjvEJM5uhPTh", "Hi all, I am closing this issue because the first reported case was fixed in op-node 1.1.4. The minimum supported version for mainnet is 1.3.2. If you are running into this issue on a modern version of op-node, please open a new issue with information about the node versions & network you're running.", "2024-02-12T16:54:01Z", "2024-02-12T16:54:01Z", "trianglesphere", "2025-08-30 07:37:53"]
["IC_kwDOFpg0Ns504O9D", "I_kwDOFpg0Ns6AGcAz", "Needs to change picture name to `logo.svg`.", "2024-02-23T08:17:00Z", "2024-02-23T08:17:00Z", "b0gdaniy", "2025-08-30 08:39:20"]
["IC_kwDOI7W0xc50vB5j", "I_kwDOI7W0xc5_3Heo", "no content", "2024-02-22T01:15:16Z", "2024-02-22T01:15:16Z", "MSilb7", "2025-08-30 08:39:21"]
["IC_kwDOI7W0xc50vCAO", "I_kwDOI7W0xc5_2uSO", "no content", "2024-02-22T01:15:54Z", "2024-02-22T01:15:54Z", "MSilb7", "2025-08-30 08:39:21"]
["IC_kwDOKIwiaM50fNxS", "I_kwDOKIwiaM5_uxhA", "![image](https://github.com/ethereum-optimism/docs/assets/27230181/35f0f1e5-57d0-43b2-8fac-7153cbfd80fd)\r\nshould we just update the link?", "2024-02-20T14:31:26Z", "2024-02-20T14:31:26Z", "imtipi", "2025-08-30 08:39:21"]
["IC_kwDOKIwiaM50f6_f", "I_kwDOKIwiaM5_uxhA", "It can't just update the link because the link is dynamic and changes over time.\r\n\r\nExcept we will only say \"latest\" and not fix the date in docs.", "2024-02-20T15:57:22Z", "2024-02-20T15:57:22Z", "Chomtana", "2025-08-30 08:39:21"]
["IC_kwDOKIwiaM50gC8A", "I_kwDOKIwiaM5_uxhA", "> It can't just update the link because the link is dynamic and changes over time.\r\n> \r\n> Except we will only say \"latest\" and not fix the date in docs.\r\n\r\nacceptable i think,except i don't know how to display the sha256sum,have to change it to the link as well\r\nor,even just display https://datadirs.optimism.io/ and let user choose on their own(if they want other version)", "2024-02-20T16:13:09Z", "2024-02-20T16:13:09Z", "imtipi", "2025-08-30 08:39:21"]
["IC_kwDOKIwiaM50grU2", "I_kwDOKIwiaM5_uxhA", "https://github.com/ethereum-optimism/docs/pull/498/files\r\n\r\nWill fix", "2024-02-20T17:42:41Z", "2024-02-20T17:42:41Z", "smartcontracts", "2025-08-30 08:39:21"]
["IC_kwDOKIsnqM50sUul", "I_kwDOKIsnqM5_2O3z", "Closed by https://github.com/ethereum-optimism/superchain-ops/pull/79", "2024-02-21T19:42:29Z", "2024-02-21T19:42:29Z", "mds1", "2025-08-30 08:39:34"]
["IC_kwDODjvEJM5089Yl", "I_kwDODjvEJM6ALPSi", "I agree that using an `Ownable2Step` is a great idea. We do want to move in this direction but it would require a bunch of tooling updates for management of OP Mainnet. We use [this](https://github.com/ethereum-optimism/superchain-ops) repo for these sorts of upgrades. We are currently focused on shipping fault proofs and use simulation + state diff assertions to ensure that ownership transfers happen safely", "2024-02-23T23:35:47Z", "2024-02-23T23:35:47Z", "tynes", "2025-08-30 08:40:03"]
["IC_kwDODjvEJM50-XV3", "I_kwDODjvEJM6ALPSi", "Is there a template / public-facing repository the OP team generally uses for fault proofs and state diff assertions that OP-stack teams can use?", "2024-02-24T18:39:37Z", "2024-02-24T18:39:37Z", "pegahcarter", "2025-08-30 08:40:03"]
["IC_kwDODjvEJM50_YTe", "I_kwDODjvEJM6ALPSi", "See [this](https://github.com/ethereum-optimism/optimism/blob/3179d49f1cdbf7cea384544f250b4762a43222f1/packages/contracts-bedrock/scripts/Deploy.s.sol#L104) for an example of generating a state diff. [superchain-ops](https://github.com/ethereum-optimism/superchain-ops) contains a [template](https://github.com/ethereum-optimism/superchain-ops/tree/main/tasks/templates/00-default) for gnosis safe multisig operations\r\n\r\nFault proofs are not live yet but you can follow along with their progress in this repo", "2024-02-25T00:44:09Z", "2024-02-25T02:34:19Z", "tynes", "2025-08-30 08:40:03"]
["IC_kwDODjvEJM51AOxp", "I_kwDODjvEJM6ALPSi", "@tynes are the fault proofs to show that proxy storage stays the same?  I'm looking for something like hardhat's [`validateUpgrade`](https://docs.openzeppelin.com/upgrades-plugins/1.x/api-hardhat-upgrades#validate-upgrade).", "2024-02-25T16:40:08Z", "2024-02-25T16:40:08Z", "pegahcarter", "2025-08-30 08:40:03"]
["IC_kwDODjvEJM4_b4xn", "I_kwDODjvEJM5FdnDe", "yeah, the default call tracer doesn't handle null values correctly. Until this is fixed, for the time being, might I suggest using this [tracer](https://gist.github.com/Inphi/48c104db351a73a8ce0567362ac59129). The script does the same thing as the default call_tracer. You can supply a custom tracer to `debug_traceTransaction` by setting the \"tracer\" parameter to the script content.", "2022-03-10T16:46:27Z", "2022-03-10T16:46:27Z", "Inphi", "2025-08-30 08:40:03"]
["IC_kwDODjvEJM4_cNaU", "I_kwDODjvEJM5FdnDe", "@Inphi, thanks for the quick response!\r\n\r\nHow would be the way to send the code inside `params/tracer`?\r\nI've tried a few ways, but getting always:\r\n`{\"jsonrpc\":\"2.0\",\"id\":null,\"error\":{\"code\":-32600,\"message\":\"failed to parse request\"}}`", "2022-03-10T18:20:35Z", "2022-03-10T18:20:35Z", "qk-santi", "2025-08-30 08:40:03"]
["IC_kwDODjvEJM4_cPYH", "I_kwDODjvEJM5FdnDe", "It's tricky to do this directly on the cmd line as the tracer script has to be escaped. If you don't mind using python, you can use this [convenient script](https://gist.github.com/Inphi/f76ce40c8737f84a7d3accfe14365f3f) that does it for you:\r\n```bash\r\nETH_RPC_URL=https://your-node python3 trace_tx.py <txhash> <path_to_tracer.js>\r\n```", "2022-03-10T18:30:24Z", "2022-03-10T18:30:49Z", "Inphi", "2025-08-30 08:40:03"]
["IC_kwDODjvEJM4_cT13", "I_kwDODjvEJM5FdnDe", "@Inphi Great! It works like a charm :smile:\r\nLeaving this gist with a test request and the code escaped in case it is useful for anyone:\r\nhttps://gist.github.com/qk-santi/c73f01788c2bf75d09cc330efe4d730d", "2022-03-10T18:52:55Z", "2022-03-10T18:52:55Z", "qk-santi", "2025-08-30 08:40:03"]
["IC_kwDODjvEJM50wjU2", "I_kwDODjvEJM5FdnDe", "I would like to reopen that issue as @Inphi solution was really temporary `Until this is fixed, for the time being, might I suggest` today we are trying to build a professional indexer which is compatible with 10s of EVMs chain and our indexer is stuck at block\r\n\r\n```\r\n{  \r\n    \"method\": \"debug_traceBlockByNumber\",                                   \r\n    \"params\": [\r\n        \"0x15679\",\r\n        {\r\n            \"tracer\": \"callTracer\"\r\n        }\r\n    ],\r\n    \"id\": 0,\r\n    \"jsonrpc\": \"2.0\"\r\n  }\r\n  ```\r\n  \r\n  Getting incorrect response\r\n  \r\n  ```\r\n  {\r\n    \"jsonrpc\": \"2.0\",\r\n    \"id\": 0,\r\n    \"result\": [\r\n        {\r\n            \"result\": {\r\n                \"type\": \"\",\r\n                \"from\": \"\",\r\n                \"gas\": \"\",\r\n                \"gasUsed\": \"\",\r\n                \"input\": \"\"\r\n            }\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nThis is how Etherscan see this https://optimistic.etherscan.io/tx/0xcf6e46a1f41e1678fba10590f9d092690c5e8fd2e85a3614715fb21caa74655d - we should have also some error information line `deployer address not whitelisted: 0x6C3F14DA26556585706c02af737a44E67Dc6954D` ", "2024-02-22T07:52:27Z", "2024-02-22T07:53:35Z", "MRabenda", "2025-08-30 08:40:03"]
["IC_kwDOKSJyfM51TTL_", "I_kwDOKSJyfM5-ctjp", "Update - we will only be adding Mixpanel at the app level starting with Dapp Console", "2024-02-28T00:59:52Z", "2024-02-28T00:59:52Z", "tarunkhasnavis", "2025-08-30 08:40:23"]
["IC_kwDOH2Qg5s5mFxrx", "I_kwDOH2Qg5s5wkxky", "same issues here with mainnet node", "2023-09-10T11:37:15Z", "2023-09-10T11:37:15Z", "0xChupaCabra", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5mJxb7", "I_kwDOH2Qg5s5wkxky", "Can you both provide the following information: \r\n\r\nSystem Specs:\r\n\r\n- OS:\r\n- `op-geth` package version (or commit hash):\r\n- Archive Node or Full Node?:\r\n- `op-node` package version (or commit hash):\r\n \r\nConfig:\r\n\r\n`op-geth` config:\r\n\r\n`op-node`config:\r\n\r\nAdd any other context about the problem here:\r\n\r\n", "2023-09-11T13:03:19Z", "2023-09-11T13:03:19Z", "sbvegan", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5mng_p", "I_kwDOH2Qg5s5wkxky", "System specs\r\n\r\n* Ubuntu 20.04.5 LTS\r\n* op-geth: v1.101200.1\r\n* syncmode=full\r\n* op-node: v1.1.4\r\n \r\nConfig:\r\nExtract from docker-compose.yml\r\n```\r\nop-geth:\r\n    # see https://github.com/ethereum-optimism/op-geth/releases\r\n    image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:v1.101200.1\r\n    restart: unless-stopped\r\n    stop_grace_period: 30s\r\n    # see https://community.optimism.io/docs/developers/bedrock/node-operator-guide/#configuration\r\n    # and https://community.optimism.io/docs/developers/bedrock/public-testnets/#goerli-upgrade-rehearsal\r\n    command: |\r\n      --datadir=/data\r\n      --networkid=420\r\n      --http\r\n      --http.addr=0.0.0.0\r\n      --http.vhosts=*\r\n      --http.corsdomain=*\r\n      --ws\r\n      --ws.port=8546\r\n      --ws.addr=0.0.0.0\r\n      --ws.origins=*\r\n      --authrpc.addr=0.0.0.0\r\n      --authrpc.port=8551\r\n      --authrpc.jwtsecret=\"/config/jwt.hex\"\r\n      --authrpc.vhosts=*\r\n      --rollup.disabletxpoolgossip=true\r\n      --rollup.sequencerhttp=https://goerli-sequencer.optimism.io\r\n      --nodiscover\r\n      --syncmode=full\r\n      --maxpeers=0\r\n\r\n  op-node:\r\n    # see https://github.com/ethereum-optimism/optimism/releases\r\n    image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:v1.1.4\r\n    restart: unless-stopped\r\n    command: |\r\n      op-node\r\n      --network=goerli\r\n      --l1=${L1RPC:-https://eth-goerli.somedomain.tld}\r\n      --l2=http://op-geth:8551\r\n      --rpc.addr=0.0.0.0\r\n      --rpc.port=9545\r\n      --l2.jwt-secret=/config/jwt.hex\r\n      --l1.trustrpc=true\r\n```\r\n\r\nNot sure what other context could be useful. Here is complete log output with verbosity set to debug:\r\n```\r\nop-geth_1  | INFO [09-15|17:43:27.709] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nop-geth_1  | WARN [09-15|17:43:27.710] Using deprecated resource file, please move this file to the 'geth' subdirectory of datadir. file=/data/nodekey\r\nop-geth_1  | DEBUG[09-15|17:43:27.710] FS scan times                            list=\"35.018\u00b5s\" set=771ns diff=\"1.794\u00b5s\"\r\nop-geth_1  | DEBUG[09-15|17:43:27.712] Sanitizing Go's GC trigger               percent=100\r\nop-geth_1  | INFO [09-15|17:43:27.713] Set global gas cap                       cap=50,000,000\r\nop-geth_1  | INFO [09-15|17:43:27.713] Initializing the KZG library             backend=gokzg\r\nop-geth_1  | INFO [09-15|17:43:27.739] Allocated trie memory caches             clean=154.00MiB dirty=256.00MiB\r\nop-geth_1  | INFO [09-15|17:43:27.769] Using leveldb as the backing database \r\nop-geth_1  | INFO [09-15|17:43:27.769] Allocated cache and file handles         database=/data/chaindata cache=512.00MiB handles=524,288\r\nop-geth_1  | INFO [09-15|17:43:27.866] Using LevelDB as the backing database \r\nop-geth_1  | INFO [09-15|17:43:27.866] Found legacy ancient chain path          location=/data/chaindata/ancient\r\nop-geth_1  | DEBUG[09-15|17:43:27.866] Chain freezer table opened               database=/data/chaindata/ancient table=headers items=12,483,072 size=113.09MiB\r\nop-geth_1  | DEBUG[09-15|17:43:27.867] Chain freezer table opened               database=/data/chaindata/ancient table=hashes  items=12,483,072 size=380.95MiB\r\nop-geth_1  | DEBUG[09-15|17:43:27.867] Chain freezer table opened               database=/data/chaindata/ancient table=bodies  items=12,483,072 size=1.53GiB\r\nop-geth_1  | DEBUG[09-15|17:43:27.867] Chain freezer table opened               database=/data/chaindata/ancient table=receipts items=12,483,072 size=1.06GiB\r\nop-geth_1  | DEBUG[09-15|17:43:27.868] Chain freezer table opened               database=/data/chaindata/ancient table=diffs    items=12,483,072 size=23.49MiB\r\nop-geth_1  | INFO [09-15|17:43:27.868] Opened ancient database                  database=/data/chaindata/ancient readonly=false\r\nop-geth_1  | DEBUG[09-15|17:43:27.871] Ancient blocks frozen already            number=12,573,071 hash=8e2f89..27762d frozen=12,483,072\r\nop-geth_1  | INFO [09-15|17:43:28.091]  \r\nop-geth_1  | INFO [09-15|17:43:28.091] --------------------------------------------------------------------------------------------------------------------------------------------------------- \r\nop-geth_1  | INFO [09-15|17:43:28.091] Chain ID:  420 (OP-Goerli) \r\nop-geth_1  | INFO [09-15|17:43:28.091] Consensus: Optimism \r\nop-geth_1  | INFO [09-15|17:43:28.091]  \r\nop-geth_1  | INFO [09-15|17:43:28.091] Pre-Merge hard forks (block based): \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Homestead:                   #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/homestead.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Tangerine Whistle (EIP 150): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/tangerine-whistle.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Spurious Dragon/1 (EIP 155): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/spurious-dragon.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Spurious Dragon/2 (EIP 158): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/spurious-dragon.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Byzantium:                   #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/byzantium.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Constantinople:              #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/constantinople.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Petersburg:                  #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/petersburg.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Istanbul:                    #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/istanbul.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Muir Glacier:                #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/muir-glacier.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Berlin:                      #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/berlin.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - London:                      #4061224  (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/london.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Arrow Glacier:               #4061224  (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/arrow-glacier.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Gray Glacier:                #4061224  (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/gray-glacier.md) \r\nop-geth_1  | INFO [09-15|17:43:28.091]  \r\nop-geth_1  | INFO [09-15|17:43:28.091] Merge configured: \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Hard-fork specification:    https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/paris.md \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Network known to be merged: true \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Total terminal difficulty:  0 \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Merge netsplit block:       #4061224  \r\nop-geth_1  | INFO [09-15|17:43:28.091]  \r\nop-geth_1  | INFO [09-15|17:43:28.091] Post-Merge hard forks (timestamp based): \r\nop-geth_1  | INFO [09-15|17:43:28.091]  - Regolith:                    @1679079600 \r\nop-geth_1  | INFO [09-15|17:43:28.091]  \r\nop-geth_1  | INFO [09-15|17:43:28.091] --------------------------------------------------------------------------------------------------------------------------------------------------------- \r\nop-geth_1  | INFO [09-15|17:43:28.091]  \r\nop-geth_1  | INFO [09-15|17:43:28.093] Loaded most recent local block           number=12,573,071 hash=8e2f89..27762d td=0 age=1mo2w4d\r\nop-geth_1  | INFO [09-15|17:43:28.093] Loaded most recent local finalized block number=12,572,804 hash=793b08..90d4e2 td=0 age=1mo2w4d\r\nop-geth_1  | INFO [09-15|17:43:28.106] Initialising Ethereum protocol           network=420 dbversion=8\r\nop-geth_1  | DEBUG[09-15|17:43:28.106] Reinjecting stale transactions           count=0\r\nop-geth_1  | INFO [09-15|17:43:28.106] Loaded local transaction journal         transactions=0 dropped=0\r\nop-geth_1  | INFO [09-15|17:43:28.106] Regenerated local transaction journal    transactions=0 accounts=0\r\nop-geth_1  | INFO [09-15|17:43:28.107] Chain post-merge, sync via beacon client \r\nop-geth_1  | INFO [09-15|17:43:28.107] Gasprice oracle is ignoring threshold set threshold=2\r\nop-geth_1  | WARN [09-15|17:43:28.107] Sanitizing invalid optimism gasprice oracle min priority fee suggestion provided=<nil> updated=100,000,000\r\nop-geth_1  | WARN [09-15|17:43:28.107] Unclean shutdown detected                booted=2023-08-29T08:49:00+0000 age=2w3d8h\r\nop-geth_1  | WARN [09-15|17:43:28.107] Unclean shutdown detected                booted=2023-09-15T17:38:39+0000 age=4m49s\r\nop-geth_1  | WARN [09-15|17:43:28.107] Engine API enabled                       protocol=eth\r\nop-geth_1  | INFO [09-15|17:43:28.107] Starting peer-to-peer node               instance=Geth/v0.1.0-unstable-36831023/linux-amd64/go1.20.8\r\nop-geth_1  | DEBUG[09-15|17:43:28.111] TCP listener up                          addr=[::]:30303\r\nop-geth_1  | DEBUG[09-15|17:43:28.114] IPCs registered                          namespaces=admin,debug,web3,eth,txpool,miner,net,engine\r\nop-geth_1  | INFO [09-15|17:43:28.114] IPC endpoint opened                      url=/data/geth.ipc\r\nop-geth_1  | DEBUG[09-15|17:43:28.114] Allowed origin(s) for WS RPC interface [*] \r\n```\r\nand then an endless stream of:\r\n```\r\nop-geth_1  | DEBUG[09-15|17:45:06.716] Dereferenced trie from memory database   nodes=38  size=15.35KiB  time=\"71.489\u00b5s\"  gcnodes=1,487,236 gcsize=555.72MiB  gctime=3.038144861s livenodes=99296 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.718] Dereferenced trie from memory database   nodes=60  size=20.06KiB  time=\"99.813\u00b5s\"  gcnodes=1,487,296 gcsize=555.74MiB  gctime=3.038244494s livenodes=99296 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.721] Dereferenced trie from memory database   nodes=80  size=31.99KiB  time=\"157.996\u00b5s\" gcnodes=1,487,376 gcsize=555.77MiB  gctime=3.03840234s  livenodes=99303 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.722] Dereferenced trie from memory database   nodes=15  size=5.85KiB   time=\"27.203\u00b5s\"  gcnodes=1,487,391 gcsize=555.78MiB  gctime=3.038429423s livenodes=99303 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.723] Dereferenced trie from memory database   nodes=62  size=20.45KiB  time=\"141.574\u00b5s\" gcnodes=1,487,453 gcsize=555.80MiB  gctime=3.038570746s livenodes=99303 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.724] Dereferenced trie from memory database   nodes=15  size=5.85KiB   time=\"27.633\u00b5s\"  gcnodes=1,487,468 gcsize=555.81MiB  gctime=3.038598229s livenodes=99303 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.733] Dereferenced trie from memory database   nodes=39  size=15.42KiB  time=\"71.569\u00b5s\"  gcnodes=1,487,507 gcsize=555.82MiB  gctime=3.038669597s livenodes=99303 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.735] Dereferenced trie from memory database   nodes=62  size=23.64KiB  time=\"182.303\u00b5s\" gcnodes=1,487,569 gcsize=555.84MiB  gctime=3.038851609s livenodes=99305 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.736] Dereferenced trie from memory database   nodes=15  size=5.85KiB   time=\"28.134\u00b5s\"  gcnodes=1,487,584 gcsize=555.85MiB  gctime=3.038879603s livenodes=99305 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.736] Dereferenced trie from memory database   nodes=15  size=5.85KiB   time=\"26.711\u00b5s\"  gcnodes=1,487,599 gcsize=555.86MiB  gctime=3.038906184s livenodes=99305 livesize=21.46MiB\r\nop-geth_1  | DEBUG[09-15|17:45:06.736] Dereferenced trie from memory database   nodes=15  size=5.85KiB   time=\"39.427\u00b5s\"  gcnodes=1,487,614 gcsize=555.86MiB  gctime=3.03894544s  livenodes=99305 livesize=21.46MiB\r\n```", "2023-09-15T17:45:39Z", "2023-09-15T17:47:31Z", "d10r", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oDFTX", "I_kwDOH2Qg5s5wkxky", "> My optimism-goerli node got stuck with op-geth outputting this:\r\n> \r\n> ```\r\n> op-geth_1  | INFO [09-09|10:28:46.160] Regenerating historical state            block=12,566,031 target=12,572,804 remaining=6773    elapsed=7m20.25015045s\r\n> op-geth_1  | INFO [09-09|10:28:54.177] Regenerating historical state            block=12,568,349 target=12,572,804 remaining=4455    elapsed=7m28.26743038s\r\n> op-geth_1  | INFO [09-09|10:29:02.191] Regenerating historical state            block=12,570,850 target=12,572,804 remaining=1954    elapsed=7m36.281140218s\r\n> op-geth_1  | INFO [09-09|10:29:09.168] Historical state regenerated             block=12,572,804 elapsed=7m43.258507511s nodes=107.24MiB preimages=0.00B\r\n> op-geth_1  | INFO [09-09|10:29:09.832] Starting work on payload                 id=0x06e60832d5e72719\r\n> op-geth_1  | WARN [09-09|10:29:17.419] State not available, ignoring new payload \r\n> op-geth_1  | WARN [09-09|10:29:27.424] State not available, ignoring new payload \r\n> op-geth_1  | WARN [09-09|10:29:37.428] State not available, ignoring new payload\r\n> ```\r\n> \r\n> I can connect to the http RPC interface, but the head block is stuck at 12573071.\r\n> \r\n> op-node meanwhile shows:\r\n> \r\n> ```\r\n> op-node_1  | t=2023-09-09T10:43:36+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x7488a42e0d5460fd50245a28a5d9adda1ff4792a32e5e88cbfc8d4f0a42da6cb:14414074\r\n> op-node_1  | t=2023-09-09T10:43:37+0000 lvl=warn msg=\"did not finish previous block building, starting new building now\" prev_onto=0x793b08cf542ab32b409c6bc08d14387af7954b8dcb4f8642668a23cb3390d4e2:12572804 prev_payload_id=0x06e60832d5e72719 new_onto=0x793b08cf542ab32b409c6bc08d14387af7954b8dcb4f8642668a23cb3390d4e2:12572804\r\n> op-node_1  | t=2023-09-09T10:43:37+0000 lvl=warn msg=\"Derivation process temporary error\"     attempts=121 err=\"engine stage failed: temp: temporarily cannot insert new safe block: failed to complete building on top of L2 chain 0x793b08cf542ab32b409c6bc08d14387af7954b8dcb4f8642668a23cb3390d4e2:12572804, id: 0x06e60832d5e72719, error (1): execution payload cannot be validated yet, latest valid hash is <nil>\"\r\n> op-node_1  | t=2023-09-09T10:43:38+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xd9716eeabed250e8016cfc49bd3e498b171f544286c0b34ebeaf5adf03d01b65:14414075 peer=16Uiu2HAmNiobrb62Y6p5bzmqL5oqnvkoK3wJmefcWAtvoGJriWWu\r\n> ```\r\n> \r\n> It's not clear to me what this means and what to do about it. I suspect it's relate to the connected goerli L1 having been unavailable for a while. But it's now good again, yet this Optimism node seems to not be recovering.\r\n\r\nmany folks in optimism discord with same issue and there is no fix. anyone finding this issue, just nuke your node because this is unfixable. \r\n\r\nop team will tell you to try debug_sethead, then tell you \"well its never worked before but maybe it will for you\" ", "2023-10-03T20:00:47Z", "2023-10-03T20:00:47Z", "eyooooo", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oDVWB", "I_kwDOH2Qg5s5wkxky", "@eyooooo im 0xChupa from vfat lol, yes indeed the only way to get our mainnet node running again was a fresh install", "2023-10-03T20:49:39Z", "2023-10-03T20:49:39Z", "0xChupaCabra", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oE9r2", "I_kwDOH2Qg5s5wkxky", "It took 10 days to resync and guess what it died again after two days when it was synced. \r\nI hardly do anything with the node except checking pool reserves and state.\r\n\r\nThis is such a shame and carelessness of the people responsible.", "2023-10-04T04:51:15Z", "2023-10-04T04:51:15Z", "zainirfan13", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oGd4h", "I_kwDOH2Qg5s5wkxky", "`Regenerating historical state`\r\nbasiclly your geth database broken...\r\nmake sure start op-node first,and shutdown op-geth last", "2023-10-04T09:49:33Z", "2023-10-04T09:49:33Z", "imtipi", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oGlWZ", "I_kwDOH2Qg5s5wkxky", "@imtipi I am sure this is not the case. It never happened before and it can't start happening to multiple people all of a sudden. \r\nthe node worked superfine for 2 months since i deployed it can't happen twice in two weeks as my server never restarts nor did I access it during this time. the service logs showed op-node and op-geth didn't restart before the \"State not available\" error. \r\n", "2023-10-04T10:08:44Z", "2023-10-04T10:09:04Z", "zainirfan13", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oG3Qe", "I_kwDOH2Qg5s5wkxky", "> `Regenerating historical state` basiclly your geth database broken... make sure start op-node first,and shutdown op-geth last\r\n\r\nAre you saying that anytime op-node is down while op-geth is running (e.g. because it's being updated in a docker-compose setting), there's a risk of the DB getting corrupted?\r\n", "2023-10-04T10:53:23Z", "2023-10-04T10:53:23Z", "d10r", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oHFKG", "I_kwDOH2Qg5s5wkxky", "> > `Regenerating historical state` basiclly your geth database broken... make sure start op-node first,and shutdown op-geth last\r\n> \r\n> Are you saying that anytime op-node is down while op-geth is running (e.g. because it's being updated in a docker-compose setting), there's a risk of the DB getting corrupted?\r\n\r\nno,you need to shutdown op-node first to make sure op-geth cannot getting data anymore", "2023-10-04T11:30:51Z", "2023-10-04T11:30:51Z", "imtipi", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oHLmf", "I_kwDOH2Qg5s5wkxky", "Doesn't really make sense to me. If I shut down op-geth first, it can't get data anyway, because it's not running.\r\nBut regardless, it's not clear to me how this translates to ops workflows. E.g. when updating nodes, I'll usually set new version tags in an .env file and then do `docker compose up -d`, which re-creates and restarts the containers where the image has changed.\r\nI now added a `depends_on: op-node` clause to the `op-geth` service configuration to ensure correct startup sequence. But afaik that won't affect shutdown order.\r\nIs this way of handling updates safe?\r\n\r\nAs I initially wrote, I suspect that in my case the data breakage was caused by the L1 being temporarily unavailable.\r\nEither way, imo this happening repeatedly points to a lack of robustness of the node software. It seems to make assumptions about the reliability of dependent services which isn't realistic, except maybe for commercial node operators with sophisticated and expensive setups guaranteeing high availability.\r\nI hope this view is shared by the team, because decentralization also requires enough people being able to run a node.", "2023-10-04T11:48:34Z", "2023-10-04T11:48:34Z", "d10r", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oHT_C", "I_kwDOH2Qg5s5wkxky", "> If I shut down op-geth first, it can't get data anyway, \r\n\r\nmore like a speeding car suddenly crashing into a wall\r\nop-geth is too fragile.", "2023-10-04T12:09:56Z", "2023-10-04T12:09:56Z", "imtipi", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oHkCE", "I_kwDOH2Qg5s5wkxky", "Geth needs to persist in-memory state on shutdown, at least with the hash-based DB format. They introduced an experimental new format, which we're working on adopting, but for now the hash-based DB format is more considered the more stable option.\r\n\r\nAfter DB corruption because of force-shutdown what ends up happening is that the \"state\" (storage/account mutations) is missing for the latest few blocks, but the block data is there.\r\n\r\nThe corrupted geth node does not expose that the state is \"missing\", and ends up misleading the op-node into trying to build on top of the latest state, which is not there.\r\n\r\nAnd so unless geth finishes the \"regenerating state\" phase (which it often does not, due to tricky assumptions around the corrupted DB) it gets stuck.\r\n\r\nIf you want to manually repair the DB, to avoid a sync from scratch, the steps to follow are:\r\n1. Stop op-geth and op-node\r\n2. Determine a block-number that a state-snapshot exists for by running `op-geth db metadata --datadir=your-dir-here`\r\n3. Start op-geth\r\n4. Run `cast rpc debug_setHead hexnumberhere` (RPC call to debug-namespace geth method, with single hex-number argument) where the `hexnumberhere` is something before the above snapshot number, to force geth to drop the stale blocks without state, to not mislead op-node again.\r\n5. Wait for geth to complete (should be fast) and determine the latest block number (e.g. run `cast block --rpc-url=http://op-geth:8545 latest`)\r\n6. Run a tool from the monorepo to correct the forkchoice of geth: `go run ./op-wheel/cmd engine set-forkchoice --unsafe=X --safe=X --finalized=X --engine=http://op-geth:8551 --engine.jwt-secret=that_jwt_file_from_op_node.txt` where `X` should be set to the latest block number.\r\n7. Start op-node again. It should start syncing again.\r\n\r\nIt's not a pretty fix, but geth DB corruption is already a bad situation, and due to issues in geth we cannot automate this recovery process. Luckily the new Path-DB format is fixing this DB corruption issue, and we will be adopting these improvements from upstream geth as soon as we can.", "2023-10-04T12:49:22Z", "2023-10-04T15:56:39Z", "protolambda", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oHm7y", "I_kwDOH2Qg5s5wkxky", "The geth DB corruption that causes \"state not available\" happens when geth shuts down, and is unrelated to any op-node or L1 interactions.\r\n\r\nYou must configure your op-geth setup to allow it to gracefully shut down, and not hit some docker/systemd/other time-out. If it does time-out, the op-geth process is killed, and state will not be fully written to disk, causing it to be not available.\r\n", "2023-10-04T12:56:19Z", "2023-10-04T12:56:19Z", "protolambda", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oIUBR", "I_kwDOH2Qg5s5wkxky", "thx @protolambda for this detailed explanation!\r\nIf it's an unorderly shutdown of op-geth which causes this issue, it's nothing specific to Optimism. I've seen the same before on various networks with geth or geth forks.\r\nI didn't however expect that to happen in my setup, as I have `stop_grace_period: 30s` in the docker-compose service definition, which I assumed to be plenty on a fast machine. Thus I assumed it to be related to the temporary L1 unavailability.\r\n\r\nWill try the recovery procedure, thanks again!", "2023-10-04T14:36:34Z", "2023-10-04T14:36:34Z", "d10r", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oIW9T", "I_kwDOH2Qg5s5wkxky", "> Will try the recovery procedure, thanks again!\r\n\r\nhope to see your fix feedback here.", "2023-10-04T14:42:53Z", "2023-10-04T14:42:53Z", "opfocus", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oIZDU", "I_kwDOH2Qg5s5wkxky", "> I didn't however expect that to happen in my setup, as I have stop_grace_period: 30s in the docker-compose service definition, which I assumed to be plenty on a fast machine.\r\n\r\nI'll default to @protolambda's opinion, but I think you're going to want to have a much longer grace period. Something like 20 minutes to be safe.", "2023-10-04T14:47:16Z", "2023-10-04T14:47:16Z", "sbvegan", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oLBn6", "I_kwDOH2Qg5s5wkxky", "> Geth needs to persist in-memory state on shutdown, at least with the hash-based DB format. They introduced an experimental new format, which we're working on adopting, but for now the hash-based DB format is more considered the more stable option.\r\n> \r\n> After DB corruption because of force-shutdown what ends up happening is that the \"state\" (storage/account mutations) is missing for the latest few blocks, but the block data is there.\r\n> \r\n> The corrupted geth node does not expose that the state is \"missing\", and ends up misleading the op-node into trying to build on top of the latest state, which is not there.\r\n> \r\n> And so unless geth finishes the \"regenerating state\" phase (which it often does not, due to tricky assumptions around the corrupted DB) it gets stuck.\r\n> \r\n> If you want to manually repair the DB, to avoid a sync from scratch, the steps to follow are:\r\n> \r\n> 1. Stop op-geth and op-node\r\n> 2. Determine a block-number that a state-snapshot exists for by running `op-geth db metadata --datadir=your-dir-here`\r\n> 3. Start op-geth\r\n> 4. Run `cast rpc debug_setHead hexnumberhere` (RPC call to debug-namespace geth method, with single hex-number argument) where the `hexnumberhere` is something before the above snapshot number, to force geth to drop the stale blocks without state, to not mislead op-node again.\r\n> 5. Wait for geth to complete (should be fast) and determine the latest block number (e.g. run `cast block --rpc-url=http://op-geth:8545 latest`)\r\n> 6. Run a tool from the monorepo to correct the forkchoice of geth: `go run ./op-wheel/cmd engine set-forkchoice --unsafe=X --safe=X --finalized=X --engine=http://op-geth:8551 --engine.jwt-secret=that_jwt_file_from_op_node.txt` where `X` should be set to the latest block number.\r\n> 7. Start op-node again. It should start syncing again.\r\n> \r\n> It's not a pretty fix, but geth DB corruption is already a bad situation, and due to issues in geth we cannot automate this recovery process. Luckily the new Path-DB format is fixing this DB corruption issue, and we will be adopting these improvements from upstream geth as soon as we can.\r\n\r\n@protolambda Could you please help me with some questions. I got the same issue from the author due to a power cut.\r\n\r\nAt step 2. I got this result. What number should I use to put in step 4 ?\r\n\r\n```\r\nsimple-optimism-node-op-geth-1  | INFO [10-04|21:27:54.903] Maximum peer count                       ETH=50 LES=0 total=50\r\nsimple-optimism-node-op-geth-1  | INFO [10-04|21:27:54.904] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nsimple-optimism-node-op-geth-1  | INFO [10-04|21:27:54.905] Set global gas cap                       cap=50,000,000\r\nsimple-optimism-node-op-geth-1  | INFO [10-04|21:27:55.139] Using leveldb as the backing database \r\nsimple-optimism-node-op-geth-1  | INFO [10-04|21:27:55.140] Allocated cache and file handles         database=/geth/geth/chaindata cache=512.00MiB handles=524,288 readonly=true\r\nsimple-optimism-node-op-geth-1  | INFO [10-04|21:27:55.644] Using LevelDB as the backing database \r\nsimple-optimism-node-op-geth-1  | INFO [10-04|21:27:55.644] Found legacy ancient chain path          location=/geth/geth/chaindata/ancient\r\nsimple-optimism-node-op-geth-1  | INFO [10-04|21:27:55.697] Opened ancient database                  database=/geth/geth/chaindata/ancient readonly=true\r\nsimple-optimism-node-op-geth-1  | +-------------------------+--------------------------------------------------------------------+\r\nsimple-optimism-node-op-geth-1  | |          FIELD          |                               VALUE                                |\r\nsimple-optimism-node-op-geth-1  | +-------------------------+--------------------------------------------------------------------+\r\nsimple-optimism-node-op-geth-1  | | databaseVersion         | 8 (0x8)                                                            |\r\nsimple-optimism-node-op-geth-1  | | headBlockHash           | 0x43e970bc8cbb5c8b035c1d4719b01aed1517389c32ff68657ad16a4358c49bb0 |\r\nsimple-optimism-node-op-geth-1  | | headFastBlockHash       | 0xf1b13813581ac504193e1c663897aaca189beaf51181aa50c18b66e2e9b89fd3 |\r\nsimple-optimism-node-op-geth-1  | | headHeaderHash          | 0xf1b13813581ac504193e1c663897aaca189beaf51181aa50c18b66e2e9b89fd3 |\r\nsimple-optimism-node-op-geth-1  | | lastPivotNumber         | <nil>                                                              |\r\nsimple-optimism-node-op-geth-1  | | len(snapshotSyncStatus) | 0 bytes                                                            |\r\nsimple-optimism-node-op-geth-1  | | snapshotDisabled        | false                                                              |\r\nsimple-optimism-node-op-geth-1  | | snapshotJournal         | 34 bytes                                                           |\r\nsimple-optimism-node-op-geth-1  | | snapshotRecoveryNumber  | <nil>                                                              |\r\nsimple-optimism-node-op-geth-1  | | snapshotRoot            | 0x6a5031570cc315f5b2816b3387d57e4201996fe974ac4888073df6aaab56ffd7 |\r\nsimple-optimism-node-op-geth-1  | | txIndexTail             | 108060460 (0x670df2c)                                              |\r\nsimple-optimism-node-op-geth-1  | | fastTxLookupLimit       | <nil>                                                              |\r\nsimple-optimism-node-op-geth-1  | | frozen                  | 110320460 items                                                    |\r\nsimple-optimism-node-op-geth-1  | | snapshotGenerator       | Done: false, Accounts: 19662, Slots:                               |\r\nsimple-optimism-node-op-geth-1  | |                         | 38039, Storage: 4056005, Marker: at                                |\r\nsimple-optimism-node-op-geth-1  | |                         | 0x0035ff3687b9ba7411e7427a575a3115a65b575b193279d17d253249505c5050 |\r\nsimple-optimism-node-op-geth-1  | | headBlock.Hash          | 0x43e970bc8cbb5c8b035c1d4719b01aed1517389c32ff68657ad16a4358c49bb0 |\r\nsimple-optimism-node-op-geth-1  | | headBlock.Root          | 0x6a5031570cc315f5b2816b3387d57e4201996fe974ac4888073df6aaab56ffd7 |\r\nsimple-optimism-node-op-geth-1  | | headBlock.Number        | 110381062 (0x6944806)                                              |\r\nsimple-optimism-node-op-geth-1  | | headHeader.Hash         | 0xf1b13813581ac504193e1c663897aaca189beaf51181aa50c18b66e2e9b89fd3 |\r\nsimple-optimism-node-op-geth-1  | | headHeader.Root         | 0x7c2379fd88278e0b28c9423a8a597cc2544ea9565a8a96cb28bd2e60e0b07a1c |\r\nsimple-optimism-node-op-geth-1  | | headHeader.Number       | 110410459 (0x694badb)                                              |\r\nsimple-optimism-node-op-geth-1  | +-------------------------+--------------------------------------------------------------------+\r\n\r\n```\r\n\r\nI tried to put this number and the return is null.\r\n\r\n```\r\ncast rpc debug_setHead 0x6944800\r\nnull\r\n```\r\n\r\nI checked the latest block and it seemed it changed but I'm not sure it's correct or not.\r\n\r\n```\r\n$ cast block --rpc-url=http://localhost:8545 latest\r\n\r\n\r\nbaseFeePerGas        50470473\r\ndifficulty           0\r\nextraData            0x\r\ngasLimit             30000000\r\ngasUsed              17933671\r\nhash                 0xea1cb8310377bb617b779978e526084d03e7479a393f27587c5acee52fe0132a\r\nlogsBloom            0x82069100b0085828201894a200820060610002a8000005e008064006582040000400180060804084004100900a10808802001048512024000418084082e560406210212804202588a05419698800804000c20816000c020608000020c02941100029138001e44008004040001800041091001104241101048d10803a0c2d041204020842980010410004820e08000702001010310860a050102400008864c81012004000040020301c00004488408010200220060001a86a00a200c4010814c484010102104b248018006044002008a480a0a2c2008103018000008240705081003286090c00040028001000220005108000122085a418505080200401231020\r\nminer                0x4200000000000000000000000000000000000011\r\nmixHash              0x0daccf3f428bddf710bcb9deb9f7bc0e223c828bb4cc36b318b24f9418f4b1b0\r\nnonce                0x0000000000000000\r\nnumber               110291757\r\nparentHash           0x440df832462d686c6b0f1310f68ccc8389dc08ba718180442de43eb387327ed3\r\nreceiptsRoot         0x510f27ef2462ab58ecb9cd0168b72a607a20a707bdbe5765c71aa0215df44a48\r\nsealFields           []\r\nsha3Uncles           0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\r\nsize                 3980\r\nstateRoot            0xb725a181eb60f09a78244e4008184a8064b7cdf13d637b6c541d999558dedcea\r\ntimestamp            1696182291\r\ntotalDifficulty      0\r\ntransactions:        [\r\n\t0xee9f43f01ccf9a136b99af35242e5b97bd2ecaf2092607c21edd34a3d9487f2a\r\n\t0x8ccc68b807bc29417d102001513e706c362bbe99ce15e68c0bccf45f8e4538bd\r\n\t0xda0c2eb564c7b05cebedf793575a1f0507b2ae48f40ae53564fa107fd16aa6aa\r\n\t0x5f1e9d3f9e530ad0d0318b96c4292306b65861ab3b8a5b01c4b36b918dce61b0\r\n\t0x62053008afc46d5a67d0d413f9c3a8bffa57589d953c41c789e08aab2a7f273e\r\n\t0x98a3e45211e60b361dde01ca7c0aa8ff3be8ed7dfb4c5d46e9dfbf32c9b8fd90\r\n\t0x1647b6a5b759566c8796e918643f059f9ed37b355ace61e7a58c90c0ce74c48e\r\n\t0x43a1bf2ccce3dbfda09bf6af8c971ccc8ebb9839bac0b4ad1465cf8c2a62a22a\r\n]\r\n\r\n```\r\n\r\nI cannot run the last step to finish. It always returns me an error code. \r\n\r\n```\r\ngo run ./op-wheel/cmd/main.go engine set-forkchoice --unsafe=110291757 --safe=110291757 --finalized=110291757 --engine=http://localhost:8551 --engine.jwt-secret=/media/dvo/0E20ECBA42049AE22/jwt.txt\r\nNo help topic for 'set-forkchoice'\r\nexit status 3\r\n\r\n```\r\n\r\nCould you please help me ? I'm new to this. Thanks a lot.\r\n", "2023-10-04T22:13:13Z", "2023-10-04T22:15:44Z", "dandavid3000", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5oTqpM", "I_kwDOH2Qg5s5wkxky", "\r\n\r\nTry converting 110291757 to a hexadecimal string    and run it under optimism path", "2023-10-06T04:59:31Z", "2023-10-10T04:57:50Z", "opfocus", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5onpun", "I_kwDOH2Qg5s5wkxky", "Completed a repair as per the instructions, and I wanted to share the process here. I might provide more test results (currently testing for stability and efforts to reduce data loss).  @sbvegan @protolambda @dandavid3000 \r\nError\r\n`INFO [10-09|15:11:18.410] Loaded most recent local block           number=4,180,757 hash=9cae61..6156d3 td=0 age=8mo3w6d\r\nINFO [10-09|15:11:18.410] Loaded most recent local finalized block number=4,180,380 hash=b57567..eb20a6 td=0 age=8mo3w6d\r\nWARN [10-09|15:11:18.411] Head state missing, repairing            number=4,180,757 hash=9cae61..6156d3 snaproot=9acd96..b8173f\r\nINFO [10-09|15:11:19.917] Loaded most recent local header          number=4,180,757 hash=9cae61..6156d3 td=0 age=8mo3w6d\r\nINFO [10-09|15:11:19.917] Loaded most recent local block           number=4,173,600 hash=29f99c..906b4b td=0 age=8mo3w6d\r\nINFO [10-09|15:11:19.917] Loaded most recent local snap block      number=4,180,757 hash=9cae61..6156d3 td=0 age=8mo3w6d\r\nINFO [10-09|15:11:19.917] Loaded most recent local finalized block number=4,180,380 hash=b57567..eb20a6 td=0 age=8mo3w6d\r\nWARN [10-09|15:11:19.917] Enabling snapshot recovery               chainhead=4,173,600 diskbase=4,173,600\r\nINFO [10-09|15:11:19.917] Initialising Ethereum protocol           network=420 dbversion=8\r\nINFO [10-09|15:11:19.918] Loaded local transaction journal         transactions=0 dropped=0\r\nINFO [10-09|15:11:19.918] Regenerated local transaction journal    transactions=0 accounts=0\r\nINFO [10-09|15:11:19.918] Chain post-merge, sync via beacon client\r\nINFO [10-09|15:11:19.918] Gasprice oracle is ignoring threshold set threshold=2\r\nWARN [10-09|15:11:19.918] Unclean shutdown detected                booted=2023-10-08T15:31:12+0000 age=23h40m7s\r\nWARN [10-09|15:11:19.918] Unclean shutdown detected                booted=2023-10-08T21:23:44+0000 age=17h47m35s\r\nWARN [10-09|15:11:19.918] Unclean shutdown detected                booted=2023-10-09T15:07:48+0000 age=3m31s\r\nWARN [10-09|15:11:19.918] Engine API enabled                       protocol=eth\r\nINFO [10-09|15:11:19.919] Starting peer-to-peer node               instance=Geth/v0.1.0-unstable-b84ba119-20230914/linux-amd64/go1.20`\r\n\r\n`\u2026\u2026..`\r\n\r\n`WARN [10-10|04:01:59.289] State not available, ignoring new payload\r\nWARN [10-10|04:01:59.373] State not available, ignoring new payload\r\nWARN [10-10|04:01:59.507] State not available, ignoring new payload\r\nWARN [10-10|04:01:59.545] State not available, ignoring new payload\r\nWARN [10-10|04:01:59.767] State not available, ignoring new payload\r\nWARN [10-10|04:01:59.788] State not available, ignoring new payload\r\nWARN [10-10|04:01:59.852] State not available, ignoring new payload\r\nWARN [10-10|04:01:59.985] State not available, ignoring new payload`\r\n\r\n`op-node log`\r\n\r\n`WARN [10-10|04:01:29.099] did not finish previous block building, starting new building now prev_onto=8d572c..2c55e4:4173601 prev_payload_id=0x232689dee9c7b372 new_onto=8d572c..2c55e4:4173601\r\nWARN [10-10|04:01:29.103] Derivation process temporary error       attempts=240,424 err=\"engine stage failed: temp: temporarily cannot insert new safe block: failed to complete building on top of L2 chain 0x8d572cfa21d619216630a433789676ac1104bcc9f99f384f765a55f4c62c55e4:4173601, id: 0x232689dee9c7b372, error (1): execution payload cannot be validated yet, latest valid hash is <nil>\"\r\nWARN [10-10|04:01:29.154] did not finish previous block building, starting new building now prev_onto=8d572c..2c55e4:4173601 prev_payload_id=0x232689dee9c7b372 new_onto=8d572c..2c55e4:4173601\r\nWARN [10-10|04:01:29.157] Derivation process temporary error       attempts=240,425 err=\"engine stage failed: temp: temporarily cannot insert new safe block: failed to complete building on top of L2 chain 0x8d572cfa21d619216630a433789676ac1104bcc9f99f384f765a55f4c62c55e4:4173601, id: 0x232689dee9c7b372, error (1): execution payload cannot be validated yet, latest valid hash is <nil>\"`\r\n- 7.Fix process\r\n    1. Stop op-geth and op-node\r\n    2. Determine a block-number that a state-snapshot exists for by running\u00a0`op-geth db metadata --datadir=your-dir-here`\r\n        \r\n        root@simple:~/op-geth# ./build/bin/geth db metadata --datadir=datadir3\r\n        INFO [10-10|04:26:42.225] Maximum peer count                       ETH=50 LES=0 total=50\r\n        INFO [10-10|04:26:42.225] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\n        INFO [10-10|04:26:42.227] Set global gas cap                       cap=50,000,000\r\n        INFO [10-10|04:26:42.227] Initializing the KZG library             backend=gokzg\r\n        INFO [10-10|04:26:42.331] Using leveldb as the backing database\r\n        INFO [10-10|04:26:42.331] Allocated cache and file handles         database=/root/op-geth/datadir3/geth/chaindata cache=512.00MiB handles=524,288 readonly=true\r\n        INFO [10-10|04:26:42.348] Using LevelDB as the backing database\r\n        INFO [10-10|04:26:42.348] Found legacy ancient chain path          location=/root/op-geth/datadir3/geth/chaindata/ancient\r\n        INFO [10-10|04:26:42.349] Opened ancient database                  database=/root/op-geth/datadir3/geth/chaindata/ancient readonly=true\r\n        +-------------------------+--------------------------------------------------------------------+\r\n        |          FIELD          |                               VALUE                                |\r\n        +-------------------------+--------------------------------------------------------------------+\r\n        | databaseVersion         | 8 (0x8)                                                            |\r\n        | headBlockHash           | 0x29f99c38cd4d8299e979a7e6ed48192df3ab191abaf6c24b49e8a8f138906b4b |\r\n        | headFastBlockHash       | 0x9cae61ae0c7030e91448b3d6214740cea54b89d81363e555b7d57daedb6156d3 |\r\n        | headHeaderHash          | 0x9cae61ae0c7030e91448b3d6214740cea54b89d81363e555b7d57daedb6156d3 |\r\n        | lastPivotNumber         | <nil>                                                              |\r\n        | len(snapshotSyncStatus) | 0 bytes                                                            |\r\n        | snapshotDisabled        | false                                                              |\r\n        | snapshotJournal         | 34 bytes                                                           |\r\n        **| snapshotRecoveryNumber  | 4173600 (0x3faf20)**                                                 |\r\n        | snapshotRoot            | 0x9acd9600b19b8feb71c03c0d39976e5d5c84c973c5e6b65fd01ce7ff05b8173f |\r\n        | txIndexTail             | 1830758 (0x1bef66)                                                 |\r\n        | fastTxLookupLimit       | <nil>                                                              |\r\n        | frozen                  | 4173601 items                                                      |\r\n        | snapshotGenerator       | Done: true, Accounts: 0,                                           |\r\n        |                         | Slots: 0, Storage: 0, Marker:                                      |\r\n        | headBlock.Hash          | 0x29f99c38cd4d8299e979a7e6ed48192df3ab191abaf6c24b49e8a8f138906b4b |\r\n        | headBlock.Root          | 0x9acd9600b19b8feb71c03c0d39976e5d5c84c973c5e6b65fd01ce7ff05b8173f |\r\n        | headBlock.Number        | 4173600 (0x3faf20)                                                 |\r\n        | headHeader.Hash         | 0x9cae61ae0c7030e91448b3d6214740cea54b89d81363e555b7d57daedb6156d3 |\r\n        | headHeader.Root         | 0xd62740e9b49f5717b7b63dcc30f53443d7bdc13491daac21ff6e020628cb6f2a |\r\n        | headHeader.Number       | 4180757 (0x3fcb15)                                                 |\r\n        +-------------------------+--------------------------------------------------------------------+\r\n        \r\n    3. Start op-geth\r\n    4. Run\u00a0`cast rpc debug_setHead hexnumberhere`\u00a0(RPC call to debug-namespace geth method, with single hex-number argument) where the\u00a0`hexnumberhere`\u00a0is something before the above snapshot number, to force geth to drop the stale blocks without state, to not mislead op-node again.\r\n        \r\n        `root@simple:~/op-geth/build/bin# ./geth attach ipc:/root/op-geth/datadir3/geth.ipc\r\n        Welcome to the Geth JavaScript console!`\r\n        \r\n        `instance: Geth/v0.1.0-unstable-b84ba119-20230914/linux-amd64/go1.20\r\n        at block: 4173600 (Sun Jan 15 2023 09:34:28 GMT+0000 (UTC))\r\n        datadir: /root/op-geth/datadir3\r\n        modules: admin:1.0 debug:1.0 engine:1.0 eth:1.0 miner:1.0 net:1.0 rpc:1.0 txpool:1.0 web3:1.0`\r\n        \r\n        `To exit, press ctrl-d or type exit`\r\n        \r\n        > `debug.setHead(\"0x3faf20\")\r\n        null`\r\n        > \r\n        \r\n        op-geth log:\r\n        \r\n        `WARN [10-10|04:31:07.212] Served eth_coinbase                      reqid=3 duration=\"111.931\u00b5s\" err=\"etherbase must be explicitly specified\"\r\n        WARN [10-10|04:35:36.014] Rewinding blockchain to block            target=4,173,600\r\n        WARN [10-10|04:35:36.916] SetHead invalidated safe block\r\n        ERROR[10-10|04:35:36.916] SetHead invalidated finalized block\r\n        INFO [10-10|04:35:36.917] Loaded most recent local block           number=4,173,600 hash=29f99c..906b4b td=0 age=8mo3w6d`\r\n        \r\n    5. Wait for geth to complete (should be fast) and determine the latest block number (e.g. run\u00a0`cast block --rpc-url=http://op-geth:8545 latest`)\r\n        \r\n        `root@simple:~# cast block --rpc-url=http://localhost:8845 latest`\r\n        \r\n        baseFeePerGas        49\r\n        difficulty           0\r\n        extraData            0x\r\n        gasLimit             25000000\r\n        gasUsed              0\r\n        hash                 0x29f99c38cd4d8299e979a7e6ed48192df3ab191abaf6c24b49e8a8f138906b4b\r\n        logsBloom            0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\r\n        miner                0x4200000000000000000000000000000000000011\r\n        mixHash              0x9caa3c8460c2c003d140ae42b9bc7279b64a501f260ec888a9e17b865c5c92a6\r\n        nonce                0x0000000000000000\r\n        number               4173600\r\n        parentHash           0x587ece9e9cd9b0534134ae8403edc2c898426db7786528c888d59137eeec0466\r\n        receiptsRoot         0x3fd774ff8fe515813d7a8f9d3748e58e857bc002823a458a93be90a3bc2e0894\r\n        sealFields           []\r\n        sha3Uncles           0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\r\n        size                 868\r\n        stateRoot            0x9acd9600b19b8feb71c03c0d39976e5d5c84c973c5e6b65fd01ce7ff05b8173f\r\n        timestamp            1673775268\r\n        withdrawalsRoot\r\n        totalDifficulty      0\r\n        transactions:        [\r\n        0xe1ad078d5ced638eb56b4647e3725d972a2352625d215788fe65e939d1877c7a\r\n        ]\r\n        \r\n    6. Run a tool from the monorepo to correct the forkchoice of geth:\u00a0`go run ./op-wheel/cmd engine set-forkchoice --unsafe=X --safe=X --finalized=X --engine=http://op-geth:8551 --engine.jwt-secret=that_jwt_file_from_op_node.txt`\u00a0where\u00a0`X`\u00a0should be set to the latest block number.\r\n        \r\n        `root@simple:~/optimism# go run ./op-wheel/cmd engine set-forkchoice --unsafe='0x3faf20' --safe='0x3faf20' --finalized='0x3faf20' --engine=ws://localhost:8551 --engine.jwt-secret=/root/op-geth/jwt.txt`\r\n        \r\n    7. Start op-node again.\r\n        \r\n        op-node log:\r\n        \r\n        `INFO [10-10|04:53:08.776] Sync progress reason=\"processed safe block derived from L1\" l2_finalized=29f99c..906b4b:4173600 l2_safe=81c235..3cfa31:4173705 l2_unsafe=81c235..3cfa31:4173705 l2_engineSyncTarget=81c235..3cfa31:4173705 l2_time=1,673,775,478 l1_derived=a47e94..75898f:8314898`\r\n        \r\n        op-geth log:\r\n        \r\n        `INFO [10-10|04:53:51.197] Imported new potential chain segment     number=4,173,870 hash=76084e..8553d2 blocks=1 txs=1 mgas=0.000 elapsed=1.237ms     mgasps=0.000   age=8mo3w6d   dirty=1.62MiB\r\n        INFO [10-10|04:53:51.198] Chain head was updated                   number=4,173,870 hash=76084e..8553d2 root=07412c..78a11d elapsed=\"150.322\u00b5s\" age=8mo3w6d`", "2023-10-10T12:01:00Z", "2023-10-10T12:05:19Z", "opfocus", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5pITH4", "I_kwDOH2Qg5s5wkxky", "**Overview**\uff1aAfter completing the aforementioned fix process, it has been observed for 7 days. @sbvegan @protolambda \r\n\r\n1. Within 30 hours after the first fix, an unexplained crash was observed, pointing to the same error as before the fix.\r\n    \r\n    op-node log:\r\n    \r\n    `WARN [10-12|05:50:12.400] Derivation process temporary error attempts=680,718 err=\"engine stage failed: temp: temporarily cannot insert new safe block: failed to complete building on top of L2 chain 0x573d201c442d663b9d262308e371503c9314582837a02a22d7f8ef9c3e0eb11a:4487626, id: 0x6e4ec0e089c0f645, error (1): execution payload cannot be validated yet, latest valid hash is <nil>\u201d`\r\n    \r\n    op-geth log:\r\n    \r\n    `INFO [10-12|06:11:52.673] Loaded most recent local block           number=4,487,625 hash=486cf4..ba17ff td=0 age=8mo3w1d\r\n    INFO [10-12|06:11:52.674] Loaded most recent local finalized block number=4,487,625 hash=486cf4..ba17ff td=0 age=8mo3w1d\r\n    INFO [10-12|06:11:52.688] Initialising Ethereum protocol           network=420 dbversion=8\r\n    INFO [10-12|06:11:52.688] Loaded local transaction journal         transactions=0 dropped=0\r\n    INFO [10-12|06:11:52.688] Regenerated local transaction journal    transactions=0 accounts=0\r\n    INFO [10-12|06:11:52.689] Chain post-merge, sync via beacon client\r\n    INFO [10-12|06:11:52.689] Gasprice oracle is ignoring threshold set threshold=2\r\n    WARN [10-12|06:11:52.690] Unclean shutdown detected                booted=2023-10-08T15:31:12+0000 age=3d14h40m\r\n    WARN [10-12|06:11:52.690] Unclean shutdown detected                booted=2023-10-08T21:23:44+0000 age=3d8h48m\r\n    WARN [10-12|06:11:52.690] Unclean shutdown detected                booted=2023-10-09T15:07:48+0000 age=2d15h4m\r\n    WARN [10-12|06:11:52.690] Unclean shutdown detected                booted=2023-10-09T15:12:19+0000 age=2d14h59m\r\n    \u2026.`\r\n    \r\n    `WARN [10-12|06:03:38.257] State not available, ignoring new payload\r\n    WARN [10-12|06:03:40.467] State not available, ignoring new payload\r\n    WARN [10-12|06:03:44.607] State not available, ignoring new payload\r\n    WARN [10-12|06:03:52.858] State not available, ignoring new payload`\r\n    \r\n2. A second fix was performed following the same process. It is concerning that the snapshotRecoveryNumber is the same as it was during the first fix, which is disappointing.\r\n    1. `root@simple:~/op-geth# ./build/bin/geth db metadata --datadir=datadir3`\r\n        \r\n        INFO [10-12|06:07:43.565] Using leveldb as the backing database\r\n        INFO [10-12|06:07:43.565] Allocated cache and file handles         database=/root/op-geth/datadir3/geth/chaindata cache=512.00MiB handles=524,288 readonly=true\r\n        INFO [10-12|06:07:43.589] Using LevelDB as the backing database\r\n        INFO [10-12|06:07:43.589] Found legacy ancient chain path          location=/root/op-geth/datadir3/geth/chaindata/ancient\r\n        INFO [10-12|06:07:43.590] Opened ancient database                  database=/root/op-geth/datadir3/geth/chaindata/ancient readonly=true\r\n        +-------------------------+--------------------------------------------------------------------+\r\n        |          FIELD          |                               VALUE                                |\r\n        +-------------------------+--------------------------------------------------------------------+\r\n        | databaseVersion         | 8 (0x8)                                                            |\r\n        | headBlockHash           | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n        | headFastBlockHash       | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n        | headHeaderHash          | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n        | lastPivotNumber         | <nil>                                                              |\r\n        | len(snapshotSyncStatus) | 0 bytes                                                            |\r\n        | snapshotDisabled        | false                                                              |\r\n        | snapshotJournal         | 377790 bytes                                                       |\r\n        | snapshotRecoveryNumber  | 4173600 (0x3faf20)                                                 |\r\n        | snapshotRoot            | 0xdc0d84bb7d6d8c923df1965115e702f63ebc49368830024ac5c069e38511d0ab |\r\n        | txIndexTail             | 2137626 (0x209e1a)                                                 |\r\n        | fastTxLookupLimit       | <nil>                                                              |\r\n        | frozen                  | 4397626 items                                                      |\r\n        | snapshotGenerator       | Done: true, Accounts: 0,                                           |\r\n        |                         | Slots: 0, Storage: 0, Marker:                                      |\r\n        | headBlock.Hash          | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n        | headBlock.Root          | 0xf94beb1ad6a20863648a225724dbb7b46ad5db92a4faa1d5fa2ea48c7830cd41 |\r\n        | headBlock.Number        | 4487625 (0x4479c9)                                                 |\r\n        | headHeader.Hash         | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n        | headHeader.Root         | 0xf94beb1ad6a20863648a225724dbb7b46ad5db92a4faa1d5fa2ea48c7830cd41 |\r\n        | headHeader.Number       | 4487625 (0x4479c9)                                                 |\r\n        +-------------------------+--------------------------------------------------------------------+\r\n        \r\n    2. root@simple:~/op-geth/build/bin# cast block --rpc-url=http://localhost:8845 latest\r\n        \r\n        baseFeePerGas        49\r\n        difficulty           0\r\n        extraData            0x\r\n        gasLimit             25000000\r\n        gasUsed              0\r\n        hash                 0x29f99c38cd4d8299e979a7e6ed48192df3ab191abaf6c24b49e8a8f138906b4b\r\n        logsBloom            0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\r\n        miner                0x4200000000000000000000000000000000000011\r\n        mixHash              0x9caa3c8460c2c003d140ae42b9bc7279b64a501f260ec888a9e17b865c5c92a6\r\n        nonce                0x0000000000000000\r\n        number               4173600\r\n        parentHash           0x587ece9e9cd9b0534134ae8403edc2c898426db7786528c888d59137eeec0466\r\n        receiptsRoot         0x3fd774ff8fe515813d7a8f9d3748e58e857bc002823a458a93be90a3bc2e0894\r\n        sealFields           []\r\n        sha3Uncles           0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\r\n        size                 868\r\n        stateRoot            0x9acd9600b19b8feb71c03c0d39976e5d5c84c973c5e6b65fd01ce7ff05b8173f\r\n        timestamp            1673775268\r\n        withdrawalsRoot\r\n        totalDifficulty      0\r\n        transactions:        [\r\n        0xe1ad078d5ced638eb56b4647e3725d972a2352625d215788fe65e939d1877c7a\r\n        ]   \r\n        \r\n    3. debug.setHead(\"0x3faf20\")\r\n    4. `root@simple:~/optimism# go run ./op-wheel/cmd engine set-forkchoice --unsafe='0x3faf20' --safe='0x3faf20' --finalized='0x3faf20' --engine=ws://localhost:8551 --engine.jwt-secret=/root/op-geth/jwt.txt`\r\n3. **It ran steadily for 5 days**, and the observation will conclude at this point.\r\n    \r\n    op-node log\uff1a\r\n    \r\n     [10-16|04:56:21.124] generated attributes in payload queue    txs=7    timestamp=1,677,051,178\r\n    INFO [10-16|04:56:21.156] inserted block                           hash=be5536..0be407 number=5,811,555 state_root=b1ff17..64bfdd timestamp=1,677,051,178 parent=10da9b..ca3bff prev_randao=f7b1b8..5a8dfb fee_recipient=0x4200000000000000000000000000000000000011 txs=7    update_safe=true\r\n    INFO [10-16|04:56:21.156] Sync progress                            reason=\"processed safe block derived from L1\" l2_finalized=5c3c34..521cff:5811144 l2_safe=be5536..0be407:5811555 l2_unsafe=be5536..0be407:5811555 l2_engineSyncTarget=be5536..0be407:5811555 l2_time=1,677,051,178 l1_derived=9dcf9b..b5deda:8535171\r\n    \r\n    op-geth log\uff1a\r\n    \r\n    INFO [10-16|04:56:55.725] Chain head was updated                   number=5,811,718 hash=cf1288..74b593 root=7a93b3..2cf0c3 elapsed=\"367.884\u00b5s\"  age=7mo3w4d\r\n    INFO [10-16|04:56:55.739] Starting work on payload                 id=0x111d45ac0b2b982d\r\n    INFO [10-16|04:56:55.753] Imported new potential chain segment     number=5,811,719 hash=10e1a8..bb9ede blocks=1 txs=2    mgas=0.959  elapsed=11.974ms     mgasps=80.054   age=7mo3w4d   dirty=433.91MiB\r\n    \r\n    **Additional Data**\r\n    \r\n    1. INFO [10-16|04:58:45.614] Loaded most recent local block           number=5,811,719 hash=10e1a8..bb9ede td=0 age=7mo3w4d\r\n    INFO [10-16|04:58:45.614] Loaded most recent local finalized block number=5,811,623 hash=98a015..0bba12 td=0 age=7mo3w4d\r\n    INFO [10-16|04:58:45.639] Initialising Ethereum protocol           network=420 dbversion=8\r\n    INFO [10-16|04:58:45.639] Loaded local transaction journal         transactions=0 dropped=0\r\n    INFO [10-16|04:58:45.639] Regenerated local transaction journal    transactions=0 accounts=0\r\n    INFO [10-16|04:58:45.640] Chain post-merge, sync via beacon client\r\n    INFO [10-16|04:58:45.640] Gasprice oracle is ignoring threshold set threshold=2\r\n    WARN [10-16|04:58:45.641] Unclean shutdown detected                booted=2023-10-08T15:31:12+0000 age=1w13h27m\r\n    WARN [10-16|04:58:45.641] Unclean shutdown detected                booted=2023-10-08T21:23:44+0000 age=1w7h35m\r\n    WARN [10-16|04:58:45.641] Unclean shutdown detected                booted=2023-10-09T15:07:48+0000 age=6d13h50m\r\n    WARN [10-16|04:58:45.641] Unclean shutdown detected                booted=2023-10-09T15:12:19+0000 age=6d13h46m\r\n    \u2026.\r\n    2. root@simple:~/op-geth/build/bin# cast block --rpc-url=http://localhost:8845 latest\r\n        \r\n        baseFeePerGas        49\r\n        difficulty           0\r\n        extraData            0x\r\n        gasLimit             25000000\r\n        gasUsed              958609\r\n        hash                 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede\r\n        logsBloom            0x0010010000000000000000001020000200000000000010000000c010000010000002000003400000004800000000000000000820000006000080000000000000080000020000000000000008000020000000000040408000000000000400000000000000000800000000000000000000008100000000140000200014200010020000040000000010000020000000800004004000000002001000000000000000200000100400400000048000000000000000000002000001020040000000008000000002000100000180000000000008100104010000600040010002000000000000000000800000000000004004000000008000000004000100000000060000\r\n        miner                0x4200000000000000000000000000000000000011\r\n        mixHash              0x6600a69dce79497b41ec85fdb37859c83d51b98a38fc783a1d1ac02e6348613d\r\n        nonce                0x0000000000000000\r\n        number               5811719\r\n        parentHash           0xcf128813e1e4083bc08e3802d62451b53147a71a7bd47cbe136587369274b593\r\n        receiptsRoot         0x6ebecbf97c99cab25fd3f2c9a356b985640404eb51930f04ef6d15c11721530c\r\n        sealFields           []\r\n        sha3Uncles           0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\r\n        size                 1174\r\n        stateRoot            0x1865470cb1a877684cd62b44148afb1634587fd28c176ff5b27406feb4f15be4\r\n        timestamp            1677051506\r\n        withdrawalsRoot\r\n        totalDifficulty      0\r\n        transactions:        [\r\n        0x95f9ee962bce647aa84f43a07e5ed5ead145703388a6b19e3387f9f24b240da6\r\n        0xa0f0bfb2900a3dbacd4af7c0749f91ba02110e6afd55df8cdab1d291f1eb0272\r\n        ]\r\n        \r\n    3. +-------------------------+--------------------------------------------------------------------+\r\n    |          FIELD          |                               VALUE                                |\r\n    +-------------------------+--------------------------------------------------------------------+\r\n    | databaseVersion         | 8 (0x8)                                                            |\r\n    | headBlockHash           | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n    | headFastBlockHash       | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n    | headHeaderHash          | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n    | lastPivotNumber         | <nil>                                                              |\r\n    | len(snapshotSyncStatus) | 0 bytes                                                            |\r\n    | snapshotDisabled        | false                                                              |\r\n    | snapshotJournal         | 459578 bytes                                                       |\r\n    | snapshotRecoveryNumber  | <nil>                                                              |\r\n    | snapshotRoot            | 0xa304ddc535a1722fb593a7cf426c3355dc115c8f371203672e083a572bbba98d |\r\n    | txIndexTail             | 3461720 (0x34d258)                                                 |\r\n    | fastTxLookupLimit       | <nil>                                                              |\r\n    | frozen                  | 5721720 items                                                      |\r\n    | snapshotGenerator       | Done: true, Accounts: 0,                                           |\r\n    |                         | Slots: 0, Storage: 0, Marker:                                      |\r\n    | headBlock.Hash          | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n    | headBlock.Root          | 0x1865470cb1a877684cd62b44148afb1634587fd28c176ff5b27406feb4f15be4 |\r\n    | headBlock.Number        | 5811719 (0x58ae07)                                                 |\r\n    | headHeader.Hash         | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n    | headHeader.Root         | 0x1865470cb1a877684cd62b44148afb1634587fd28c176ff5b27406feb4f15be4 |\r\n    | headHeader.Number       | 5811719 (0x58ae07)                                                 |\r\n    +-------------------------+--------------------------------------------------------------------+", "2023-10-16T06:01:34Z", "2023-10-16T06:01:34Z", "opfocus", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5pIT6M", "I_kwDOH2Qg5s5wkxky", "**Some questions:** Regarding the snapshotRecoveryNumber, sometimes it remains unchanged for extended periods, and at times, querying it shows 'nil' (even though the node can sync properly after a restart).", "2023-10-16T06:05:15Z", "2023-10-16T06:05:15Z", "opfocus", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5pMttH", "I_kwDOH2Qg5s5wkxky", "> Completed a repair as per the instructions, and I wanted to share the process here. I might provide more test results (currently testing for stability and efforts to reduce data loss). @sbvegan @protolambda @dandavid3000 Error `INFO [10-09|15:11:18.410] Loaded most recent local block number=4,180,757 hash=9cae61..6156d3 td=0 age=8mo3w6d INFO [10-09|15:11:18.410] Loaded most recent local finalized block number=4,180,380 hash=b57567..eb20a6 td=0 age=8mo3w6d WARN [10-09|15:11:18.411] Head state missing, repairing number=4,180,757 hash=9cae61..6156d3 snaproot=9acd96..b8173f INFO [10-09|15:11:19.917] Loaded most recent local header number=4,180,757 hash=9cae61..6156d3 td=0 age=8mo3w6d INFO [10-09|15:11:19.917] Loaded most recent local block number=4,173,600 hash=29f99c..906b4b td=0 age=8mo3w6d INFO [10-09|15:11:19.917] Loaded most recent local snap block number=4,180,757 hash=9cae61..6156d3 td=0 age=8mo3w6d INFO [10-09|15:11:19.917] Loaded most recent local finalized block number=4,180,380 hash=b57567..eb20a6 td=0 age=8mo3w6d WARN [10-09|15:11:19.917] Enabling snapshot recovery chainhead=4,173,600 diskbase=4,173,600 INFO [10-09|15:11:19.917] Initialising Ethereum protocol network=420 dbversion=8 INFO [10-09|15:11:19.918] Loaded local transaction journal transactions=0 dropped=0 INFO [10-09|15:11:19.918] Regenerated local transaction journal transactions=0 accounts=0 INFO [10-09|15:11:19.918] Chain post-merge, sync via beacon client INFO [10-09|15:11:19.918] Gasprice oracle is ignoring threshold set threshold=2 WARN [10-09|15:11:19.918] Unclean shutdown detected booted=2023-10-08T15:31:12+0000 age=23h40m7s WARN [10-09|15:11:19.918] Unclean shutdown detected booted=2023-10-08T21:23:44+0000 age=17h47m35s WARN [10-09|15:11:19.918] Unclean shutdown detected booted=2023-10-09T15:07:48+0000 age=3m31s WARN [10-09|15:11:19.918] Engine API enabled protocol=eth INFO [10-09|15:11:19.919] Starting peer-to-peer node instance=Geth/v0.1.0-unstable-b84ba119-20230914/linux-amd64/go1.20`\r\n> \r\n> `\u2026\u2026..`\r\n> \r\n> `WARN [10-10|04:01:59.289] State not available, ignoring new payload WARN [10-10|04:01:59.373] State not available, ignoring new payload WARN [10-10|04:01:59.507] State not available, ignoring new payload WARN [10-10|04:01:59.545] State not available, ignoring new payload WARN [10-10|04:01:59.767] State not available, ignoring new payload WARN [10-10|04:01:59.788] State not available, ignoring new payload WARN [10-10|04:01:59.852] State not available, ignoring new payload WARN [10-10|04:01:59.985] State not available, ignoring new payload`\r\n> \r\n> `op-node log`\r\n> \r\n> `WARN [10-10|04:01:29.099] did not finish previous block building, starting new building now prev_onto=8d572c..2c55e4:4173601 prev_payload_id=0x232689dee9c7b372 new_onto=8d572c..2c55e4:4173601 WARN [10-10|04:01:29.103] Derivation process temporary error attempts=240,424 err=\"engine stage failed: temp: temporarily cannot insert new safe block: failed to complete building on top of L2 chain 0x8d572cfa21d619216630a433789676ac1104bcc9f99f384f765a55f4c62c55e4:4173601, id: 0x232689dee9c7b372, error (1): execution payload cannot be validated yet, latest valid hash is <nil>\" WARN [10-10|04:01:29.154] did not finish previous block building, starting new building now prev_onto=8d572c..2c55e4:4173601 prev_payload_id=0x232689dee9c7b372 new_onto=8d572c..2c55e4:4173601 WARN [10-10|04:01:29.157] Derivation process temporary error attempts=240,425 err=\"engine stage failed: temp: temporarily cannot insert new safe block: failed to complete building on top of L2 chain 0x8d572cfa21d619216630a433789676ac1104bcc9f99f384f765a55f4c62c55e4:4173601, id: 0x232689dee9c7b372, error (1): execution payload cannot be validated yet, latest valid hash is <nil>\"`\r\n> \r\n> * 7.Fix process\r\n>   \r\n>   1. Stop op-geth and op-node\r\n>   2. Determine a block-number that a state-snapshot exists for by running\u00a0`op-geth db metadata --datadir=your-dir-here`\r\n>      root@simple:~/op-geth# ./build/bin/geth db metadata --datadir=datadir3\r\n>      INFO [10-10|04:26:42.225] Maximum peer count                       ETH=50 LES=0 total=50\r\n>      INFO [10-10|04:26:42.225] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\n>      INFO [10-10|04:26:42.227] Set global gas cap                       cap=50,000,000\r\n>      INFO [10-10|04:26:42.227] Initializing the KZG library             backend=gokzg\r\n>      INFO [10-10|04:26:42.331] Using leveldb as the backing database\r\n>      INFO [10-10|04:26:42.331] Allocated cache and file handles         database=/root/op-geth/datadir3/geth/chaindata cache=512.00MiB handles=524,288 readonly=true\r\n>      INFO [10-10|04:26:42.348] Using LevelDB as the backing database\r\n>      INFO [10-10|04:26:42.348] Found legacy ancient chain path          location=/root/op-geth/datadir3/geth/chaindata/ancient\r\n>      INFO [10-10|04:26:42.349] Opened ancient database                  database=/root/op-geth/datadir3/geth/chaindata/ancient readonly=true\r\n>      +-------------------------+--------------------------------------------------------------------+\r\n>      |          FIELD          |                               VALUE                                |\r\n>      +-------------------------+--------------------------------------------------------------------+\r\n>      | databaseVersion         | 8 (0x8)                                                            |\r\n>      | headBlockHash           | 0x29f99c38cd4d8299e979a7e6ed48192df3ab191abaf6c24b49e8a8f138906b4b |\r\n>      | headFastBlockHash       | 0x9cae61ae0c7030e91448b3d6214740cea54b89d81363e555b7d57daedb6156d3 |\r\n>      | headHeaderHash          | 0x9cae61ae0c7030e91448b3d6214740cea54b89d81363e555b7d57daedb6156d3 |\r\n>      | lastPivotNumber         |                                                               |\r\n>      | len(snapshotSyncStatus) | 0 bytes                                                            |\r\n>      | snapshotDisabled        | false                                                              |\r\n>      | snapshotJournal         | 34 bytes                                                           |\r\n>      **| snapshotRecoveryNumber  | 4173600 (0x3faf20)**                                                 |\r\n>      | snapshotRoot            | 0x9acd9600b19b8feb71c03c0d39976e5d5c84c973c5e6b65fd01ce7ff05b8173f |\r\n>      | txIndexTail             | 1830758 (0x1bef66)                                                 |\r\n>      | fastTxLookupLimit       |                                                               |\r\n>      | frozen                  | 4173601 items                                                      |\r\n>      | snapshotGenerator       | Done: true, Accounts: 0,                                           |\r\n>      |                         | Slots: 0, Storage: 0, Marker:                                      |\r\n>      | headBlock.Hash          | 0x29f99c38cd4d8299e979a7e6ed48192df3ab191abaf6c24b49e8a8f138906b4b |\r\n>      | headBlock.Root          | 0x9acd9600b19b8feb71c03c0d39976e5d5c84c973c5e6b65fd01ce7ff05b8173f |\r\n>      | headBlock.Number        | 4173600 (0x3faf20)                                                 |\r\n>      | headHeader.Hash         | 0x9cae61ae0c7030e91448b3d6214740cea54b89d81363e555b7d57daedb6156d3 |\r\n>      | headHeader.Root         | 0xd62740e9b49f5717b7b63dcc30f53443d7bdc13491daac21ff6e020628cb6f2a |\r\n>      | headHeader.Number       | 4180757 (0x3fcb15)                                                 |\r\n>      +-------------------------+--------------------------------------------------------------------+\r\n>   3. Start op-geth\r\n>   4. Run\u00a0`cast rpc debug_setHead hexnumberhere`\u00a0(RPC call to debug-namespace geth method, with single hex-number argument) where the\u00a0`hexnumberhere`\u00a0is something before the above snapshot number, to force geth to drop the stale blocks without state, to not mislead op-node again.\r\n>      `root@simple:~/op-geth/build/bin# ./geth attach ipc:/root/op-geth/datadir3/geth.ipc Welcome to the Geth JavaScript console!`\r\n>      `instance: Geth/v0.1.0-unstable-b84ba119-20230914/linux-amd64/go1.20 at block: 4173600 (Sun Jan 15 2023 09:34:28 GMT+0000 (UTC)) datadir: /root/op-geth/datadir3 modules: admin:1.0 debug:1.0 engine:1.0 eth:1.0 miner:1.0 net:1.0 rpc:1.0 txpool:1.0 web3:1.0`\r\n>      `To exit, press ctrl-d or type exit`\r\n>      > `debug.setHead(\"0x3faf20\")  null`\r\n>      \r\n>      \r\n>      op-geth log:\r\n>      `WARN [10-10|04:31:07.212] Served eth_coinbase                      reqid=3 duration=\"111.931\u00b5s\" err=\"etherbase must be explicitly specified\" WARN [10-10|04:35:36.014] Rewinding blockchain to block            target=4,173,600 WARN [10-10|04:35:36.916] SetHead invalidated safe block ERROR[10-10|04:35:36.916] SetHead invalidated finalized block INFO [10-10|04:35:36.917] Loaded most recent local block           number=4,173,600 hash=29f99c..906b4b td=0 age=8mo3w6d`\r\n>   5. Wait for geth to complete (should be fast) and determine the latest block number (e.g. run\u00a0`cast block --rpc-url=http://op-geth:8545 latest`)\r\n>      `root@simple:~# cast block --rpc-url=http://localhost:8845 latest`\r\n>      baseFeePerGas        49\r\n>      difficulty           0\r\n>      extraData            0x\r\n>      gasLimit             25000000\r\n>      gasUsed              0\r\n>      hash                 0x29f99c38cd4d8299e979a7e6ed48192df3ab191abaf6c24b49e8a8f138906b4b\r\n>      logsBloom            0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\r\n>      miner                0x4200000000000000000000000000000000000011\r\n>      mixHash              0x9caa3c8460c2c003d140ae42b9bc7279b64a501f260ec888a9e17b865c5c92a6\r\n>      nonce                0x0000000000000000\r\n>      number               4173600\r\n>      parentHash           0x587ece9e9cd9b0534134ae8403edc2c898426db7786528c888d59137eeec0466\r\n>      receiptsRoot         0x3fd774ff8fe515813d7a8f9d3748e58e857bc002823a458a93be90a3bc2e0894\r\n>      sealFields           []\r\n>      sha3Uncles           0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\r\n>      size                 868\r\n>      stateRoot            0x9acd9600b19b8feb71c03c0d39976e5d5c84c973c5e6b65fd01ce7ff05b8173f\r\n>      timestamp            1673775268\r\n>      withdrawalsRoot\r\n>      totalDifficulty      0\r\n>      transactions:        [\r\n>      0xe1ad078d5ced638eb56b4647e3725d972a2352625d215788fe65e939d1877c7a\r\n>      ]\r\n>   6. Run a tool from the monorepo to correct the forkchoice of geth:\u00a0`go run ./op-wheel/cmd engine set-forkchoice --unsafe=X --safe=X --finalized=X --engine=http://op-geth:8551 --engine.jwt-secret=that_jwt_file_from_op_node.txt`\u00a0where\u00a0`X`\u00a0should be set to the latest block number.\r\n>      `root@simple:~/optimism# go run ./op-wheel/cmd engine set-forkchoice --unsafe='0x3faf20' --safe='0x3faf20' --finalized='0x3faf20' --engine=ws://localhost:8551 --engine.jwt-secret=/root/op-geth/jwt.txt`\r\n>   7. Start op-node again.\r\n>      op-node log:\r\n>      `INFO [10-10|04:53:08.776] Sync progress reason=\"processed safe block derived from L1\" l2_finalized=29f99c..906b4b:4173600 l2_safe=81c235..3cfa31:4173705 l2_unsafe=81c235..3cfa31:4173705 l2_engineSyncTarget=81c235..3cfa31:4173705 l2_time=1,673,775,478 l1_derived=a47e94..75898f:8314898`\r\n>      op-geth log:\r\n>      `INFO [10-10|04:53:51.197] Imported new potential chain segment     number=4,173,870 hash=76084e..8553d2 blocks=1 txs=1 mgas=0.000 elapsed=1.237ms     mgasps=0.000   age=8mo3w6d   dirty=1.62MiB INFO [10-10|04:53:51.198] Chain head was updated                   number=4,173,870 hash=76084e..8553d2 root=07412c..78a11d elapsed=\"150.322\u00b5s\" age=8mo3w6d`\r\n\r\nThanks for the instructions. I tried your method many times and I did not succeed. Op-node keeps saying \"Walking back .....etc..\" to a really old eth tx that was too far away from the current one. It never stopped because the last command to finish the process always gave me a return error.\r\n\r\nI ended up start a fresh node. It maybe long but better and I will keep backing up the database once every 2 weeks.", "2023-10-16T17:25:54Z", "2023-10-16T17:25:54Z", "dandavid3000", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5pMwiv", "I_kwDOH2Qg5s5wkxky", "\r\n> Thanks for the instructions. I tried your method many times and I did not succeed. Op-node keeps saying \"Walking back .....etc..\" to a really old eth tx that was too far away from the current one. It never stopped because the last command to finish the process always gave me a return error.\r\n> \r\n> I ended up start a fresh node. It maybe long but better and I will keep backing up the database once every 2 weeks.\r\n\r\nWalking back.. is not bad, it takes time and I believe there is value in waiting. It is a good habit to backup your data regularly", "2023-10-16T17:33:35Z", "2023-10-16T17:33:35Z", "opfocus", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5q4Raf", "I_kwDOH2Qg5s5wkxky", "@kaliubuntu0206 can you explain what this does?\r\n\r\n```\r\n--state.scheme value                                                   ($GETH_STATE_SCHEME)\r\n          Scheme to use for storing ethereum state ('hash' or 'path')\r\n```", "2023-11-03T21:40:54Z", "2023-11-03T21:40:54Z", "sbvegan", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5tKLzM", "I_kwDOH2Qg5s5wkxky", "After synchronizing using the `--state.scheme=path` parameter, does it mean that the node will automatically prune the data?", "2023-11-29T07:54:03Z", "2023-11-29T07:54:03Z", "rayn316", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5tNvpH", "I_kwDOH2Qg5s5wkxky", "I don't think the `path` option is supported on `op-geth` yet. I don't know when/if the default database will be updated. cc @protolambda ", "2023-11-29T16:48:08Z", "2023-11-29T16:48:08Z", "sbvegan", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5tcExs", "I_kwDOH2Qg5s5wkxky", "`path` is supported, but the datadir that op-goerli and op-mainnet start with is not in that format. Other op-stack chains, which start Bedrock from genesis, can be used with the `path` setting. We are working on making it available to op-mainnet and op-goerli as well.", "2023-12-01T12:56:29Z", "2023-12-01T12:56:29Z", "protolambda", "2025-08-30 08:40:24"]
["IC_kwDOH2Qg5s5vAalw", "I_kwDOH2Qg5s5wkxky", "> **Overview**\uff1aAfter completing the aforementioned fix process, it has been observed for 7 days. @sbvegan @protolambda\r\n> \r\n> 1. Within 30 hours after the first fix, an unexplained crash was observed, pointing to the same error as before the fix.\r\n>    op-node log:\r\n>    `WARN [10-12|05:50:12.400] Derivation process temporary error attempts=680,718 err=\"engine stage failed: temp: temporarily cannot insert new safe block: failed to complete building on top of L2 chain 0x573d201c442d663b9d262308e371503c9314582837a02a22d7f8ef9c3e0eb11a:4487626, id: 0x6e4ec0e089c0f645, error (1): execution payload cannot be validated yet, latest valid hash is <nil>\u201d`\r\n>    op-geth log:\r\n>    `INFO [10-12|06:11:52.673] Loaded most recent local block           number=4,487,625 hash=486cf4..ba17ff td=0 age=8mo3w1d INFO [10-12|06:11:52.674] Loaded most recent local finalized block number=4,487,625 hash=486cf4..ba17ff td=0 age=8mo3w1d INFO [10-12|06:11:52.688] Initialising Ethereum protocol           network=420 dbversion=8 INFO [10-12|06:11:52.688] Loaded local transaction journal         transactions=0 dropped=0 INFO [10-12|06:11:52.688] Regenerated local transaction journal    transactions=0 accounts=0 INFO [10-12|06:11:52.689] Chain post-merge, sync via beacon client INFO [10-12|06:11:52.689] Gasprice oracle is ignoring threshold set threshold=2 WARN [10-12|06:11:52.690] Unclean shutdown detected                booted=2023-10-08T15:31:12+0000 age=3d14h40m WARN [10-12|06:11:52.690] Unclean shutdown detected                booted=2023-10-08T21:23:44+0000 age=3d8h48m WARN [10-12|06:11:52.690] Unclean shutdown detected                booted=2023-10-09T15:07:48+0000 age=2d15h4m WARN [10-12|06:11:52.690] Unclean shutdown detected                booted=2023-10-09T15:12:19+0000 age=2d14h59m \u2026.`\r\n>    `WARN [10-12|06:03:38.257] State not available, ignoring new payload WARN [10-12|06:03:40.467] State not available, ignoring new payload WARN [10-12|06:03:44.607] State not available, ignoring new payload WARN [10-12|06:03:52.858] State not available, ignoring new payload`\r\n> 2. A second fix was performed following the same process. It is concerning that the snapshotRecoveryNumber is the same as it was during the first fix, which is disappointing.\r\n>    \r\n>    1. `root@simple:~/op-geth# ./build/bin/geth db metadata --datadir=datadir3`\r\n>       INFO [10-12|06:07:43.565] Using leveldb as the backing database\r\n>       INFO [10-12|06:07:43.565] Allocated cache and file handles         database=/root/op-geth/datadir3/geth/chaindata cache=512.00MiB handles=524,288 readonly=true\r\n>       INFO [10-12|06:07:43.589] Using LevelDB as the backing database\r\n>       INFO [10-12|06:07:43.589] Found legacy ancient chain path          location=/root/op-geth/datadir3/geth/chaindata/ancient\r\n>       INFO [10-12|06:07:43.590] Opened ancient database                  database=/root/op-geth/datadir3/geth/chaindata/ancient readonly=true\r\n>       +-------------------------+--------------------------------------------------------------------+\r\n>       |          FIELD          |                               VALUE                                |\r\n>       +-------------------------+--------------------------------------------------------------------+\r\n>       | databaseVersion         | 8 (0x8)                                                            |\r\n>       | headBlockHash           | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n>       | headFastBlockHash       | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n>       | headHeaderHash          | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n>       | lastPivotNumber         |                                                               |\r\n>       | len(snapshotSyncStatus) | 0 bytes                                                            |\r\n>       | snapshotDisabled        | false                                                              |\r\n>       | snapshotJournal         | 377790 bytes                                                       |\r\n>       | snapshotRecoveryNumber  | 4173600 (0x3faf20)                                                 |\r\n>       | snapshotRoot            | 0xdc0d84bb7d6d8c923df1965115e702f63ebc49368830024ac5c069e38511d0ab |\r\n>       | txIndexTail             | 2137626 (0x209e1a)                                                 |\r\n>       | fastTxLookupLimit       |                                                               |\r\n>       | frozen                  | 4397626 items                                                      |\r\n>       | snapshotGenerator       | Done: true, Accounts: 0,                                           |\r\n>       |                         | Slots: 0, Storage: 0, Marker:                                      |\r\n>       | headBlock.Hash          | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n>       | headBlock.Root          | 0xf94beb1ad6a20863648a225724dbb7b46ad5db92a4faa1d5fa2ea48c7830cd41 |\r\n>       | headBlock.Number        | 4487625 (0x4479c9)                                                 |\r\n>       | headHeader.Hash         | 0x486cf4391a2a3b7a60716b4ba9a797845caad98f327b4ed0083259a0a7ba17ff |\r\n>       | headHeader.Root         | 0xf94beb1ad6a20863648a225724dbb7b46ad5db92a4faa1d5fa2ea48c7830cd41 |\r\n>       | headHeader.Number       | 4487625 (0x4479c9)                                                 |\r\n>       +-------------------------+--------------------------------------------------------------------+\r\n>    2. root@simple:~/op-geth/build/bin# cast block --rpc-url=http://localhost:8845 latest\r\n>       baseFeePerGas        49\r\n>       difficulty           0\r\n>       extraData            0x\r\n>       gasLimit             25000000\r\n>       gasUsed              0\r\n>       hash                 0x29f99c38cd4d8299e979a7e6ed48192df3ab191abaf6c24b49e8a8f138906b4b\r\n>       logsBloom            0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\r\n>       miner                0x4200000000000000000000000000000000000011\r\n>       mixHash              0x9caa3c8460c2c003d140ae42b9bc7279b64a501f260ec888a9e17b865c5c92a6\r\n>       nonce                0x0000000000000000\r\n>       number               4173600\r\n>       parentHash           0x587ece9e9cd9b0534134ae8403edc2c898426db7786528c888d59137eeec0466\r\n>       receiptsRoot         0x3fd774ff8fe515813d7a8f9d3748e58e857bc002823a458a93be90a3bc2e0894\r\n>       sealFields           []\r\n>       sha3Uncles           0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\r\n>       size                 868\r\n>       stateRoot            0x9acd9600b19b8feb71c03c0d39976e5d5c84c973c5e6b65fd01ce7ff05b8173f\r\n>       timestamp            1673775268\r\n>       withdrawalsRoot\r\n>       totalDifficulty      0\r\n>       transactions:        [\r\n>       0xe1ad078d5ced638eb56b4647e3725d972a2352625d215788fe65e939d1877c7a\r\n>       ]\r\n>    3. debug.setHead(\"0x3faf20\")\r\n>    4. `root@simple:~/optimism# go run ./op-wheel/cmd engine set-forkchoice --unsafe='0x3faf20' --safe='0x3faf20' --finalized='0x3faf20' --engine=ws://localhost:8551 --engine.jwt-secret=/root/op-geth/jwt.txt`\r\n> 3. **It ran steadily for 5 days**, and the observation will conclude at this point.\r\n>    op-node log\uff1a\r\n>    [10-16|04:56:21.124] generated attributes in payload queue    txs=7    timestamp=1,677,051,178\r\n>    INFO [10-16|04:56:21.156] inserted block                           hash=be5536..0be407 number=5,811,555 state_root=b1ff17..64bfdd timestamp=1,677,051,178 parent=10da9b..ca3bff prev_randao=f7b1b8..5a8dfb fee_recipient=0x4200000000000000000000000000000000000011 txs=7    update_safe=true\r\n>    INFO [10-16|04:56:21.156] Sync progress                            reason=\"processed safe block derived from L1\" l2_finalized=5c3c34..521cff:5811144 l2_safe=be5536..0be407:5811555 l2_unsafe=be5536..0be407:5811555 l2_engineSyncTarget=be5536..0be407:5811555 l2_time=1,677,051,178 l1_derived=9dcf9b..b5deda:8535171\r\n>    op-geth log\uff1a\r\n>    INFO [10-16|04:56:55.725] Chain head was updated                   number=5,811,718 hash=cf1288..74b593 root=7a93b3..2cf0c3 elapsed=\"367.884\u00b5s\"  age=7mo3w4d\r\n>    INFO [10-16|04:56:55.739] Starting work on payload                 id=0x111d45ac0b2b982d\r\n>    INFO [10-16|04:56:55.753] Imported new potential chain segment     number=5,811,719 hash=10e1a8..bb9ede blocks=1 txs=2    mgas=0.959  elapsed=11.974ms     mgasps=80.054   age=7mo3w4d   dirty=433.91MiB\r\n>    **Additional Data**\r\n>    \r\n>    1. INFO [10-16|04:58:45.614] Loaded most recent local block           number=5,811,719 hash=10e1a8..bb9ede td=0 age=7mo3w4d\r\n>       INFO [10-16|04:58:45.614] Loaded most recent local finalized block number=5,811,623 hash=98a015..0bba12 td=0 age=7mo3w4d\r\n>       INFO [10-16|04:58:45.639] Initialising Ethereum protocol           network=420 dbversion=8\r\n>       INFO [10-16|04:58:45.639] Loaded local transaction journal         transactions=0 dropped=0\r\n>       INFO [10-16|04:58:45.639] Regenerated local transaction journal    transactions=0 accounts=0\r\n>       INFO [10-16|04:58:45.640] Chain post-merge, sync via beacon client\r\n>       INFO [10-16|04:58:45.640] Gasprice oracle is ignoring threshold set threshold=2\r\n>       WARN [10-16|04:58:45.641] Unclean shutdown detected                booted=2023-10-08T15:31:12+0000 age=1w13h27m\r\n>       WARN [10-16|04:58:45.641] Unclean shutdown detected                booted=2023-10-08T21:23:44+0000 age=1w7h35m\r\n>       WARN [10-16|04:58:45.641] Unclean shutdown detected                booted=2023-10-09T15:07:48+0000 age=6d13h50m\r\n>       WARN [10-16|04:58:45.641] Unclean shutdown detected                booted=2023-10-09T15:12:19+0000 age=6d13h46m\r\n>       \u2026.\r\n>    2. root@simple:~/op-geth/build/bin# cast block --rpc-url=http://localhost:8845 latest\r\n>       baseFeePerGas        49\r\n>       difficulty           0\r\n>       extraData            0x\r\n>       gasLimit             25000000\r\n>       gasUsed              958609\r\n>       hash                 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede\r\n>       logsBloom            0x0010010000000000000000001020000200000000000010000000c010000010000002000003400000004800000000000000000820000006000080000000000000080000020000000000000008000020000000000040408000000000000400000000000000000800000000000000000000008100000000140000200014200010020000040000000010000020000000800004004000000002001000000000000000200000100400400000048000000000000000000002000001020040000000008000000002000100000180000000000008100104010000600040010002000000000000000000800000000000004004000000008000000004000100000000060000\r\n>       miner                0x4200000000000000000000000000000000000011\r\n>       mixHash              0x6600a69dce79497b41ec85fdb37859c83d51b98a38fc783a1d1ac02e6348613d\r\n>       nonce                0x0000000000000000\r\n>       number               5811719\r\n>       parentHash           0xcf128813e1e4083bc08e3802d62451b53147a71a7bd47cbe136587369274b593\r\n>       receiptsRoot         0x6ebecbf97c99cab25fd3f2c9a356b985640404eb51930f04ef6d15c11721530c\r\n>       sealFields           []\r\n>       sha3Uncles           0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\r\n>       size                 1174\r\n>       stateRoot            0x1865470cb1a877684cd62b44148afb1634587fd28c176ff5b27406feb4f15be4\r\n>       timestamp            1677051506\r\n>       withdrawalsRoot\r\n>       totalDifficulty      0\r\n>       transactions:        [\r\n>       0x95f9ee962bce647aa84f43a07e5ed5ead145703388a6b19e3387f9f24b240da6\r\n>       0xa0f0bfb2900a3dbacd4af7c0749f91ba02110e6afd55df8cdab1d291f1eb0272\r\n>       ]\r\n>    3. +-------------------------+--------------------------------------------------------------------+\r\n>       |          FIELD          |                               VALUE                                |\r\n>       +-------------------------+--------------------------------------------------------------------+\r\n>       | databaseVersion         | 8 (0x8)                                                            |\r\n>       | headBlockHash           | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n>       | headFastBlockHash       | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n>       | headHeaderHash          | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n>       | lastPivotNumber         |                                                               |\r\n>       | len(snapshotSyncStatus) | 0 bytes                                                            |\r\n>       | snapshotDisabled        | false                                                              |\r\n>       | snapshotJournal         | 459578 bytes                                                       |\r\n>       | snapshotRecoveryNumber  |                                                               |\r\n>       | snapshotRoot            | 0xa304ddc535a1722fb593a7cf426c3355dc115c8f371203672e083a572bbba98d |\r\n>       | txIndexTail             | 3461720 (0x34d258)                                                 |\r\n>       | fastTxLookupLimit       |                                                               |\r\n>       | frozen                  | 5721720 items                                                      |\r\n>       | snapshotGenerator       | Done: true, Accounts: 0,                                           |\r\n>       |                         | Slots: 0, Storage: 0, Marker:                                      |\r\n>       | headBlock.Hash          | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n>       | headBlock.Root          | 0x1865470cb1a877684cd62b44148afb1634587fd28c176ff5b27406feb4f15be4 |\r\n>       | headBlock.Number        | 5811719 (0x58ae07)                                                 |\r\n>       | headHeader.Hash         | 0x10e1a84cdde4c8cb504a15c33f78820d70ad7689ca6fe4c6aa3b847c7cbb9ede |\r\n>       | headHeader.Root         | 0x1865470cb1a877684cd62b44148afb1634587fd28c176ff5b27406feb4f15be4 |\r\n>       | headHeader.Number       | 5811719 (0x58ae07)                                                 |\r\n>       +-------------------------+--------------------------------------------------------------------+\r\n\r\nI had a similar problem after prune state.Very nice and it worked!Thx!", "2023-12-19T09:09:07Z", "2023-12-19T09:11:30Z", "algtm", "2025-08-30 08:40:24"]
["IC_kwDOKIwiaM51J6n2", "I_kwDOKIwiaM5_3o6_", "cc @K-Ho ", "2024-02-26T23:50:18Z", "2024-02-26T23:50:18Z", "cpengilly", "2025-08-30 08:40:24"]
["IC_kwDOLB-lzc51h7Q5", "I_kwDOLB-lzc6A3Gvv", "We discussed this with the analytics/product folks and including multiple blobs per tx is definitely on our radar. It can be done without protocol changes; we already retrieve all of the referenced blobs, and have a way of stitching packets together. We can simply use the channel-frames to separate the data into blobs, the blob contents do not have to be literally concatenated.\r\n", "2024-02-29T19:35:17Z", "2024-02-29T19:35:17Z", "protolambda", "2025-08-30 08:40:30"]
["IC_kwDODjvEJM5088-j", "I_kwDODjvEJM6AQdd5", "It is unclear how much code has been changed with Manta. They may be using an older version of the op-node", "2024-02-23T23:32:40Z", "2024-02-23T23:32:40Z", "tynes", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM51ZxRP", "I_kwDODjvEJM6AQdd5", "@junha-ahn please use an op-node version of at least 1.5.1. Manta needs to stay up to date.", "2024-02-28T19:23:37Z", "2024-02-28T19:23:37Z", "trianglesphere", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM51jK3W", "I_kwDODjvEJM58CFKs", "#490  Bridge Security Analysis for any bridge risks through tokens being swapped.", "2024-02-29T23:38:20Z", "2024-02-29T23:38:43Z", "philliprossii", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM5yzP4i", "I_kwDODjvEJM57RtvG", "What's the status of this?", "2024-02-05T01:01:33Z", "2024-02-05T01:01:33Z", "smartcontracts", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM5vt9u-", "I_kwDODjvEJM56udNo", "Hi there! Logs contain this line prior to shutdown:\r\n\r\n```\r\nDec 29 10:07:07 systemd[1]: base-sepolia.service: Main process exited, code=killed, status=9/KILL\r\n```\r\n\r\nThis means that Geth was forcibly shut down, without having time to write data to disk. Please set up your service file to wait longer on shutdown (at least 5 minutes). Forcibly killing Geth will cause data corruption regardless of which state scheme is used.", "2024-01-02T17:24:00Z", "2024-01-02T17:24:00Z", "mslipper", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM51klEm", "I_kwDODjvEJM56udNo", "@mslipper which PR/code fix this bug? We are working Java consensus client and also found this problem.", "2024-03-01T05:13:42Z", "2024-03-01T05:13:42Z", "GrapeBaBa", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM51kmqb", "I_kwDODjvEJM56udNo", "Can this issue be resolved using the debug_setHead interface for rollback?", "2024-03-01T05:20:58Z", "2024-03-01T05:20:58Z", "thinkAfCod", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM5qtMGg", "I_kwDODjvEJM51nqc1", "You should consider deploying to an orchestration framework like kubernetes instead of implementing that event handling yourself. Kubernetes is battle tested and it restarts containers when they fail", "2023-11-02T07:51:15Z", "2023-11-02T07:51:15Z", "tynes", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM5qtNUm", "I_kwDODjvEJM51nqc1", "@tynes Could I analysis the reason of failure and combat some possible DB corruption programmatically? https://github.com/ethereum/go-ethereum/issues/16607. Since the simple retrial would loop the failure", "2023-11-02T07:55:56Z", "2023-11-02T07:55:56Z", "unknown", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM5quLTh", "I_kwDODjvEJM51nqc1", "Yes you could likely do that although I do not have any good suggestions on how to do that off of the top of my head. It really depends on your setup. Are you experiencing DB corruption issues? I am not aware of this being a huge issue right now", "2023-11-02T10:45:19Z", "2023-11-02T10:45:19Z", "tynes", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM5quPdM", "I_kwDODjvEJM51nqc1", "@tynes No I was just curious if your team have faced and tried to solve the problem when it happened ( especially while running a node inside containers )", "2023-11-02T10:57:13Z", "2023-11-02T10:57:13Z", "unknown", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM5yzULt", "I_kwDODjvEJM51nqc1", "I think the docs will hopefully soon get a section that gives advice for running a prod OP Stack chain.", "2024-02-05T01:24:31Z", "2024-02-05T01:24:31Z", "smartcontracts", "2025-08-30 08:40:36"]
["IC_kwDODjvEJM5qhBQV", "I_kwDODjvEJM51YpHU", "> Since I don't like dependencies\r\n\r\nWhat about `node_modules`? That is where ethereumjs would live.\r\n\r\nIt should be possible to reimplement the [L2 genesis generation](https://github.com/ethereum-optimism/optimism/blob/96a24cc3f6f5bbbb8f42011988208619a41344fd/op-chain-ops/genesis/layer_two.go#L17) in typescript, read through the code to see what it does. Note that it will be getting new features over time so you will need to maintain it.", "2023-10-31T11:35:36Z", "2023-10-31T11:35:36Z", "tynes", "2025-08-30 08:40:36"]
["IC_kwDOKIwiaM51_BsT", "I_kwDOKIwiaM6BVDlW", "There's a draft for this here: https://github.com/ethereum-optimism/docs/pull/329. Needs to be rebased.", "2024-03-05T19:07:39Z", "2024-03-05T19:08:15Z", "smartcontracts", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52Lt4X", "I_kwDOKIwiaM6A21Zp", "feature request shifting to internal tracking system, ticket is closed for now until feature can be properly scoped and planned.", "2024-03-07T07:44:41Z", "2024-03-07T07:44:41Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52T6CY", "I_kwDOKIwiaM5_5voG", "feature request shifting to internal tracking system, ticket is closed for now until feature can be properly scoped and planned.", "2024-03-08T02:19:16Z", "2024-03-08T02:19:16Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52T5KY", "I_kwDOKIwiaM5_3DV7", "feature request shifting to internal tracking system, ticket is closed for now until feature can be properly scoped and planned.", "2024-03-08T02:14:17Z", "2024-03-08T02:14:17Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM51I1Cf", "I_kwDOKIwiaM5-szFZ", "All but point (4) gets addressed in https://github.com/ethereum-optimism/docs/pull/515", "2024-02-26T20:54:17Z", "2024-02-26T20:54:17Z", "sbvegan", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52KdoP", "I_kwDOKIwiaM5-szFZ", "#4 is a feature request since it's not natively supported in nextra. we'll have to put in for a design change, so I'll track this separately. closing this ticket.", "2024-03-07T05:54:10Z", "2024-03-07T05:54:10Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52KlYs", "I_kwDOKIwiaM57ukDh", "page updated as of jan 8th and issue fixed :) ", "2024-03-07T06:03:06Z", "2024-03-07T06:03:06Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM5zGLtQ", "I_kwDOKIwiaM57b9U5", "The nextra maintainer (who is also maintainer of swr) has an example of how to implement this here: https://github.com/vercel/swr-site/blob/88d1b8affa7f1c344f3aaba229b51867b2cedc8d/components/docsearch.js", "2024-02-07T00:15:10Z", "2024-02-07T00:15:10Z", "roninjin10", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52T4Bt", "I_kwDOKIwiaM57b9U5", "\nfeature request shifting to internal tracking system, ticket is closed for now until feature can be properly scoped and planned.", "2024-03-08T02:08:08Z", "2024-03-08T02:08:08Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52T3UB", "I_kwDOKIwiaM55upIj", "\nfeature request shifting to internal tracking system, ticket is closed for now until feature can be properly scoped and planned.", "2024-03-08T02:04:10Z", "2024-03-08T02:04:10Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52T2lW", "I_kwDOKIwiaM54-xLQ", "feature request shifting to internal tracking system, ticket is closed for now until feature can be properly scoped and planned.", "2024-03-08T02:00:23Z", "2024-03-08T02:00:23Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM5upmF7", "I_kwDOKIwiaM54-trf", "possibly use RSS feeds from op-geth, op-node, and monorepo release notes (batcher, proposer, etc.) -- bring them into one MDX file within docs in a way that devs can manipulate\r\n\r\n- toggle on all of them, toggle on some of them, check to display just batcher etc.", "2023-12-14T18:45:18Z", "2023-12-14T18:45:18Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52LoNN", "I_kwDOKIwiaM54-trf", "shifted feature to internal tracking system, closing this ticket", "2024-03-07T07:36:40Z", "2024-03-07T07:36:40Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52Hm8A", "I_kwDOKIwiaM53nb_p", "fixed in feb with new gas estimation pages", "2024-03-06T20:14:52Z", "2024-03-06T20:14:52Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52HmYO", "I_kwDOKIwiaM53nX3S", "done as of Jan 10", "2024-03-06T20:13:23Z", "2024-03-06T20:13:23Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52MPK-", "I_kwDOKIwiaM5ylXYT", "feature request shifting to internal tracking system, ticket is closed for now until feature can be properly scoped and planned.", "2024-03-07T08:29:05Z", "2024-03-07T08:29:05Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52HdWK", "I_kwDOKIwiaM5x0HQD", "@annieke just pinging you to make sure this page is still needed and to check accuracy (+ add any new questions)", "2024-03-06T19:47:58Z", "2024-03-06T19:47:58Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52H4-8", "I_kwDOKIwiaM5x0HQD", "@cpengilly let's deprioritize this! ", "2024-03-06T21:01:52Z", "2024-03-06T21:01:52Z", "annieke", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM5wZeC3", "I_kwDOKIwiaM5xyqcG", "I think this can be best resolved by the protocol docs refresh, we can just link to the protocol docs then", "2024-01-10T21:01:37Z", "2024-01-10T21:01:37Z", "smartcontracts", "2025-08-30 08:41:28"]
["IC_kwDOKIwiaM52HZ7M", "I_kwDOKIwiaM5xtUM1", "closing this since most of the estimating gas docs have been recently revised + 2 new ones added", "2024-03-06T19:39:29Z", "2024-03-06T19:39:29Z", "cpengilly", "2025-08-30 08:41:28"]
["IC_kwDOLB-lzc514KCC", "I_kwDOLB-lzc591ZEI", "I think this is fixed in the [latest version](https://github.com/ethereum-optimism/specs/blob/11099e9908bb7bfa640d73b2a3a2349bef9ab7a1/specs/protocol/span-batches.md#L32-L34), fixed in [this PR](https://github.com/ethereum-optimism/specs/pull/50/files).", "2024-03-04T23:36:31Z", "2024-03-04T23:37:07Z", "snario", "2025-08-30 08:41:46"]
["IC_kwDODjvEJM52ZQA3", "I_kwDODjvEJM6BvTLl", "Make sure to clean and rebuild everything. This problem does not occur on fresh builds in CI so it leads me to believe it is an issue with forge caching. `pnpm clean` in `contracts-bedrock`", "2024-03-08T19:53:45Z", "2024-03-08T19:53:45Z", "tynes", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM52bk7G", "I_kwDODjvEJM6BvTLl", "thank you for the suggestion @tynes ! I've tried `pnpm clean` in the folder but unfortunately still seeing the same error :thinking: However, since this is working in ci I close the issue and I'll try to further investigate what's happening with my setup.\r\nThanks once again :pray: ", "2024-03-09T18:22:17Z", "2024-03-09T18:22:17Z", "seaona", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM52YuGT", "I_kwDODjvEJM6BuyoP", "Also running into a similar issue but this time pointing to mainnet: \r\n\r\n```\r\nblaine@blaines-MacBook-Pro batch_decoder % ./batch_decoder fetch --start 19392100 --end 19392120 --inbox 0xff00000000000000000000000000000000000010 --sender 0x6887246668a3b87F54DeB3b94Ba47a6f63F32985 --l1 https://eth-mainnet.g.alchemy.com/v2/<your_api_key_here>\r\nFetched block:  19392108\r\nFetched block:  19392100\r\nFetched block:  19392102\r\nFetched block:  19392107\r\nFetched block:  19392104\r\nFetched block:  19392103\r\nFetched block:  19392109\r\nFetched block:  19392105\r\nFetched block:  19392101\r\nFound a transaction (0x8aae90ceb2bfd1a687221e7227c65dd3ffd41c732cc795dab4cfe16d105ca88c) from an invalid sender (0x6088B06c5a187058434655B71057a9ee93E13d0d)\r\nFetched block:  19392117\r\nFetched block:  19392112\r\nFetched block:  19392111\r\nFetched block:  19392110\r\nFetched block:  19392114\r\nFetched block:  19392118\r\nFetched block:  19392115\r\nFetched block:  19392113\r\nFetched block:  19392116\r\nFetched block:  19392106\r\nFetched block:  19392119\r\nFetched batches in range [19392100,19392120). Found 4 valid & 2 invalid batches\r\n```", "2024-03-08T18:23:09Z", "2024-03-08T18:23:09Z", "blmalone", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM52TvMV", "I_kwDODjvEJM6Bn6d2", "Simply send another deposit to assume the identity of the aliased account on L2", "2024-03-08T01:26:18Z", "2024-03-08T01:26:18Z", "tynes", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM519tXW", "I_kwDODjvEJM6BUWpq", "Please open an issue or pull request to the [specs](https://github.com/ethereum-optimism/specs) repo to discuss this kind of thing. To have this seriously considered, you will need to provide more detail, including security considerations", "2024-03-05T16:03:12Z", "2024-03-05T16:03:12Z", "tynes", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM519t70", "I_kwDODjvEJM6BUWpq", "Will re-open with more detail in the specs repo", "2024-03-05T16:04:05Z", "2024-03-05T16:04:05Z", "henridevieux", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM51oMO1", "I_kwDODjvEJM6A92ci", "Interacting with the implementation of the portal will never result in a balance transfer to L2. It would break the idea of being able to upgrade the contracts. It could be possible to have the tx revert on L1 if calling the implementation directly, this is a footgun for users. Pretty easy to implement `(address(this) == implementation)` where `implementation` is an `immutable` and set in the constructor", "2024-03-01T16:14:08Z", "2024-03-01T16:14:08Z", "tynes", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM51oXBI", "I_kwDODjvEJM6A92ci", "Exactly.", "2024-03-01T16:40:00Z", "2024-03-01T16:40:00Z", "pegahcarter", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM5rgWrR", "I_kwDODjvEJM52Ly-w", "Hello, you can track the whole process here https://github.com/ethereum-optimism/optimism/pull/7349", "2023-11-09T11:20:26Z", "2023-11-09T11:20:26Z", "unknown", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM51-9kr", "I_kwDODjvEJM52Ly-w", "Please refer to the Ecotone Upgrade Proposal for detailed information about the proposed changes for 4844: https://gov.optimism.io/t/upgrade-proposal-5-ecotone-network-upgrade/7669", "2024-03-05T18:56:35Z", "2024-03-05T18:56:35Z", "smartcontracts", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM5ZALeI", "I_kwDODjvEJM5iNq1n", "Very nice ", "2023-04-02T04:17:44Z", "2023-04-02T04:17:44Z", "kamlesh11839", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM5eo6AI", "I_kwDODjvEJM5iNq1n", "I would like to work on this @fgimenez @tynes if not resolved :)", "2023-06-12T17:40:51Z", "2023-06-12T17:40:51Z", "ashutosh887", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM5epGD1", "I_kwDODjvEJM5iNq1n", "hey @ashutosh887 there's an approved PR for this issue here https://github.com/ethereum-optimism/optimism/pull/5329", "2023-06-12T18:09:49Z", "2023-06-12T18:09:49Z", "fgimenez", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM5ipCKe", "I_kwDODjvEJM5iNq1n", "HI @fgimenez \r\nIs the PR merged?\r\n\r\n\r\ncc: @tynes \r\n\r\n> hey @ashutosh887 there's an approved PR for this issue here #5329\r\n\r\n", "2023-07-28T03:23:29Z", "2023-07-28T03:23:29Z", "ashutosh887", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM5iphgt", "I_kwDODjvEJM5iNq1n", "nope, not merged", "2023-07-28T05:38:39Z", "2023-07-28T05:38:39Z", "fgimenez", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM5nzs6q", "I_kwDODjvEJM5iNq1n", "Looks like this issue can be closed @tynes ", "2023-09-30T00:40:51Z", "2023-09-30T00:40:51Z", "Sabnock01", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM5ToZLT", "I_kwDODjvEJM5ZZEW1", "Hello, It looks like when the environment runs `run yarn build` it is trying to run a command that is not compatible with the AWS Graviton instances. It could also be due to some missing dependencies that are not specified in the Dockerfile that are required to run the command. Without more information about the specific error message and the environment in which the Dockerfile is being run, it is difficult to say for sure what the issue is.\r\n\r\nThis may not be super helpful considering you are running it within aws, but I often have issues building docker containers on my M1 Mac, and the issue is almost always fixed by changing the second line \r\n`FROM node:16-alpine3.14` \r\nto \r\n\r\n```\r\nFROM node:16-slim\r\nRUN apt-get update\r\nRUN apt-get install -y openssl\r\n```\r\n\r\nI hope this helps you get closer to resolving the issue.", "2023-01-25T04:58:53Z", "2023-01-25T04:58:53Z", "meaghanfitzgerald", "2025-08-30 08:41:47"]
["IC_kwDODjvEJM51--BN", "I_kwDODjvEJM5ZZEW1", "Closing as dupe of #3606 ", "2024-03-05T18:57:50Z", "2024-03-05T18:57:50Z", "smartcontracts", "2025-08-30 08:41:47"]
["IC_kwDOH2Qg5s53Htpc", "I_kwDOH2Qg5s6CXiw1", "Worth noting for the record that halting is an opt-in thing. It's actually very useful for a full node to prevent it following a non-upgraded chain if it misses an update and then having to completely resync to get back to the correct chain because it no longer has world states old enough to handle the very large reorg.\r\n\r\nAlso, the recommended protocol version is updated prior to the required protocol version being updated which causes op-geth to log an error about being out of date but not halt.\r\n\r\n```\r\n    --rollup.halt value                                                    ($GETH_ROLLUP_HALT)\r\n          Opt-in option to halt on incompatible protocol version requirements of the given\r\n          level (major/minor/patch/none), as signaled through the Engine API by the rollup\r\n          node\r\n```\r\n\r\nI believe you can achieve a policy of only ever logging by using the options from `op-node`:\r\n```\r\n--rollup.halt value                                                    ($OP_NODE_ROLLUP_HALT)\r\n          Opt-in option to halt on incompatible protocol version requirements of the given\r\n          level (major/minor/patch/none), as signaled onchain in L1\r\n\r\n    --rollup.load-protocol-versions     (default: false)                   ($OP_NODE_ROLLUP_LOAD_PROTOCOL_VERSIONS)\r\n          Load protocol versions from the superchain L1 ProtocolVersions contract (if\r\n          available), and report in logs and metrics\r\n```\r\n\r\nYou can instruct it to load protocol versions but not opt in to rollup halt and op-node will use the JSON-RPC call to also check that its op-geth is up to date.\r\n", "2024-03-14T21:26:40Z", "2024-03-14T21:26:40Z", "ajsutton", "2025-08-30 08:44:07"]
["IC_kwDOH2Qg5s53MhQc", "I_kwDOH2Qg5s6CXiw1", "How is this opt-in? I did not set the `GET_ROLLUP_HALT` env or --rollup.halt on either Optimism or Base nodes that I operate, but I did encounter this behavior.", "2024-03-15T14:19:54Z", "2024-03-15T14:19:54Z", "feld", "2025-08-30 08:44:07"]
["IC_kwDOH2Qg5s53XVzr", "I_kwDOH2Qg5s6CXiw1", "If you look at https://github.com/ethereum-optimism/op-geth/blob/425e757c51a1148cf6e3451157f1b666b2242b81/eth/backend.go#L616-L625 if `s.config.RollupHaltOnIncompatibleProtocolVersion` is not set, it does not halt. That config var is set in https://github.com/ethereum-optimism/op-geth/blob/352fbe634837af931b6d5128901b31cd2550bffe/cmd/utils/flags.go#L1848 to the value set in the `rollup.halt` flag which has now default value: https://github.com/ethereum-optimism/op-geth/blob/352fbe634837af931b6d5128901b31cd2550bffe/cmd/utils/flags.go#L888-L892\r\n\r\nSo it's off by default.  If op-geth halted then something must have set that flag.\r\n\r\nWhat version of op-geth were you running?", "2024-03-17T20:29:11Z", "2024-03-17T20:29:11Z", "ajsutton", "2025-08-30 08:44:07"]
["IC_kwDOKSJyfM52kmFH", "I_kwDOKSJyfM6BWtAo", "Here's the image, lmk if you need different size: \r\n<img width=\"820\" alt=\"Superchain Dev Preview\" src=\"https://github.com/ethereum-optimism/ecosystem/assets/140648764/5519d16c-06fc-4734-a775-e1210ed6a1b2\">\r\n", "2024-03-11T19:46:55Z", "2024-03-11T19:46:55Z", "shaunop", "2025-08-30 08:44:07"]
["IC_kwDOKSJyfM52xq2T", "I_kwDOKSJyfM6BWtAo", "@tarunkhasnavis Updated this image to account for new \"Superchain\" logo. \r\n\r\n<img width=\"800\" alt=\"Superchain Dev Preview\" src=\"https://github.com/ethereum-optimism/ecosystem/assets/140648764/9c3267d6-2df3-4b06-9e0c-2b9cf76c26a7\">\r\n\r\n", "2024-03-12T23:24:17Z", "2024-03-12T23:24:17Z", "shaunop", "2025-08-30 08:44:07"]
["IC_kwDODjvEJM52u-is", "I_kwDODjvEJM6CB8_A", "We are planning on fully deprecating the sdk so it was decided to not update it. It doesn't have great test coverage and adds a massive amount of size to any frontend bundle.\r\n\r\nViem is recommended now. I do have an old branch someplace where I did the migration, it took maybe 45 mins or so of replacing stuff\r\n\r\ncc @roninjin10 ", "2024-03-12T16:10:30Z", "2024-03-12T16:10:30Z", "tynes", "2025-08-30 09:45:22"]
["IC_kwDODjvEJM52vZ01", "I_kwDODjvEJM6CB8_A", "Correct. @sambacha there are no plans to upgrade the optimism sdk. More users actually use ethers v5 still than ethers v6 atm so this sdk is considered the ethers v5 sdk.\r\n\r\nIf anybody was looking to contribute an ethers v6 sdk get in contact with me. [We would want to build it here](https://github.com/ethers-io/ext-utils-optimism).  Ideally both ethers and viem support the op stack but natively not via a custom sdk.\r\n\r\nHave you tried viem @sambacha ? [Viem has native OP-Stack support](https://viem.sh/op-stack)", "2024-03-12T17:00:57Z", "2024-03-12T17:00:57Z", "roninjin10", "2025-08-30 09:45:22"]
["IC_kwDODjvEJM52vdXr", "I_kwDODjvEJM6CB8_A", "@sambacha feel free to dm me on tg if ethers v5 is blocking you", "2024-03-12T17:08:27Z", "2024-03-12T17:08:27Z", "roninjin10", "2025-08-30 09:45:22"]
["IC_kwDODjvEJM52vfos", "I_kwDODjvEJM6CB8_A", "Thanks for the clarification and quick response! Cheers", "2024-03-12T17:13:33Z", "2024-03-12T17:13:33Z", "sambacha", "2025-08-30 09:45:22"]
["IC_kwDOJ_r-bs5zj8WV", "I_kwDOJ_r-bs5-9fh0", "I see two obvious options:\r\n\r\n1. Remove this chain entirely\r\n2. Add this chain to a global list of \"retired\" / sunset chains, which can have all of their checks skipped. \r\n\r\nI prefer 1, unless there are good reasons to keep some of the config around. ", "2024-02-12T14:38:36Z", "2024-02-12T14:38:36Z", "geoknee", "2025-08-30 09:45:36"]
["IC_kwDOH2Qg5s54IAcw", "I_kwDOH2Qg5s6DPC2n", "This is mostly covered here:\r\nhttps://docs.optimism.io/builders/node-operators/management/blobs#configure-a-blob-archiver-archive-nodes", "2024-03-22T15:39:16Z", "2024-03-22T15:39:16Z", "quickchase", "2025-08-30 09:46:16"]
["IC_kwDOH2Qg5s54IZcs", "I_kwDOH2Qg5s6DPC2n", "@quickchase \r\nThank you very much.\r\nSo, looks it needs to rely on archived beacon blob data provider to spin up a new node if I don't use snapsync.", "2024-03-22T16:32:36Z", "2024-03-22T16:34:20Z", "HiroyukiNaito", "2025-08-30 09:46:16"]
["IC_kwDOLB-lzc54DbDZ", "I_kwDOLB-lzc6DNMDT", "With the recent addition of the db in op-node for safe heads, could this make sense to live as a subsystem directly in op-node? It would then enable things like a p2p network protocol for requesting output preimages", "2024-03-22T01:43:56Z", "2024-03-22T01:43:56Z", "tynes", "2025-08-30 09:46:21"]
["IC_kwDOLB-lzc53Qj-2", "I_kwDOLB-lzc6CYe60", "Is there an easy way to do this with mdbook?", "2024-03-16T00:45:57Z", "2024-03-16T00:45:57Z", "tynes", "2025-08-30 09:46:21"]
["IC_kwDOLB-lzc53tQQB", "I_kwDOLB-lzc6CYe60", "> Is there an easy way to do this with mdbook?\r\n\r\nmdbook has this option built in: https://rust-lang.github.io/mdBook/format/configuration/renderers.html#outputhtmlredirect\r\n\r\nGenerating a sitemap.xml file would be useful in tracking changes regardless if `output.redirects` is maintained, as it would capture all the links for the site. However large changes would be more difficult to track in a single PR. In those cases just doing some simple search based off of the user inputted URL and the latest sitemap could provide a best-effort approximation of what the user is looking for.\r\n\r\nA more Lazy Approach:\r\n\r\nThe browser should find the valid path that has the shortest *edit distance* (re: Levenshtein distance) from the path that was requested from the submitted URL based off of the latest `sitemap.xml` file. \r\nThen present the suggestion (the shortest edit distance result based off of the sitemap.xml links) as as suggested link (i.e. the path determined by the levenstein algo) on the 404 page. \r\n\r\nBasicaly, \"Did you mean < suggested path link > for  < users original requested link > ?\"\r\n\r\n", "2024-03-19T23:42:30Z", "2024-03-19T23:42:30Z", "sambacha", "2025-08-30 09:46:21"]
["IC_kwDOLB-lzc53fpPk", "I_kwDOLB-lzc5-FBkb", "I really like the per-hardfork categorization. It's consistent with the ethereum L1 consensus-specs, and makes it easy to extend specs incrementally. That said, a type of global overview-doc, with categorized features, and a type of mini-changelog with links to sections of the individual specs, would be really useful for navigation.\r\nOtherwise I would have to search 5 different `state-transition.md` docs until I find the state-transition feature change I am looking for.\r\n", "2024-03-18T19:45:29Z", "2024-03-18T19:45:29Z", "protolambda", "2025-08-30 09:46:21"]
["IC_kwDOJ_r-bs52NK2_", "I_kwDOJ_r-bs6BfY5Y", "While I agree the inputs are not secret, they aren't terribly meaningful outside of the contributing chain's repo either. For example `DEPLOYMENTS_DIR` and other file paths. To me it feels like some of these variables at least still make sense as env vars. ", "2024-03-07T10:03:44Z", "2024-03-07T10:03:44Z", "geoknee", "2025-08-30 09:48:20"]
["IC_kwDOJ_r-bs52OJRR", "I_kwDOJ_r-bs6BfY5Y", "Yea that's fair. I do like the idea of using this issue as a step towards a dedicated `input` directory, and to your point `DEPLOYMENTS_DIR` is really not the input we care about\u2014the contract addresses it holds are. So perhaps we do use env vars, and use those env vars to also generate the input file.\r\n\r\nI'm also ok with keeping it simple and removing the input file approach for now if we think it's too early", "2024-03-07T12:35:46Z", "2024-03-07T12:35:46Z", "mds1", "2025-08-30 09:48:20"]
["IC_kwDOJ_r-bs52YSay", "I_kwDOJ_r-bs6BfY5Y", "Got a straw man implementation here https://github.com/ethereum-optimism/superchain-registry/pull/118 \r\n\r\nHappy to go after TOML and other refactors too, but that might be best broken up and done in follow ups?", "2024-03-08T17:05:56Z", "2024-03-08T17:05:56Z", "geoknee", "2025-08-30 09:48:20"]
["IC_kwDOJ_r-bs53eXTn", "I_kwDOJ_r-bs6BfY5Y", "Let's mark this as completed by #118 for now, and we can revisit pending feedback ", "2024-03-18T16:54:59Z", "2024-03-18T16:54:59Z", "mds1", "2025-08-30 09:48:20"]
["IC_kwDODjvEJM53Xs5n", "I_kwDODjvEJM6Chx6w", "Fixed!  Operating during the early morning hours can result in lower gas fees. Thank you.\r\n", "2024-03-18T01:04:56Z", "2024-03-18T01:04:56Z", "lichong2005", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM53b16o", "I_kwDODjvEJM6Chx6w", "Closing as user resolved their own issue. ", "2024-03-18T12:30:38Z", "2024-03-18T12:30:38Z", "blmalone", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM5xLnB1", "I_kwDODjvEJM58fZ3r", "@PeterBenc are you able to share a reproducible example? Link to the repo this is happening in or a minimal reproducible example?", "2024-01-18T16:59:59Z", "2024-01-18T16:59:59Z", "roninjin10", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM5xLoK0", "I_kwDODjvEJM58fZ3r", "@roninjin10 sorry, no, I haven't been able to reproduce it, it just kept happening randomly in circleCI or in heroku. Its a private repo so cannot share more.\r\nJust posting the issue here, so you guys (and other in future) are aware that this happens.", "2024-01-18T17:02:45Z", "2024-01-18T17:02:45Z", "PeterBenc", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM5xLpz0", "I_kwDODjvEJM58fZ3r", "> @roninjin10 sorry, no, I haven't been able to reproduce it, it just kept happening randomly in circleCI or in heroku. Its a private repo so cannot share more. Just posting the issue here, so you guys (and other in future) are aware that this happens.\r\n\r\nBy randomly do you mean it sometimes would not happen sometimes would happen? Are you able to share the install command that this is failing on and/or more of the logging context this error happened on? Are you just running `yarn install` or something else?", "2024-01-18T17:06:27Z", "2024-01-18T17:06:27Z", "roninjin10", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM5xLqxf", "I_kwDODjvEJM58fZ3r", "What version of npm are you using?", "2024-01-18T17:08:52Z", "2024-01-18T17:08:52Z", "roninjin10", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM50IzHz", "I_kwDODjvEJM58fZ3r", "We are having the same issue in ci when doing `npm ci`. It fails randomly >50% of the time.", "2024-02-16T14:16:44Z", "2024-02-16T14:16:44Z", "acgith", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM52bxmD", "I_kwDODjvEJM58fZ3r", "Same here, fails on Vercel. The `only-allow` script likely shouldn't be added to these packages that can be installed by arbitrary consumers using `npm`, `yarn`, etc.", "2024-03-09T22:05:10Z", "2024-03-09T22:16:22Z", "alecananian", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM52byQx", "I_kwDODjvEJM58fZ3r", "I will approve a pr opened that removes it. preinstall scripts aren\u2019t supposed to ever run for npm packages so it\u2019s odd that it\u2019s causing issues ", "2024-03-09T22:18:13Z", "2024-03-09T22:18:13Z", "roninjin10", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM52jlGB", "I_kwDODjvEJM58fZ3r", "@roninjin10 thanks for merging #9805 !\r\n\r\nwe just recently added the dependency to the @thirdweb-dev/sdk package which is causing issues with yarn/npm for our users (this is what prompted @alecananian PR)\r\n\r\nAny chance we can get #9812 merged today to get the latest release?", "2024-03-11T17:34:23Z", "2024-03-11T17:34:23Z", "joaquim-verges", "2025-08-30 09:48:38"]
["IC_kwDODjvEJM54FVyN", "I_kwDODjvEJM58fZ3r", "Hey @roninjin10 , do you have an ETA for releasing this merge? Our pipelines fail most of the time because of this issue", "2024-03-22T09:13:15Z", "2024-03-22T09:13:15Z", "0xfrosty", "2025-08-30 09:48:38"]
["IC_kwDOLB-lzc51RUNc", "I_kwDOLB-lzc6AaUKd", "Hey @tuxcanfly, thanks for the suggestion. We do have a last PR to complete the op-plasma implementation that will add a proper plasma switch as part of system config. Once that is done, the flag will tell derivation whether to interpret the first byte of the tx data as a derivation version or commitment version at a given block. Will make sure to reflect that in the spec. ", "2024-02-27T19:44:47Z", "2024-02-27T19:44:47Z", "tchardin", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc513QPq", "I_kwDOLB-lzc6AaUKd", "@tchardin Do you mean \"at a given block\" or do you mean \"for a given block\"? My understanding is there is no concept of going between plasma and rollup mode. \"At a given block\" implies you can switch between the two\r\n\r\n@tuxcanfly Do you feel like this is sufficient?", "2024-03-04T20:42:04Z", "2024-03-04T20:42:04Z", "tynes", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc513ifx", "I_kwDOLB-lzc6AaUKd", "After chat with @trianglesphere, we're going to go for bumping the batcher tx version byte to 1 and use that to detect whether tx data should be interpreted as plasma commitment and then keep a derivationVersion0 on the input batch to forward to the frame decoder. (Commitment type prefix is kept after the batcher tx type)", "2024-03-04T21:30:43Z", "2024-03-04T21:33:11Z", "tchardin", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc514Fm0", "I_kwDOLB-lzc6AaUKd", "@tchardin there is no batcher tx version right now, so this would be a new addition correct?", "2024-03-04T23:18:43Z", "2024-03-04T23:18:43Z", "tuxcanfly", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc514J6n", "I_kwDOLB-lzc6AaUKd", "https://github.com/ethereum-optimism/specs/blob/main/specs/protocol/derivation.md#batcher-transaction-format this is a bit ambiguous because it's also called derivationVersion in the implementation so in rollup mode the derivation version byte (`0`) is the batcher tx version byte too.", "2024-03-04T23:36:02Z", "2024-03-04T23:36:02Z", "tchardin", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc514aY2", "I_kwDOLB-lzc6AaUKd", "@tchardin actually now that the rollup config includes `usePlasma` flag I'm not totally sure if this is required. We would need to move the version byte out of the frame `txData` into the plasma input. The additional byte also adds to the cost of L1 calldata that is submitted very frequently.", "2024-03-05T00:24:29Z", "2024-03-05T00:24:29Z", "tuxcanfly", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc52IP6I", "I_kwDOLB-lzc6AaUKd", "@tchardin would this enable switching between rollup mode and plasma mode if we switch the derivation mode based on the version byte on a frame-by-frame basis?", "2024-03-06T21:57:41Z", "2024-03-06T21:57:41Z", "tuxcanfly", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc52JYIw", "I_kwDOLB-lzc6AaUKd", "We can also enable challenges only for plasma mode by having a pre-condition that the version byte must be non-zero. This also derisks bugs or issues with the smart contract system itself, by having a fallback to the rollup mode.", "2024-03-07T01:24:39Z", "2024-03-07T01:24:39Z", "tuxcanfly", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc52Q8q5", "I_kwDOLB-lzc6AaUKd", "@tchardin re: last PR to complete the op-plasma implementation, will get merged before ecotone?", "2024-03-07T18:15:34Z", "2024-03-07T18:15:34Z", "emilianobonassi", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc52Ygb1", "I_kwDOLB-lzc6AaUKd", "@tuxcanfly still pending review but plan is using a tx data type byte as in https://github.com/latticexyz/redstone/pull/20/files, this means op plasma chain can still process regular input from L1 DA but not the way around, so a rollup mode chain would skip input commitments.\r\n\r\n@emilianobonassi ecotone is already merged. If you mean before ecotone is activated, then yes hopefully will be merged before March 14.", "2024-03-08T17:43:08Z", "2024-03-08T17:43:08Z", "tchardin", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc53o_6m", "I_kwDOLB-lzc6AaUKd", "@tchardin @tuxcanfly i think we can close since this is merged, is that correct?\r\n\r\nhttps://github.com/ethereum-optimism/optimism/pull/9845", "2024-03-19T13:51:13Z", "2024-03-19T13:51:13Z", "emilianobonassi", "2025-08-30 09:49:06"]
["IC_kwDOLB-lzc54UG4e", "I_kwDOLB-lzc6AaUKd", "Implemented in\r\n\r\nhttps://github.com/ethereum-optimism/optimism/pull/9845\r\nhttps://github.com/ethereum-optimism/specs/pull/90", "2024-03-25T17:32:05Z", "2024-03-25T17:32:05Z", "trianglesphere", "2025-08-30 09:49:06"]
["IC_kwDODjvEJM54T1oY", "I_kwDODjvEJM6DZV_3", "Try updating your version of foundry", "2024-03-25T16:54:30Z", "2024-03-25T16:54:30Z", "tynes", "2025-08-30 09:50:32"]
["IC_kwDODjvEJM54ZGsW", "I_kwDODjvEJM6DZV_3", "> Try updating your version of foundry\r\n\r\nThank you @tynes , that helps!", "2024-03-26T08:49:00Z", "2024-03-26T08:49:00Z", "cifer76", "2025-08-30 09:50:32"]
["IC_kwDODjvEJM5yzTrM", "I_kwDODjvEJM5z1RXZ", "Was this fixed?", "2024-02-05T01:22:21Z", "2024-02-05T01:22:21Z", "smartcontracts", "2025-08-30 09:50:32"]
["IC_kwDODjvEJM544gP6", "I_kwDODjvEJM5z1RXZ", "fixed in https://github.com/ethereum-optimism/optimism/pull/8235\r\nand https://github.com/ethereum-optimism/optimism/pull/8040\r\n", "2024-03-30T13:57:05Z", "2024-03-30T13:57:05Z", "protolambda", "2025-08-30 09:50:32"]
["IC_kwDOKIwiaM52Hp6P", "I_kwDOKIwiaM53oxv3", "@annieke are you feeling like we still need this? if so, I will prioritize for Q2", "2024-03-06T20:22:26Z", "2024-03-06T20:22:26Z", "cpengilly", "2025-08-30 10:51:35"]
["IC_kwDOKIwiaM55NGju", "I_kwDOKIwiaM53oxv3", "closing, duplicate issue", "2024-04-03T03:48:53Z", "2024-04-03T03:48:53Z", "cpengilly", "2025-08-30 10:51:35"]
["IC_kwDOH2Qg5s55I0zJ", "I_kwDOH2Qg5s6EVNHQ", "Notyed incorrectluy had flag setting network id for 420", "2024-04-02T15:24:03Z", "2024-04-02T15:24:03Z", "angusscott", "2025-08-30 10:51:37"]
["IC_kwDOJ_r-bs54oknK", "I_kwDOJ_r-bs6D1dqd", "For any new chains being added which bump into this issue before it is resolved, they can be added to the exclusion list here:\r\n\r\nhttps://github.com/ethereum-optimism/superchain-registry/blob/fdd0d21715eea9fdfd48eac191e23d1249c59899/validation/superchain-version_test.go#L55-L66\r\n\r\n", "2024-03-27T20:21:57Z", "2024-03-27T20:21:57Z", "geoknee", "2025-08-30 10:52:01"]
["IC_kwDOJ_r-bs54oy-X", "I_kwDOJ_r-bs6D1dqd", "From @mds1:\r\n\r\n> this is just etherscan detecting the wrong address as the implementation, because we use a nonstandard proxy pattern for this contract. The actual implementation lives in the AddressManager, which is at `0x9bFE9c5609311DF1c011c47642253B78a4f33F4B` on sepolia. So you can find the implementation address with:\r\n> ```\r\n> $ cast call 0x9bFE9c5609311DF1c011c47642253B78a4f33F4B 'getAddress(string)(address)' 'OVM_L1CrossDomainMessenger' --rpc-url $SEPOLIA_RPC_URL\r\n> 0xD3494713A5cfaD3F5359379DfA074E2Ac8C6Fd65\r\n> ```\r\n> and you'll see that address has 2.3.0 as the semver.\r\n> \r\n> If you pick a [random tx](https://dashboard.tenderly.co/tx/sepolia/0x8a11c8cb9260c8b11ef2e8ed7e5605bb4fefc1af5587bb767640c268593d9588) to that proxy and look at the trace, you'll see one staticcall addressManager to get the implementation address then a delegatecall to that address", "2024-03-27T20:49:55Z", "2024-03-27T23:15:03Z", "geoknee", "2025-08-30 10:52:01"]
["IC_kwDOJ_r-bs54pKwr", "I_kwDOJ_r-bs6D1dqd", "Does this mean we can revert https://github.com/ethereum-optimism/superchain-registry/pull/148 given this issue is closed? Will the test pass for race?", "2024-03-27T21:55:16Z", "2024-03-27T21:55:16Z", "zchn", "2025-08-30 10:52:01"]
["IC_kwDOJ_r-bs55GuI0", "I_kwDOJ_r-bs6D1dqd", "The test will not pass for race because we currently require contract versions to match op-mainnet or op-sepolia. We may soon relax that so that a wider list of versions is acceptable, in which case we can unskip the test for race and some other chains. ", "2024-04-02T11:43:51Z", "2024-04-02T11:43:51Z", "geoknee", "2025-08-30 10:52:01"]
["IC_kwDODjvEJM5tgx5w", "I_kwDODjvEJM54gR1w", "I wonder what it would take to use biome for linting", "2023-12-03T02:03:58Z", "2023-12-03T02:03:58Z", "tynes", "2025-08-30 10:52:23"]
["IC_kwDODjvEJM5thSW8", "I_kwDODjvEJM54gR1w", "We could use it for linting and I would actually recommend it. It does not have EVERY rule we have in our eslint config but it has all the ones that actually matter.\r\n\r\nI personally use biome for linting and formatting on all projects these days and it's blazingly fast even on large monorepos\r\n<img width=\"610\" alt=\"image\" src=\"https://github.com/ethereum-optimism/optimism/assets/35039927/872627b0-1749-4eb9-8cf9-666bcfc1d840\">\r\n", "2023-12-03T10:53:40Z", "2023-12-03T10:53:40Z", "roninjin10", "2025-08-30 10:52:23"]
["IC_kwDODjvEJM5tyWPP", "I_kwDODjvEJM54gR1w", "I would def prefer removing eslint in favor of biome to reduce the number of deps that we depend on. If it has all the rules that matters, then I would be supportive of using it", "2023-12-06T01:14:01Z", "2023-12-06T01:14:01Z", "tynes", "2025-08-30 10:52:23"]
["IC_kwDODjvEJM5zH1pV", "I_kwDODjvEJM54gR1w", "Hi @tynes ! Did some digging around, there are some rules which are simply unavailable. If there's a list of absolutely required \u2229 unavailable rules, I could open a PR with biomejs and get it accepted or work off of a fork. Are there any such rules? I'd love to get this closed. :)", "2024-02-07T07:27:07Z", "2024-02-07T07:27:07Z", "AbhinavMir", "2025-08-30 10:52:23"]
["IC_kwDODjvEJM5ifwj-", "I_kwDODjvEJM5sSvXB", "Do you want to add additional \"replica\" nodes to the docker compose setup? ie full nodes that are just syncing the network? What is your usecase?", "2023-07-26T21:04:17Z", "2023-07-26T21:04:17Z", "tynes", "2025-08-30 10:52:23"]
["IC_kwDODjvEJM5ihGVl", "I_kwDODjvEJM5sSvXB", "i think im not totally sure what i really need, but, before bedrock, i had this services working with hardhat as L1\r\n\r\n```\r\nl1\r\ndeployer\r\ndtl\r\nl2\r\nrelayer\r\nfault-detector\r\nsubmitter\r\ngasoracle\r\nverifier\r\nreplica\r\nreplica-healthcheck\r\n# and all influxdb services\r\n```\r\nand now just i need to understand\r\n\r\n```\r\nl1\r\nl2\r\nop-batcher\r\nop-proposer\r\nop-node\r\nstateviz\r\n```\r\n+ make+python+nodejs, i really love you xD\r\n\r\nNow we have a realistict L1+L2+NODE(op-node+op-geth) datadirs and block production, but i think we can make more flexible all docker based configurations\r\n\r\nI will list 2 \"maybe\" interesting fixes for this \r\n\r\n1.  still exists `Dockerfile.packages` with different services so is possible to maintain last known configurations as separated docker file, people like \"blockscout explorer\" have a directory with differents users ready `docker-compose.something.yml` files, so with python and make, all is easy to implement and document at this point\r\n\r\n2. I can understand what a L2-node is a sum of op-node+op-geth connected between datadir and genesis from L2 ( maybe im saying something wrong) so, if this is true we can have a separated file named `docker-compose.l2-nodes.yml`, configured with cappabilities to use \"deploy\" tag from docker compose to generate all what user want it\r\n\r\n![image](https://github.com/ethereum-optimism/optimism/assets/11871932/cc29758f-c724-4f02-9fda-111c4c737583)\r\ni based point 2 of this image from this ref https://community.optimism.io/docs/developers/bedrock/node-operator-guide/#deployment-overview\r\n\r\n### Scenarios\r\n\r\n+ Concurrency, ye.. is distributed, but still dapps connect to different endpoints (RPC urls) so can be interesting\r\n+ Ready docker compose configuration for DAPP (not a web, not a contract, just a little config with many comments \"not source compiler users\"\r\n+ Custom blockchain update % of total nodes with new GETH versions and/or L1/L2 upgrades\r\n\r\n\r\n### Just a note\r\n\r\nI did this in local and is working but not at 100% , im testing some things.\r\n\r\n **be carefull if some reader want to copy this in the future**\r\n```\r\nversion: '3.9'\r\nname: optimism\r\n\r\n# Networks to be used by services\r\nnetworks:\r\n  op-network:\r\n    name: op-network\r\n    driver: bridge\r\n\r\n# This Compose file is expected to be used with the devnet-up.sh script.\r\n# The volumes below mount the configs generated by the script into each\r\n# service.\r\n\r\nvolumes:\r\n  l1_data:\r\n  l2_data:\r\n  op_log:\r\n  geth_data:\r\n\r\n\r\nservices:\r\n  l1:\r\n    container_name: l1\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile.l1\r\n    ports:\r\n      - \"8545:8545\"\r\n      - \"8546:8546\"\r\n      - \"9551:8551\"\r\n      - \"7060:6060\"\r\n    volumes:\r\n      - \"l1_data:/db\"\r\n      - \"${PWD}/../.devnet/genesis-l1.json:/genesis.json\"\r\n      - \"${PWD}/test-jwt-secret.txt:/config/test-jwt-secret.txt\"\r\n\r\n  l2:\r\n    container_name: l2\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile.l2\r\n    ports:\r\n      - \"9545:8545\"\r\n      - \"9552:8551\"\r\n      - \"8060:6060\"\r\n    volumes:\r\n      - \"l2_data:/db\"\r\n      - \"${PWD}/../.devnet/genesis-l2.json:/genesis.json\"\r\n      - \"${PWD}/test-jwt-secret.txt:/config/test-jwt-secret.txt\"\r\n    entrypoint:  # pass the L2 specific flags by overriding the entry-point and adding extra arguments\r\n      - \"/bin/sh\"\r\n      - \"/entrypoint.sh\"\r\n      - \"--authrpc.jwtsecret=/config/test-jwt-secret.txt\"\r\n\r\n  op-node:\r\n    container_name: op-node\r\n    depends_on:\r\n      - l1\r\n      - l2\r\n    build:\r\n      context: ../\r\n      dockerfile: ./op-node/Dockerfile\r\n    command: >\r\n      op-node\r\n      --l1=ws://l1:8546\r\n      --l2=http://l2:8551\r\n      --l2.jwt-secret=/config/test-jwt-secret.txt\r\n      --sequencer.enabled\r\n      --sequencer.l1-confs=0\r\n      --verifier.l1-confs=0\r\n      --p2p.sequencer.key=8b3a350cf5c34c9194ca85829a2df0ec3153be0318b5e2d3348e872092edffba\r\n      --rollup.config=/rollup.json\r\n      --rpc.addr=0.0.0.0\r\n      --rpc.port=8545\r\n      --p2p.listen.ip=0.0.0.0\r\n      --p2p.listen.tcp=9003\r\n      --p2p.listen.udp=9003\r\n      --p2p.scoring.peers=light\r\n      --p2p.ban.peers=true\r\n      --snapshotlog.file=/op_log/snapshot.log\r\n      --p2p.priv.path=/config/p2p-node-key.txt\r\n      --metrics.enabled\r\n      --metrics.addr=0.0.0.0\r\n      --metrics.port=7300\r\n      --pprof.enabled\r\n      --rpc.enable-admin\r\n    ports:\r\n      - \"7545:8545\"\r\n      - \"9003:9003\"\r\n      - \"7300:7300\"\r\n      - \"6060:6060\"\r\n    volumes:\r\n      - \"${PWD}/p2p-sequencer-key.txt:/config/p2p-sequencer-key.txt\"\r\n      - \"${PWD}/p2p-node-key.txt:/config/p2p-node-key.txt\"\r\n      - \"${PWD}/test-jwt-secret.txt:/config/test-jwt-secret.txt\"\r\n      - \"${PWD}/../.devnet/rollup.json:/rollup.json\"\r\n      - op_log:/op_log\r\n\r\n  op-proposer:\r\n    container_name: op-proposer\r\n    depends_on:\r\n      - l1\r\n      - l2\r\n      - op-node\r\n    build:\r\n      context: ../\r\n      dockerfile: ./op-proposer/Dockerfile\r\n    ports:\r\n      - \"6062:6060\"\r\n      - \"7302:7300\"\r\n    environment:\r\n      OP_PROPOSER_L1_ETH_RPC: http://op-l1:8545\r\n      OP_PROPOSER_ROLLUP_RPC: http://op-node:8545\r\n      OP_PROPOSER_POLL_INTERVAL: 1s\r\n      OP_PROPOSER_NUM_CONFIRMATIONS: 1\r\n      OP_PROPOSER_MNEMONIC: test test test test test test test test test test test junk\r\n      OP_PROPOSER_L2_OUTPUT_HD_PATH: \"m/44'/60'/0'/0/1\"\r\n      OP_PROPOSER_L2OO_ADDRESS: \"${L2OO_ADDRESS}\"\r\n      OP_PROPOSER_PPROF_ENABLED: \"true\"\r\n      OP_PROPOSER_METRICS_ENABLED: \"true\"\r\n      OP_PROPOSER_ALLOW_NON_FINALIZED: \"true\"\r\n\r\n  op-batcher:\r\n    container_name: op-batcher\r\n    depends_on:\r\n      - l1\r\n      - l2\r\n      - node\r\n    build:\r\n      context: ../\r\n      dockerfile: ./op-batcher/Dockerfile\r\n    ports:\r\n      - \"6061:6060\"\r\n      - \"7301:7300\"\r\n      - \"6545:8545\"\r\n    environment:\r\n      OP_BATCHER_L1_ETH_RPC: http://l1:8545\r\n      OP_BATCHER_L2_ETH_RPC: http://l2:8545\r\n      OP_BATCHER_ROLLUP_RPC: http://op-node:8545\r\n      OP_BATCHER_MAX_CHANNEL_DURATION: 1\r\n      OP_BATCHER_SUB_SAFETY_MARGIN: 4 # SWS is 15, ChannelTimeout is 40\r\n      OP_BATCHER_POLL_INTERVAL: 1s\r\n      OP_BATCHER_NUM_CONFIRMATIONS: 1\r\n      OP_BATCHER_MNEMONIC: test test test test test test test test test test test junk\r\n      OP_BATCHER_SEQUENCER_HD_PATH: \"m/44'/60'/0'/0/2\"\r\n      OP_BATCHER_PPROF_ENABLED: \"true\"\r\n      OP_BATCHER_METRICS_ENABLED: \"true\"\r\n      OP_BATCHER_RPC_ENABLE_ADMIN: \"true\"\r\n\r\n  stateviz:\r\n    build:\r\n      context: ../\r\n      dockerfile: ./ops-bedrock/Dockerfile.stateviz\r\n    command:\r\n      - stateviz\r\n      - -addr=0.0.0.0:8080\r\n      - -snapshot=/op_log/snapshot.log\r\n      - -refresh=10s\r\n    ports:\r\n      - \"9090:8080\"\r\n    volumes:\r\n      - op_log:/op_log:ro\r\n\r\n  create-geth-genesis:\r\n    container_name: create-geth-genesis\r\n    depends_on:\r\n      - l1\r\n      - l2\r\n      - op-node\r\n    build:\r\n      context: ./../op-geth\r\n      dockerfile: ./Dockerfile\r\n    volumes:\r\n      - \"geth_data:/home/datadir/\"\r\n      - \"${PWD}/../.devnet/genesis-l2.json:/config/genesis.json\"\r\n    command: init --datadir /home/datadir /config/genesis.json\r\n\r\n  op-geth:\r\n    container_name: op-geth\r\n    depends_on:\r\n      create-geth-genesis:\r\n        condition: service_completed_successfully\r\n    build:\r\n      context: ./../op-geth\r\n      dockerfile: ./Dockerfile\r\n    ports:\r\n      - \"8549:8545\"\r\n      - \"8550:8546\"\r\n    volumes:\r\n      - \"geth_data:/home/datadir/\"\r\n      - \"${PWD}/test-jwt-secret.txt:/config/jwt.txt\"\r\n      - \"${PWD}/../.devnet/genesis-l2.json:/config/genesis.json\"\r\n    environment:\r\n      L1_RPC: http://l1:8545\r\n      L2_RPC: http://l2:8545\r\n      SEQUENCER_URL: http://op-node:8545\r\n    command: >\r\n      --http\r\n      --http.corsdomain=\"*\"\r\n      --http.vhosts=\"*\"\r\n      --http.addr=0.0.0.0\r\n      --http.api=web3,debug,eth,txpool,net,engine,miner\r\n      --ws\r\n      --ws.addr=0.0.0.0\r\n      --ws.port=8546\r\n      --ws.origins=\"*\"\r\n      --ws.api=web3,debug,eth,txpool,net,engine,miner\r\n      --syncmode=full\r\n      --gcmode=archive\r\n      --nodiscover\r\n      --maxpeers=0\r\n      --networkid=901\r\n      --authrpc.vhosts=\"*\"\r\n      --authrpc.addr=0.0.0.0\r\n      --authrpc.port=8551\r\n      --authrpc.jwtsecret=/config/jwt.txt\r\n      --rollup.sequencerhttp=http://op-node:8545\r\n      --datadir /home/datadir\r\n#     --snapshot=false\r\n#     --rollup.disabletxpoolgossip=true\r\n\r\n```\r\n\r\nlet me know if i'm misunderstanding something please @tynes ", "2023-07-27T03:05:39Z", "2023-07-27T03:08:00Z", "netzulo", "2025-08-30 10:52:23"]
["IC_kwDODjvEJM5ihL8L", "I_kwDODjvEJM5sSvXB", "Are you proposing that we add blockscout to the docker compose setup?\r\n\r\n> i think im not totally sure what i really need\r\n\r\nIt would be helpful for us to know what problem you need solved :)\r\n\r\nThe statements that you made about the new system are generally correct\r\n", "2023-07-27T03:47:19Z", "2023-07-27T03:47:19Z", "tynes", "2025-08-30 10:52:23"]
["IC_kwDODjvEJM5ihV2r", "I_kwDODjvEJM5sSvXB", "close this. i need to think more about my request, and no sense keep it open. XD totally agree", "2023-07-27T04:48:18Z", "2023-07-27T04:48:18Z", "netzulo", "2025-08-30 10:52:23"]
["IC_kwDOKIwiaM56Vml2", "I_kwDOKIwiaM6E2HO3", "hi @22Xy we are working to setup for language/localization support and would appreciate your help! I think we'll be using Crowdin once we're all setup for translating the developer docs, but we'll def share details with the collective when we get to that point. I'll also pass this note along to our DevRel team to see if we can get this project mobilized sooner! \ud83d\udc9f ", "2024-04-12T20:27:46Z", "2024-04-12T20:27:46Z", "cpengilly", "2025-08-30 10:54:14"]
["IC_kwDOKIwiaM56Vncc", "I_kwDOKIwiaM6E2HO3", "closing this ticket for now, but we will be in touch with the community soon!", "2024-04-12T20:30:28Z", "2024-04-12T20:30:28Z", "cpengilly", "2025-08-30 10:54:14"]
["IC_kwDOKIwiaM550CHU", "I_kwDOKIwiaM6Enuhv", "blank request, closing the issue", "2024-04-08T21:37:02Z", "2024-04-08T21:37:02Z", "cpengilly", "2025-08-30 10:54:14"]
["IC_kwDOKIwiaM56NPxY", "I_kwDOKIwiaM6C3ZWx", "Thank you for the great detail on this one @opfocus sorry for the delay", "2024-04-11T18:37:24Z", "2024-04-11T18:37:24Z", "sbvegan", "2025-08-30 10:54:14"]
["IC_kwDOKIwiaM56aecg", "I_kwDOKIwiaM6CN7dK", "shifted to feature prior matrix for tracking, so closing this issue", "2024-04-13T21:03:08Z", "2024-04-13T21:03:08Z", "cpengilly", "2025-08-30 10:54:14"]
["IC_kwDOKIwiaM56aeMn", "I_kwDOKIwiaM6B8M9A", "duplicate issue", "2024-04-13T20:57:57Z", "2024-04-13T20:57:57Z", "cpengilly", "2025-08-30 10:54:14"]
["IC_kwDOKIwiaM52HY28", "I_kwDOKIwiaM5xtURp", "@sbvegan @ZakAyesh wondering if this could be a small issue that Zak could take on to get acclimated to writing docs? \ud83d\ude4f\ud83c\udffe ", "2024-03-06T19:37:00Z", "2024-03-06T19:39:55Z", "cpengilly", "2025-08-30 10:54:14"]
["IC_kwDOKIwiaM550DB_", "I_kwDOKIwiaM5xtURp", "closing", "2024-04-08T21:40:00Z", "2024-04-08T21:40:00Z", "cpengilly", "2025-08-30 10:54:14"]
["IC_kwDODjvEJM56Bpdt", "I_kwDODjvEJM6FOd8x", "I am also encountering this issue.\r\nAnd the optimism version I am using is v1.7.0,  the op-contract version I am using is v1.2.0.", "2024-04-10T11:11:53Z", "2024-04-10T11:11:53Z", "522923331", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM56H3tm", "I_kwDODjvEJM6FOd8x", "Solve the problem by adding the `--with-gas-price` flags.\r\n\r\nWhole command:\r\n```\r\nforge script scripts/Deploy.s.sol:Deploy --private-key $GS_ADMIN_PRIVATE_KEY --broadcast --rpc-url $L1_RPC_URL --with-gas-price 1.7gwei\r\n```\r\nSpecific gas price values can be found in the browser: [Sepolia Blockscout Gas Tracker](https://eth-sepolia.blockscout.com/gas-tracker)", "2024-04-11T03:53:36Z", "2024-04-11T03:53:36Z", "420516460", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM56Igvc", "I_kwDODjvEJM6FOd8x", "I didn't add the `--with-gas-price` flag when I successfully deployed the L1 contract half a month ago\uff0cAnd the guide also does not use this flag.\r\nDo you know why it is needed now?  @420516460 \r\n", "2024-04-11T07:05:26Z", "2024-04-11T07:05:26Z", "522923331", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM56Iynz", "I_kwDODjvEJM6FOd8x", "> I didn't add the `--with-gas-price` flag when I successfully deployed the L1 contract half a month ago\uff0cAnd the guide also does not use this flag. Do you know why it is needed now? @420516460\r\n\r\nyep, i have deployed it successfully before without the need to use `--with-gas-price` flag, in guide use '--slow'.\r\n\r\nDid you need to run `forge script scripts/Deploy.s.sol:Deploy --sig 'sync()' --rpc-url $L1_RPC_URL` when you deployed it before? in the lastest version, these command no longer needs to be executed\r\n\r\nIn optimism v1.7.2 version, there have been many changes to op contracts, for example use `OptimismPortal2` instead of `OptimismPortal`.\r\n\r\n in my opinion, It should be that more contracts need to be deployed and there is a huge gap in `estimate gas`, you can see my error log, it prompts: \r\n ```\r\nEstimated gas price: 0.001137278 gwei\r\n ```\r\n in [sepolia blockscout](https://eth-sepolia.blockscout.com/gas-tracker), the slowest gas price is 3.5gwei, that is a big difference!!! so i think that is the key reason.", "2024-04-11T07:47:46Z", "2024-04-11T07:48:09Z", "420516460", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5envjn", "I_kwDODjvEJM5oXV90", "I think one way to fix this is to add a `logIndex: Number` parameter to some of the `CrossChainMessenger` functions to pick out the withdrawal message from the transaction receipt https://github.com/across-protocol/optimism/commit/2bbdb4dc69d8b563b14fdc498f68ef9e71dc71e1#diff-77164cc5c071d1488e1cf72c0f91fdd99dd8976edb5004c83198bbd517039572", "2023-06-12T14:35:36Z", "2023-06-12T14:35:36Z", "nicholaspai", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5epvuq", "I_kwDODjvEJM5oXV90", "hi, we're aware of this issue and are looking to ship a fix in ~a week! thank you for the report", "2023-06-12T20:03:11Z", "2023-06-12T20:03:11Z", "annieke", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5eu9Bx", "I_kwDODjvEJM5oXV90", "> [](https://github.com/across-protocol/optimism/commit/2bbdb4dc69d8b563b14fdc498f68ef9e71dc71e1#diff-77164cc5c071d1488e1cf72c0f91fdd99dd8976edb5004c83198bbd517039572)\r\n\r\nThis would work but we will get something clean out for ya'll", "2023-06-13T13:54:34Z", "2023-06-13T13:54:34Z", "roninjin10", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5e4iqB", "I_kwDODjvEJM5oXV90", "> > \r\n> \r\n> This would work but we will get something clean out for ya'll\r\n\r\ncc @annieke it also seems there is an additional bug that finalized withdrawals are not getting marked with the correct `RELAYED` status\r\n\r\nI've been able to manually finalize withdrawals locally but I don't see the status updated in the SDK's `getMessageStatus` method\r\n\r\nExample L2 withdrawal:\r\n- https://optimistic.etherscan.io/tx/0xda9e9c8dfc7718bc1499e1e64d8df6cddbabc46e819475a6c755db286a41b9fa\r\n\r\nL1 finalization: \r\n- https://etherscan.io/tx/0x9bc7b3a1056d4a2829ea253e290877fb25d61f07153b638767d2bb8d44e520d3\r\n\r\nPerhaps because these are pre-bedrock?\r\n\r\nUPDATE:\r\n- I've confirmed that these errors are only showing up on pre bedrock withdrawal. I have yet to finalize a post bedrock withdrawal yet so I will know soon", "2023-06-14T19:49:03Z", "2023-06-14T20:15:28Z", "nicholaspai", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5e-y_C", "I_kwDODjvEJM5oXV90", "Hey @nicholaspai, small update.   Based on my investigation of the SDK the \"cleaner solution\" I was hoping to do is a little too risky of a refactor to do as it might cause regressions.   I am currently going with the idea you suggested where you would pass in a `withdrawaIndex`.   By using `populateTransaction` you would be able to aggregate them into a multicall.   The work is being done in this pr https://github.com/ethereum-optimism/optimism/pull/6024 and should be released soon", "2023-06-15T18:10:05Z", "2023-06-15T18:10:05Z", "roninjin10", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5e-zf6", "I_kwDODjvEJM5oXV90", "@nicholaspai regarding the finalization issue you mentioned, prebedrock withdrawals are getting correct message status in our production code atm.    I will test it out myself to try to repro with the withdrawal you shared", "2023-06-15T18:11:54Z", "2023-06-15T18:11:54Z", "roninjin10", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5e-zwy", "I_kwDODjvEJM5oXV90", "> Hey @nicholaspai, small update. Based on my investigation of the SDK the \"cleaner solution\" I was hoping to do is a little too risky of a refactor to do as it might cause regressions. I am currently going with the idea you suggested where you would pass in a `withdrawaIndex`. By using `populateTransaction` you would be able to aggregate them into a multicall. The work is being done in this pr #6024 and should be released soon\r\n\r\nNice thanks. Curious what do you think of the `MESSAGE_STATUS` bug I think exists ([comment](https://github.com/ethereum-optimism/optimism/issues/5983#issuecomment-1591880321) for pre Bedrock withdrawals?\r\n\r\nIt's a bit annoying to have to fix since these withdrawals shouldn't grow over time but there might be some other dApps/deves who run into that bug? We were able to manually withdraw the pre bedrock withdrawals and added some logic to just ignore those withdrawals.", "2023-06-15T18:12:55Z", "2023-06-15T18:12:55Z", "nicholaspai", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5e-6y8", "I_kwDODjvEJM5oXV90", "Yea we will have to repro because all of our legacy withdrawals are working.    My only guess is that it requires the combination of being both a multicall and a legacy withdrawal to repro but I will try to repro today", "2023-06-15T18:37:47Z", "2023-06-15T18:37:47Z", "roninjin10", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5e_Byt", "I_kwDODjvEJM5oXV90", "> Yea we will have to repro because all of our legacy withdrawals are working. My only guess is that it requires the combination of being both a multicall and a legacy withdrawal to repro but I will try to repro today\r\n\r\noh yeah the legacy withdrawal works, the status is just not set correctly from `READY_TO_RELAY` to `RELAYED`", "2023-06-15T19:02:55Z", "2023-06-15T19:02:55Z", "nicholaspai", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5e_nzI", "I_kwDODjvEJM5oXV90", "Right on our end this sdk method is returning RELAYED correctly (haven't tested your withdrawal yet will in a bit)", "2023-06-15T21:18:33Z", "2023-06-15T21:18:49Z", "roninjin10", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5fBa7P", "I_kwDODjvEJM5oXV90", "@nicholaspai thanks for the second report!  I have reproed on your withdrawals but also on testnet.   This is a regression.  You were correct and I was wrong because this issue affects ALL legacy withdrawals.  We'll get a fix in for both issues in asap.", "2023-06-16T07:03:57Z", "2023-06-16T07:45:46Z", "roninjin10", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5fGqIJ", "I_kwDODjvEJM5oXV90", "hey @nicholaspai, I got both fixes in. So you don't have to wait for our normal release you can `npm install @eth-optimism/sdk@0.0.0-20230617021031` to use the version of the sdk with the fixes in it right away.  We should have an official release sometime next week.\r\n\r\nHave a great weekend!", "2023-06-17T02:14:45Z", "2023-06-17T02:14:45Z", "roninjin10", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5fM9E2", "I_kwDODjvEJM5oXV90", "> hey @nicholaspai, I got both fixes in. So you don't have to wait for our normal release you can `npm install @eth-optimism/sdk@0.0.0-20230617021031` to use the version of the sdk with the fixes in it right away. We should have an official release sometime next week.\r\n> \r\n> Have a great weekend!\r\n\r\nthanks! We'll just wait for the normal release as we're running successfully on a fork of the `eth-optimism/sdk` for now", "2023-06-19T13:51:19Z", "2023-06-19T13:51:19Z", "nicholaspai", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5fWmln", "I_kwDODjvEJM5oXV90", "Resolved by this tag: https://github.com/ethereum-optimism/optimism/releases/tag/%40eth-optimism%2Fsdk%403.0.0", "2023-06-21T00:12:11Z", "2023-06-21T00:12:11Z", "annieke", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM5feIKI", "I_kwDODjvEJM5oXV90", "Oops, should be tracking this PR https://github.com/ethereum-optimism/optimism/pull/6053 instead; reopening!", "2023-06-21T21:55:08Z", "2023-06-21T21:55:08Z", "annieke", "2025-08-30 10:54:40"]
["IC_kwDODjvEJM56IhiW", "I_kwDODjvEJM5oXV90", "Error: withdrawal index 0 out of bounds. There are 0 withdrawals this is waht i get\r\n", "2024-04-11T07:07:41Z", "2024-04-11T07:07:41Z", "ahyammona", "2025-08-30 10:54:40"]
["IC_kwDOJ_r-bs55KKg7", "I_kwDOJ_r-bs6EJseS", "I notice there are also these addresses https://github.com/ethereum-optimism/superchain-registry/pull/110/files which should potentially also be added to the script. ", "2024-04-02T18:00:40Z", "2024-04-02T18:00:40Z", "geoknee", "2025-08-30 10:54:42"]
["IC_kwDOJ_r-bs55KKso", "I_kwDOJ_r-bs6EJseS", "@zchn do you know if these addresses can be scraped from the optimism monrepo in the same way as the other addresses?", "2024-04-02T18:01:12Z", "2024-04-02T18:01:12Z", "geoknee", "2025-08-30 10:54:42"]
["IC_kwDOJ_r-bs55Kkzu", "I_kwDOJ_r-bs6EJseS", "We can execute some calls to read this data from the chain based on what we know is already in the deployments directory:\r\n- ProxyAdminOwner: Call e.g. `OptimismPortalProxy.admin()` to get the ProxyAdmin, then `ProxyAdmin.owner()` to get ProxyAdminOwner\r\n- SystemConfigOwner: Call `SystemConfigProxy.owner()`\r\n- Guardian: Call `OptimismPortal.superchainConfig()` to get the SuperchainConfig, then `SuperchainConfig.guardian()` to get the guardian. For bedrock chains (like Mode sepolia) you'd call `OptimismPortal.GUARDIAN()` directly, as there is no SuperchainConfig\r\n- Challenger: `L2OutputOracle.CHALLENGER()` for bedrock chains, `L2OutputOracle.CHALLENGER()` and `L2OutputOracle.challenger()` both work for upgrade 4 (extended pause, op-contracts/v1.2.0) and ugprade 6 (op-contracts/v1.3.0). For Fault proof chains (OP Sepolia), the L2OO is removed and I'm not sure where the challenger role now lives, cc @zchn \r\n\r\nFrom the `add-chain.sh` script `cast call` is the easiest way to do this, e.g. `cast call $OptimismPortalProxy \"admin()(address)\" -r $RPC_URL`", "2024-04-02T18:50:36Z", "2024-04-02T18:51:48Z", "mds1", "2025-08-30 10:54:42"]
["IC_kwDOJ_r-bs55LFOj", "I_kwDOJ_r-bs6EJseS", "Re: reading this information from chain in `add-chain.sh`.\r\n\r\n1. We \"need\" this information to perform checks on it in `CheckSecurityConfigs.s.sol`. But, those checks are actually just reading the information from the chain in the same way. We know that they will pass, unless the output of the script is tampered with or the on chain information changes between running the `add-chain.sh` script and running the test. Is the point of those particular checks in `CheckSecurityConfigs.s.sol` to ensure the newly added addresses are \"statically queryable\" by consumers of the registry (they don't need to make network requests)?\r\n\r\n2. I think we have a goal to only ask contributors to give us minimal information in general, and generate extra data from that minimal data. See https://github.com/ethereum-optimism/security-pod/issues/74, https://github.com/ethereum-optimism/security-pod/issues/75. Putting network calls into `add-chain.sh` seems to be driving in the opposite direction to our goal. But perhaps we need to accept that in the short term and revisit it as part of the longer term refactoring of the repo?\r\n\r\n3. We should also consider updating this list of addresses parsed and exported by the Go module:\r\nhttps://github.com/ethereum-optimism/superchain-registry/blob/4a8560435b4a38dffbf5cc30b6b15eeaae002b1e/superchain/superchain.go#L105-L117", "2024-04-02T19:30:02Z", "2024-04-02T19:30:02Z", "geoknee", "2025-08-30 10:54:42"]
["IC_kwDOJ_r-bs55LT7u", "I_kwDOJ_r-bs6EJseS", "1. Yea my understanding is those checks ensure validity and consistency of al the data provided, because of the fact that chains don't currently provide a \"minimal set of data\" with us auto-generating the rest into output files.\r\n2. I believe you are saying \"putting network calls into `add-chain.sh` takes us in the opposite direction of requiring minimal data, because it results in more data being committed by the contributor\"? If so, I'd argue we already have a fixed set of required input data for contributors, and new network calls in `add-chain.sh` doesn't expand that, and just makes it easier for contributors to automatically fetch data that's currently missing by default (ProxyAdminOwner / SystemConfigOwner / Guardian / Challenger addresses). But to your point, longer term it may be better to have a clear `input` and `output` split where contributors PR a minimal set of data into `input` and we autogenerate everything needed in output, and run checks against that output data. \r\n3. If they aren't currently needed by any consumers, my initial reaction is to avoid it because that takes us further from a goal of https://github.com/ethereum-optimism/superchain-registry/issues/36 by giving go consumers more information", "2024-04-02T20:08:46Z", "2024-04-02T20:08:46Z", "mds1", "2025-08-30 10:54:42"]
["IC_kwDOKIwiaM57P9q2", "I_kwDOKIwiaM6DLsqQ", "duplicate issue, closing", "2024-04-20T21:12:44Z", "2024-04-20T21:12:44Z", "cpengilly", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM52KfOS", "I_kwDOKIwiaM5_Xzzx", "@noobmanog can you pls clarify what page has broken links? \ud83d\ude4f\ud83c\udffe It is not clear from the PR. thank you!", "2024-03-07T05:56:43Z", "2024-03-07T05:56:43Z", "cpengilly", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM57M0mk", "I_kwDOKIwiaM5_Xzzx", "closing, no followup from original poster", "2024-04-19T17:04:41Z", "2024-04-19T17:04:41Z", "cpengilly", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM57P8UM", "I_kwDOKIwiaM56QXa_", "duplicate issue, closing", "2024-04-20T20:41:44Z", "2024-04-20T20:41:44Z", "cpengilly", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM52KOqn", "I_kwDOKIwiaM54-wSX", "@sbvegan I think you might be working on these already. Is that correct? If so, maybe we can combine issues.", "2024-03-07T05:30:47Z", "2024-03-07T05:30:47Z", "cpengilly", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM53IalJ", "I_kwDOKIwiaM54-wSX", "shifted to Q2", "2024-03-15T00:32:54Z", "2024-03-15T00:32:54Z", "cpengilly", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM57P_N1", "I_kwDOKIwiaM54-wSX", "closed, duplicate issue", "2024-04-20T21:41:26Z", "2024-04-20T21:41:26Z", "cpengilly", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM5s11zp", "I_kwDOKIwiaM53pGFw", "Hi. I'm new and have only recently started studying. And the very first and most obvious problem for me was these two points. \"Beginning on OP Minet\" and \"First Contract.\" \r\nIn the \"Beginning on OP Minet\" section, I wanted to find a clear understanding of the structure of the development environment and the relationships between its components. And after that, startening practicing and understanding how it works.", "2023-11-24T20:10:00Z", "2023-11-24T20:10:00Z", "Anton-dot911", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM5s2FgV", "I_kwDOKIwiaM53pGFw", "see [pull 208](https://github.com/ethereum-optimism/docs/pull/208)\r\nrequest for assignment, review, approval and closure.", "2023-11-24T22:34:01Z", "2023-11-24T22:40:44Z", "accessor-io", "2025-08-30 10:56:54"]
["IC_kwDOKIwiaM5uiiPG", "I_kwDOKIwiaM53pGFw", "> Hi. I'm new and have only recently started studying. And the very first and most obvious problem for me was these two points. \"Beginning on OP Minet\" and \"First Contract.\" In the \"Beginning on OP Minet\" section, I wanted to find a clear understanding of the structure of the development environment and the relationships between its components. And after that, startening practicing and understanding how it works.\r\n\r\nThanks for the feedback @Anton-dot911. We've recently updated the [first contract tutorial](https://docs.optimism.io/builders/dapp-developers/tutorials/first-contract), and we'd love to get your feedback on it. We are still working on the getting starting guide and need to scope the ask/request a bit more first.", "2023-12-13T19:03:54Z", "2023-12-13T19:03:54Z", "cpengilly", "2025-08-30 10:56:54"]
["IC_kwDOLB-lzc56vB-z", "I_kwDOLB-lzc6F34Bn", "These are known as [preinstalls](https://specs.optimism.io/protocol/preinstalls.html) so are in a separate section. We use this terminology to differentiate from standard OP Stack predeploys because preinstalls are written by third party developers and consequently we can't assume they have the same security guarantees. \r\n\r\nHowever:\r\n- The create2deployer is in the predeploys section, but should be moved to preinstalls\r\n- The preinstalls are missing an entry from the specification overview (screenshot below)\r\n\r\nSo I'll leave this issue open to account for those\r\n\r\n<img width=\"328\" alt=\"image\" src=\"https://github.com/ethereum-optimism/specs/assets/17163988/55f8f798-7e81-4602-9d3d-ab3b5d9c5c68\">\r\n", "2024-04-16T13:51:12Z", "2024-04-16T13:51:12Z", "mds1", "2025-08-30 10:57:01"]
["IC_kwDOLB-lzc56vOpS", "I_kwDOLB-lzc6F34Bn", "thanks @mds1 - closing", "2024-04-16T14:14:20Z", "2024-04-16T14:14:20Z", "emilianobonassi", "2025-08-30 10:57:01"]
["IC_kwDOKIwiaM57MRAn", "I_kwDOKIwiaM6GTBN6", "I'd like to work on this", "2024-04-19T15:29:07Z", "2024-04-19T15:29:07Z", "richardgreg", "2025-08-30 11:59:05"]
["IC_kwDOKIwiaM57Mq35", "I_kwDOKIwiaM6GTBN6", "Hi @cpengilly, I would love to work on this", "2024-04-19T16:35:59Z", "2024-04-19T16:35:59Z", "JeffreyJoel", "2025-08-30 11:59:05"]
["IC_kwDOKIwiaM57MxfP", "I_kwDOKIwiaM6GTBN6", "hi @richardgreg it's yours!\r\n@JeffreyJoel I'll ping you for a diff issue to see if you're interested \ud83d\udc9f ", "2024-04-19T16:55:42Z", "2024-04-19T16:55:42Z", "cpengilly", "2025-08-30 11:59:05"]
["IC_kwDOKIwiaM57OGgP", "I_kwDOKIwiaM6GTBN6", "@cpengilly, no problem", "2024-04-19T21:16:53Z", "2024-04-19T21:16:53Z", "JeffreyJoel", "2025-08-30 11:59:05"]
["IC_kwDOKIwiaM57P90p", "I_kwDOKIwiaM6GTBN6", "@JeffreyJoel would you be interested in issue #590 ?", "2024-04-20T21:15:39Z", "2024-04-20T21:15:39Z", "cpengilly", "2025-08-30 11:59:05"]
["IC_kwDOKIwiaM57MzX-", "I_kwDOKIwiaM6EHxwW", "per @sbvegan: We could add a general callout that we're testing fault proofs on OP Sepolia, point to the blog post, and say that withdrawal times currently reflect the mainnet time", "2024-04-19T17:01:20Z", "2024-04-19T17:01:20Z", "cpengilly", "2025-08-30 11:59:05"]
["IC_kwDOLB-lzc57Qmo0", "I_kwDOLB-lzc59-9ij", "For singular batch, it's `rlp_encode(SingularBatchType ++ rlp_encode(SingularBatch)) ++ ... ++ rlp_encode(SingularBatchType ++ rlp_encode(SingularBatch))`.\r\n\r\nFor span batch, it's `rlp_encode(SpanBatchType ++ RawSpanBatch.encode(RawSpanBatch))`.", "2024-04-21T08:03:41Z", "2024-04-21T08:11:40Z", "zhiqiangxu", "2025-08-30 11:59:26"]
["IC_kwDOLB-lzc57p7pI", "I_kwDOLB-lzc59-9ij", "Exactly. This is how it should be described in the spec to be 100% clear and unambiguous. It should also be described what happens if the buffer contains additional data after the outer `rlp_encode(...)`. This is probably an error?", "2024-04-24T10:07:39Z", "2024-04-24T10:07:39Z", "Wollac", "2025-08-30 11:59:26"]
["IC_kwDOLB-lzc574szb", "I_kwDOLB-lzc59-9ij", "This is what exactly [this pr](https://github.com/ethereum-optimism/optimism/pull/10290) is trying to do, but it may cause chain split.", "2024-04-26T01:17:57Z", "2024-04-26T01:18:12Z", "zhiqiangxu", "2025-08-30 11:59:26"]
["IC_kwDODjvEJM58C02Q", "I_kwDODjvEJM6HIPZF", "How I solved it:\r\n\r\nUpdated my node version to 20+\r\nUpdated npm version\r\nUpdated pnpm to 9+\r\n\r\nInstalled rust because I faced cargo errors using \r\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\r\n\r\npnpm install:foundry\r\npnpm update:foundry\r\npnpm check:foundry\r\n\r\n\r\nFoundry installation took me more than an hour.\r\n\r\nAfter that ran the basic steps to rebuild for op-node \r\n\r\npnpm install\r\npnpm build\r\nmake op-node", "2024-04-27T18:01:12Z", "2024-04-27T18:01:12Z", "zainirfan13", "2025-08-30 11:59:33"]
["IC_kwDODjvEJM57d89u", "I_kwDODjvEJM6Gk-5j", "See discussion in https://github.com/ethereum-optimism/specs/discussions/140", "2024-04-23T05:38:33Z", "2024-04-23T05:38:33Z", "tynes", "2025-08-30 11:59:33"]
["IC_kwDODjvEJM56qd3M", "I_kwDODjvEJM6Fwk-A", "Generating artifacts in this style is out of date. No need to call `sync()` anymore, just run the following command:\r\n\r\n```bash\r\nforge script -vvv scripts/Deploy.s.sol:Deploy\r\n```\r\n\r\nAnd include `--broadcast` and private key material for live deploys", "2024-04-15T23:01:13Z", "2024-04-15T23:01:13Z", "tynes", "2025-08-30 11:59:33"]
["IC_kwDODjvEJM560oM2", "I_kwDODjvEJM6Fwk-A", "> Generating artifacts in this style is out of date. No need to call `sync()` anymore, just run the following command:\r\n> \r\n> ```shell\r\n> forge script -vvv scripts/Deploy.s.sol:Deploy\r\n> ```\r\n> \r\n> And include `--broadcast` and private key material for live deploys\r\n\r\ni 'm running the command:\r\n`forge script -vvv scripts/Deploy.s.sol:Deploy  --rpc-url $L1_RPC_URL --broadcast --private-key $GS_ADMIN_PRIVATE_KEY `\r\nit seems not work well. Without calling 'sync()', the command is just the same as the deploy command, and no artifacts are generated, which is needed when generating the L2 config files. \r\n", "2024-04-17T07:50:31Z", "2024-04-17T07:51:08Z", "wy51084915", "2025-08-30 11:59:33"]
["IC_kwDODjvEJM566Uu9", "I_kwDODjvEJM6Fwk-A", "you dont need the artifacts anymore to generate the genesis and rollup files. The op-node now takes a new flag `--l1-deploments` which will take a json file as an input like [this](https://github.com/ethereum-optimism/superchain-registry/blob/main/superchain/extra/addresses/sepolia/base.json). so just create a json file and put your contract addresses", "2024-04-17T19:57:25Z", "2024-04-17T19:57:25Z", "nitantchhajed", "2025-08-30 11:59:33"]
["IC_kwDODjvEJM566X2J", "I_kwDODjvEJM6Fwk-A", "you can refer to this - https://github.com/ethereum-optimism/developers/discussions/329#discussion-6441347", "2024-04-17T20:03:46Z", "2024-04-17T20:03:46Z", "nitantchhajed", "2025-08-30 11:59:33"]
["IC_kwDODjvEJM57nk4V", "I_kwDODjvEJM6Fwk-A", "Thanks for the help @nitantchhajed !", "2024-04-24T03:50:56Z", "2024-04-24T03:50:56Z", "tynes", "2025-08-30 11:59:33"]
["IC_kwDOKIsnqM56yjvY", "I_kwDOKIsnqM6F3SOR", "Thanks for the suggestion! Can you expand on your vision here a bit more? We have some ideas for how to improve tooling in this repo, but it's mainly around safety checks around state changes, for example running various assertions about the state diff in a `_postCheck` method. See [here](https://github.com/ethereum-optimism/superchain-ops/blob/main/tasks/sep/metal-001-MCP-L1/SignFromJson.s.sol) for an example. \r\n\r\nI'm not too familiar with forge-proposal-simulator, but it seems to basically use a similar approach with the `_validate` method?", "2024-04-16T23:37:09Z", "2024-04-16T23:37:09Z", "mds1", "2025-08-30 12:01:02"]
["IC_kwDOKIsnqM57tKPU", "I_kwDOKIsnqM6F3SOR", "The Forge Proposal Simulator is designed to help eliminate bugs in governance proposals. It does this in two ways:\r\n\r\n1. Explicitly ask the developer to write validations tailored to each proposal, which adds safety checks and lets you know the actions that were run made the expected state changes.\r\n2. Allows you to wire together the governance proposals into the integration tests. Let's say optimism has a huge integration test suite just for the smart contracts in another repo, which I would imagine to be the case. With a little bit of work connecting this framework to your integration tests, now all governance proposals will run against your integration test suite. Say there is a bug caused by a change in a governance proposal where some part of the smart contract system on L1, or L2 will not work after the proposal passes. If an integration test existed for that behavior, you would be able to catch this bug in the governance proposal before it goes live.\r\n\r\nTo properly integrate this tool and get the full benefits, you would likely need to move your governance proposal creation to the repo where the smart contracts and integration test suite lives, but I'm sure there's other ways around this if you don't want to move things around too much.\r\n\r\nIn most smart contract protocols today, governance proposals and deployment scripts are some of the least audited parts of the codebase because they are expected to be secure, or not do anything incorrectly. These assumptions often turn out to be wrong, so we built this tool to help solve this pain point and hopefully get rid of bugs that could be easily caught with checks and a run against the existing integration test suite post proposal.\r\n\r\nThe main benefit you would get from day 1 using this tool is there would not be a need to write a [JSON](https://github.com/ethereum-optimism/superchain-ops/blob/main/tasks/sep/metal-001-MCP-L1/input.json) file that specifies all targets, calls, parameters, and values. Instead, you could write the proposal straight in readable Solidity, which would then be extracted into calls for the multisig wallet to make via multicall. This is what a proposal would look like from an example in our docs: \r\n<img width=\"776\" alt=\"Screenshot 2024-04-24 at 10 00 03\u202fAM\" src=\"https://github.com/ethereum-optimism/superchain-ops/assets/34463580/3a1d6845-a52b-4d3e-89a3-88d2911ebb51\">\r\nYou just write the proposal in plain solidity, and then the tool extracts the calldata, pranks as the multisig, and applies the state changes. It includes support for Gnosis Safes, Openzeppelin Timelock Controllers and Governor Bravo/Timelock DAO combinations. Once you've generated a multisig proposal, and run everything locally, all tests and validations pass, it provides a convenient calldata logging functionality, which you can then take, and paste straight into the Gnosis Safe UI.\r\n\r\nYou can learn more about Forge Proposal Simulator in the [docs](https://docs.soliditylabs.io/forge-proposal-simulator), [reading the code](https://github.com/solidity-labs-io/forge-proposal-simulator/tree/main), or reach out directly and I can walk you through things.", "2024-04-24T17:05:59Z", "2024-04-24T17:05:59Z", "ElliotFriedman", "2025-08-30 12:01:02"]
["IC_kwDOKIsnqM57tUWc", "I_kwDOKIsnqM6F3SOR", "Another benefit is you could consolidate and make your addresses system easier to use https://github.com/ethereum-optimism/superchain-ops/blob/main/tasks/sep/metal-001-MCP-L1/SignFromJson.s.sol#L396-L404\r\n\r\nInstead would look more like this `addresses.getAddress(\"ADDRESS_NAME\")`.\r\n\r\nhttps://docs.soliditylabs.io/forge-proposal-simulator/guides/multisig-proposal#setting-up-the-addresses-json", "2024-04-24T17:31:46Z", "2024-04-24T17:31:46Z", "ElliotFriedman", "2025-08-30 12:01:02"]
["IC_kwDODjvEJM560lZs", "I_kwDODjvEJM6F9bTM", "It seems your l1 rpc service is rate-limited.", "2024-04-17T07:47:09Z", "2024-04-17T07:47:09Z", "zhiqiangxu", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM5607Qk", "I_kwDODjvEJM6F9bTM", "> It seems your l1 rpc service is rate-limited.\r\n\r\nyes! i have found the error on  [docs.infura.io](url). Thanks!\r\nBut this may not be the real reason. I am using the l1 rpc service on infura and i have checked the requests sent to the infura node. All batch requests failed while other requests survived. i guess the point is that \"batch item count exceeded\", i will try to limit the batch-size tomorrow.", "2024-04-17T08:27:35Z", "2024-04-17T09:32:40Z", "wy51084915", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56_Wf6", "I_kwDODjvEJM6F9bTM", "@wy51084915 You can fine tune the batch size by the `l1.rpc-max-batch-size` parameter.", "2024-04-18T09:26:13Z", "2024-04-18T09:26:29Z", "zhiqiangxu", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56_X2V", "I_kwDODjvEJM6F9bTM", "> @wy51084915 You can fine tune the batch size by the `l1.rpc-max-batch-size` parameter.\r\n\r\nyes! That is what i planed to do yesterday and that worked!\r\nThanks for your reply!", "2024-04-18T09:29:15Z", "2024-04-18T09:29:15Z", "wy51084915", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM564txd", "I_kwDODjvEJM6F0HL9", "Oops, the real reason seems to be mis-configuration of `max_channel_duration`", "2024-04-17T16:14:43Z", "2024-04-17T16:14:43Z", "zhiqiangxu", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56cSob", "I_kwDODjvEJM6FqP7k", "Also loses the level if you specify `--log.format=logfmt` or `--log.format=json`.  The logfmt and json handlers created default to setting their level to `info` which is from the `op-geth` code.  The JSON version seems to have been fixed in the latest `op-geth` (https://github.com/ethereum-optimism/op-geth/blob/d9f03bf5a4c0f3ae18cc8a4a0dd764509cdf6cbb/log/handler.go#L117-L141) but that hasn't been pulled into the monorepo yet.  The logfmt version has a separate method that allows specifying the level.", "2024-04-15T00:17:31Z", "2024-04-15T00:17:31Z", "ajsutton", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56Jkdb", "I_kwDODjvEJM6FWFZa", "can you show more specify foundry version? in path xxx/optimism to run command:\r\n`forge --version`\r\n\r\nwill get like this: \r\n`forge 0.2.0 (4603195 2024-04-10T00:17:25.834634784Z)`\r\n\r\nand what is your optimism branch version number? is it `tutorials/chain`?", "2024-04-11T09:50:36Z", "2024-04-11T09:55:26Z", "420516460", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56NokB", "I_kwDODjvEJM6FWFZa", "i have got the same problem \r\n\r\nThe forge version is \r\nforge 0.2.0 (14daacf 2024-04-09T00:17:09.802796000Z)\r\n\r\nand yes it is the tutorials/chain", "2024-04-11T19:41:07Z", "2024-04-11T19:41:07Z", "Ajmali124", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56PU8l", "I_kwDODjvEJM6FWFZa", "> i have got the same problem\r\n> \r\n> The forge version is forge 0.2.0 (14daacf 2024-04-09T00:17:09.802796000Z)\r\n> \r\n> and yes it is the tutorials/chain\r\n\r\nyour version is right, have you checked if contracts have been deployed to L1? \r\n\r\nin the latest version of optimism(v1.7.2+), we don\u2018t rely on the artifacts, so it's fine in latest version no need to run this command.\r\n\r\ni m not sure this changes if have been sync to the branch `tutorials/chain`, i will retry what you did", "2024-04-12T02:08:09Z", "2024-04-12T02:09:05Z", "420516460", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56Qy4f", "I_kwDODjvEJM6FWFZa", "> can you show more specify foundry version? in path xxx/optimism to run command: `forge --version`\r\n> \r\n> will get like this: `forge 0.2.0 (4603195 2024-04-10T00:17:25.834634784Z)`\r\n> \r\n> and what is your optimism branch version number? is it `tutorials/chain`?\r\n\r\nyes, i got forge version == 0.2.0 and i'm under the branch `tutorials/chain`\r\ni finally choose to skip the step and just copy the artifacts from its peer folder(sepolia), which may be generated by optimism earlier, and that worked.", "2024-04-12T07:58:51Z", "2024-04-12T07:59:24Z", "wy51084915", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56Q08a", "I_kwDODjvEJM6FWFZa", "> > i have got the same problem\r\n> > The forge version is forge 0.2.0 (14daacf 2024-04-09T00:17:09.802796000Z)\r\n> > and yes it is the tutorials/chain\r\n> \r\n> your version is right, have you checked if contracts have been deployed to L1?\r\n> \r\n> in the latest version of optimism(v1.7.2+), we don\u2018t rely on the artifacts, so it's fine in latest version no need to run this command.\r\n> \r\n> i m not sure this changes if have been sync to the branch `tutorials/chain`, i will retry what you did\r\n\r\nyes , the deployment is successful , but under the branch \"tutorials/chain\", to generate the L2's genesis files need the artifacts.So i have to get it.", "2024-04-12T08:04:06Z", "2024-04-12T08:04:06Z", "wy51084915", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56Q8iR", "I_kwDODjvEJM6FWFZa", "> > > i have got the same problem\r\n> > > The forge version is forge 0.2.0 (14daacf 2024-04-09T00:17:09.802796000Z)\r\n> > > and yes it is the tutorials/chain\r\n> > \r\n> > \r\n> > your version is right, have you checked if contracts have been deployed to L1?\r\n> > in the latest version of optimism(v1.7.2+), we don\u2018t rely on the artifacts, so it's fine in latest version no need to run this command.\r\n> > i m not sure this changes if have been sync to the branch `tutorials/chain`, i will retry what you did\r\n> \r\n> yes , the deployment is successful , but under the branch \"tutorials/chain\", to generate the L2's genesis files need the artifacts.So i have to get it.\r\n\r\nwooo nice! yep i have met some similar issues, the `artifacts` are practically no longer needed at this time, but the documentation hasn't been updated yet.\r\n\r\ni m guessing you need to change this when you run the `go run cmd/main.go genesis l2 \\` step:\r\n```\r\n...\r\n --deployment-dir . /packages/contracts-bedrock/deployments/getting-started/ \\\r\n...\r\n```\r\nthis flag need `artifacts`.\r\n\r\n", "2024-04-12T08:22:51Z", "2024-04-12T08:22:51Z", "420516460", "2025-08-30 12:01:11"]
["IC_kwDODjvEJM56RJA9", "I_kwDODjvEJM6FWFZa", "> > > > i have got the same problem\r\n> > > > The forge version is forge 0.2.0 (14daacf 2024-04-09T00:17:09.802796000Z)\r\n> > > > and yes it is the tutorials/chain\r\n> > > \r\n> > > \r\n> > > your version is right, have you checked if contracts have been deployed to L1?\r\n> > > in the latest version of optimism(v1.7.2+), we don\u2018t rely on the artifacts, so it's fine in latest version no need to run this command.\r\n> > > i m not sure this changes if have been sync to the branch `tutorials/chain`, i will retry what you did\r\n> > \r\n> > \r\n> > yes , the deployment is successful , but under the branch \"tutorials/chain\", to generate the L2's genesis files need the artifacts.So i have to get it.\r\n> \r\n> wooo nice! yep i have met some similar issues, the `artifacts` are practically no longer needed at this time, but the documentation hasn't been updated yet.\r\n> \r\n> i m guessing you need to change this when you run the `go run cmd/main.go genesis l2 \\` step:\r\n> \r\n> ```\r\n> ...\r\n>  --deployment-dir . /packages/contracts-bedrock/deployments/getting-started/ \\\r\n> ...\r\n> ```\r\n> \r\n> this flag need `artifacts`.\r\n\r\nyes, just change the `--deployment-dir . /packages/contracts-bedrock/deployments/getting-started/ ` to `--deployment-dir . /packages/contracts-bedrock/deployments/sepolia/` , and everything will be fine", "2024-04-12T08:47:20Z", "2024-04-12T08:47:20Z", "wy51084915", "2025-08-30 12:01:11"]
["IC_kwDOJ_r-bs56K-0u", "I_kwDOJ_r-bs6FYP-Z", "Note that `semver.yaml` is already somewhat in this direction. However:\r\n\r\n* it lives in `superchain` module \r\n* it is consumed by downstream packages such as `op-chain-ops`\r\n* yaml not toml", "2024-04-11T13:29:44Z", "2024-04-11T13:29:44Z", "geoknee", "2025-08-30 12:01:29"]
["IC_kwDOJ_r-bs562lyx", "I_kwDOJ_r-bs6FYP-Z", "Suggestion from @mds1 about using scientific notation for numbers to make them easier for humans to parse:\r\n\r\nhttps://github.com/ethereum-optimism/superchain-registry/pull/185#discussion_r1568032360\r\n", "2024-04-17T12:20:06Z", "2024-04-17T12:20:06Z", "geoknee", "2025-08-30 12:01:29"]
["IC_kwDOI7W0xc50rnkr", "I_kwDOI7W0xc5_3H7x", "very very goooood", "2024-02-21T18:14:16Z", "2024-02-21T18:14:16Z", "habibiman62", "2025-08-30 12:01:47"]
["IC_kwDOI7W0xc50vB8n", "I_kwDOI7W0xc5_3H7x", "no content", "2024-02-22T01:15:35Z", "2024-02-22T01:15:35Z", "MSilb7", "2025-08-30 12:01:47"]
["IC_kwDOI7W0xc58JPOQ", "I_kwDOI7W0xc5_3H7x", "ok . tnx", "2024-04-29T13:44:42Z", "2024-04-29T13:44:42Z", "habibiman62", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s56VgHr", "I_kwDOH2Qg5s6FkCNz", "Found the issue: see https://github.com/ethereum-optimism/op-geth/blob/9e50b7db766762138a0db913fe85e7a2cb82c832/core/state_transition.go#L442\r\n\r\nWhen the tracer reports gasUsed, it reports it based on the gasUsed that was determined based on remaining gas in `CaptureTxEnd`. For deposits we don't have refunds, hence 0 remaining gas. But that doesn't mean all gas was used. Hence the incorrect tracer value.\r\n\r\nIn the implementation the gas-price is 0, and so a 0 refund is given to deposits, without skipping the refund code-path.\r\n\r\nI think we can reduce the diff, and just report the actual `remainingGas` to the tracer.\r\nWould like some feedback here from @trianglesphere and/or @axelKingsley.\r\n", "2024-04-12T20:09:36Z", "2024-04-12T20:09:36Z", "protolambda", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s56WLNz", "I_kwDOH2Qg5s6FkCNz", "@smartcontracts actually added the `CaptureTxEnd(0)` pretty recently to handle the case of failed deposit transactions.", "2024-04-12T22:21:03Z", "2024-04-12T22:21:03Z", "trianglesphere", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s58VYNf", "I_kwDOH2Qg5s6FkCNz", "Bump \ud83d\ude47\u200d\u2642\ufe0f ", "2024-04-30T16:53:23Z", "2024-04-30T16:53:23Z", "quickchase", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s554ICy", "I_kwDOH2Qg5s6FGkDf", "Try replacing --networkid=10 with --op-network=op-mainnet", "2024-04-09T11:11:21Z", "2024-04-09T11:11:21Z", "opfocus", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s554XRD", "I_kwDOH2Qg5s6FGkDf", "The service is up and running, synchronising headers now.", "2024-04-09T11:25:48Z", "2024-04-09T11:25:48Z", "YuXiaoCoder", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s555N9W", "I_kwDOH2Qg5s6FGkDf", "@opfocus \r\nHow long will this process take?\r\n```log\r\nINFO [04-09|20:20:21.614] Forkchoice requested sync to new head    number=118,533,222 hash=fedefe..c029ba\r\nINFO [04-09|20:20:24.252] Forkchoice requested sync to new head    number=118,533,223 hash=177908..522036\r\nINFO [04-09|20:20:26.248] Forkchoice requested sync to new head    number=118,533,224 hash=0f1105..158aec\r\nINFO [04-09|20:20:27.930] Forkchoice requested sync to new head    number=118,533,225 hash=8849a9..6897c1\r\n```", "2024-04-09T12:21:08Z", "2024-04-09T12:21:08Z", "YuXiaoCoder", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s556-ZJ", "I_kwDOH2Qg5s6FGkDf", "You also need peers for snap sync, you should ideally remove\r\n\r\n```text\r\n--nodiscover --maxpeers=0\r\n```", "2024-04-09T15:36:38Z", "2024-04-09T15:36:38Z", "quickchase", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s557C7p", "I_kwDOH2Qg5s6FGkDf", "> @opfocus How long will this process take?\r\n> \r\n> ```\r\n> INFO [04-09|20:20:21.614] Forkchoice requested sync to new head    number=118,533,222 hash=fedefe..c029ba\r\n> INFO [04-09|20:20:24.252] Forkchoice requested sync to new head    number=118,533,223 hash=177908..522036\r\n> INFO [04-09|20:20:26.248] Forkchoice requested sync to new head    number=118,533,224 hash=0f1105..158aec\r\n> INFO [04-09|20:20:27.930] Forkchoice requested sync to new head    number=118,533,225 hash=8849a9..6897c1\r\n> ```\r\n\r\nSorry, I have not run  a mainnet node . You can observe the log of the op-node `sync process `to get a rough estimate.", "2024-04-09T15:45:32Z", "2024-04-09T15:45:32Z", "opfocus", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s55-IPH", "I_kwDOH2Qg5s6FGkDf", "> You also need peers for snap sync, you should ideally remove\r\n> \r\n> ```\r\n> --nodiscover --maxpeers=0\r\n> ```\r\nI removed this flag and geth started syncing up, thanks\r\n", "2024-04-10T02:05:58Z", "2024-04-10T02:05:58Z", "YuXiaoCoder", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s55-IS3", "I_kwDOH2Qg5s6FGkDf", "> > @opfocus How long will this process take?\r\n> > ```\r\n> > INFO [04-09|20:20:21.614] Forkchoice requested sync to new head    number=118,533,222 hash=fedefe..c029ba\r\n> > INFO [04-09|20:20:24.252] Forkchoice requested sync to new head    number=118,533,223 hash=177908..522036\r\n> > INFO [04-09|20:20:26.248] Forkchoice requested sync to new head    number=118,533,224 hash=0f1105..158aec\r\n> > INFO [04-09|20:20:27.930] Forkchoice requested sync to new head    number=118,533,225 hash=8849a9..6897c1\r\n> > ```\r\n> \r\n> Sorry, I have not run a mainnet node . You can observe the log of the op-node `sync process `to get a rough estimate.\r\n\r\nThank you very much, I will check it out", "2024-04-10T02:06:24Z", "2024-04-10T02:06:24Z", "YuXiaoCoder", "2025-08-30 12:01:47"]
["IC_kwDOH2Qg5s58IjOe", "I_kwDOH2Qg5s6EHl4C", "\"missing trie node\" happens when the node is started after an unclean shutdown. Try the \"path\" DB scheme instead of the \"hash\" DB scheme (see upstream geth docs) to not be as prone to this type of DB issue in geth, don't force-shutdown geth, and resync with snap-sync to get back up and running.", "2024-04-29T12:34:48Z", "2024-04-29T12:34:48Z", "protolambda", "2025-08-30 12:01:47"]
["IC_kwDOFpg0Ns5743VM", "I_kwDOFpg0Ns6Domz2", "@trador-rising \r\n\r\nHi there! Thanks for your patience waiting for a reply! Here's an example of a token that is doing something similar (different symbol and name on L1 compared to Base). https://github.com/ethereum-optimism/ethereum-optimism.github.io/pull/697\r\n\r\nIn the near future we'll be transitioning away from the official Base bridge in favor of further decentralizing and directing users to other bridges in the community, e.g. https://superbridge.app/base\r\n\r\nCommunity bridges have their own processes for enabling you to include your own token, but being part of the Superchain Token List (this repo) is an important step.\r\n\r\nHope that helps clarify! :)", "2024-04-26T02:13:23Z", "2024-04-26T02:13:23Z", "wbnns", "2025-08-30 12:01:58"]
["IC_kwDOFpg0Ns576XOG", "I_kwDOFpg0Ns6Domz2", "Hi @wbnns thanks very much for the reply! It's exciting to know that this will be possible. Are you sure https://github.com/ethereum-optimism/ethereum-optimism.github.io/pull/697 is the correct link? It looks like this token has the same ticker and name on Base and ETH: Open Ecosystem Token (OPN)", "2024-04-26T08:28:18Z", "2024-04-26T08:28:18Z", "trador-rising", "2025-08-30 12:01:58"]
["IC_kwDOFpg0Ns5779l_", "I_kwDOFpg0Ns6Domz2", "@trador-rising Ah, indeed, apologies -- I am mistaken here. :/ I'm not sure I have another reference then where a different name and symbol has been used. I would consider keeping it the same based on this, and possibly deploying a new native token with the desired symbol and name that can be airdropped/claimed by existing holders.", "2024-04-26T12:44:18Z", "2024-04-26T12:45:43Z", "wbnns", "2025-08-30 12:01:58"]
["IC_kwDOFpg0Ns5745QQ", "I_kwDOFpg0Ns6DDP3N", "@faces-of-eth Come hang out in the Base channel on Farcaster, there are a lot of memecoins regularly launching there :) https://warpcast.com/~/channel/base", "2024-04-26T02:20:23Z", "2024-04-26T02:20:23Z", "wbnns", "2025-08-30 12:01:58"]
["IC_kwDOFpg0Ns5ynCuK", "I_kwDOFpg0Ns59arhq", "@chicken-juju we've received a grant from the Optimism Foundation to build a Superchain bridge interface. You can find it here https://superbridge.app, we support Base and all the tokens in this token list.", "2024-02-02T05:09:08Z", "2024-02-02T05:09:08Z", "AlexBHarley", "2025-08-30 12:01:58"]
["IC_kwDOFpg0Ns5745zl", "I_kwDOFpg0Ns59arhq", "@chicken-juju \r\n\r\n+1 https://superbridge.app/base that Alex mentioned \ud83d\udc4d", "2024-04-26T02:22:05Z", "2024-04-26T02:22:05Z", "wbnns", "2025-08-30 12:01:58"]
["IC_kwDOJ_r-bs58wR59", "I_kwDOJ_r-bs6HrHK3", "Closing in favor of https://github.com/ethereum-optimism/client-pod/issues/801", "2024-05-03T13:32:20Z", "2024-05-03T13:32:20Z", "bitwiseguy", "2025-08-30 12:02:12"]
["IC_kwDOJ_r-bs55rjCE", "I_kwDOJ_r-bs6E1rlu", "We should also perform a check that all existing files are consistent with the conventions above and modify them if so", "2024-04-07T12:43:35Z", "2024-04-07T12:43:35Z", "mds1", "2025-08-30 12:02:12"]
["IC_kwDOKIwiaM583XPL", "I_kwDOKIwiaM6G9zAq", "first item addressed, along with few other related updates. second item is not a duplicate, but added clarification around that (base fee and blob base fee are two parameters that must be set by chain ops)", "2024-05-05T17:32:43Z", "2024-05-05T17:32:43Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM58gyBM", "I_kwDOKIwiaM6GpLiw", "Hey @sbvegan, can I work on this issue? And just to confirm, should this update be applied to both the \"Published to Ethereum / Safe\" and \"Finalized\" subsections?", "2024-05-01T19:23:52Z", "2024-05-01T19:23:52Z", "MukulKolpe", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57P92f", "I_kwDOKIwiaM6DLjA0", "@JeffreyJoel would you be interested in taking this issue?", "2024-04-20T21:16:12Z", "2024-04-20T21:16:12Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57Q2Bx", "I_kwDOKIwiaM6DLjA0", "@cpengilly, I am open to working on this, but I would like a bit more clarity on this.\r\n", "2024-04-21T11:39:10Z", "2024-04-21T11:39:10Z", "JeffreyJoel", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM583d5D", "I_kwDOKIwiaM6DLjA0", "@JeffreyJoel I agree! let's find you a diff issue to work on. We'll be in touch soon! \ud83d\udc9f ", "2024-05-05T19:05:21Z", "2024-05-05T19:05:21Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM583pYQ", "I_kwDOKIwiaM6DLjA0", "@cpengilly, I'm sorry for not being able to work on this sooner", "2024-05-05T21:48:06Z", "2024-05-05T21:48:06Z", "JeffreyJoel", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57P-aR", "I_kwDOKIwiaM6Crp1W", "@sbvegan I only see a few legacy contracts (with 'legacy' in the title). Are there others? Also, since this page is autogenerated from superchain registry, we wouldnt' be able to mark these on our end. We'd need to put in a request to that team to add it on their side (so it auto-populates in the docs)", "2024-04-20T21:24:05Z", "2024-04-20T21:24:05Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM583e9W", "I_kwDOKIwiaM6Crp1W", "@smartcontracts FV", "2024-05-05T19:21:14Z", "2024-05-05T19:21:14Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM583flN", "I_kwDOKIwiaM6Crp1W", "pinged an engineer for confirmation/context", "2024-05-05T19:31:36Z", "2024-05-05T19:31:36Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM54V4UR", "I_kwDOKIwiaM6CLrHD", "I can take this. can you assign this to me?", "2024-03-25T22:05:52Z", "2024-03-25T22:05:52Z", "estheroche", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM54pS4E", "I_kwDOKIwiaM6CLrHD", "Hey @estheroche, thanks for taking this on! Let me know if you need more context", "2024-03-27T22:24:56Z", "2024-03-27T22:24:56Z", "sbvegan", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57QzJX", "I_kwDOKIwiaM6CLrHD", "@cpengilly please help, i have been facing an error trying to install [install pnpm] on my Macbook terminal. \r\n\r\nmac@Macs-MacBook-Pro-4 docs %  install pnpm \r\nusage: install [-bCcpSsv] [-B suffix] [-f flags] [-g group] [-m mode]\r\n               [-o owner] file1 file2\r\n       install [-bCcpSsv] [-B suffix] [-f flags] [-g group] [-m mode]\r\n               [-o owner] file1 ... fileN directory\r\n       install -d [-v] [-g group] [-m mode] [-o owner] directory ...\r\n       \r\n       this is the error.\r\n\r\nplease how can i rectify this? so that i can contribute", "2024-04-21T10:55:15Z", "2024-04-21T10:55:15Z", "estheroche", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57WyQ1", "I_kwDOKIwiaM6CLrHD", "@estheroche sorry! when we didn't hear back, I took on the issue myself, so let's find you another issue! In the meantime, I will have devsupport reach out to resolve the pnpm install error. \ud83d\udc9f ", "2024-04-22T14:05:07Z", "2024-04-22T14:05:07Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57bmAR", "I_kwDOKIwiaM6CLrHD", "@cpengilly thank you for your response\r\n", "2024-04-22T19:51:08Z", "2024-04-22T19:51:08Z", "estheroche", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57bou3", "I_kwDOKIwiaM6CLrHD", "@cpengilly i will be expecting feedback.", "2024-04-22T19:54:40Z", "2024-04-22T19:54:40Z", "estheroche", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57b1_K", "I_kwDOKIwiaM6CLrHD", "@estheroche I believe we might have fixed the issue. Can you try `pnpm install `again and let us know?", "2024-04-22T20:28:48Z", "2024-04-22T20:29:17Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM57hje0", "I_kwDOKIwiaM6CLrHD", "@cpengilly its working now. thank you", "2024-04-23T13:58:38Z", "2024-04-23T13:58:38Z", "estheroche", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM514CJO", "I_kwDOKIwiaM5-S3KD", "Release notes:\r\n- https://github.com/ethereum-optimism/optimism/releases/tag/op-contracts%2Fv1.0.0\r\n- https://github.com/ethereum-optimism/optimism/releases/tag/op-contracts%2Fv1.1.0\r\n- https://github.com/ethereum-optimism/optimism/releases/tag/op-contracts%2Fv1.2.0\r\n- https://github.com/ethereum-optimism/optimism/releases/tag/op-contracts%2Fv1.3.0-rc.1", "2024-03-04T23:05:32Z", "2024-03-04T23:05:32Z", "sbvegan", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM514IP0", "I_kwDOKIwiaM5-S3KD", "https://github.com/ethereum-optimism/optimism?tab=readme-ov-file#development-and-release-process", "2024-03-04T23:29:24Z", "2024-03-04T23:29:24Z", "sbvegan", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM52KanK", "I_kwDOKIwiaM5-S3KD", "@ZakAyesh @sbvegan also adding to list for Mach completion (Q1)", "2024-03-07T05:49:35Z", "2024-03-07T05:49:35Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM59INHU", "I_kwDOKIwiaM5-S3KD", "completed a while ago", "2024-05-07T21:00:39Z", "2024-05-07T21:00:39Z", "sbvegan", "2025-08-30 13:04:25"]
["IC_kwDOKIwiaM52KWRR", "I_kwDOKIwiaM58uzH9", ">Whenever hard fork happens, this json should be updated.\r\nWondering if this can be automated in some way? @smartcontracts @sbvegan ", "2024-03-07T05:42:36Z", "2024-03-07T05:42:36Z", "cpengilly", "2025-08-30 13:04:25"]
["IC_kwDODjvEJM58EtVl", "I_kwDODjvEJM6G8nWQ", ">   \"l1ChainID\": 17000,\r\n\r\n> --l2-allocs ../packages/contracts-bedrock/state-dump-17000.json\r\n\r\nWhile the panic is not expected behavior of the tooling, this particular usage here of the L2 genesis tool seems wrong. The L1 state dump should not be used as the L2 chain allocs.\r\n", "2024-04-28T19:17:02Z", "2024-04-28T19:17:02Z", "protolambda", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM58G8PG", "I_kwDODjvEJM6G8nWQ", "@protolambda I see, thanks for the clarification. How then should the l2-allocs be generated?", "2024-04-29T08:55:30Z", "2024-04-29T08:55:30Z", "Padraic-O-Mhuiris", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM58I4Eq", "I_kwDODjvEJM6G8nWQ", "The manual command is:\r\n```bash\r\nCONTRACT_ADDRESSES_PATH=path_to_your_deployed_l1_addresses.json  forge script --chain-id $MY_L2_CHAIN_ID scripts/L2Genesis.s.sol:L2Genesis --sig runWithAllUpgrades() --private-key 0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\r\n```\r\n\r\nAnd then the generated state-dump JSON object is wrapped in JSON like this: ` { \"accounts\": forge_dump }`.\r\nThat wrapped object can then be passed to the L2 genesis command.\r\n\r\nThe above is how the devnet is started. The private key matches the first dev account, to execute the L2 genesis with (zeroed out at the end).\r\n\r\n@tynes does this look right? I think we need to update the docs.\r\n", "2024-04-29T13:10:11Z", "2024-04-29T13:10:11Z", "protolambda", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM52FxqT", "I_kwDODjvEJM6BXGak", "However you are configuring your `op-node`, the RPC is pointing to mainnet when it should be pointing to the local L2 node", "2024-03-06T16:10:08Z", "2024-03-06T16:10:08Z", "tynes", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM52GKLD", "I_kwDODjvEJM6BXGak", "**Resolved:**\r\n\r\nI was able to get it working. I deleted the `datadir`, imported the `genesis.json` file manually then ran the `op-geth` node. At that point you'll notice the logs show the correct chainId: \r\n\r\n```\r\nINFO [03-06|11:45:03.662] ---------------------------------------------------------------------------------------------------------------------------------------------------------\r\nINFO [03-06|11:45:03.662] Chain ID:  42069 (unknown)\r\nINFO [03-06|11:45:03.662] Consensus: Optimism\r\n```\r\n\r\nWhereas before it was defaulting the Chain ID to `1`, mainnet. \r\n\r\n**Steps to fix:** \r\n\r\n```\r\ncd op-geth\r\n\r\nrm -rf datadir\r\n\r\n./build/bin/geth \\\r\n  --datadir ./datadir \\\r\n  --http \\\r\n  --http.corsdomain=\"*\" \\\r\n  --http.vhosts=\"*\" \\\r\n  --http.addr=0.0.0.0 \\\r\n  --http.api=web3,debug,eth,txpool,net,engine \\\r\n  --ws \\\r\n  --ws.addr=0.0.0.0 \\\r\n  --ws.port=8546 \\\r\n  --ws.origins=\"*\" \\\r\n  --ws.api=debug,eth,txpool,net,engine \\\r\n  --syncmode=full \\\r\n  --gcmode=archive \\\r\n  --nodiscover \\\r\n  --maxpeers=0 \\\r\n  --networkid=42069 \\\r\n  --authrpc.vhosts=\"*\" \\\r\n  --authrpc.addr=0.0.0.0 \\\r\n  --authrpc.port=8551 \\\r\n  --authrpc.jwtsecret=./jwt.txt \\\r\n  --rollup.disabletxpoolgossip=true \\\r\n  init genesis.json\r\n\r\n./build/bin/geth \\\r\n  --datadir ./datadir \\\r\n  --http \\\r\n  --http.corsdomain=\"*\" \\\r\n  --http.vhosts=\"*\" \\\r\n  --http.addr=0.0.0.0 \\\r\n  --http.api=web3,debug,eth,txpool,net,engine \\\r\n  --ws \\\r\n  --ws.addr=0.0.0.0 \\\r\n  --ws.port=8546 \\\r\n  --ws.origins=\"*\" \\\r\n  --ws.api=debug,eth,txpool,net,engine \\\r\n  --syncmode=full \\\r\n  --gcmode=archive \\\r\n  --nodiscover \\\r\n  --maxpeers=0 \\\r\n  --networkid=42069 \\\r\n  --authrpc.vhosts=\"*\" \\\r\n  --authrpc.addr=0.0.0.0 \\\r\n  --authrpc.port=8551 \\\r\n  --authrpc.jwtsecret=./jwt.txt \\\r\n  --rollup.disabletxpoolgossip=true\r\n```\r\n\r\nThis may require a PR to update the documentation with a command that imports the `genesis.json` file. I can look into doing this. ", "2024-03-06T16:48:26Z", "2024-03-06T16:48:26Z", "blmalone", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM5ul5W4", "I_kwDODjvEJM55pODM", "I\"m also affected. What seems even weirder to me is that building the missing image locally doesn't seem to help either.\r\n\r\nRunning the following from https://github.com/ethereum-optimism/optimism/blob/develop/CONTRIBUTING.md#building-the-rest-of-the-system\r\n\r\n```\r\ncd ops-bedrock\r\nexport COMPOSE_DOCKER_CLI_BUILD=1\r\nexport DOCKER_BUILDKIT=1\r\ndocker compose build\r\n```\r\nLeads to the same error as OP\r\n```\r\n...\r\n => => loading layer 2a3c70920cf9 196.61kB / 19.33MB                                                                                                                                     0.3s\r\n => [op-node internal] load build definition from Dockerfile                                                                                                                             0.2s\r\n => => transferring dockerfile: 464B                                                                                                                                                     0.0s\r\n => ERROR [op-node internal] load metadata for us-docker.pkg.dev/oplabs-tools-artifacts/images/op-stack-go:devnet                                                                        0.2s\r\n------\r\n > [op-node internal] load metadata for us-docker.pkg.dev/oplabs-tools-artifacts/images/op-stack-go:devnet:\r\n------\r\nfailed to solve: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-stack-go:devnet: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-stack-go:devnet: not found\r\n```\r\n\r\nBuilding the missing image locally with\r\n\r\n```\r\ndocker compose build op_stack_go_builder\r\n```\r\nsucceeds:\r\n```\r\n$ docker images|grep op-stack-go     \r\nus-docker.pkg.dev/oplabs-tools-artifacts/images/op-stack-go   devnet            04ee7a199867   4 minutes ago   330MB\r\n```\r\n\r\nBut I still get the same error when running `docker compose build`.\r\n\r\nI think everyone who starts building now is affected. A colleague who seems to have downloaded the \"existing but still not resolvable\" image earlier is able to build.", "2023-12-14T08:51:03Z", "2023-12-14T08:53:10Z", "sveitser", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM5u_jww", "I_kwDODjvEJM55pODM", "Hi, I noticed @opfocus created a PR in the docs repo to update things there. Do you get the same issue with the updated docs?", "2023-12-19T05:40:09Z", "2023-12-19T05:40:09Z", "smartcontracts", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM5vGIE-", "I_kwDODjvEJM55pODM", "Looks the PR linked in this issue. Seems not fix the issue.", "2023-12-20T05:26:43Z", "2023-12-20T05:26:43Z", "boundless-forest", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM5vGKaY", "I_kwDODjvEJM55pODM", "> Looks the PR linked in this issue. Seems not fix the issue.\r\n\r\nWhich commit and branch are you currently using\r\n\r\nI don't have much knowledge about this. If it were me, I would probably first check for possible version issues with Docker Compose. Docker Engine [link](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository) Compose [link](https://docs.docker.com/compose/install/linux/#install-the-plugin-manually).", "2023-12-20T05:39:05Z", "2023-12-20T13:41:40Z", "opfocus", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM5zdxK-", "I_kwDODjvEJM55pODM", "A temporary workaround is to go into the compose-file and update all references to us-docker.pkg.dev/oplabs-tools-artifacts/images/op-stack-go:devnet with a reference to your own image. \r\n\r\nE.g myDockerHubRepo/op-stack-go:devnet\r\n\r\nThe compose file is: https://github.com/ethereum-optimism/optimism/blob/develop/ops-bedrock/docker-compose.yml", "2024-02-10T21:28:09Z", "2024-02-10T21:28:09Z", "OCookman", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM58Tjjh", "I_kwDODjvEJM55pODM", "why not use dockerhub\r\n", "2024-04-30T14:32:58Z", "2024-04-30T14:32:58Z", "KunPengRen", "2025-08-30 13:06:23"]
["IC_kwDODjvEJM58ieQ4", "I_kwDODjvEJM55pODM", "> why not use dockerhub\r\n\r\nchange to  rss3/op-stack-go:07317038d021591298ec2b237328d587e2fca93f", "2024-05-02T02:11:15Z", "2024-05-02T02:11:15Z", "KunPengRen", "2025-08-30 13:06:23"]
["IC_kwDOKIwiaM51kYT0", "I_kwDOKIwiaM6AeJUT", "suggestion: I wouldn't keep an node config overview page at all. I'd take the node config overview section and relocate to the [general node overview page](https://docs.optimism.io/builders/node-operators/overview), linking from there to the new op-node config and op-geth config pages \ud83d\ude04 \r\ncc @sbvegan ", "2024-03-01T04:07:29Z", "2024-03-01T04:07:46Z", "cpengilly", "2025-08-30 13:07:05"]
["IC_kwDOLB-lzc59Q-5p", "I_kwDOLB-lzc6IJbal", "> The Holocene is the name given to the last 11,700 years* of the Earth's history \u2014 the time since the end of the last major glacial epoch, or \"ice age.\"\r\n\r\n> A giant planet, sometimes referred to as a jovian planet, is a diverse type of planet much larger than Earth. Giant planets are usually primarily composed of low-boiling point materials (volatiles), rather than rock or other solid matter, but massive solid planets can also exist. \r\n\r\n> Orogeny is a mountain-building process that takes place at a convergent plate margin when plate motion compresses the margin. An orogenic belt or orogen develops as the compressed plate crumples and is uplifted to form one or more mountain ranges. \r\n\r\nLove this, I am learning so much", "2024-05-08T22:28:07Z", "2024-05-08T22:28:07Z", "mds1", "2025-08-30 13:07:13"]
["IC_kwDOLB-lzc59myVz", "I_kwDOLB-lzc6IJbal", "Happy to use almost all of these new names, they sound mostly cooler than the old names in Github. Since we didn't use any of the higher fork names in any code or specs yet, I don't see a problem with changing these at the current moment.\r\n\r\n@protolambda mentioned 3 name clashes with existing projects, maybe we just use the old names for these (Krypton, Lava, Nebula).\r\n\r\nFor reference, this is what we agreed to last year\r\n<img width=\"529\" alt=\"image\" src=\"https://github.com/ethereum-optimism/specs/assets/2607372/6b1f2123-87bf-40f7-b8a8-3ad723fb2d9a\">\r\n\r\nWhen these names are settled, they should be updated at https://github.com/orgs/ethereum-optimism/projects/47/settings/fields/57300142", "2024-05-13T11:24:02Z", "2024-05-13T11:24:02Z", "sebastianst", "2025-08-30 13:07:13"]
["IC_kwDOLB-lzc59nbvW", "I_kwDOLB-lzc6IJbal", "@sebastianst I updated the tracker to match the names in PR #187 - thanks for helping to land this to the specs repo!", "2024-05-13T12:47:55Z", "2024-05-13T12:47:55Z", "tynes", "2025-08-30 13:07:13"]
["IC_kwDOLB-lzc59qy8Z", "I_kwDOLB-lzc6IJbal", "Thanks @tynes approved the specs pr.", "2024-05-13T17:35:11Z", "2024-05-13T17:35:11Z", "sebastianst", "2025-08-30 13:07:13"]
["IC_kwDOLB-lzc59rGWV", "I_kwDOLB-lzc6IJbal", "Hardfork names as proposed in #187 look good to me. Thanks for putting this together.\r\n", "2024-05-13T17:57:24Z", "2024-05-13T17:57:24Z", "protolambda", "2025-08-30 13:07:13"]
["IC_kwDODjvEJM59Rcyw", "I_kwDODjvEJM6ITDvz", "I have found a diff between the mode deployment and other OP stack chains: \r\n\r\nOther OP stack chains have 0x4200000000000000000000000000000000000007 as the L2CrossDomainMessenger address and Mode's is deployed at:\r\nL2CrossDomainMessenger 0xC0d3c0d3c0D3c0D3C0d3C0D3C0D3c0d3c0d30007\r\n\r\nIs it possible that this diff is what is causing the SDK to not work for Mode?", "2024-05-09T00:46:53Z", "2024-05-09T00:46:53Z", "jtfirek", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59TrNC", "I_kwDODjvEJM6ITDvz", "> I have found a diff between the mode deployment and other OP stack chains:\r\n> \r\n> Other OP stack chains have 0x4200000000000000000000000000000000000007 as the L2CrossDomainMessenger address and Mode's is deployed at: L2CrossDomainMessenger 0xC0d3c0d3c0D3c0D3C0d3C0D3C0D3c0d3c0d30007\r\n> \r\n> Is it possible that this diff is what is causing the SDK to not work for Mode?\r\n\r\n`0xC0d3c0d3c0D3c0D3C0d3C0D3C0D3c0d3c0d30007` is the implementation and not the proxy. You should interact with `0x4200000000000000000000000000000000000007`.\r\n\r\nI am not sure why this problem would exist on mode and not other chains. It is likely a bug in the sdk. cc @smartcontracts. Perhaps something with an off by one \r\n\r\nYou should consider using [viem](https://github.com/wevm/viem) for this as the sdk has been deprecated and is no longer maintained", "2024-05-09T09:37:08Z", "2024-05-09T09:37:08Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59VRR6", "I_kwDODjvEJM6ITDvz", "Do you have any code examples for constructing the proof for withdraws via viem @tynes ? ", "2024-05-09T14:04:15Z", "2024-05-09T14:04:15Z", "jtfirek", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59W3n0", "I_kwDODjvEJM6ITDvz", "> Do you have any code examples for constructing the proof for withdraws via viem @tynes ?\r\n\r\nhttps://github.com/wevm/viem/blob/d6ae8fa770679306d1eb86b5fb24d3bf2c764aa6/site/pages/op-stack/guides/withdrawals.md\r\n\r\nThis took a quick search thru their docs page to find", "2024-05-09T17:52:31Z", "2024-05-09T17:52:31Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59ZDLS", "I_kwDODjvEJM6ITDvz", "Thanks", "2024-05-10T02:07:11Z", "2024-05-10T02:07:11Z", "jtfirek", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59H3mt", "I_kwDODjvEJM6IAiCO", "These values are hardcoded as we do not recommend changing them from these values. It is possible to create a set of values that breaks deposits", "2024-05-07T20:08:19Z", "2024-05-07T20:08:19Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59H4hH", "I_kwDODjvEJM6H_W4q", "Can you include a link to your transaction hash?", "2024-05-07T20:10:51Z", "2024-05-07T20:10:51Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59JjRp", "I_kwDODjvEJM6H_W4q", "https://exp.testnet.jibchain.net/tx/0xf4e5cbaaed3d840d7db32e58411ba2c53f9f6c1ed2ff1a485fee514764cf1993?tab=internal\r\n\r\nCan you provide example code for withdraw ?\r\nmay be opsdk not support custome token right now.\r\nhttps://github.com/alt-research/optimism-tutorial/blob/main/cross-dom-bridge-eth/index.js\r\n", "2024-05-08T03:07:32Z", "2024-05-08T03:07:32Z", "dome", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59Mdiv", "I_kwDODjvEJM6H_W4q", "It looks like you are attempting to withdraw through the `L2StandardBridge` - that doesn't work with the native asset on the L2. You will want to send a transaction to the `L2ToL1MessagePasser` [contract](https://github.com/ethereum-optimism/optimism/blob/c41bb739f15005412f227a130474433e168faf8c/packages/contracts-bedrock/src/L2/L2ToL1MessagePasser.sol#L73) to facilicate a withdrawal of the native asset", "2024-05-08T12:00:52Z", "2024-05-08T12:00:52Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59MhEL", "I_kwDODjvEJM6H_W4q", "I updated the FAQ in https://github.com/ethereum-optimism/specs/discussions/140 with more information on how to deposit and withdrawal", "2024-05-08T12:09:17Z", "2024-05-08T12:09:17Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59MhXA", "I_kwDODjvEJM6H_W4q", "Thanks it's work. (littel revert)\r\nhttps://exp.testnet.jibchain.net/tx/0x1096fa0d522ea088f1d2f200dba461e849984f2231f8e9e9f6ef9a7eac0e746d?tab=internal", "2024-05-08T12:09:58Z", "2024-05-08T12:09:58Z", "dome", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59Mhi1", "I_kwDODjvEJM6H_W4q", "fix in frontend. ", "2024-05-08T12:10:25Z", "2024-05-08T12:10:25Z", "dome", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59MieP", "I_kwDODjvEJM6H_W4q", "Can you confirm if this is impacting you? https://github.com/ethereum-optimism/optimism/pull/10440#discussion_r1593150252", "2024-05-08T12:12:52Z", "2024-05-08T12:12:52Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59OMSh", "I_kwDODjvEJM6H_W4q", "deposit not effect", "2024-05-08T15:42:05Z", "2024-05-08T15:42:05Z", "dome", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM571xxW", "I_kwDODjvEJM6G29Fp", "cc @sbvegan - are we sure that the docs say to check out a particular commit? We can remove calling `sync()` as part of the deployment", "2024-04-25T16:28:11Z", "2024-04-25T16:28:11Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM58v5M0", "I_kwDODjvEJM6G29Fp", "> when I deploy a local node with the branch `feat/custom-gas-token`, I got an error like this:\r\n> \r\n> forge script scripts/Deploy.s.sol:Deploy --sig 'sync()' --rpc-url $L1_RPC_URL\r\n> \r\n> [\u2806] Compiling... No files changed, compilation skipped Error: Function `sync()` is not implemented in your script\r\n\r\nWere you able to deploy with this branch? If yes can you please tell me the step as well? I am also trying to deploy custom erc 20 as a gas token\r\n", "2024-05-03T12:35:01Z", "2024-05-03T12:35:38Z", "priyanshuthapliyal55", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59Trtf", "I_kwDODjvEJM6G29Fp", "See the setup guide link here: https://github.com/ethereum-optimism/specs/discussions/140#discussion-6530747", "2024-05-09T09:38:30Z", "2024-05-09T09:38:30Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM53KfYM", "I_kwDODjvEJM6CaXNr", "As the error message shows: \r\n```\r\nfoundry is missing from /soft/optimism/versions.json\r\n```\r\n\r\nSuggesting to check whether foundry is installed properly, also `packages/contracts-bedrock/scripts/verify-foundry-install.sh` can do this.", "2024-03-15T09:16:37Z", "2024-03-15T09:16:37Z", "friendwu", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM53Kgym", "I_kwDODjvEJM6CaXNr", "yes l check\r\n***@***.*** optimism]# forge --version\r\nforge 0.2.0 (3fa0270 2024-03-15T00:16:56.190232509Z)\r\n***@***.*** optimism]# sh /soft/optimism/packages/contracts-bedrock/scripts/checks/check-foundry-install.sh\r\n'foundry' is missing from /soft/optimism/versions.json\r\n***@***.*** optimism]#\r\n\r\nIs there a requirement for the Forge version?\r\n\r\n\r\n\r\n***@***.***\r\nFrom: friendwu\r\nDate: 2024-03-15 17:16\r\nTo: ethereum-optimism/optimism\r\nCC: lemons; Author\r\nSubject: Re: [ethereum-optimism/optimism] op build error (Issue #9870)\r\nAs the error message shows:\r\nfoundry is missing from /soft/optimism/versions.json\n\r\nSuggesting to check whether foundry is installed properly, also packages/contracts-bedrock/scripts/verify-foundry-install.sh can do this.\r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you authored the thread.Message ID: ***@***.***>\r\n", "2024-03-15T09:20:15Z", "2024-03-15T09:20:15Z", "zhouhanker", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM540kOC", "I_kwDODjvEJM6CaXNr", "I met the same issue and solve it by installing `jq`.\r\nCheck more at https://github.com/ethereum-optimism/optimism/commit/8432a71b8d70099cfed8d8df7c8a0b2c96e0e313.", "2024-03-29T10:33:38Z", "2024-03-29T10:33:38Z", "smilenow", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM55Drlb", "I_kwDODjvEJM6CaXNr", "okay I'll give it a try!", "2024-04-02T03:30:22Z", "2024-04-02T03:30:22Z", "zhouhanker", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5xszIA", "I_kwDODjvEJM570vuK", "also happens now for me, op-node version is v1.4.0", "2024-01-24T07:43:13Z", "2024-01-24T07:43:13Z", "wangjiangw", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5yzPUv", "I_kwDODjvEJM570vuK", "Not sure if you can easily recover if you missed the upgrade, may need to resync.", "2024-02-05T00:58:04Z", "2024-02-05T00:58:04Z", "smartcontracts", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5yzU7S", "I_kwDODjvEJM54ZN5Q", "What happened to this? cc @tynes ", "2024-02-05T01:27:39Z", "2024-02-05T01:27:39Z", "smartcontracts", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM59Tsnc", "I_kwDODjvEJM54ZN5Q", "this has been completed", "2024-05-09T09:40:47Z", "2024-05-09T09:40:47Z", "tynes", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5ujpvx", "I_kwDODjvEJM50BLgK", "Hi @tynes,\r\nIs it still accurate? Happy to jump on this one :D ", "2023-12-13T23:14:06Z", "2023-12-13T23:14:06Z", "Ekzer", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5yzTx8", "I_kwDODjvEJM50BLgK", "Is this still something we want to do? @tynes ", "2024-02-05T01:23:00Z", "2024-02-05T01:23:00Z", "smartcontracts", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5jh6zX", "I_kwDODjvEJM5tqEM2", "@roninjin10 can you assign me?\r\n", "2023-08-08T15:26:49Z", "2023-08-08T15:26:49Z", "Sah314", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5jkSBd", "I_kwDODjvEJM5tqEM2", "@roninjin10 I want to work on that issue ", "2023-08-08T23:46:38Z", "2023-08-08T23:46:38Z", "aliaaquib", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5jkYRU", "I_kwDODjvEJM5tqEM2", "@Sah314 looks like you got first dibs", "2023-08-09T00:25:09Z", "2023-08-09T00:25:09Z", "roninjin10", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5jkZnf", "I_kwDODjvEJM5tqEM2", "@aliaaquib if @Sah314 isn't able to do it you are next up. Keep eyes on issues tab I will be trying to make more good first issues moving forward", "2023-08-09T00:33:57Z", "2023-08-09T00:33:57Z", "roninjin10", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5jr3jY", "I_kwDODjvEJM5tqEM2", "@Sah314 let me know if you still plan on finishing https://github.com/ethereum-optimism/optimism/pull/6649 it was pretty close", "2023-08-10T02:25:43Z", "2023-08-10T02:25:43Z", "roninjin10", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5jwGEo", "I_kwDODjvEJM5tqEM2", "@aliaaquib you are up! Let me know if you need help. #6649 was mostly complete so could pick up there", "2023-08-10T16:35:04Z", "2023-08-10T16:35:04Z", "roninjin10", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5jwpAA", "I_kwDODjvEJM5tqEM2", "> @aliaaquib you are up! Let me know if you need help. #6649 was mostly complete so could pick up there\r\n\r\nSure ", "2023-08-10T18:20:37Z", "2023-08-10T18:20:37Z", "aliaaquib", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5j0Ysl", "I_kwDODjvEJM5tqEM2", "@roninjin10 There is no `config` file in `packages/configs` can you please have a look.", "2023-08-11T12:23:56Z", "2023-08-11T12:23:56Z", "aliaaquib", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM5j1dKE", "I_kwDODjvEJM5tqEM2", "Yes the previous assignee force pushed away the changes. The previous assignee did add the config files as can be seen here https://github.com/ethereum-optimism/optimism/commit/b12d1b126a8dc72e9a15cef51fd5e0e572884af3#diff-7073466fab8833385922059f8cdd3e64df6171df380abde93492b9ed4d5d695e", "2023-08-11T15:16:53Z", "2023-08-11T15:16:53Z", "roninjin10", "2025-08-30 13:07:56"]
["IC_kwDODjvEJM542kTk", "I_kwDODjvEJM5tqEM2", "I have no clue what this is or why I\u2019m even here ", "2024-03-29T18:17:56Z", "2024-03-29T18:17:56Z", "Realrobwoodx", "2025-08-30 13:07:56"]
["IC_kwDOLB-lzc584_za", "I_kwDOLB-lzc6FTmEJ", "Could this be handled at the dependency set selection level? If a chain A wants to add a chain B to its dependency set, it needs to investigate its properties, including DA.\r\n\r\nThen we could have a unified \"batch inbox\" interface on L1 that supports different DA systems (e.g. it might only have commitments to batches posted, and/or it might be a pure \"pull\" interface to be used by the fault proof to get bytes out of a batch).\r\n\r\nI believe there was work done on that by Espresso? Celestia also has an OP stack fork where they post Celestia blockhashes on L1 I believe.", "2024-05-06T07:00:51Z", "2024-05-19T21:03:32Z", "norswap", "2025-08-30 13:09:48"]
["IC_kwDOLB-lzc5-YN7x", "I_kwDOLB-lzc6FTmEJ", "> Could this be handled at the dependency set selection level?\r\n\r\nIt totally could, it would be a social consensus kind of thing. Because the chain operators have this ability, it may not be something that app devs align with. The app devs may want to express that they are only interested in accepting messages from a particular type of blockspace, which is why making the \"type of chain\" legible from within the EVM is important.\r\n", "2024-05-20T11:34:56Z", "2024-05-20T11:34:56Z", "tynes", "2025-08-30 13:09:48"]
["IC_kwDODjvEJM5-8RVo", "I_kwDODjvEJM6J8Qn1", "`forge script --resume` can handle this.", "2024-05-24T14:50:55Z", "2024-05-24T14:50:55Z", "zhiqiangxu", "2025-08-30 14:11:10"]
["IC_kwDODjvEJM5-cPjc", "I_kwDODjvEJM6JgLaS", "Yes, see the Fault Proofs governance proposal here: https://gov.optimism.io/t/upgrade-proposal-fault-proofs/8161\r\n\r\nCheck the `OptimismPortal2` and the `DisputeGameFactory`/`FaultDisputeGame` contracts for concrete implementations.", "2024-05-20T22:38:50Z", "2024-05-20T22:38:50Z", "smartcontracts", "2025-08-30 14:11:10"]
["IC_kwDODjvEJM5-fq4o", "I_kwDODjvEJM6JgLaS", "If you are running a pre fault proof system, you could imagine setting the `proposer` to a smart contract that has logic around multiple proposers, although it would require custom code. Like @smartcontracts mentioned, the fault proof based system is the real solution to this", "2024-05-21T09:51:55Z", "2024-05-21T09:51:55Z", "tynes", "2025-08-30 14:11:10"]
["IC_kwDODjvEJM5-iw5S", "I_kwDODjvEJM6JgLaS", "Makes sense.  Thanks guys.", "2024-05-21T16:49:44Z", "2024-05-21T16:49:44Z", "pegahcarter", "2025-08-30 14:11:10"]
["IC_kwDODjvEJM59Mak4", "I_kwDODjvEJM6IMpti", "The L2 chain id is pulled from the deploy config, see [here](https://github.com/ethereum-optimism/optimism/blob/c41bb739f15005412f227a130474433e168faf8c/packages/contracts-bedrock/deploy-config/mainnet.json#L6)", "2024-05-08T11:53:18Z", "2024-05-08T11:53:18Z", "tynes", "2025-08-30 14:11:10"]
["IC_kwDODjvEJM5191M_", "I_kwDODjvEJM6BToJm", "We will shortly be rewriting the L2 genesis generation in solidity to decouple releases of the predeploys from releases of `op-node`. You are correct that our storage setting library doesn't support mappings right now, its possible to add but we didn't need it so we didn't implement it at the time", "2024-03-05T16:17:47Z", "2024-03-05T16:17:47Z", "tynes", "2025-08-30 14:11:10"]
["IC_kwDODjvEJM59WiT4", "I_kwDODjvEJM6IQyyB", "WIP: https://github.com/ethereum-optimism/optimism/pull/10458", "2024-05-09T17:04:40Z", "2024-05-09T17:04:40Z", "axelKingsley", "2025-08-30 14:11:28"]
["IC_kwDODjvEJM59edgc", "I_kwDODjvEJM6HzWjw", "https://github.com/ethereum-optimism/optimism/pull/10458\r\n", "2024-05-10T19:11:22Z", "2024-05-10T19:11:22Z", "axelKingsley", "2025-08-30 14:11:28"]
["IC_kwDODjvEJM58ploW", "I_kwDODjvEJM6HhyOl", "`--l1.rethdb` has to be set to a non-empty value, and the build flag should be used, for the reth DB to be used. There's no such thing as a `reth_db` RPC Kind as far as I am aware, but this may have been simplified as part of prior reth-db related changes.\r\n", "2024-05-02T18:54:09Z", "2024-05-02T18:54:09Z", "protolambda", "2025-08-30 14:11:28"]
["IC_kwDODjvEJM59XF_z", "I_kwDODjvEJM6G90hy", "@trianglesphere would love to take this on! ", "2024-05-09T18:32:37Z", "2024-05-09T18:32:37Z", "ethenotethan", "2025-08-30 14:11:28"]
["IC_kwDODjvEJM5yNFSF", "I_kwDODjvEJM59lLvy", "The for-loop in the resolveClaim() may have a similar issue https://github.com/ethereum-optimism/optimism/blob/546fb2c7a5796b7fe50b0b7edc7666d3bd281d6f/packages/contracts-bedrock/src/dispute/FaultDisputeGame.sol#L394 (although with bond, such attack will be much more expensive)", "2024-01-30T04:01:06Z", "2024-01-30T04:01:06Z", "qizhou", "2025-08-30 14:11:28"]
["IC_kwDODjvEJM5ySeEV", "I_kwDODjvEJM59lLvy", "Thanks for the issue! Great spot, and glad that people are already taking a look at these contracts. This is a known issue, but was pushed aside for a bit due to the cost-to-grief. We'll likely make some improvements here based on your suggestions to improve readability and make this path more FV friendly \ud83d\ude04 \r\n\r\nThis issue would prevent the game from resolving, locking all bonds up within the game above the subgame that's bricked. However, it would not cause the game to resolve incorrectly - it would effectively be bricked due to all sub-games not being resolved.\r\n\r\nThe impact of this is low-medium in severity, and the attack is costly for the malicious party:\r\n* If the game proposes a correct output and the game gets bricked, anyone can come along and just create a new output proposal, which costs on average $10-15. The one with the 6k+ subgames on a node won't finalize, but that's no problem - withdrawers could re-prove their withdrawal against a future game's output root. To sustain the DoS, the attacker would need to get 1 claim w/ 6k+ subgames in each dispute game that is active.\r\n* If the game proposes an incorrect output, the game can never finalize anyways since all subgames of the root can't be resolved. There's no incentive for a malicious actor to perform this attack on a game with a malicious root claim.\r\n\r\nBonds are not currently defined in the `getRequiredBond` function, but early work on them suggests that they will be fairly high to cover the costs of hardware and to maintain a high probability of collateralization between the time of creation and a possible counter. Let's say they're `2 ETH` per interaction - not including execution or native transaction costs associated with the creation of the 6,000 subgames, it would cost the attacker `$28,092,120 USD` to create the 6,000 subgames in that single dispute game alone at today's spot ETH price. The honest actors would still be able to counter and take the bonds of these 6,000 subgames, unless the counters themselves had 6,000 more responses, since credit is accumulated as each subgame is resolved + subgames are resolved bottom-up. This creates a very uneven incentive - since the honest actors will always respond to these spammed sub-games up until the instruction step at the leaves, the attacker would have to continue bricking claims lower and lower, for far more money than the honest challenger had to pay.", "2024-01-30T16:37:28Z", "2024-01-30T21:16:31Z", "clabby", "2025-08-30 14:11:28"]
["IC_kwDODjvEJM5yUO4K", "I_kwDODjvEJM59lLvy", "@clabby We should ensure there is a section in the specs that includes your reasoning as to why this is safe. It will help to centralize the source of truth for the design in a single place", "2024-01-30T21:15:00Z", "2024-01-30T21:15:00Z", "tynes", "2025-08-30 14:11:28"]
["IC_kwDODjvEJM5-AvLj", "I_kwDODjvEJM59lLvy", "This was resolved via https://github.com/ethereum-optimism/optimism/pull/10248 - claims can now be resolved in batches, ensuring it is always possible to resolve claims by spreading the work over multiple transactions if necessary.  The cost of this work is incentivised by the bonds that are paid out as part of resolution (ie the winner of the bond is incentivised to resolve the claims).", "2024-05-16T06:23:16Z", "2024-05-16T06:23:16Z", "ajsutton", "2025-08-30 14:11:28"]
["IC_kwDOH2Qg5s5_KcF2", "I_kwDOH2Qg5s6J9890", "Well problem gone, with recreate the Node from scratch with snap-sync, but the same config were used.", "2024-05-27T13:04:03Z", "2024-05-27T13:04:03Z", "valamidev", "2025-08-30 14:11:56"]
["IC_kwDODjvEJM5_val-", "I_kwDODjvEJM6Kuc8J", "The docker builds are done via the `docker-bake.hcl` in the top level of the repository.  Simplest option to build them is to run `make golang-docker` in the top level dir.", "2024-06-01T00:04:06Z", "2024-06-01T00:04:06Z", "ajsutton", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_MX65", "I_kwDODjvEJM6KPaYc", "We do not want to check bytecode into the repo itself as that creates large and unnecessary diffs. It was a massive pain for devs to maintain the `op-bindings` package which is why it was deleted. It is not up to individual services to generate bindings, see this [justfile](https://github.com/babylonchain/optimism/blob/7cbda018196b58a71e2c0b4bc9e31a289235074e/op-chain-ops/justfile) for an example of how to do so. We no longer need the bytecode as the L2 genesis is created using a foundry script", "2024-05-27T19:08:15Z", "2024-05-27T19:08:15Z", "tynes", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_MyIz", "I_kwDODjvEJM6KPaYc", "@tynes Thanks for your reply. \r\nBut how to handle the case that needs the deploy method in the Go bindings file? E.g. for the [abi_test.go](https://github.com/ethereum-optimism/optimism/blob/develop/op-proposer/proposer/abi_test.go#L42) in the op-proposer, this test function calls the deploy method `bindings.DeployL2OutputOracle(opts, backend)` which is in the [bindings](https://github.com/ethereum-optimism/optimism/blob/develop/op-e2e/bindings/l2outputoracle.go#L53). If using the justfile you mention to generate bindings file, it would not generate this deploy method. To generate the deploy method it needs to run `abigen` with the option `--bin <bytecode>`.", "2024-05-27T21:20:26Z", "2024-05-27T21:20:57Z", "lesterli", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M8XM", "I_kwDODjvEJM6KPaYc", "The maintainers of that service will need to regenerate their bindings with new bytecode. I don't recommend following that sort of pattern for unit tests, `op-e2e` is guaranteed to have the latest bytecode present. Its possible to update the example that I linked with a `--bin $(jq .bytecode < forge-artifact.json)` flag (just an example, would need to build the right `jq` query). That is an old test and the e2e tests do cover submitting outputs to the `L2OutputOracle`. Once fault proofs are live, we will delete the `L2OutputOracle` from the codebase as it will be considered legacy. There is a permissioned fault proof game type that offers similar security properties as the existing `L2OutputOracle`", "2024-05-27T22:17:20Z", "2024-05-27T22:17:20Z", "tynes", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M868", "I_kwDODjvEJM6KPaYc", "Got it, thanks.", "2024-05-27T22:20:49Z", "2024-05-27T22:20:49Z", "lesterli", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M9U8", "I_kwDODjvEJM6KPaYc", "thanks @tynes for the quick response! a few more questions:\r\n\r\n1. do you know when we can expect the fault proof be ready (roughly)?\r\n2. it's likely we can't wait for the fault proof. in this case, which branch do you recommend us to build upon?\r\n3. is there anything we can read to understand the new system architecture? will there still be the 4 components (e.g. op-geth, op-node, op-submitter, op-proposer)?\r\n4. how will fault proof affect existing OP Stack based L2s? will is be easy for them to upgrade or do you expect most of them still will be on the legacy version for a while?\r\n\r\nhappy to chat more to give you more context on what we are building. my TG: zidongz", "2024-05-27T22:23:41Z", "2024-05-27T22:25:25Z", "bap2pecs", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M_nn", "I_kwDODjvEJM6KPaYc", "The fault proof release is currently out for [governance vote](https://gov.optimism.io/t/final-protocol-upgrade-7-fault-proofs/8161). This should contain details about how the system is changing. See the [docs](https://docs.optimism.io/stack/protocol/fault-proofs/overview) for more information. If the vote passes, then it will go live within 2-3 weeks on OP Mainnet. It has been live on our testnet for some time. The upgrade code can be found in https://github.com/ethereum-optimism/superchain-ops/pull/206. It should be possible for OP Stack chains to inherit our work depending on the sorts of changes to the codebase that were made. Chains that opt into revenue share for retroPGF will receive more help with the upgrade\r\n", "2024-05-27T22:40:22Z", "2024-05-27T22:40:22Z", "tynes", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M__j", "I_kwDODjvEJM6KPaYc", "thanks @tynes! we will start to look into the links you shared here", "2024-05-27T22:43:26Z", "2024-05-27T22:43:26Z", "bap2pecs", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_NJue", "I_kwDODjvEJM6KPaYc", "It works fine @tynes suggestions. Close it.", "2024-05-28T00:01:43Z", "2024-05-28T00:01:43Z", "lesterli", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5--Qep", "I_kwDODjvEJM6J6Azu", "Are you sure the `IMPL_SALT` is different? We really need a way to just deploy proxies and configure them to the shared implementations", "2024-05-24T19:42:45Z", "2024-05-24T19:42:51Z", "tynes", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_Fw2y", "I_kwDODjvEJM6J6Azu", "Yes, I am sure IMPLT_SALT is different.\r\nI tried all these below values but still getting the same above error. I am using \"tutorials/chain\" branch\r\n\r\nexport IMPL_SALT=$(openssl rand -hex 32)\r\nexport IMPL_SALT=$(openssl rand -hex 64)\r\nexport IMPL_SALT=$(openssl rand -hex 128)\r\nexport IMPL_SALT=$(openssl rand -hex 32768)\r\nexport IMPL_SALT=$(openssl rand -hex 9899)\r\nexport IMPL_SALT=$(openssl rand -hex 91357)\r\nexport IMPL_SALT=$(openssl rand -hex 76426)\r\nexport IMPL_SALT=$(openssl rand -hex 33554431)\r\n", "2024-05-26T13:11:23Z", "2024-05-26T13:12:05Z", "mohit6b", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_LnRY", "I_kwDODjvEJM6J6Azu", "Similar issue here:\r\nrunning 1.7.6\r\n```\r\n    \u251c\u2500 [8937393460516496946] 0xa6B71E26C5e0845f74c812102Ca7114b6a896AB2::createProxyWithNonce(0xd9Db270c1B5E3Bd161E8c8503c55cEABeE709552, 0xb63e800d0000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000140000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000000000000000000005c51ae978595a5c367ac03ca5e33bd6cb4b876f20000000000000000000000000000000000000000000000000000000000000000, 57493239949579596273554895313127079449756098686046701190193136806895817983202 [5.749e76])\r\n    \u2502   \u251c\u2500 [8937393460516461789] \u2192 new <unknown>@0x25Facc9A623Ea37609d2293C3413a11E709795f9\r\n    \u2502   \u2502   \u2514\u2500 \u2190 [CreateCollision] 0 bytes of code\r\n    \u2502   \u2514\u2500 \u2190 [Revert] revert: Create2 call failed\r\n    \u2514\u2500 \u2190 [Revert] revert: Create2 call failed\r\n```\r\n\r\n<img width=\"924\" alt=\"image\" src=\"https://github.com/ethereum-optimism/optimism/assets/2582555/e6b93672-fcf8-450e-8ae0-7f4236dd812a\">\r\n\r\n\r\n```\r\nIMPL_SALT=$(openssl rand -hex 32) DEPLOYMENT_CONTEXT=deployer DEPLOY_CONFIG_PATH=deploy-config/deployer.json forge script -vvv scripts/Deploy.s.sol:Deploy ...\r\n```", "2024-05-27T15:55:10Z", "2024-05-27T15:59:11Z", "InoMurko", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M8u_", "I_kwDODjvEJM6J6Azu", "This looks like its an issue with the deployment of the safe, its not the implementations that are clashing but the safe. This is something that needs to be fixed. cc @maurelian ", "2024-05-27T22:19:45Z", "2024-05-27T22:19:45Z", "tynes", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M9FT", "I_kwDODjvEJM6J6Azu", "Okay. Can you please suggest an alternative till it gets fixed? @tynes \r\nI am also trying with branch \"op-contracts/v1.3.0\" but getting the same issue. ", "2024-05-27T22:21:54Z", "2024-05-27T22:21:54Z", "mohit6b", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M-ns", "I_kwDODjvEJM6J6Azu", "Try https://github.com/ethereum-optimism/optimism/pull/10660", "2024-05-27T22:33:06Z", "2024-05-27T22:33:06Z", "tynes", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_M_wV", "I_kwDODjvEJM6J6Azu", "Okay, thanks. Let me try this one.\r\nShould i use --sig 'sync()' function or not?", "2024-05-27T22:41:24Z", "2024-05-27T22:41:24Z", "mohit6b", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_NAh7", "I_kwDODjvEJM6J6Azu", "> Should i use --sig 'sync()' function or not?\r\n\r\nThis has been deprecated, you shouldn't need to call that function. See [this](https://oplabs.notion.site/EXTERNAL-How-to-run-a-Custom-Gas-Token-chain-d88c68306f934790be051f87213a0b1d) document for updated instructions, these should be reflected in our devdocs soon", "2024-05-27T22:47:55Z", "2024-05-27T22:47:55Z", "tynes", "2025-08-30 14:12:18"]
["IC_kwDODjvEJM5_TxVE", "I_kwDODjvEJM6J6Azu", "The script is still failing and I am getting below error.\r\n\r\nAlso, it only allows for sepolia in deploy-config. How should I configure my custom testnet?\r\n\r\n\r\n\r\n \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500 [427] 0x2f5497904A8Dc32F078AfF9eFB5e061015367836::decimals() [staticcall]\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500 \u2190 [Return] 18\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500 [3421] 0x2f5497904A8Dc32F078AfF9eFB5e061015367836::name() [staticcall]\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500 \u2190 [Return] \"Op Token\"\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500 [3464] 0x2f5497904A8Dc32F078AfF9eFB5e061015367836::symbol() [staticcall]\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500 \u2190 [Return] \"OPT\"\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500 [707] Proxy::setGasPayingToken(0x2f5497904A8Dc32F078AfF9eFB5e061015367836, 18, 0x4f7020546f6b656e000000000000000000000000000000000000000000000000, 0x4f50540000000000000000000000000000000000000000000000000000000000)\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500 [212] OptimismPortal2::setGasPayingToken(0x2f5497904A8Dc32F078AfF9eFB5e061015367836, 18, 0x4f7020546f6b656e000000000000000000000000000000000000000000000000, 0x4f50540000000000000000000000000000000000000000000000000000000000) [delegatecall]\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500 \u2190 [Revert] EvmError: Revert\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500 \u2190 [Revert] EvmError: Revert\r\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500 \u2190 [Revert] EvmError: Revert\r\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500 \u2190 [Revert] revert: Proxy: delegatecall to new implementation contract failed\r\n    \u2502   \u2502   \u2502   \u2514\u2500 \u2190 [Revert] revert: Proxy: delegatecall to new implementation contract failed\r\n    \u2502   \u2502   \u2514\u2500 \u2190 [Revert] revert: GS013\r\n    \u2502   \u2514\u2500 \u2190 [Revert] revert: GS013\r\n    \u2514\u2500 \u2190 [Revert] revert: GS013\r\n\r\n\r\n\r\n== Logs ==\r\n  Writing artifact to deployments/artifact.json\r\n  Connected to network with chainid 11155111\r\n  Commit hash: \ufffd\ufffdvAVqy\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\r\n  DeployConfig: reading file /Users/mohitvashistha/optimism/packages/contracts-bedrock/deploy-config/sepolia.json\r\n  Deploying a fresh OP Stack including SuperchainConfig\r\n  start of L1 Deploy!\r\n  Deploying Safe\r\n  Saving SafeProxyFactory: 0xa6B71E26C5e0845f74c812102Ca7114b6a896AB2\r\n  Saving SafeSingleton: 0xd9Db270c1B5E3Bd161E8c8503c55cEABeE709552\r\n  Saving SystemOwnerSafe: 0x076246e59A402cdc80D1D9923E5629DC659C5045\r\n  New safe: SystemOwnerSafe deployed at 0x076246e59A402cdc80D1D9923E5629DC659C5045\r\n    Note that this safe is owned by the deployer key\r\n  deployed Safe!\r\n  Setting up Superchain\r\n  Deploying AddressManager\r\n  Saving AddressManager: 0x4994F291bb9910fE6Aa3B1085180192b52E0490b\r\n  AddressManager deployed at 0x4994F291bb9910fE6Aa3B1085180192b52E0490b\r\n  Deploying ProxyAdmin\r\n  Saving ProxyAdmin: 0x5434DD370992B5FC72a6F0aaF70Dd81E2cb3E802\r\n  ProxyAdmin deployed at 0x5434DD370992B5FC72a6F0aaF70Dd81E2cb3E802\r\n  ProxyAdmin ownership transferred to Safe at: 0x076246e59A402cdc80D1D9923E5629DC659C5045\r\n  Deploying ERC1967 proxy for SuperchainConfigProxy\r\n  Saving SuperchainConfigProxy: 0xE9c13dA229b6865da0AC0a345fd34a945253939A\r\n     at 0xE9c13dA229b6865da0AC0a345fd34a945253939A\r\n  Saving SuperchainConfig: 0x3C63FDeE5DA2781dCc23a34F817071aE47277727\r\n  SuperchainConfig deployed at 0x3C63FDeE5DA2781dCc23a34F817071aE47277727\r\n  Running chain assertions on the SuperchainConfig\r\n  Deploying ERC1967 proxy for ProtocolVersionsProxy\r\n  Saving ProtocolVersionsProxy: 0xFd3f08CBc07753F9AE29EAFC9ea5793e539Bc878\r\n     at 0xFd3f08CBc07753F9AE29EAFC9ea5793e539Bc878\r\n  Deploying ProtocolVersions implementation\r\n  Saving ProtocolVersions: 0xC5c9BB5434612250b78b07BCB2287166152d972E\r\n  ProtocolVersions deployed at 0xC5c9BB5434612250b78b07BCB2287166152d972E\r\n  Running chain assertions on the ProtocolVersions\r\n  Upgrading and initializing ProtocolVersions proxy\r\n  ProtocolVersions version: 1.0.0\r\n  Running chain assertions on the ProtocolVersions\r\n  set up superchain!\r\n  Deploying OP Chain\r\n  Deploying proxies\r\n  Deploying ERC1967 proxy for OptimismPortalProxy\r\n  Saving OptimismPortalProxy: 0xbF243F9462EB1de5e8824e6baB60b43cAB39cA9A\r\n     at 0xbF243F9462EB1de5e8824e6baB60b43cAB39cA9A\r\n  Deploying ERC1967 proxy for SystemConfigProxy\r\n  Saving SystemConfigProxy: 0x75917B7F267F47c2fFF6ad196E6516754eDf39e2\r\n     at 0x75917B7F267F47c2fFF6ad196E6516754eDf39e2\r\n  Deploying proxy for L1StandardBridge\r\n  Saving L1StandardBridgeProxy: 0x24214FE6FF029E98A67eeE3b33adA694BD4a533a\r\n  L1StandardBridgeProxy deployed at 0x24214FE6FF029E98A67eeE3b33adA694BD4a533a\r\n  Deploying proxy for L1CrossDomainMessenger\r\n  Saving L1CrossDomainMessengerProxy: 0x3BB0353Cecc69E00B8Ba560F8143a1b6277f1c2E\r\n  L1CrossDomainMessengerProxy deployed at 0x3BB0353Cecc69E00B8Ba560F8143a1b6277f1c2E\r\n  Deploying ERC1967 proxy for OptimismMintableERC20FactoryProxy\r\n  Saving OptimismMintableERC20FactoryProxy: 0x48b224034DA08140001A2Ab5dBd2C0c0248Fa178\r\n     at 0x48b224034DA08140001A2Ab5dBd2C0c0248Fa178\r\n  Deploying ERC1967 proxy for L1ERC721BridgeProxy\r\n  Saving L1ERC721BridgeProxy: 0xd0D0bAA677Dea50ba0d74FD75e35D2593C6D6080\r\n     at 0xd0D0bAA677Dea50ba0d74FD75e35D2593C6D6080\r\n  Deploying ERC1967 proxy for DisputeGameFactoryProxy\r\n  Saving DisputeGameFactoryProxy: 0xbCBfd98B1a6e0dD8D28E24D0B00Cf038Dbe8b487\r\n     at 0xbCBfd98B1a6e0dD8D28E24D0B00Cf038Dbe8b487\r\n  Deploying ERC1967 proxy for L2OutputOracleProxy\r\n  Saving L2OutputOracleProxy: 0x2FE5ed399dd9C14408D2b00F61e6d3868Bea656c\r\n     at 0x2FE5ed399dd9C14408D2b00F61e6d3868Bea656c\r\n  Deploying ERC1967 proxy for DelayedWETHProxy\r\n  Saving DelayedWETHProxy: 0x1bc6daf1950A4Eb7c5db16ff7B4FCCe13Ab56052\r\n     at 0x1bc6daf1950A4Eb7c5db16ff7B4FCCe13Ab56052\r\n  Deploying ERC1967 proxy for AnchorStateRegistryProxy\r\n  Saving AnchorStateRegistryProxy: 0x6a488E1A84C6dE3A2DB8D04CEd5FE52d0D94D4b1\r\n     at 0x6a488E1A84C6dE3A2DB8D04CEd5FE52d0D94D4b1\r\n  Transferring AddressManager ownership to ProxyAdmin\r\n  AddressManager ownership transferred to 0x5434DD370992B5FC72a6F0aaF70Dd81E2cb3E802\r\n  Deploying implementations\r\n  Deploying L1CrossDomainMessenger implementation\r\n  Saving L1CrossDomainMessenger: 0xf54d8C2377448A188Eee08CaED4ff70D612dc42a\r\n  L1CrossDomainMessenger deployed at 0xf54d8C2377448A188Eee08CaED4ff70D612dc42a\r\n  Running chain assertions on the L1CrossDomainMessenger\r\n  Deploying OptimismMintableERC20Factory implementation\r\n  Saving OptimismMintableERC20Factory: 0x13865c6A1bDad1708F3CE7A097eeee79a60071B2\r\n  OptimismMintableERC20Factory deployed at 0x13865c6A1bDad1708F3CE7A097eeee79a60071B2\r\n  Running chain assertions on the OptimismMintableERC20Factory\r\n  Deploying SystemConfig implementation\r\n  Saving SystemConfig: 0xC829e6A687bb3dC85148bCD41e439C05714D3adA\r\n  SystemConfig deployed at 0xC829e6A687bb3dC85148bCD41e439C05714D3adA\r\n  Running chain assertions on the SystemConfig\r\n  Deploying L1StandardBridge implementation\r\n  Saving L1StandardBridge: 0x6d84EDb112Ca1C012C1b8832202845b51CbE1C1B\r\n  L1StandardBridge deployed at 0x6d84EDb112Ca1C012C1b8832202845b51CbE1C1B\r\n  Running chain assertions on the L1StandardBridge\r\n  Deploying L1ERC721Bridge implementation\r\n  Saving L1ERC721Bridge: 0x2E83D8E3557C17B434FCB88F182CF730379B2fe5\r\n  L1ERC721Bridge deployed at 0x2E83D8E3557C17B434FCB88F182CF730379B2fe5\r\n  Running chain assertions on the L1ERC721Bridge\r\n  Deploying OptimismPortal implementation\r\n  Saving OptimismPortal: 0x7E17F3b7F2245648F4A82c0760875FbC0E110A15\r\n  OptimismPortal deployed at 0x7E17F3b7F2245648F4A82c0760875FbC0E110A15\r\n  Running chain assertions on the OptimismPortal\r\n  Guardian has no code: 0xfd1D2e729aE8eEe2E146c033bf4400fE75284301\r\n  Deploying L2OutputOracle implementation\r\n  Saving L2OutputOracle: 0x1Cf3815989cbEd3d81c41Fa98E775d8F77eF1429\r\n  L2OutputOracle deployed at 0x1Cf3815989cbEd3d81c41Fa98E775d8F77eF1429\r\n  Running chain assertions on the L2OutputOracle\r\n  Deploying OptimismPortal2 implementation\r\n  Saving OptimismPortal2: 0x513E2C5beE10F32346D33243Ba1fD87B87667B49\r\n  OptimismPortal2 deployed at 0x513E2C5beE10F32346D33243Ba1fD87B87667B49\r\n  Running chain assertions on the OptimismPortal2\r\n  Guardian has no code: 0xfd1D2e729aE8eEe2E146c033bf4400fE75284301\r\n  Deploying DisputeGameFactory implementation\r\n  Saving DisputeGameFactory: 0xD65D3789384Efb31FcD6a8D694C43D49C0D33356\r\n  DisputeGameFactory deployed at 0xD65D3789384Efb31FcD6a8D694C43D49C0D33356\r\n  Running chain assertions on the DisputeGameFactory\r\n  Deploying DelayedWETH implementation\r\n  Saving DelayedWETH: 0xa0C58eB144EeC561d698fFCB192cB02baa333A8A\r\n  DelayedWETH deployed at 0xa0C58eB144EeC561d698fFCB192cB02baa333A8A\r\n  Running chain assertions on the DelayedWETH\r\n  Deploying PreimageOracle implementation\r\n  Saving PreimageOracle: 0x2f669A729e40A8006eFac22bBDA515d21A12643D\r\n  PreimageOracle deployed at 0x2f669A729e40A8006eFac22bBDA515d21A12643D\r\n  Deploying Mips implementation\r\n  Saving Mips: 0x868a938026ffc715514f7bC82AD962440be91860\r\n  MIPS deployed at 0x868a938026ffc715514f7bC82AD962440be91860\r\n  Deploying AnchorStateRegistry implementation\r\n  Saving AnchorStateRegistry: 0x5c8e5A31f53Bd6E0bB9694a142c3353750605a21\r\n  AnchorStateRegistry deployed at 0x5c8e5A31f53Bd6E0bB9694a142c3353750605a21\r\n  Initializing implementations\r\n  WARNING: FPAC is enabled. Initializing the OptimismPortal proxy with the OptimismPortal2.\r\n  Upgrading and initializing OptimismPortal2 proxy\r\n  OptimismPortal2 version: 3.8.0\r\n  Running chain assertions on the OptimismPortal2\r\n  Guardian has no code: 0xfd1D2e729aE8eEe2E146c033bf4400fE75284301\r\n  Upgrading and initializing SystemConfig proxy\r\nError: \r\nscript failed: revert: GS013", "2024-05-28T18:39:32Z", "2024-05-28T18:39:32Z", "mohit6b", "2025-08-30 14:12:18"]
["IC_kwDOLB-lzc5_Mf2h", "I_kwDOLB-lzc6KETPe", "Thanks for looking at the spec and giving this feedback @ryanschneider! You are definitely correct that due to the current definition it is possible to have two different events that are emitted result in the same serialized message.\r\n\r\nGiven the following definition:\r\n```golang\r\nmsg := make([]byte, 0)\r\nfor _, topic := range log.Topics {\r\n    msg = append(msg, topic.Bytes()...)\r\n}\r\nmsg = append(msg, log.Data...)\r\n```\r\n\r\nGiven the following events:\r\n\r\n```solidity\r\nevent Foo(bytes32 indexed);\r\nevent Bar(bytes32, bytes32) anonymous;\r\n```\r\n\r\nYou cannot distinguish the difference in the message between:\r\n\r\n```solidity\r\nemit Foo(bytes32(0));\r\nemit Bar(0x54b5e571557045df8a69633aeac8e206d92b03f48563fab6a430493d299de727, bytes32(0));\r\n```\r\n\r\nUnder the current scheme, one would need to call back to the `CrossL2Inbox` for auth, in particular [origin()](https://github.com/ethereum-optimism/optimism/blob/329b8699bd8dd94285b8117304f20d37564ed873/packages/contracts-bedrock/src/L2/CrossL2Inbox.sol#L75) would let you know which contract emitted the event. We definitely want to enable interactions with untrusted contracts as much as possible. I am trying to wrap my head around how different the security model would be in practice if we had some sort of additional unique identifier within the message that would enable the events to be distinguished because an untrusted contract could still just emit the same event at the wrong place.\r\n\r\nFor example, emitting a `Burn` event that can be used to `Mint` on a remote chain, how do we know that its actually subtracting from the users balance? This leads me to believe there is some fundamental amount of distrust that you need to have when interacting with contracts on remote chains, which is not much different than interacting with untrusted contracts on a local chain.", "2024-05-27T19:45:50Z", "2024-05-27T19:45:50Z", "tynes", "2025-08-30 14:12:46"]
["IC_kwDOH2Qg5s6AawHi", "I_kwDOH2Qg5s6LeS8o", "Responded on upstream issue.", "2024-06-07T09:51:55Z", "2024-06-07T09:51:55Z", "protolambda", "2025-08-30 14:14:35"]
["IC_kwDOLB-lzc6AUndT", "I_kwDOLB-lzc6LA0ul", "This would also enable the development of a based rollups using the OP Stack", "2024-06-06T16:01:50Z", "2024-06-06T16:01:50Z", "tynes", "2025-08-30 14:15:03"]
["IC_kwDOLB-lzc6AMdGU", "I_kwDOLB-lzc6KEHUJ", "Initiating Messages are Events. You would call an executing message on the destination chain with an `Identifier` pointing to the Event or log in the source chain.\r\n\r\nPlease correct me if I did not understand the attack, I have been going through the spec as well. It would work in this way:\r\n\r\nAssume Chain B has Chain A in the dependency set, and Chain A has Chain B in the dependency set. Assume these chains trust each other, and consider pre-confirmations to be safe, allowing sub-second interop among both chains with minimal latency.\r\n\r\n1. On block X on chain A, there is an NFT collection that lets you mint another NFT if you already have the NFT.\r\n```\r\nfunction mint() public payable {\r\n    if (IERC721(collectionAddress).balanceOf(msg.sender) > 0) {\r\n        // User owns an NFT in collection C\r\n        _mintNFT(msg.sender);\r\n    } else if (msg.value >= 10 ether) {\r\n        // User pays 10 ETH\r\n        _mintNFT(msg.sender);\r\n    } else {\r\n        // Revert transaction with error\r\n        revert(\"Must hold NFT from collection C or pay 10 ETH\");\r\n    }\r\n}\r\n```\r\n2. On block X + 1, Alice mints an NFT sends the NFT to their wallet on Chain B. They send a \"transferNFT\" transaction on Chain A, that locks the NFT in the contract, and emits an event `TransferNFT`.\r\n```\r\nfunction transferNFT(address to, uint256 tokenId) public {\r\n    // Ensure the caller is the owner of the NFT\r\n    require(ownerOf(tokenId) == msg.sender, \"Caller is not the owner\");\r\n\r\n    // Transfer the NFT\r\n    _transfer(msg.sender, to, tokenId);\r\n\r\n    // Emit the TransferNFT event\r\n    emit TransferNFT(msg.sender, to, tokenId);\r\n}\r\n```\r\n 3. Now we can call the [CrossL2Inbox](https://specs.optimism.io/interop/predeploys.html#crossl2inbox) with the [Identifier](https://specs.optimism.io/interop/messaging.html#message-identifier), and specify a target address on Chain B, that will create some sort of wrapped or bridged NFT with the same token ID and send it to Alice's wallet on Chain B. It emits an event `MintBridgedNFT ` on block X + 1. \r\n ```\r\n // Define the BridgedNFT contract\r\ncontract BridgedNFT is ERC721 {\r\n\r\n    // Define the MintBridgedNFT event\r\n    event MintBridgedNFT(address indexed to, uint256 indexed tokenId);\r\n\r\n    // Function to mint bridged NFT\r\n    function mintBridgedNFT(address to, uint256 tokenId) public {\r\n        // Mint the bridged NFT\r\n        _safeMint(to, tokenId);\r\n        // Emit the MintBridgedNFT event\r\n        emit MintBridgedNFT(to, tokenId);\r\n    }\r\n}\r\n```\r\n\r\nNot sure I am seeing how we could use `MintBridgedNFT` emitted on Chain B to mint more free NFTs on Chain A. Could you clarify?\r\n\r\nAlso, [cyclic dependencies](https://specs.optimism.io/interop/messaging.html#cyclic-dependencies) are expected as Chain A must be able to communicate with Chain B and vice versa all the time, and should be possible to resolve if we look at them as a directed graph with vertices as blocks and edges as dependencies.\r\n", "2024-06-05T18:56:13Z", "2024-06-05T18:57:04Z", "SkandaBhat", "2025-08-30 14:15:03"]
["IC_kwDOLB-lzc6AMj5a", "I_kwDOLB-lzc6KEHUJ", "Correct, we need to enforce a relative message ordering to support intra-block (same timestamp) messaging. Otherwise the initiating message can be after the executing message, and break core assumptions.\r\n\r\nIntra-block messaging is an active area of research, and can be trivially disabled by tweaking the timestamp invariant in the smart-contract. Still, this is a good feature, so we are looking for ways to enforce the intra-block ordering elegantly.\r\n\r\nThe not-so-elegant solution would be to enforce a total ordering by event-log index, but when you consider cross-L2 intra-block messaging, this creates an undesired amount of incompatible cross-L2 messages. Ideally we use some form of relative ordering, to allow messages to interact with eachother, while keeping all dependencies in a form of a DAG (i.e. no cycles between messages, but possible still back and forth interactions between two blocks).\r\n\r\nNote that this ordering can be enforced outside of the EVM; just like the dependency correctness is validated by the verifier, the ordering can also be checked.\r\n \r\n", "2024-06-05T19:12:22Z", "2024-06-05T19:12:22Z", "protolambda", "2025-08-30 14:15:03"]
["IC_kwDOLB-lzc532jeE", "I_kwDOLB-lzc6Cg6-S", "Strong +1, this also would enable permisionless payouts for sequencer revenue on the l2 layer", "2024-03-20T22:41:13Z", "2024-03-20T22:41:13Z", "kahuang", "2025-08-30 14:15:03"]
["IC_kwDOLB-lzc538mT5", "I_kwDOLB-lzc6Cg6-S", "+1, especially the automatic targeting. ideally with just setting 2 parameters, margin and max expense. the first for the target profit, the second for indicating the max subsidize cost in case costs > fees. this way, the chain have a floor in expenses and a margin target when profitable automatically adapting user fees.", "2024-03-21T13:57:34Z", "2024-03-21T13:57:34Z", "emilianobonassi", "2025-08-30 14:15:03"]
["IC_kwDOLB-lzc5_dXmt", "I_kwDOLB-lzc6Cg6-S", "Revenue of the chain can be calculated using the fee vaults (total processed + current balance) which avoids needing to observe all receipts\r\n\r\nI think all thats needed is to checkpoint the last payout state in the fee payout contracts:\r\n- what was the cumulative rev at the last payout state\r\n- what was the cumulative da cost at the last payout state\r\n\r\nThen fee payout can calculate current rev (minus last payout state), and current da cost (minus last payout state) to calculate current profit (if any).\r\n\r\nThese values can also be re-used for a profit %age PID?", "2024-05-29T23:18:17Z", "2024-05-29T23:18:17Z", "kahuang", "2025-08-30 14:15:03"]
["IC_kwDOLB-lzc5_dXs1", "I_kwDOLB-lzc6Cg6-S", "Although I think the first step is just getting teh da cost accumulator into op-node so its observable on l2, perhaps with the ability to force an additional value into the accumulator (so we can backfill previous da costs?)", "2024-05-29T23:18:47Z", "2024-05-29T23:19:36Z", "kahuang", "2025-08-30 14:15:03"]
["IC_kwDOLB-lzc5_7Xe3", "I_kwDOLB-lzc6Cg6-S", "> Revenue of the chain can be calculated using the fee vaults (total processed + current balance) which avoids needing to observe all receipts\r\n> \r\n> I think all thats needed is to checkpoint the last payout state in the fee payout contracts:\r\n> \r\n> * what was the cumulative rev at the last payout state\r\n> * what was the cumulative da cost at the last payout state\r\n> \r\n> Then fee payout can calculate current rev (minus last payout state), and current da cost (minus last payout state) to calculate current profit (if any).\r\n> \r\n> These values can also be re-used for a profit %age PID?\r\n\r\nIt is not ideal to use `totalProcessed` for multiple reasons:\r\n- Enshrines more storage slots as consensus critical, meaning that it becomes an area of possible bugs, especially when the contracts are upgradable\r\n- The `FeeVault` contracts have a `receive` function, meaning that one can inflate the value of `totalProcessed`, this opens the door for out of band attacks on consensus\r\n- It requires a poke of the `FeeVault` to update `totalProcessed`, meaning that transactions will need to be sent to update these values. If the sequencer censors those updates, it can attack consensus\r\n- It means that we cannot ever allow for a dynamic fee recipient, such as proposed in a future version of span batches\r\n\r\nI think we can use the block header to get the [base fee](https://github.com/ethereum/go-ethereum/blob/a6751d6fc830b9a1edc963df2c05908f4392e39c/core/types/block.go#L84) + [the total gas used](https://github.com/ethereum/go-ethereum/blob/a6751d6fc830b9a1edc963df2c05908f4392e39c/core/types/block.go#L77) to get around needing to read all of the receipts, although this will not account for tips, which I think is fine under this design.", "2024-06-03T23:03:23Z", "2024-06-03T23:03:23Z", "tynes", "2025-08-30 14:15:03"]
["IC_kwDODjvEJM5_afmG", "I_kwDODjvEJM6J0oJx", "I added information on this to the contracts package here: https://github.com/ethereum-optimism/optimism/pull/10678", "2024-05-29T15:08:33Z", "2024-05-29T15:08:33Z", "tynes", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5_aomY", "I_kwDODjvEJM6J0oJx", "I added information on this to the `op-node` package here: https://github.com/ethereum-optimism/optimism/pull/10679", "2024-05-29T15:25:08Z", "2024-05-29T15:25:08Z", "tynes", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5_dtgU", "I_kwDODjvEJM6J0oJx", "@tynes Thanks for the update, that's very useful! \r\n\r\nAnother big concern for production deployment is how to choose proper configuration values, it will be great if there're guides about this.", "2024-05-30T01:12:42Z", "2024-05-30T01:13:28Z", "zhiqiangxu", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5_vsRI", "I_kwDODjvEJM6J0oJx", "I recommend checking out the values used by OP Mainnet. Some of the values have since become hardcoded as part of consensus as it only adds cognitive overhead being able to modify them and we found sane values", "2024-06-01T01:48:51Z", "2024-06-01T01:48:51Z", "tynes", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM53qW6D", "I_kwDODjvEJM6Cz-wL", "Looks like the batch inbox isn't configured which is leaving `tx.to` unset which is resulting in deployment txs", "2024-03-19T16:10:26Z", "2024-03-19T16:10:26Z", "tynes", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM53uM8s", "I_kwDODjvEJM6Cz-wL", "```\r\n{\r\n  \"genesis\": {\r\n    \"l1\": {\r\n      \"hash\": \"0x6363a6f10143a8354c22be0e5782bc648a6fcfab3802dc2ee3230881228bf358\",\r\n      \"number\": 5495359\r\n    },\r\n    \"l2\": {\r\n      \"hash\": \"0x9b2a860ad408fc483c1caa5d3617dec6ccdc37d6ca465a6978b20294086bebe0\",\r\n      \"number\": 0\r\n    },\r\n    \"l2_time\": 1710562272,\r\n    \"system_config\": {\r\n      \"batcherAddr\": \"0xb773ad428540d19fb6f4cddd131e780b87838ad1\",\r\n      \"overhead\": \"0x0000000000000000000000000000000000000000000000000000000000000834\",\r\n      \"scalar\": \"0x00000000000000000000000000000000000000000000000000000000000f4240\",\r\n      \"gasLimit\": 30000000,\r\n      \"baseFeeScalar\": 0,\r\n      \"blobBaseFeeScalar\": 0\r\n    }\r\n  },\r\n  \"block_time\": 2,\r\n  \"max_sequencer_drift\": 600,\r\n  \"seq_window_size\": 3600,\r\n  \"channel_timeout\": 300,\r\n  \"l1_chain_id\": 11155111,\r\n  \"l2_chain_id\": 42069,\r\n  \"regolith_time\": 0,\r\n  \"canyon_time\": 0,\r\n  \"batch_inbox_address\": \"0xff00000000000000000000000000000000042069\",\r\n  \"deposit_contract_address\": \"0x579973566dc062223746c0d283473f75cf8d9b9e\",\r\n  \"l1_system_config_address\": \"0x446be58aef502fff94f77a0adf3852b2e1fbdcbe\",\r\n  \"protocol_versions_address\": \"0x0000000000000000000000000000000000000000\"\r\n}\r\n```\r\nThis is my rollup configuration, batch_inbox_address is set.\r\n@tynes \r\n\r\n> Looks like the batch inbox isn't configured which is leaving `tx.to` unset which is resulting in deployment txs\r\n\r\n", "2024-03-20T03:13:07Z", "2024-03-21T03:09:59Z", "mrdotparasyte", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5_2diY", "I_kwDODjvEJM6Cz-wL", "@zmovane what L1 RPC endpoint were you using? Did you come across this again? We experienced this a couple of times internally and switching to a properly synced L1 geth node fixed it in our cases.", "2024-06-03T11:43:33Z", "2024-06-03T11:43:33Z", "sebastianst", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5_57Un", "I_kwDODjvEJM6Cz-wL", "Looks like this is an issue with Erigon, sent a bug report https://github.com/ledgerwatch/erigon/issues/10607", "2024-06-03T18:45:50Z", "2024-06-03T18:45:50Z", "sebastianst", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5_-KBm", "I_kwDODjvEJM6Cz-wL", "It got fixed on their end.", "2024-06-04T09:06:27Z", "2024-06-04T09:06:27Z", "sebastianst", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM6AI8Hj", "I_kwDODjvEJM6Cz-wL", "Fix is in [Erigon/v2.60.1](https://github.com/ledgerwatch/erigon/releases/tag/v2.60.1)", "2024-06-05T13:04:25Z", "2024-06-05T13:04:25Z", "sebastianst", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5Q6T51", "I_kwDODjvEJM5ZUNDv", "Little update on the matter: the same temporary error happened this weekend at a different height using the same L1 RPC.\r\nThis time, re-running op-node made it resync and fixed the issue.\r\nI'll take some time to try and make the bug reproducible.", "2022-12-19T11:01:21Z", "2022-12-19T11:01:21Z", "Kelvyne", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5RuI3s", "I_kwDODjvEJM5ZUNDv", "I believe this was fixed by #4494", "2023-01-04T15:09:27Z", "2023-01-04T15:09:27Z", "Kelvyne", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5lne99", "I_kwDODjvEJM5ZUNDv", "I am facing a similar issue but don't know how to resolve it. Can you give a little more detail on how to fix this. I am setting up a base mainnet node and sync is stuck from a day. logs of op-node container shows the same err warning.", "2023-09-04T08:37:40Z", "2023-09-04T08:37:40Z", "Mrigesh901", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5r8iU1", "I_kwDODjvEJM5ZUNDv", "This happens on op-goerli now", "2023-11-14T19:16:11Z", "2023-11-14T19:16:11Z", "koraykoska", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5xsyoJ", "I_kwDODjvEJM5ZUNDv", "also happens now, op-node version is v1.4.0", "2024-01-24T07:41:38Z", "2024-01-24T07:41:38Z", "wangjiangw", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM52tFxP", "I_kwDODjvEJM5ZUNDv", "I am seeing these error right now in my node:\r\n```\r\nMar 12 09:42:10 ip-my-server bash[4347]: t=2024-03-10T09:42:10+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=27896 err=\"temp: failed to fetch receipts of L1 block 0x8132291a8005c6d4e325b42f588149e80e5e15ac88ff6850ee54ab5aeca5e18a:18916133 (parent: 0x2b15a6b95357ebfa4e3a525dc048b218d62104adf2e4922db3e3702afe54c257:18916132) for L1 sysCfg update: invalid receipts: receipt 160 has unexpected tx index 0\"\r\n```\r\n\r\nHow to fix that?", "2024-03-12T12:23:53Z", "2024-03-12T12:23:53Z", "Rogalek", "2025-08-30 14:15:17"]
["IC_kwDODjvEJM5_4cPu", "I_kwDODjvEJM5ZUNDv", "Having same issue here with Blast (optimism fork)\r\nIs it possible to clear the specific cache on my L1 for this specific block?", "2024-06-03T15:23:45Z", "2024-06-03T15:23:45Z", "sahfd4eiugf", "2025-08-30 14:15:17"]
["IC_kwDOH2Qg5s50Zqsj", "I_kwDOH2Qg5s5_rxwj", "@sbvegan FYI", "2024-02-19T17:00:58Z", "2024-02-19T17:00:58Z", "0xmariniere", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s500kux", "I_kwDOH2Qg5s5_rxwj", "## Reproducing\r\n\r\nI was able to reproduce the same behavior, but I'm not familiar with tracing. My first note is that, `op-geth` will be very close to `geth`, see https://op-geth.optimism.io/. In the [geth docs](https://geth.ethereum.org/docs/developers/evm-tracing/built-in-tracers#native-tracers) I'm only seeing `callTracer` and not `flatCallTracer`. We're flagging this internally.\r\n\r\n### Actual Behavior - `flatCallTracer`\r\n\r\nRequest:\r\n\r\n```\r\nsoyboy@soyboys-MacBook-Air ~ % curl -X POST \\\r\n--data '[\r\n  {\r\n    \"jsonrpc\": \"2.0\",\r\n    \"method\": \"debug_traceBlockByNumber\",\r\n    \"params\": [\r\n      \"0x660473A\",\r\n      {\r\n        \"tracer\": \"flatCallTracer\"\r\n      }\r\n    ],\r\n    \"id\": 0\r\n  }\r\n]' \\\r\n-H \"Content-Type: application/json\" \\\r\n<internal-developer-rpc-endpoint> | jq\r\n```\r\n\r\nResponse:\r\n\r\n```\r\n[\r\n  {\r\n    \"jsonrpc\": \"2.0\",\r\n    \"error\": {\r\n      \"code\": -32000,\r\n      \"message\": \"unrecognized call frame type: STOP\"\r\n    },\r\n    \"id\": 0\r\n  }\r\n]\r\n```\r\n\r\n### Actual Behavior - `callTracer`\r\n\r\nRequest:\r\n\r\n```\r\ncurl -X POST \\\r\n--data '[\r\n  {\r\n    \"jsonrpc\": \"2.0\",\r\n    \"method\": \"debug_traceBlockByNumber\",\r\n    \"params\": [\r\n      \"0x660473A\",\r\n      {\r\n        \"tracer\": \"callTracer\"\r\n      }\r\n    ],\r\n    \"id\": 0\r\n  }\r\n]' \\\r\n-H \"Content-Type: application/json\" \\\r\n<internal-developer-rpc-endpoint> | jq\r\n```\r\n\r\nResponse:\r\n\r\n```\r\n[\r\n  {\r\n    \"jsonrpc\": \"2.0\",\r\n    \"result\": [\r\n      {\r\n        \"result\": {\r\n          \"calls\": [\r\n            {\r\n              \"from\": \"0x4200000000000000000000000000000000000015\",\r\n              \"gas\": \"0xe9a0d\",\r\n              \"gasUsed\": \"0x6af8\",\r\n              \"input\": \"0x015d8eb900000000000000000000000000000000000000000000000000000000010e36800000000000000000000000000000000000000000000000000000000064b467e700000000000000000000000000000000000000000000000000000003584922358bb001d5a5f8872522e82ed02f58008f78d434eedd0a5e9745b2a74bf2f94a2a00000000000000000000000000000000000000000000000000000000000000000000000000000000000000006887246668a3b87f54deb3b94ba47a6f63f3298500000000000000000000000000000000000000000000000000000000000000bc00000000000000000000000000000000000000000000000000000000000a6fe0\",\r\n              \"to\": \"0xc0d3c0d3c0d3c0d3c0d3c0d3c0d3c0d3c0d30015\",\r\n              \"type\": \"DELEGATECALL\",\r\n              \"value\": \"0x0\"\r\n            }\r\n          ],\r\n          \"from\": \"0xdeaddeaddeaddeaddeaddeaddeaddeaddead0001\",\r\n          \"gas\": \"0xf4240\",\r\n          \"gasUsed\": \"0xc539\",\r\n          \"input\": \"0x015d8eb900000000000000000000000000000000000000000000000000000000010e36800000000000000000000000000000000000000000000000000000000064b467e700000000000000000000000000000000000000000000000000000003584922358bb001d5a5f8872522e82ed02f58008f78d434eedd0a5e9745b2a74bf2f94a2a00000000000000000000000000000000000000000000000000000000000000000000000000000000000000006887246668a3b87f54deb3b94ba47a6f63f3298500000000000000000000000000000000000000000000000000000000000000bc00000000000000000000000000000000000000000000000000000000000a6fe0\",\r\n          \"to\": \"0x4200000000000000000000000000000000000015\",\r\n          \"type\": \"CALL\",\r\n          \"value\": \"0x0\"\r\n        },\r\n        \"txHash\": \"0x53c37d6a4bff2e8adec4f681caf0a5f50af9946cfcc37545f86821237b150573\"\r\n      },\r\n      {\r\n        \"result\": {\r\n          \"from\": \"0x0000000000000000000000000000000000000000\",\r\n          \"gas\": \"0x0\",\r\n          \"gasUsed\": \"0x520c\",\r\n          \"input\": \"0x\",\r\n          \"type\": \"STOP\"\r\n        },\r\n        \"txHash\": \"0xa47277b3d92384e70906bed705723a15c91d9f0fff2fc602824d14ea201801c1\"\r\n      },\r\n      // omitted the rest of the response \r\n    ],\r\n    \"id\": 0\r\n  }\r\n]\r\n```", "2024-02-22T17:36:32Z", "2024-02-22T17:47:38Z", "sbvegan", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s501zDl", "I_kwDOH2Qg5s5_rxwj", "I cannot find `flatCallTracer` https://geth.ethereum.org/docs/interacting-with-geth/rpc/ns-debug#traceconfig. Where did you find this tracer configuration option?", "2024-02-22T20:28:40Z", "2024-02-22T20:28:40Z", "sbvegan", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5018Ns", "I_kwDOH2Qg5s5_rxwj", "Hey @sbvegan - for reference that tracer was introduced in Geth 1.12.0 - https://github.com/ethereum/go-ethereum/releases/tag/v1.12.0 (in release notes you can find - https://github.com/ethereum/go-ethereum/pull/27304) and is supported by Optimism as of now\r\n\r\nhttps://ethereum.org/pl/developers/docs/evm/opcodes <- STOP opcode should not be returned by flatCallTracer", "2024-02-22T20:55:01Z", "2024-02-27T08:54:35Z", "MRabenda", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s51L3TR", "I_kwDOH2Qg5s5_rxwj", "@sbvegan yeah seems like they did not add this in their documentation but as @MRabenda pointed out; it is indeed supported by `geth`.\r\n\r\nAny updates on this?", "2024-02-27T08:41:31Z", "2024-02-27T08:41:31Z", "achmand", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s51McD2", "I_kwDOH2Qg5s5_rxwj", "@sbvegan we did additional investigation and issue is deeper, not in flatCallTracer. Please try to execute \r\n\r\n```\r\n{  \r\n    \"method\": \"debug_traceTransaction\",                                   \r\n    \"params\": [\r\n        \"0xa47277b3d92384e70906bed705723a15c91d9f0fff2fc602824d14ea201801c1\",\r\n        {\r\n            \"tracer\": \"callTracer\"\r\n        }\r\n    ],\r\n    \"id\": 0,\r\n    \"jsonrpc\": \"2.0\"\r\n  }\r\n  ```\r\n  \r\n  As a response you will get \r\n  \r\n  ```\r\n  {\r\n    \"jsonrpc\": \"2.0\",\r\n    \"id\": 0,\r\n    \"result\": {\r\n        \"from\": \"0x0000000000000000000000000000000000000000\",\r\n        \"gas\": \"0x0\",\r\n        \"gasUsed\": \"0x520c\",\r\n        \"input\": \"0x\",\r\n        \"type\": \"STOP\"\r\n    }\r\n}\r\n```\r\n\r\nAnd probably this is core issue here\r\n  As a response you will get ", "2024-02-27T10:04:07Z", "2024-02-27T10:04:07Z", "MRabenda", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s51OAHx", "I_kwDOH2Qg5s5_rxwj", "Yep I got the same response, I'll work on getting this in front of a client engineer:\r\n\r\n## Reproducing\r\n\r\n```\r\ncurl -X POST \\                 \r\n--data '[\r\n  {  \r\n    \"method\": \"debug_traceTransaction\",                                   \r\n    \"params\": [                      \r\n        \"0xa47277b3d92384e70906bed705723a15c91d9f0fff2fc602824d14ea201801c1\",\r\n        {\r\n            \"tracer\": \"callTracer\"\r\n        }\r\n    ],\r\n    \"id\": 0,\r\n    \"jsonrpc\": \"2.0\"\r\n  }\r\n]' \\\r\n-H \"Content-Type: application/json\" \\\r\n<internal-endpoint> | jq\r\n\r\n# response\r\n\r\n[\r\n  {\r\n    \"jsonrpc\": \"2.0\",\r\n    \"result\": {\r\n      \"from\": \"0x0000000000000000000000000000000000000000\",\r\n      \"gas\": \"0x0\",\r\n      \"gasUsed\": \"0x520c\",\r\n      \"input\": \"0x\",\r\n      \"type\": \"STOP\"\r\n    },\r\n    \"id\": 0\r\n  }\r\n]\r\n```", "2024-02-27T13:55:33Z", "2024-02-27T13:55:33Z", "sbvegan", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s6BINSo", "I_kwDOH2Qg5s5_rxwj", "It was solved by https://github.com/ethereum-optimism/op-geth/pull/265/files", "2024-06-13T17:36:41Z", "2024-06-13T17:36:41Z", "MRabenda", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5zjZDm", "I_kwDOH2Qg5s5-kIDn", "Have you observed any anomalies with L1RPC?", "2024-02-12T13:13:21Z", "2024-02-12T13:13:21Z", "opfocus", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5zjyCF", "I_kwDOH2Qg5s5-kIDn", "> Have you observed any anomalies with L1RPC?\r\n\r\nNo, we are using Geth it is healthy and synced up to blockheight.", "2024-02-12T14:14:54Z", "2024-02-12T14:14:54Z", "valamidev", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5zjzoY", "I_kwDOH2Qg5s5-kIDn", "What makes me very curious why this `12h 6m` delay is happening:\r\n```\r\nINFO [02-12|14:15:13.993] Chain head was updated                   number=116,052,473 hash=cddcb0..833f29 root=859c8b..ff58fe elapsed=\"207.15\u00b5s\"  age=12h6m30s\r\nINFO [02-12|14:15:14.073] Starting work on payload                 id=0x49439aae3eda3936\r\nINFO [02-12|14:15:14.077] Imported new potential chain segment     number=116,052,474 hash=530161..bdd5c2 blocks=1 txs=1 mgas=0.047 elapsed=1.687ms     mgasps=27.802  age=12h6m29s  snapdiffs=1.41MiB    triedirty=3.14MiB\r\nINFO [02-12|14:15:14.079] Chain head was updated                   number=116,052,474 hash=530161..bdd5c2 root=544bcf..80c0f0 elapsed=\"415.963\u00b5s\" age=12h6m29s\r\n```\r\n\r\nMore than 2 weeks passed and the Node is always keep this \"distance\" from the blockheight, sometimes more but never less than `12h 6m`.", "2024-02-12T14:18:50Z", "2024-02-12T14:19:01Z", "valamidev", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5zn0m4", "I_kwDOH2Qg5s5-kIDn", "Can you provide information about the _sync progress_ entry in the op-node log and whether it indicates that batches have been dropped?", "2024-02-13T00:02:57Z", "2024-02-13T00:02:57Z", "opfocus", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5zr66J", "I_kwDOH2Qg5s5-kIDn", "> Can you provide information about the _sync progress_ entry in the op-node log and whether it indicates that batches have been dropped?\r\n\r\nIt seems nominal:\r\n```\r\nt=2024-02-13T09:25:48+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x7731b5df70446de2dd1d4d59e403020cd8c2d4d27485e0ab48a8fa7ddb263a92:116108785\r\nt=2024-02-13T09:25:48+0000 lvl=info msg=\"Reading channel\"                        channel=353a41d213a6c21f1195d40dc405aad6 frames=1\r\nt=2024-02-13T09:25:48+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0x6aa89646e9615ae08d2f397c02a33bd47839d22a59133f88c24f5a7a2c3d896d:19215990 originBehind=false\r\nt=2024-02-13T09:25:49+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0xb6a6eb14ef77d88c702355adc32066acb5da335e3d7f50e0bfbd9d94ff089f60:19215991 originBehind=false\r\nt=2024-02-13T09:25:49+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0xe08f2450624a5efd8e4303535af191661a8fe1abc7c74bb2f3bb45225db25324:19215992 originBehind=false\r\nt=2024-02-13T09:25:49+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x8b588f1a01a4081fb8e10860f1066eb927a41d0d87a5b69262d9f0c3d1e4c1db:116108786 peer=16Uiu2HAmJMyUxRaVq689PEAcyWQFS5LYyZUA4zZmboUtnsBFsX3i\r\nt=2024-02-13T09:25:50+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x8b588f1a01a4081fb8e10860f1066eb927a41d0d87a5b69262d9f0c3d1e4c1db:116108786\r\nt=2024-02-13T09:25:50+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0x7cb7d061bd76c75e36b74b5e59ba836a1653cf69877834d83c190bd3fb2da7f7:19215993 originBehind=false\r\nt=2024-02-13T09:25:50+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0xcddd5c64e83fa83111f2829d5f352cc049f29d8021a723d0d03e93c8104416a1:19215994 originBehind=false\r\nt=2024-02-13T09:25:51+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x34d4a344476fcdc1f407d81ab8f05fdfa11d37aff4afa213895f5f20708628ac:116108787 peer=16Uiu2HAm423f1zgf1HqGbL9x5zYJmkzgK1wjgqC23AiGRHEh5ynA\r\nt=2024-02-13T09:25:51+0000 lvl=warn msg=\"failed to serve p2p sync request\"       serve=payloads_by_number peer=16Uiu2HAmS-51339 remote=/ip4/54.160.33.54/tcp/9003     req=116,108,778 err=\"peer requested unknown block by number: not found\"\r\nt=2024-02-13T09:25:51+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0xf450f5bedf3d5d7a0f553c1ad84af30ada8ac4429150befeb9b1896fd8d809f8:19215995 originBehind=false\r\nt=2024-02-13T09:25:51+0000 lvl=info msg=\"created new channel\"                    origin=0xf450f5bedf3d5d7a0f553c1ad84af30ada8ac4429150befeb9b1896fd8d809f8:19215995 channel=1313f9c6c7e5b229abd0675b6eab3a37 length=118,456 frame_number=0 is_last=true\r\nt=2024-02-13T09:25:51+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x34d4a344476fcdc1f407d81ab8f05fdfa11d37aff4afa213895f5f20708628ac:116108787\r\nt=2024-02-13T09:25:51+0000 lvl=info msg=\"Reading channel\"                        channel=1313f9c6c7e5b229abd0675b6eab3a37 frames=1\r\nt=2024-02-13T09:25:52+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0x726ba79c5c7c5a76a11673bda2df9d81dd62e8e58bb15b9b65bb9bcea684a043:19215996 originBehind=false\r\nt=2024-02-13T09:25:52+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0x5070f5560f49c3587a8233d432a70597e39b96e924e2bf6265e14a67a341e55b:19215997 originBehind=false\r\nt=2024-02-13T09:25:53+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0x52bd4507e8a34135731e439e6b7373fbf07829eaa4305788b8a21c4f7c0f031a:19215998 originBehind=false\r\nt=2024-02-13T09:25:53+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xe80138762b60536b3000636a72f545909e8680e9051ea4ae4ef48d5c9337ea1d:116108788 peer=16Uiu2HAm8Xug1ovyDgEya1JMJU5o1MsJ5qD1cA2pVePoB1NwafJv\r\nt=2024-02-13T09:25:53+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0x4578ccbb5800c90d3231d300c897c3ddd6ae84a8baf1550f0ad2158cb9e97ea4:19215999 originBehind=false\r\nt=2024-02-13T09:25:53+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0x2676a1507e780a6f309638e5564c5a74cf6b79ebf55f215fc0dac574a5b41308:19216000 originBehind=false\r\nt=2024-02-13T09:25:53+0000 lvl=info msg=\"created new channel\"                    origin=0x2676a1507e780a6f309638e5564c5a74cf6b79ebf55f215fc0dac574a5b41308:19216000 channel=d9f7bae7bca84f59ab4b16dab8274c92 length=119,244 frame_number=0 is_last=true\r\nt=2024-02-13T09:25:53+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xe80138762b60536b3000636a72f545909e8680e9051ea4ae4ef48d5c9337ea1d:116108788\r\nt=2024-02-13T09:25:53+0000 lvl=info msg=\"Reading channel\"                        channel=d9f7bae7bca84f59ab4b16dab8274c92 frames=1\r\nt=2024-02-13T09:25:54+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0xcffb438205989e7b2efe00b9fc137c336133211af8d95b2805fe936a3089445f:19216001 originBehind=false\r\nt=2024-02-13T09:25:54+0000 lvl=info msg=\"Advancing bq origin\"                    origin=0x9801571404954fb1807766e9ebc9bf862fc84e93172f52bbf2c591f36e37facd:19216002 originBehind=false\r\n```", "2024-02-13T09:26:53Z", "2024-02-13T09:26:53Z", "valamidev", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5-485N", "I_kwDOH2Qg5s5-kIDn", "Problem found, there was issue with finding P2P peers.", "2024-05-24T08:05:08Z", "2024-05-24T08:05:08Z", "valamidev", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5-8u2X", "I_kwDOH2Qg5s5-kIDn", "> blem found, there was issue with finding P2P peers.\r\n\r\n\r\nhow did you solve it", "2024-05-24T15:37:59Z", "2024-05-24T15:37:59Z", "opfocus", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s5-9Err", "I_kwDOH2Qg5s5-kIDn", "> > blem found, there was issue with finding P2P peers.\r\n> \r\n> how did you solve it\r\n\r\nWe had issue with forwarding ports and `nodiscovery`", "2024-05-24T16:24:06Z", "2024-05-24T16:24:06Z", "valamidev", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s6Av6xP", "I_kwDOH2Qg5s5-kIDn", "> > > blem found, there was issue with finding P2P peers.\r\n> > \r\n> > \r\n> > how did you solve it\r\n> \r\n> We had issue with forwarding ports and `nodiscovery`\r\n\r\nI am facing the same issue.\r\nCould you please explain a bit more? Thank you.", "2024-06-11T07:56:48Z", "2024-06-11T07:56:48Z", "zeGzD", "2025-08-30 15:17:16"]
["IC_kwDOH2Qg5s6AzBUA", "I_kwDOH2Qg5s5-kIDn", "> > > > blem found, there was issue with finding P2P peers.\r\n> > > \r\n> > > \r\n> > > how did you solve it\r\n> > \r\n> > \r\n> > We had issue with forwarding ports and `nodiscovery`\r\n> \r\n> I am facing the same issue. Could you please explain a bit more? Thank you.\r\n\r\nI think he means to remove this configuration` --nodiscover `on op-geth.", "2024-06-11T14:06:08Z", "2024-06-11T14:06:21Z", "opfocus", "2025-08-30 15:17:16"]
["IC_kwDOLB-lzc52mS8R", "I_kwDOLB-lzc6AdB2E", "An initial + very minimal diff to the `OptimismPortal` to add custom gas token:\r\n\r\n```diff\r\ndiff --git a/packages/contracts-bedrock/src/L1/OptimismPortal.sol b/packages/contracts-bedrock/src/L1/OptimismPortal.sol\r\nindex 30e6bd471..b6f511756 100644\r\n--- a/packages/contracts-bedrock/src/L1/OptimismPortal.sol\r\n+++ b/packages/contracts-bedrock/src/L1/OptimismPortal.sol\r\n@@ -14,6 +14,7 @@ import { AddressAliasHelper } from \"src/vendor/AddressAliasHelper.sol\";\r\n import { ResourceMetering } from \"src/L1/ResourceMetering.sol\";\r\n import { ISemver } from \"src/universal/ISemver.sol\";\r\n import { Constants } from \"src/libraries/Constants.sol\";\r\n+import { SafeTransferLib } from \"solady/utils/SafeTransferLib.sol\";\r\n \r\n /// @custom:proxied\r\n /// @title OptimismPortal\r\n@@ -64,6 +65,13 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n     /// @custom:network-specific\r\n     SystemConfig public systemConfig;\r\n \r\n+    /// @notice Address of the gas paying token.\r\n+    /// @custom:network-specific\r\n+    address public gasPayingToken;\r\n+\r\n+    /// @notice The total amount of gas paying asset in the contract.\r\n+    uint256 internal _balance;\r\n+\r\n     /// @notice Emitted when a transaction is deposited from L1 to L2.\r\n     ///         The parameters of this event are read by the rollup node and used to derive deposit\r\n     ///         transactions on L2.\r\n@@ -100,6 +108,7 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n             _l2Oracle: L2OutputOracle(address(0)),\r\n             _systemConfig: SystemConfig(address(0)),\r\n             _superchainConfig: SuperchainConfig(address(0))\r\n+            _gasPayingToken: address(0),\r\n         });\r\n     }\r\n \r\n@@ -107,10 +116,13 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n     /// @param _l2Oracle Contract of the L2OutputOracle.\r\n     /// @param _systemConfig Contract of the SystemConfig.\r\n     /// @param _superchainConfig Contract of the SuperchainConfig.\r\n+    /// @param _gasPayingToken ERC20 token used to pay gas on L2.\r\n+    ///        Set to `address(0)` for ether.\r\n     function initialize(\r\n         L2OutputOracle _l2Oracle,\r\n         SystemConfig _systemConfig,\r\n-        SuperchainConfig _superchainConfig\r\n+        SuperchainConfig _superchainConfig,\r\n+        address _gasPayingToken\r\n     )\r\n         public\r\n         initializer\r\n@@ -118,6 +130,7 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n         l2Oracle = _l2Oracle;\r\n         systemConfig = _systemConfig;\r\n         superchainConfig = _superchainConfig;\r\n+        gasPayingToken = _gasPayingToken;\r\n         if (l2Sender == address(0)) {\r\n             l2Sender = Constants.DEFAULT_L2_SENDER;\r\n         }\r\n@@ -173,6 +186,15 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n         return _byteCount * 16 + 21000;\r\n     }\r\n \r\n+    /// @notice Retuns the balance of the contract in the smallest units.\r\n+    function balance() public pure returns (uint256) {\r\n+        if (gasPayingToken == address(0)) {\r\n+            return address(this).balance;\r\n+        } else {\r\n+            return _balance;\r\n+        }\r\n+    }\r\n+\r\n     /// @notice Accepts value so that users can send ETH directly to this contract and have the\r\n     ///         funds be deposited to their address on L2. This is intended as a convenience\r\n     ///         function for EOAs. Contracts should call the depositTransaction() function directly\r\n@@ -344,7 +366,20 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n         //   2. The amount of gas provided to the execution context of the target is at least the\r\n         //      gas limit specified by the user. If there is not enough gas in the current context\r\n         //      to accomplish this, `callWithMinGas` will revert.\r\n-        bool success = SafeCall.callWithMinGas(_tx.target, _tx.gasLimit, _tx.value, _tx.data);\r\n+        bool success = true;\r\n+        // If either is the gas paying asset or if the custom gas token chain withdrawal is a call (no value)\r\n+        if (gasPayingToken == address(0) || _tx.value == 0) {\r\n+            success = SafeCall.callWithMinGas(_tx.target, _tx.gasLimit, _tx.value, _tx.data);\r\n+        } else {\r\n+            _balance -= _tx.value;\r\n+\r\n+            // this reverts on failure\r\n+            SafeTransferLib.safeTransfer({\r\n+                token: gasPayingToken\r\n+                to: _tx.target,\r\n+                value: _tx.value\r\n+            });\r\n+        }\r\n \r\n         // Reset the l2Sender back to the default value.\r\n         l2Sender = Constants.DEFAULT_L2_SENDER;\r\n@@ -361,6 +396,39 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n         }\r\n     }\r\n \r\n+    // This is the entrypoint to depositing an ERC20 token as a custom gas token.\r\n+    function depositERC20Transaction(\r\n+        address _to\r\n+        uint256 _mint,\r\n+        uint256 _value,\r\n+        uint64 _gasLimit,\r\n+        bool _isCreation,\r\n+        bytes memory _data\r\n+    ) public metered(_gasLimit) {\r\n+        // Can only be called if an ERC20 token is used for gas paying on L2\r\n+        require(gasPayingToken != address(0), \"OptimismPortal: only custom gas token\");\r\n+\r\n+        // Pulls ownership of custom gas token to portal\r\n+        SafeTransferLib.safeTransferFrom2({\r\n+            token: gasPayingToken,\r\n+            from: msg.sender,\r\n+            to: address(this),\r\n+            amount: _mint,\r\n+        });\r\n+\r\n+        // Overflow protection here ensures safety on L2 from overflows in balance\r\n+        _balance += _mint;\r\n+\r\n+        _depositTransaction({\r\n+            _to: _to,\r\n+            _mint: _mint,\r\n+            _value: _value,\r\n+            _gasLimit: _gasLimit,\r\n+            _isCreation: _isCreation,\r\n+            _data: _data\r\n+        });\r\n+    }\r\n+\r\n     /// @notice Accepts deposits of ETH and data, and emits a TransactionDeposited event for use in\r\n     ///         deriving deposit transactions. Note that if a deposit is made by a contract, its\r\n     ///         address will be aliased when retrieved using `tx.origin` or `msg.sender`. Consider\r\n@@ -381,6 +449,28 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n         payable\r\n         metered(_gasLimit)\r\n     {\r\n+        if (gasPayingToken != address(0)) {\r\n+            require(msg.value == 0, \"OptimismPortal: cannot send ETH with custom gas token\");\r\n+        }\r\n+\r\n+        _depositTransaction({\r\n+            _to: _to,\r\n+            _mint: msg.value,\r\n+            _value: _value,\r\n+            _gasLimit: _gasLimit,\r\n+            _isCreation: _isCreation,\r\n+            _data: _data\r\n+        });\r\n+    }\r\n+\r\n+    function _depositTransaction(\r\n+        address _to,\r\n+        uint256 _mint,\r\n+        uint256 _value,\r\n+        uint64 _gasLimit,\r\n+        bool _isCreation,\r\n+        bytes memory _data\r\n+    ) internal {\r\n         // Just to be safe, make sure that people specify address(0) as the target when doing\r\n         // contract creations.\r\n         if (_isCreation) {\r\n@@ -406,7 +496,7 @@ contract OptimismPortal is Initializable, ResourceMetering, ISemver {\r\n         // Compute the opaque data that will be emitted as part of the TransactionDeposited event.\r\n         // We use opaque data so that we can update the TransactionDeposited event in the future\r\n         // without breaking the current interface.\r\n-        bytes memory opaqueData = abi.encodePacked(msg.value, _value, _gasLimit, _isCreation, _data);\r\n+        bytes memory opaqueData = abi.encodePacked(_mint, _value, _gasLimit, _isCreation, _data);\r\n \r\n         // Emit a TransactionDeposited event so that the rollup node can derive a deposit\r\n         // transaction for this deposit.\r\n```", "2024-03-12T01:33:01Z", "2024-03-12T01:33:01Z", "tynes", "2025-08-30 15:17:18"]
["IC_kwDOLB-lzc54dq7E", "I_kwDOLB-lzc6AdB2E", "+1, need a standard here that is forwards compatible with interop/fault proofs etc", "2024-03-26T17:22:21Z", "2024-03-26T17:22:21Z", "kahuang", "2025-08-30 15:17:18"]
["IC_kwDOLB-lzc57KJLf", "I_kwDOLB-lzc6AdB2E", "We've previously deployed an Optimism-based rollup (pre-Bedrock) with ERC-20 as gas token, [here is the implementation](https://github.com/drinkius/GTON-Network/commit/89426812c9989bf344def1b8596477b88adb5a21#diff-88187d88043d4fc46e2c5c3c799e49f8ce4dcd351ae417c961d9f733fc3458a4) that we've come up with to keep the diff to a minimum\r\n\r\nIt seems that there might be more changes if we try to keep WETH predeploy address to always be same as L1 native token, in our implementation we avoided that and updates mostly were confined to contracts. Also, we should keep in mind that ERC-20 gas token rollups might be used on top of other L2s so not sure adhering to WETH naming might be good, it's better to have WNATIVE or something similar\r\n\r\nI'm still not sure how to combine two implementations in one repo painlessly without disrupting existing rollup implementation and reusing existing tests, since really the logics might stay the same but slight differences in contracts' naming would require duplicating the calling logics", "2024-04-19T10:09:37Z", "2024-04-19T10:09:37Z", "drinkius", "2025-08-30 15:17:18"]
["IC_kwDOLB-lzc6BJrhM", "I_kwDOLB-lzc6AdB2E", "@drinkius Appreciate you sharing this, we have implemented custom gas token support, see https://github.com/ethereum-optimism/specs/discussions/140", "2024-06-13T21:08:03Z", "2024-06-13T21:08:03Z", "tynes", "2025-08-30 15:17:18"]
["IC_kwDODjvEJM6A1xA_", "I_kwDODjvEJM6L6Cxi", "My branch was before the commit where the indexer was removed, I'm dumb lol", "2024-06-11T20:47:21Z", "2024-06-11T20:47:21Z", "fahimahmedx", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM6A1lSA", "I_kwDODjvEJM6IiOCP", "Its better to ask this question here: https://github.com/ethereum-optimism/developers/discussions", "2024-06-11T20:11:23Z", "2024-06-11T20:11:23Z", "tynes", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5yzVHb", "I_kwDODjvEJM55pBEO", "Generally it's best to look at Mission Requests or to look at anything tagged with `good-first-issue`.", "2024-02-05T01:28:33Z", "2024-02-05T01:28:33Z", "smartcontracts", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5mZM-3", "I_kwDODjvEJM5wq4uZ", "> I am deploying my OP-Stack chain on the Kubernetes cluster. I am using helm charts and op-geth, op-node docker Images. my op-geth is running successfully in a pod.\r\n> \r\n> but when I am trying to run the op-node I am getting RPC error. I am using op-node Docker Image from release-[1.1.4](https://us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:v1.1.4) .\r\n> \r\n> **my values.yaml file -**\r\n> \r\n> ```\r\n> replicaCount: 1\r\n> \r\n> image:\r\n>   repository: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node\r\n>   pullPolicy: IfNotPresent\r\n>   # Overrides the image tag whose default is the chart appVersion.\r\n>   tag: d826cb018955d342779ccc0ebdf878cfa746e765\r\n> \r\n> nameOverride: \"\"\r\n> fullnameOverride: \"\"\r\n> \r\n> service:\r\n>   type: LoadBalancer\r\n> \r\n> resources:\r\n>   {}\r\n>   # We usually recommend not to specify default resources and to leave this as a conscious\r\n>   # choice for the user. This also increases chances charts run on environments with little\r\n>   # resources, such as Minikube. If you do want to specify resources, uncomment the following\r\n>   # lines, adjust them as necessary, and remove the curly braces after 'resources:'.\r\n>   # limits:\r\n>   #   cpu: 100m\r\n>   #   memory: 128Mi\r\n>   # requests:\r\n>   #   cpu: 100m\r\n>   #   memory: 128Mi\r\n> \r\n> nodeSelector: {}\r\n> \r\n> tolerations: []\r\n> \r\n> affinity: {}\r\n> \r\n> #######################################\r\n> # op-node settings for Goerli testnet.\r\n> #######################################\r\n> livenessProbe: true\r\n> readinessProbe: true\r\n> \r\n> secretForVolumeMountDir: /secrets\r\n> configmapsForVolumeMountDir: /configmaps\r\n> \r\n> l1:\r\n>   rpcAddr: https://eth-goerli.g.alchemy.com/v2/e0CsbXjGCT0xVVFc9MyaE7-olvSVAh4S\r\n>   rpcKind: alchemy\r\n> \r\n> l2:\r\n>   rpcAddr: http://0.0.0.0:8551\r\n>   jwtSecret:\r\n>     file: abf3a433bfe1f8faa262ad82b3ec5fa572e9c045a4c44bd3b21998e77fd3632a\r\n>     filename: jwt-secret.txt\r\n> \r\n> sequencer:\r\n>   enabled: true\r\n>  # l1Confirmations: 3\r\n>   p2pKey: 87ab7e053da865130601efdb8fdd22d0197bab98fb7435fadac73cab6ed94fcc\r\n> \r\n> #verifier:\r\n> #  l1Confirmations: 3\r\n> \r\n> p2p:\r\n>   enabled: true\r\n>   priv:\r\n>     file:\r\n>     filename: p2ppriv.txt\r\n>   listen:\r\n>     ip: 0.0.0.0\r\n>     tcp:\r\n>       name: p2p-tcp-port\r\n>       containerPort: 9003\r\n>       hostPort: 9003\r\n>     udp:\r\n>       name: p2p-udp-port\r\n>       containerPort: 9003\r\n>       hostPort: 9003\r\n> \r\n> rollupConfig:\r\n>   filename: rollup.json\r\n> \r\n> rpc:\r\n>   port:\r\n>     name: rpc-port\r\n>     containerPort: 8547\r\n>     hostPort: 8547\r\n>   addr: 0.0.0.0\r\n>   enableAdmin: true\r\n> \r\n> metrics:\r\n>   enabled: true\r\n>   addr: 0.0.0.0\r\n>   port:\r\n>     name: metrics-port\r\n>     containerPort: 7300\r\n>     hostPort: 7300\r\n> \r\n> # rest application arguments\r\n> args: []\r\n> ```\r\n> \r\n> **Error -**\r\n> \r\n> ![image](https://user-images.githubusercontent.com/96972634/267003057-25232562-e849-4f65-abbd-4029ea1daa0f.png)\r\n> \r\n> I checked my Alchemy RPC, it has no issues with my L1 Goerli RPC.\r\n> \r\n> **helm chart that I'm using -** https://artifacthub.io/packages/helm/op-chain-charts/op-node\r\n\r\nwhich op-node  version are you using", "2023-09-13T15:42:46Z", "2023-09-13T15:42:46Z", "opfocus", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5mcyn2", "I_kwDODjvEJM5wq4uZ", "The latest release https://github.com/ethereum-optimism/optimism/releases/tag/v1.1.4\r\n", "2023-09-14T06:13:52Z", "2023-09-14T06:13:52Z", "nitantchhajed", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5me_ei", "I_kwDODjvEJM5wq4uZ", "> The latest release https://github.com/ethereum-optimism/optimism/releases/tag/v1.1.4\r\n\r\nIt looks like you are deploying a rollup node, not a replica node\uff0c right\uff1f\r\n", "2023-09-14T12:55:29Z", "2023-09-14T12:55:29Z", "opfocus", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5mkQnc", "I_kwDODjvEJM5wq4uZ", "> > The latest release https://github.com/ethereum-optimism/optimism/releases/tag/v1.1.4\r\n> \r\n> It looks like you are deploying a rollup node, not a replica node\uff0c right\uff1f\r\n\r\nyes, a rollup node", "2023-09-15T07:05:03Z", "2023-09-15T07:05:03Z", "nitantchhajed", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM6A1iEp", "I_kwDODjvEJM5wq4uZ", "This is the best place to get this kind of help: https://github.com/ethereum-optimism/developers/discussions", "2024-06-11T20:03:19Z", "2024-06-11T20:03:19Z", "tynes", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5jNv1Y", "I_kwDODjvEJM5tOyy_", "Thank you for sharing this! cc @binjix23\r\n\r\nIt is also possible to send ether to L2 by simply using metamask and doing a transfer to the OptimismPortalProxy smart contract", "2023-08-03T19:42:43Z", "2023-08-03T19:42:43Z", "tynes", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5jaf-Q", "I_kwDODjvEJM5tOyy_", "Open sourcing the bridge UI is work that is in progress. Stay tuned for updates. cc @zainbacchus @nitaliano ", "2023-08-07T13:46:36Z", "2023-08-07T13:46:36Z", "roninjin10", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5sz3qk", "I_kwDODjvEJM5tOyy_", "> **Is your feature request related to a problem? Please describe.** The OP-Stack chain tutorial doesn't provide the UI for the bridge, which makes it difficult for non-so tech users to bridge using the sdk.\r\n> \r\n> **Describe the solution you'd like** I would like you guys to take a look into the fuctional UI that I've made for my OP-Stack chain, then we can work together to Implement this for the universal OP-Stack chain by making the needed changes into it.\r\n> \r\n> http://testnet-bridge.raceconomy.com/\r\n> \r\n> https://github.com/nitantchhajed/op-stack-bridge.git\r\n\r\nCan we use this bridge ui for local devnet that we run using docker?", "2023-11-24T11:27:52Z", "2023-11-24T11:27:52Z", "priyanshuthapliyal", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5ik5ej", "I_kwDODjvEJM5svm_e", "I would be open to a PR that renames it to `L2_OUTPUT_ORACLE_ADDRESS` which is explicit and explains exactly what it is but I want to be careful of code churn, just changing things for the sake of changing things", "2023-07-27T15:21:29Z", "2023-07-27T15:21:29Z", "tynes", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5ilO_i", "I_kwDODjvEJM5svm_e", "i can make for you an initial PR and you just review for missing changes what i can't know, if you are agree, i can help you a lit :-D", "2023-07-27T16:11:54Z", "2023-07-27T16:12:20Z", "netzulo", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5inrIn", "I_kwDODjvEJM5svm_e", "We'd also need to maintain backwards compatibility.  Just changing it will silently ignore the old `L2OO_ADDRESS` and silently break existing users.", "2023-07-27T20:56:55Z", "2023-07-27T20:56:55Z", "ajsutton", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5i3zEj", "I_kwDODjvEJM5svm_e", "fwiw, this env var is only really used internally by our devnet tooling. The only way that it would break things for people is if they are not using our orchestration scripts, which they would have to be doing a lot of manual things in that case, so its unlikely to be breaking things", "2023-07-31T17:10:00Z", "2023-07-31T17:10:00Z", "tynes", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5jNqhs", "I_kwDODjvEJM5svm_e", "![image](https://github.com/ethereum-optimism/optimism/assets/11871932/b36c9b47-a960-4c58-9530-db64cc109d81)\r\n\r\nfor each search i found some interesting point to research and i have some doubts\r\n![image](https://github.com/ethereum-optimism/optimism/assets/11871932/fc387487-1ed5-495b-81d9-db29ba675359)\r\n![image](https://github.com/ethereum-optimism/optimism/assets/11871932/b0e791ce-44a0-4262-870b-a37bfd7195f8)\r\n\r\n+ a flag in proposer `l2oo-address` , we can rename too to `l2-output-oracle-address` . I know is a long name but in favor of legibility . are you agree?\r\n\r\n+ the environment variable will be from `L2OO_ADDRESS` to `L2_OUTPUT_ORACLE_ADDRESS`\r\n\r\n\r\n\r\nim missing some behaviour to review ? if not, can i make the PR?\r\n", "2023-08-03T19:32:39Z", "2023-08-03T19:32:39Z", "netzulo", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5aX8nB", "I_kwDODjvEJM5j5Z1n", "You also need to run the `op-proposer`. We are working on getting this added to the docs. https://github.com/ethereum-optimism/optimism/pull/5391", "2023-04-20T12:16:26Z", "2023-04-20T12:16:26Z", "smartcontracts", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5ac0DD", "I_kwDODjvEJM5j5Z1n", "####  After running op-propose, the state of the message can be changed and can be withdrawn normally, but it takes a long time (about 15h) for the message to change from the STATE_ROOT_NOT_PUBLISHED state to the READY_TO_PROVE state. The time for my op-propose to submit the message is 2mins\r\n```bash\r\n./bin/op-batcher \\\r\n    --l2-eth-rpc=http://localhost:8565 \\\r\n    --rollup-rpc=http://localhost:8577 \\\r\n    --poll-interval=1s \\\r\n    --sub-safety-margin=6 \\\r\n    --num-confirmations=1 \\\r\n    --safe-abort-nonce-too-low-count=3 \\\r\n    --resubmission-timeout=30s \\\r\n    --rpc.addr=0.0.0.0 \\\r\n    --rpc.port=8578 \\\r\n    --rpc.enable-admin \\\r\n    --max-channel-duration=120 \\\r\n    --target-l1-tx-size-bytes=2048 \\\r\n    --l1-eth-rpc=https://eth-goerli.g.alchemy.com/v2/\\\r\n    --private-key=\r\n```\r\n```bash\r\n\t./bin/op-node \\\r\n\t--l2=http://localhost:8571 \\\r\n\t--l2.jwt-secret=./jwt.txt \\\r\n\t--sequencer.enabled \\\r\n\t--sequencer.l1-confs=3 \\\r\n\t--verifier.l1-confs=3 \\\r\n\t--rollup.config=./rollup.json \\\r\n\t--rpc.addr=0.0.0.0 \\\r\n\t--rpc.port=8577 \\\r\n\t--p2p.disable \\\r\n\t--rpc.enable-admin \\\r\n\t--p2p.listen.ip=0.0.0.0 \\\r\n\t--p2p.listen.tcp=9033 \\\r\n\t--p2p.listen.udp=9033 \\\r\n\t--p2p.sequencer.key=b3 \\\r\n\t--l1=https://eth-goerli.g.alchemy.com/v2/... \\\r\n\t--l1.rpckind=alchemy\r\n```\r\n\r\n```bash\r\n./bin/op-proposer \\\r\n    --poll-interval 12s \\\r\n    --rpc.port 8560 \\\r\n    --rollup-rpc http://localhost:8547 \\\r\n    --l2oo-address $L2OO_ADDR \\\r\n    --private-key $PROPOSER_KEY \\\r\n    --l1-eth-rpc $L1_RPC\r\n```\r\n```bash\r\nversion = 1.0.3\r\n```", "2023-04-21T08:55:50Z", "2023-04-21T08:55:50Z", "wuzhanfly", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5aeH2E", "I_kwDODjvEJM5j5Z1n", "Hmmm strange. What is your `finalizationPeriodSeconds` in your deploy config?", "2023-04-21T13:35:24Z", "2023-04-21T13:35:24Z", "smartcontracts", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5afA9G", "I_kwDODjvEJM5j5Z1n", "getting-started.json \r\n\r\n```bash\r\n{\r\n  \"finalizationPeriodSeconds\": 12,\r\n}\r\n```", "2023-04-21T16:37:34Z", "2023-04-21T16:37:34Z", "wuzhanfly", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5ec5N3", "I_kwDODjvEJM5j5Z1n", "Same Issue with me too, It takes hours and my code gets stuck at **Line number 64**, after that line executes successfully, it gives error of alchemy RPC Connection Timed out. I have my OP-Proposer running since the start and my challenge period is 60 seconds. here is the link of L2 transaction \r\nhttps://testnet.racescan.io/tx/0x149286a4fbd31a03c1eefbdc6627159b32a38de928221d3bb752dcbfcc998111\r\n\r\nMy Contracts -\r\nAddressManager: \"0xBd3cbCe7Dbc6Bb7c33b36D4B5E2adf3F186701E4\",   \r\n            L1CrossDomainMessenger: \"0x3B251e2998009AB5034228CB71E95D71D9869322\",    \r\n            L1StandardBridge: \"0x40DB2D3424286bEc850Aeec030cA6cCf17D07636\",   \r\n            OptimismPortal: \"0xfEEE0247901eCE2FD8a72371e0C8F4F5b081CacA\",  \r\n            L2OutputOracle: \"0xe2A596B5cb0134b60FC0396c9cA82867F5b35073\", \r\n\r\n![image](https://github.com/ethereum-optimism/optimism/assets/96972634/fb4400ab-2383-46f1-a3d9-1523cec51320)\r\n", "2023-06-09T14:04:57Z", "2023-06-09T14:04:57Z", "nitantchhajed", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5ekamw", "I_kwDODjvEJM5j5Z1n", "@smartcontracts any solution for this withdraw issue ??\r\n", "2023-06-12T05:23:56Z", "2023-06-12T05:23:56Z", "nitantchhajed", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5enq05", "I_kwDODjvEJM5j5Z1n", "@wuzhanfly I solved this Issue on my side, first of all I updated to latest codebase by using `git pull` and then I stopped all the components starting from `op-proposer` to `op-geth`. then I changed  `--max-channel-duration=120` to `5` in `op-batcher`, then I restarted all the components. then it took me 24 minutes to withdraw completely. I think because of `\"l2OutputOracleSubmissionInterval\": 120` in `getting-started.json` it takes 120 L1 blocks submission Interval. so `120 x 12 (L1 Block time) = 1440 seconds = 24 Minutes`. I don't know if I am correct or not regarding the withdrawal time, but doing this reduced my withdrawal time.\r\n\r\nPlease feel free to correct me, if I am wrong.\r\n\r\n\r\n\r\n![image](https://github.com/ethereum-optimism/optimism/assets/96972634/bfe37dfe-4ec6-47f6-84d2-c716a89fa7cb)\r\n\r\n", "2023-06-12T14:26:55Z", "2023-06-12T14:27:58Z", "nitantchhajed", "2025-08-30 15:17:31"]
["IC_kwDODjvEJM5k5SBG", "I_kwDODjvEJM5j5Z1n", "Hi guys I have the same problem, and I'm not sure changing `l2OutputOracleSubmissionInterval` alone is enough.\r\n\r\nWhat I notice:\r\n1. `l2OutputOracleSubmissionInterval` is first used to wait for **L1 blocks** (below I've set it to 60 or ~12mins on Sepolia)\r\n   ```\r\n   t=2023-08-25T03:24:13+0000 lvl=info msg=\"L2 Output Submitter started\"\r\n   t=2023-08-25T03:24:30+0000 lvl=dbug msg=\"proposer submission interval has not elapsed\" currentBlockNumber=0 nextBlockNumber=60\r\n   t=2023-08-25T03:24:37+0000 lvl=dbug msg=\"proposer submission interval has not elapsed\" currentBlockNumber=0 nextBlockNumber=60\r\n   ...\r\n   ```\r\n2. Then, `l2OutputOracleSubmissionInterval` is also used for the number of **L2 blocks** to put in each state root txn published on L1\r\n   ```\r\n   Txn1   proposeL2Output _l2BlockNumber=60\r\n   Txn1   proposeL2Output _l2BlockNumber=120\r\n   Txn1   proposeL2Output _l2BlockNumber=180\r\n   ...\r\n   ```\r\n\r\nThis means if L2 is significantly faster (e.g., 2 seconds) than the L1 (e.g., Sepolia 12 seconds), then Proposer already falls behind right out the gate (due to step 1), and continues to lag even farther with each step 2 that passes.\r\n\r\nFor example, I have a testnet running for a week (using `l2OutputOracleSubmissionInterval=120`) and the Proposer has gradually fallen behind on publishing to the tune of 2.5 days' worth of L2 blocks.\r\n\r\nAppreciate any help on this, thanks.", "2023-08-25T04:26:08Z", "2023-08-25T04:52:34Z", "antonio-altr", "2025-08-30 15:17:31"]
["IC_kwDOLB-lzc6Btl1l", "I_kwDOLB-lzc6CgnpI", "## Assumptions and Requirements\r\n\r\n- Chain IDs are assumed to be unique\r\n- Batch inbox address must be unique per chain\r\n- Batch inbox address must be deterministic \r\n- Maybe: Forward compatibility with https://github.com/ethereum-optimism/specs/issues/221\r\n\r\n## Proposal\r\n\r\n- Initial scheme is version 0\r\n- Batch inbox address is `versionByte || keccak256(bytes32(chainId))[:19]`, where `||` denotes concatenation and `chainId` is a uint256\r\n\r\nThis supports 256 iterations, and makes the version extractable from the address.\r\n\r\n## Example:\r\n\r\nFor Chain ID 123, we first compute the hash to be `0x5569044719a1ec3b04d0afa9e7a5310c7c0473331d13dc9fafe143b2c4e8148a`\r\n\r\n- Solidity: `keccak256(bytes.concat(bytes32(uint256(123))))`\r\n- Cast: `cast keccak 0x000000000000000000000000000000000000000000000000000000000000007b`\r\n\r\nNext we take the first 19 bytes of the hash: `0x5569044719a1ec3b04d0afa9e7a5310c7c0473`\r\n\r\nThen we perform the concatenation with version 0 to get a batch inbox address of `0x005569044719a1ec3b04d0afa9e7a5310c7c0473`\r\n\r\n## Questions\r\n\r\n1. If this convention is agreed on, do we want existing chains to migrate towards this, or do we only use this convention for new chains going forward, e.g. as part of the OP Stack Manager (https://github.com/ethereum-optimism/specs/pull/236). I lean towards not migrating as it's much simpler, since #221 is the preferred long term solution anyway. \r\n2. This scheme can be made compatible with https://github.com/ethereum-optimism/specs/issues/221 by deploying to vanity addresses that have the expected versionByte prefix. However, requiring a vanity address as part of the spec feels leaky and suboptimal. Do we only use this new proposal until #221 is implemented?", "2024-06-18T14:13:57Z", "2024-06-18T14:51:00Z", "mds1", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6Bu_kp", "I_kwDOLB-lzc6CgnpI", "I am on board with this as a default but ultimately if we support #221 we will need to have the ability to modify the batch inbox address on the `SystemConfig` and have the `op-node` listen to those modifications.  I am not opinionated on what the default standard is as long as its adopted when people do not explicitly say what their batch inbox is. We could always make the L2 chain id a getter on the system config and not need to worry about encoding it in the batch inbox\r\n\r\nA side thought is the batch inbox in a way is a file path in a file system. Nothing super useful or actionable about that statement, just throwing it out there", "2024-06-18T17:05:23Z", "2024-06-18T17:05:23Z", "tynes", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BvCwJ", "I_kwDOLB-lzc6CgnpI", "> We could always make the L2 chain id a getter on the system config and not need to worry about encoding it in the batch inbox\r\n\r\nI do think we should keep the batch inbox address discoverable via SystemConfig, this way external tools like L2Beat can easily find it to e.g. verify data is actually being posted there.\r\n\r\n---\r\n\r\nIt sounds like we agree with a simple, minimal solution for now as a stopgap for #221. The only place in the specs where the current convention is mentioned is in the standard config, so I'll open a PR to change that to the above proposal", "2024-06-18T17:13:08Z", "2024-06-18T17:13:56Z", "mds1", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL0t", "I_kwDOLB-lzc6Mkd-i", "Previous work for this can be found [here](https://github.com/ethereum-optimism/optimism/pull/6903). We considered having a ring buffer instead of a mapping that grows forever. We decided to not merge this due to the inclusion of [eip-4788](https://eips.ethereum.org/EIPS/eip-4788) in the op stack. The proofs are larger when using 4788 but its still possible. I have been wondering if its a good idea to open a RIP that adds a precompile/predeploy for this functionality so that developers can have a more consistent experience between various L2s", "2024-03-01T17:26:30Z", "2024-03-01T17:26:30Z", "tynes", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL0x", "I_kwDOLB-lzc6Mkd-i", "Do you have an idea of how much it would cost to post the updated mapping on L1 once we we're using blobs or no?  Since blobs are pruned, my worry would be this additional overhead cost.", "2024-03-01T20:29:14Z", "2024-03-01T20:29:14Z", "pegahcarter", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL00", "I_kwDOLB-lzc6Mkd-i", "Blobs have no impact on this because L2 outputs are not posted to L1, it is the L2 inputs that are posted to L1. Meaning L2 txs (inputs) are posted to L1, not the L2 state (outputs)\r\n\r\nYour implementation will cause additional state growth for every L1 block forever. It may be a tradeoff you are willing to take, but generally core devs use ring buffers for these sorts of constructions so that the max storage usage is bounded", "2024-03-01T21:10:38Z", "2024-03-01T21:10:38Z", "tynes", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL04", "I_kwDOLB-lzc6Mkd-i", "Does changing state on L2 have the same impact on L1 as changing state on L1?  Ie. adding new variables increases state vs. modifying existing state variables does not.  I've been digging through the docs but can't seem to find a distinction between the two.", "2024-03-01T22:53:52Z", "2024-03-01T22:53:52Z", "pegahcarter", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL09", "I_kwDOLB-lzc6Mkd-i", "The op stack batch submits inputs to L1, not state diffs. You can make as many modifications to L2 state as you want in a given L2 transaction and the batch size will be the same given the same sized tx that makes less modifications to L2 state", "2024-03-04T23:54:15Z", "2024-03-04T23:54:15Z", "tynes", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL1E", "I_kwDOLB-lzc6Mkd-i", "Okay, that makes sense.  What I'm asking is about the difference in:\r\n- modifying existing state variables\r\n- adding new state variables\r\nFor a transaction on L2.  Do new variables to the L2  increase L2 state but not L1 state?", "2024-03-06T18:15:57Z", "2024-03-06T18:15:57Z", "pegahcarter", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL1G", "I_kwDOLB-lzc6Mkd-i", "Adding new variable to L2 predeploys only increase L2 state", "2024-03-07T17:10:28Z", "2024-03-07T17:10:28Z", "tynes", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL1J", "I_kwDOLB-lzc6Mkd-i", "IMO it's worth having a predeploy to provide some bridge back to L1 state.", "2024-03-07T18:38:56Z", "2024-03-07T18:38:56Z", "pegahcarter", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL1L", "I_kwDOLB-lzc6Mkd-i", "> Previous work for this can be found [here](https://github.com/ethereum-optimism/optimism/pull/6903). We considered having a ring buffer instead of a mapping that grows forever. We decided to not merge this due to the inclusion of [eip-4788](https://eips.ethereum.org/EIPS/eip-4788) in the op stack. The proofs are larger when using 4788 but its still possible. I have been wondering if its a good idea to open a RIP that adds a precompile/predeploy for this functionality so that developers can have a more consistent experience between various L2s\r\n\r\nhttps://github.com/Dedaub/audits/blob/main/Ethereum%20Foundation/EIP%204788%20Bytecode%20Audit.pdf", "2024-03-12T12:43:52Z", "2024-03-12T12:43:52Z", "sambacha", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL1R", "I_kwDOLB-lzc6Mkd-i", "> Previous work for this can be found [here](https://github.com/ethereum-optimism/optimism/pull/6903). We considered having a ring buffer instead of a mapping that grows forever. We decided to not merge this due to the inclusion of [eip-4788](https://eips.ethereum.org/EIPS/eip-4788) in the op stack. The proofs are larger when using 4788 but its still possible. I have been wondering if its a good idea to open a RIP that adds a precompile/predeploy for this functionality so that developers can have a more consistent experience between various L2s\r\n\r\nI vote for adding this feature since the cost is fixed and trivial, and the usage is much easier compared to eip-4788 :)", "2024-03-23T04:06:59Z", "2024-03-23T04:06:59Z", "zhiqiangxu", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL1V", "I_kwDOLB-lzc6Mkd-i", "@tynes IMO is the state bloat of a mapping storing every hash is worth it- otherwise, additional contracts will need to store historical block hashes and the additional redundancy may exceed storing the hash once in a single place.\r\n\r\nIt also doesn't make sense to skip block hashes (to save space) as oracles, for example, rely on different intervals and may need a specific block.\r\n\r\nIf we're storing block hashes, we should also store the timestamp associated with the hash.", "2024-03-25T16:22:41Z", "2024-03-25T16:22:41Z", "pegahcarter", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnL1Y", "I_kwDOLB-lzc6Mkd-i", "I think its worth pursuing this feature through the Rollup Improvement Process see https://github.com/ethereum/RIPs/issues/16", "2024-03-25T16:47:13Z", "2024-03-25T16:47:13Z", "tynes", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnF9_", "I_kwDOLB-lzc6MkXZ9", "cc @protolambda ", "2024-02-05T00:51:48Z", "2024-02-05T00:51:48Z", "smartcontracts", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnF-G", "I_kwDOLB-lzc6MkXZ9", "I believe `D` is deposits, and `B` is batches. Deposits come at the same time as that the L1 origin is recognized in the L2 chain. This always happens before the resulting L2 block is batch-submitted to the L1. The batch-data then allows other nodes to reconstruct the exact same L2 block.", "2024-02-05T22:51:17Z", "2024-02-05T22:51:17Z", "protolambda", "2025-08-30 15:20:02"]
["IC_kwDOLB-lzc6BnF-K", "I_kwDOLB-lzc6MkXZ9", "> I believe `D` is deposits, and `B` is batches. Deposits come at the same time as that the L1 origin is recognized in the L2 chain. This always happens before the resulting L2 block is batch-submitted to the L1. The batch-data then allows other nodes to reconstruct the exact same L2 block.\r\n\r\nMy confusion is whether B1 is the batches generated in Epoch1. If so, it shouldn't be submitted in the block of L1 Epoch1, because by the time the first L2 block of Epoch1 is generated, the corresponding L1 origin, which is the block of L1 Epoch1, has already been generated. B1 should be submitted in the blocks of the subsequent sequencing window size, just like B2 B3 in the diagram.", "2024-02-06T10:17:22Z", "2024-02-06T10:19:51Z", "Nickqiaoo", "2025-08-30 15:20:02"]
["IC_kwDOJ_r-bs6Bq9Wy", "I_kwDOJ_r-bs6Irr6C", "Via #257 ", "2024-06-18T08:31:56Z", "2024-06-18T08:31:56Z", "geoknee", "2025-08-30 15:20:08"]
["IC_kwDOJ_r-bs6Bq_OG", "I_kwDOJ_r-bs6GKgAm", "Via #257 ", "2024-06-18T08:36:03Z", "2024-06-18T08:36:03Z", "geoknee", "2025-08-30 15:20:08"]
["IC_kwDOJ_r-bs6B3Q_c", "I_kwDOJ_r-bs5_26en", "@geoknee ok to close this one?", "2024-06-19T13:39:46Z", "2024-06-19T13:39:46Z", "BlocksOnAChain", "2025-08-30 15:20:08"]
["IC_kwDOKIsnqM6B3hNY", "I_kwDOKIsnqM6LKDY-", "Can I work on this issue? ", "2024-06-19T14:08:18Z", "2024-06-19T14:08:18Z", "ShubhSensei", "2025-08-30 15:20:09"]
["IC_kwDOKIsnqM6CCDDX", "I_kwDOKIsnqM6LKDY-", "Ah, this documentation is actually up to date now and was fixed in the original PR. Future presigned pauses will only need 1 state override as currently listed in `PRESIGNED-PAUSE.md`", "2024-06-20T21:15:39Z", "2024-06-20T21:15:39Z", "mds1", "2025-08-30 15:20:09"]
["IC_kwDODjvEJM6BnK5L", "I_kwDODjvEJM6MNcUN", "Your PR was auto closed from being stale, I reopened it. Closing this issue", "2024-06-17T22:17:42Z", "2024-06-17T22:17:42Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6A1yqx", "I_kwDODjvEJM6L6Et0", "Appreciate you opening this issue. Its likely that these are flakes as the tests do pass in CI", "2024-06-11T20:53:07Z", "2024-06-11T20:53:07Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnB-G", "I_kwDODjvEJM6L6Et0", "Closing this due to the tests all passing in CI", "2024-06-17T21:51:46Z", "2024-06-17T21:51:46Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6ALQLL", "I_kwDODjvEJM6LMdjE", "It has been building in CI. I would be very surprised if `jq` was removed from `apk`", "2024-06-05T16:03:34Z", "2024-06-05T16:03:34Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6APG6R", "I_kwDODjvEJM6LMdjE", "> It has been building in CI. I would be very surprised if `jq` was removed from `apk`\r\n\r\nThat's weird, in theory docker should build everywhere, now it always reports the above error on my macbook.", "2024-06-06T05:32:08Z", "2024-06-06T05:32:08Z", "zhiqiangxu", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BmAba", "I_kwDODjvEJM6LMdjE", "This is likely a transient issue, closing this as it works in CI", "2024-06-17T19:05:21Z", "2024-06-17T19:05:21Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5_cCgb", "I_kwDODjvEJM6KAopC", "@cromaniuc Please provide the raw json response when querying `eth_getTransactionReceipt` on your L1 Erigon node for the following two txs (blob and non-blob tx in block https://etherscan.io/block/19938706)\r\n* blob-tx: https://etherscan.io/tx/0x708f0b86850cd74dfcfd1c4771b9c0f2ebf21e66caa769f1e53eb9bf2f1bd209\r\n* non-blob-tx: https://etherscan.io/tx/0x1dd898311b716a687ce0ffca0e89e2ec50e6c900c3da38629a77543d8470e243", "2024-05-29T18:50:11Z", "2024-05-29T18:50:32Z", "sebastianst", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5_gad-", "I_kwDODjvEJM6KAopC", "Seems like a related fix was just merged in erigon: https://github.com/ledgerwatch/erigon/pull/10551\r\nPlease upgrade erigon and try again once this fix is released.", "2024-05-30T09:59:24Z", "2024-05-30T09:59:24Z", "sebastianst", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnI_C", "I_kwDODjvEJM6KAopC", "Closing this due to it being an issue with erigon rather than op-node", "2024-06-17T22:11:53Z", "2024-06-17T22:11:53Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM59H4SR", "I_kwDODjvEJM6IAzV6", "This definitely slipped through review, I think its unlikely that we will migrate away from this now as there are more important things to focus on. You are right its better to standardize", "2024-05-07T20:10:13Z", "2024-05-07T20:10:13Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6A1g7K", "I_kwDODjvEJM6HO3x9", "Are you still having this issue?", "2024-06-11T20:00:20Z", "2024-06-11T20:00:20Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnHmA", "I_kwDODjvEJM6HO3x9", "Closing due to inactivity", "2024-06-17T22:07:40Z", "2024-06-17T22:07:40Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM571y3v", "I_kwDODjvEJM6FYv3K", "We have been working on a related idea in https://github.com/ethereum-optimism/optimism/pull/9985\r\n\r\nThis would need to be fully specified in the specs repo before it could be considered for inclusion in the codebase :)", "2024-04-25T16:30:42Z", "2024-04-25T16:30:42Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM572Pk0", "I_kwDODjvEJM6FYv3K", "That's great!  It would be a real convenience if every set of L1 contracts for a given L2 were deterministic.  It would probably help with bridging back to L1 as well.\r\n\r\nWhat I was suggesting is more like this:\r\n```Solidity\r\nfunction deployProxyAndInitialize(\r\n    address _owner,\r\n    address _implementation,\r\n    bytes memory _data\r\n) returns (Proxy iProxy) {\r\n    iProxy = new Proxy(msg.sender);\r\n    iProxy.upgradeToAndCall({ _implementation: _implementation, _data: _data });\r\n    if (_owner != msg.sender) {\r\n        iProxy.changeAdmin(_owner);\r\n    }\r\n}\r\n```\r\nAnd then there is no risk of a rogue deployer or dangling `initialize()`.\r\n\r\nUntil foundry offers atomic deployments (https://github.com/foundry-rs/foundry/issues/7452) it would be beneficial to use an alternative method.", "2024-04-25T17:39:12Z", "2024-04-25T17:39:37Z", "pegahcarter", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnJoz", "I_kwDODjvEJM6FYv3K", "See https://github.com/ethereum-optimism/specs/pull/236 for thoughts on this", "2024-06-17T22:13:50Z", "2024-06-17T22:13:50Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM56VLHg", "I_kwDODjvEJM6FVuH_", "Hola sabes qe tengo un token el metamask de 3 WLD y no puedo retirarlo ", "2024-04-12T19:09:59Z", "2024-04-12T19:09:59Z", "Diegomiguelvidela", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM56qd_2", "I_kwDODjvEJM6FVuH_", "cc @smartcontracts ", "2024-04-15T23:01:48Z", "2024-04-15T23:01:48Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM57AFTO", "I_kwDODjvEJM6FVuH_", "@ahyammona you should give more context on what you are doing. My guess is that you are using `waitForMessageReceipt` wrong.\r\n\r\nYou should only use `waitForMessageReceipt` for cross chain messages (deposit and withdraw).\r\n\r\nIf you are trying to wait for example erc20 tx, make sure you use `tx.wait()`", "2024-04-18T11:14:00Z", "2024-04-18T11:14:00Z", "danielsimao", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnJxf", "I_kwDODjvEJM6FVuH_", "The sdk is no longer maintained ", "2024-06-17T22:14:14Z", "2024-06-17T22:14:14Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6CDzu-", "I_kwDODjvEJM6Mirkc", "In practice does this look like making the sequencing window legible to the EVM so that we can revert in EVM?\n\nRight now there is a standardness of the sequencing window but it isn't legible from within EVM. We would need to set it via the `SystemConfig` or hardcode it to be a certain value as part of the hardfork. If hardcoded, then we can hardcode the same value to be used in the require statement, preventing too new of cross chain messages when its a deposit tx.\n\nI made a design doc about making it dynamic - https://github.com/ethereum-optimism/design-docs/pull/14\n\nIts less work to hardcode it to a specific value but impacts things like L3s since the sequencing window is defined by base layer blocks and not time. If we made it dynamic, there there may be unknown side effects/edge cases", "2024-06-21T05:38:21Z", "2024-06-21T05:38:21Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6CEBKt", "I_kwDODjvEJM6Mirkc", "Not entirely sure I have all the details in my head, but we could also expose whether the current tx is a deposit transaction (which we need to for devnet 0 workaround to disable cross-L2 deposits) and then the `L2Inbox` contract could call the `SystemConfig` to get the current sequencer window.", "2024-06-21T06:28:20Z", "2024-06-21T06:28:20Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6CH4Bm", "I_kwDODjvEJM6Mirkc", "Closing this in favor of discussion in https://github.com/ethereum-optimism/optimism/issues/10867 which is closely related and describes the current proposed solution for deposit handling in the comments", "2024-06-21T16:51:53Z", "2024-06-21T16:51:53Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BlHuy", "I_kwDODjvEJM6Mirl1", "The predeploys are nearly done. The only thing left that needs to be handled for the devnet release is https://github.com/ethereum-optimism/design-docs/pull/13", "2024-05-20T18:31:01Z", "2024-05-20T18:31:16Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BmvI0", "I_kwDODjvEJM6Mirl1", "Closing this as complete", "2024-06-17T21:00:26Z", "2024-06-17T21:00:26Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM542avF", "I_kwDODjvEJM6EEXIu", "![242171bf7834d095](https://github.com/ethereum-optimism/optimism/assets/8406232/e6fb8d6d-9516-4a3d-b057-b764083e7917)\r\n", "2024-03-29T17:35:45Z", "2024-03-29T17:35:45Z", "clabby", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM542bN1", "I_kwDODjvEJM6EEXIu", "Impressive it's over 9000", "2024-03-29T17:37:45Z", "2024-03-29T17:37:45Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM542jaq", "I_kwDODjvEJM6EEXIu", "![image (7)](https://github.com/ethereum-optimism/optimism/assets/69322641/dbda4844-5b91-4b4d-b4f9-2ba75959f801)\r\n", "2024-03-29T18:13:55Z", "2024-03-29T18:13:55Z", "Harpethr", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM55U2ac", "I_kwDODjvEJM6EEXIu", "Wooooo", "2024-04-03T20:20:38Z", "2024-04-03T20:20:38Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM56LP1M", "I_kwDODjvEJM6EEXIu", ":1st_place_medal: ", "2024-04-11T14:03:33Z", "2024-04-11T14:03:33Z", "pegahcarter", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnHaI", "I_kwDODjvEJM6EEXIu", "Looking forward to #20000", "2024-06-17T22:07:05Z", "2024-06-17T22:07:05Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM55bAhz", "I_kwDODjvEJM6D4Qlu", "As I undestand, you do not have the blobs in the beacon. Needs to set up the lighthouse with `-prune-blobs=false` parameter.\r\nhttps://docs.optimism.io/builders/node-operators/management/blobs", "2024-04-04T12:49:48Z", "2024-04-04T12:49:48Z", "andreyvg", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM55tery", "I_kwDODjvEJM6D4Qlu", "Nope, `--l1.beacon` and `--l1.beacon-archiver` are pointing to an archive node (Lighthouse is running with `-prune-blobs=false` and `-reconstruct-historic-states`)", "2024-04-08T06:34:41Z", "2024-04-08T06:35:06Z", "cromaniuc", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnLLH", "I_kwDODjvEJM6D4Qlu", "Recent changes likely fixed this issue", "2024-06-17T22:18:36Z", "2024-06-17T22:18:36Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnGdQ", "I_kwDODjvEJM6Dqnvh", "Better to ask this question here: https://github.com/ethereum-optimism/developers/discussions", "2024-06-17T22:04:14Z", "2024-06-17T22:04:14Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM55bCJN", "I_kwDODjvEJM6DanwA", "I have the same error. Please advise.", "2024-04-04T12:52:23Z", "2024-04-04T12:52:23Z", "andreyvg", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM55wacw", "I_kwDODjvEJM6DanwA", "I would be grateful for any input", "2024-04-08T13:16:20Z", "2024-04-08T13:16:20Z", "harveyff", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM57VuLE", "I_kwDODjvEJM6DanwA", "I have the same problem, my node is trying to sync on optimism mainnet.\r\n```\r\nApr 22 12:34:29 my-server bash[42857]: t=2024-04-22T12:34:29+0000 lvl=warn msg=\"failed to serve p2p sync request\" serve=payloads_by_number peer=16Uiu2HAm3-2217 remote=/ip4/103.180.28.215/tcp/9222 req=119083463 err=\"peer requested unknown block by number: not found\"\r\nApr 22 12:34:29 my-server bash[42857]: t=2024-04-22T12:34:29+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xbd0958a2df614ed945e84b8eff181ceefe4cc561f7fc90bcca788301ce9a670e:119095246 peer=16Uiu2HAmTrP55nReSSHC8seJ29DFg1vQKmFXhtdW4GsSkXBPgHSv\r\nApr 22 12:34:29 my-server bash[42857]: t=2024-04-22T12:34:29+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xbd0958a2df614ed945e84b8eff181ceefe4cc561f7fc90bcca788301ce9a670e:119095246\r\nApr 22 12:34:29 my-server bash[42857]: t=2024-04-22T12:34:29+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0x6ace165f0b87522a32551d1e8b311a217dae69111c1b3c67d9ab3198e140f326:119054636\r\nApr 22 12:34:29 my-server bash[42857]: t=2024-04-22T12:34:29+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0x6ef225c362ae4c35a42faae1ad843e6195a97489c84fd0ec695f4211a538992d:119054637\r\n```\r\n\r\nFor almost week I have the same block.", "2024-04-22T12:38:24Z", "2024-04-22T12:38:24Z", "Rogalek", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM57ZEr4", "I_kwDODjvEJM6DanwA", "Syncing op-sepolia from scratch with v1.7.3 node and v1.101311.0 geth, I've now encountered this, running unpruned Prysm nodes on v5.0.3, stuck at 8366372, running op-node with   --l1.beacon-archiver=$OP_NODE__L1_BEACON_ARCHIVE and \r\n  --l1.beacon=$OP_NODE__L1_BEACON \r\n\r\n```\r\n848314\r\nt=2024-04-22T16:34:45+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=8749 err=\"engine stage failed: temp: failed to fetch blobs: failed to get blob sidecars for L1BlockRef 0x799e89369a21a862a5ae506841e56b1cd4895f5d7b7fabe66add300a6c1cb315:5335452: expected 1 sidecars but got 0\"\r\nt=2024-04-22T16:34:45+0000 lvl=info msg=\"connected to peer\" peer=16Uiu2HAmBwxxZjBESz7xVU6p595znCfddbgdQpXSfLueSu7eBfwp addr=/ip4/80.64.208.80/tcp/50477\r\nt=2024-04-22T16:34:45+0000 lvl=info msg=\"Starting P2P sync client event loop\" peer=16Uiu2HAmBwxxZjBESz7xVU6p595znCfddbgdQpXSfLueSu7eBfwp\r\nt=2024-04-22T16:34:46+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xedae620245ddfd90675654cb80418b806169cfb0d2d06953198eaf8677880ac5:11000573 peer=16Uiu2HAmPoBKuQkThkP8kMR7iRayNbvHNVRD34iCJCUYaY9z7h3j\r\nt=2024-04-22T16:34:46+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xedae620245ddfd90675654cb80418b806169cfb0d2d06953198eaf8677880ac5:11000573\r\nt=2024-04-22T16:34:46+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0x812a3c67ad94aeed70aa0281cf5ea3a30318282ab028896cbcfb12f71254788e:10848315\r\nt=2024-04-22T16:34:48+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xd340032962ed15e3fbd8385f4ab82a912b63717ccacf6e59ebafb070f3999082:11000574 peer=16Uiu2HAmPoBKuQkThkP8kMR7iRayNbvHNVRD34iCJCUYaY9z7h3j\r\nt=2024-04-22T16:34:48+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xd340032962ed15e3fbd8385f4ab82a912b63717ccacf6e59ebafb070f3999082:11000574\r\nt=2024-04-22T16:34:50+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xcc5ff1d6836e15b251a05c3a3f17e254624d8d638668ee95f6504b21a1aae482:11000575 peer=16Uiu2HAmPoBKuQkThkP8kMR7iRayNbvHNVRD34iCJCUYaY9z7h3j\r\nt=2024-04-22T16:34:50+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xcc5ff1d6836e15b251a05c3a3f17e254624d8d638668ee95f6504b21a1aae482:11000575\r\nt=2024-04-22T16:34:50+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0x7212fa89cbbe661de2b84370d07f36eb4c7fb7312a4c5b2a1a6b2dade81dfd83:10848316\r\nt=2024-04-22T16:34:52+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x2d8640ce0136c70e466b07eaa8165c024cfd68d1e58cf41c38bed2c7a5d1ad35:11000576 peer=16Uiu2HAmPoBKuQkThkP8kMR7iRayNbvHNVRD34iCJCUYaY9z7h3j\r\nt=2024-04-22T16:34:52+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x2d8640ce0136c70e466b07eaa8165c024cfd68d1e58cf41c38bed2c7a5d1ad35:11000576\r\nt=2024-04-22T16:34:53+0000 lvl=info msg=\"connected to peer\" peer=16Uiu2HAmPxjjB561cMjM6kSkuRAwQr4nhY6KHozyo2KDHzvSxBEf addr=/ip4/222.106.187.48/tcp/49582\r\nt=2024-04-22T16:34:53+0000 lvl=info msg=\"Starting P2P sync client event loop\" peer=16Uiu2HAmPxjjB561cMjM6kSkuRAwQr4nhY6KHozyo2KDHzvSxBEf\r\nt=2024-04-22T16:34:54+0000 lvl=info msg=\"disconnected from peer\" peer=16Uiu2HAmPxjjB561cMjM6kSkuRAwQr4nhY6KHozyo2KDHzvSxBEf addr=/ip4/222.106.187.48/tcp/49582\r\nt=2024-04-22T16:34:54+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xcb485f734cdd7a970c0769ed41daf822ac5ac36913fca4051b4df6a4f28fe62f:11000577 peer=16Uiu2HAmPoBKuQkThkP8kMR7iRayNbvHNVRD34iCJCUYaY9z7h3j\r\nt=2024-04-22T16:34:54+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xcb485f734cdd7a970c0769ed41daf822ac5ac36913fca4051b4df6a4f28fe62f:11000577\r\nt=2024-04-22T16:34:54+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0xf807ad98a50744edae53a2b0a1fe49ec635c084639e80cdf1babeea952879b2e:10848317\r\nt=2024-04-22T16:34:54+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0xc2e99d085142c27808e8a8cb07d6366050639ee3f7dd9363c48655e20f40cd51:10848318\r\nt=2024-04-22T16:34:55+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=8750 err=\"engine stage failed: temp: failed to fetch blobs: failed to get blob sidecars for L1BlockRef 0x799e89369a21a862a5ae506841e56b1cd4895f5d7b7fabe66add300a6c1cb315:5335452: expected 1 sidecars but got 0\"\r\n```", "2024-04-22T16:39:38Z", "2024-04-22T16:42:50Z", "icculp", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM57dmBf", "I_kwDODjvEJM6DanwA", "There is no way to do it. Let me take the liberty to say, is there really a team maintaining this chain?", "2024-04-23T03:48:39Z", "2024-04-23T03:48:39Z", "harveyff", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM57jdzu", "I_kwDODjvEJM6DanwA", "Have the same issue but with `op-mainnet`.\r\n\r\n```\r\nt=2024-04-23T16:36:37+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=8595 err=\"stage 0 failed resetting: temp: failed to find the L2 Heads to start from: failed to fetch current L2 forkchoice state: failed to find the finalized L2 block: failed to parse L1 info deposit tx from L2 block: data is unexpected length: 260\"\r\nt=2024-04-23T16:36:37+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xb70f0a88ef99d630db1db6c7f7715f403ddc73f98331d63b8e9ba5bb4a8aa434:119145710 peer=16Uiu2HAmJiNges1tqVoFW7g8habSsMwJgqUP6gbL3ofMPvmoTY3d\r\nt=2024-04-23T16:36:37+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xb70f0a88ef99d630db1db6c7f7715f403ddc73f98331d63b8e9ba5bb4a8aa434:119145710\r\nt=2024-04-23T16:36:40+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x2d7036730b236076762e9a678396af3202c1072e2731bc0850d9ccb1999236c4:119145711 peer=16Uiu2HAkxfLbtojJACcSDKzdG6QHd8q4TG9MtNCwEhqaneGTCvDw\r\nt=2024-04-23T16:36:40+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x2d7036730b236076762e9a678396af3202c1072e2731bc0850d9ccb1999236c4:119145711\r\nt=2024-04-23T16:36:40+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0x7be94fcf6e7578b2084ca23ff632fdaeb7a7a0f33077b2638d47708526866409:119115374\r\nt=2024-04-23T16:36:40+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0xf4b048ae7655cf4a6a00efb90fed454e7933087b2ebf67115b8df56eb9b084c7:119115375\r\nt=2024-04-23T16:36:40+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0xd6b7e1a40315e78024b2c16d8cf8a6ee38bb73a3ddf5243b4a05da4751f8012b:119115376\r\nt=2024-04-23T16:36:40+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0xd7e58f60eaa99dfd749290515834d954aa2c3edc112dd2c91fa8e0938473b501:119115377\r\nt=2024-04-23T16:36:40+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0xc4bb1ef23c53d7b7cc4d6e28cb6908a41a0e5736bfe2407042afcaa0360e5659:119115378\r\nt=2024-04-23T16:36:40+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0x918350ad148b03ffadb504e6cb91d9d42164db3a3323cc120c892d0e12f970bc:119115379\r\nt=2024-04-23T16:36:41+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xb47f958bbbca8c1c3b45dfe78150137bcd5a5cb7e6327da0e220e86267e5a286:119145712 peer=16Uiu2HAm7dhyfPKohuLyiZ2nUopuNUUdZhJSwxHymwbrGkuuJdZ7\r\nt=2024-04-23T16:36:41+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xb47f958bbbca8c1c3b45dfe78150137bcd5a5cb7e6327da0e220e86267e5a286:119145712\r\nt=2024-04-23T16:36:44+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xd1f3e5440b113c5653aa5c64549dc3b7fa77af29ff61fc36f054562ab7753d42:119145713 peer=16Uiu2HAkxfLbtojJACcSDKzdG6QHd8q4TG9MtNCwEhqaneGTCvDw\r\nt=2024-04-23T16:36:44+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xd1f3e5440b113c5653aa5c64549dc3b7fa77af29ff61fc36f054562ab7753d42:119145713\r\nt=2024-04-23T16:36:44+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0xebcc7ea0713a8146e498cbfee0e29dc9bc3af70cdce0cac448eda4c8e3423e33:119115380\r\nt=2024-04-23T16:36:44+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0xd51a637d122f5db1ee8b3a86ff69ab78b50850ce93a66f5e356a8f644c55b9e0:119115381\r\nt=2024-04-23T16:36:45+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xe897155cd8ae6fd2a07bdaa6db27dec141a91a4ec34743e9db7be3be7c8fa976:119145714 peer=16Uiu2HAm7dhyfPKohuLyiZ2nUopuNUUdZhJSwxHymwbrGkuuJdZ7\r\nt=2024-04-23T16:36:45+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xe897155cd8ae6fd2a07bdaa6db27dec141a91a4ec34743e9db7be3be7c8fa976:119145714\r\nt=2024-04-23T16:36:45+0000 lvl=info msg=\"Dropping payload from payload queue because the payload queue is too large\" id=0x56f18506d6b33b33561f0b0ffc085d65729ff5e87021cae98c791a42c06761d3:119115382\r\n\r\n```\r\nPlease help @trianglesphere @geoknee @sebastianst @tynes ", "2024-04-23T16:40:05Z", "2024-04-23T16:40:36Z", "alexqrid", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM57jtrs", "I_kwDODjvEJM6DanwA", "> There is no way to do it. Let me take the liberty to say, is there really a team maintaining this chain?\r\n\r\nIt moves forward for me using an external beacon service, so something is off about the way these beacon endpoints are configured and the docs seem incomplete about how to configure and enable the needed data. I tried prysm without pruning, ie synced from genesis and no pruning flags enabled but no dice. So I tried lighthouse using checkpoint sync and turning prune-blobs to false, and enabing genesis-backfill. It does not seem that it backfills the blobs, however. I'm now trying lighthouse to sync from genesis with the same flags, we'll see if it works in a while. You can test your beacon endpoint like this:\r\n`curl http://192.168.0.17:55702/eth/v1/beacon/blob_sidecars/5335452`\r\n\r\nto test the blob sidecars, change the block num to what the logs say in the derivation error to see if your beacon endpoint has it. @alexqrid your error seems to indicate you're using checkpoint sync with no backfill if I was to guess, not even about the blob sidecars. `t=2024-04-23T16:36:37+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=8595 err=\"stage 0 failed resetting: temp: failed to find the L2 Heads to start from: failed to fetch current L2 forkchoice state: failed to find the finalized L2 block: failed to parse L1 info deposit tx from L2 block: data is unexpected length: 260\"`\r\n\r\n@Rogalek you need to look earlier in your logs for an error like `Derivation process temporary error` to see what its saying, it may after a certain point stop showing that and just sync optimistically for a while, at least so I've found, especially if I change from consensus-layer sync to execution-layer sync, it'll just optimistically queue payloads blocks forever and never update geth head", "2024-04-23T17:11:18Z", "2024-04-23T17:11:18Z", "icculp", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM57kKoD", "I_kwDODjvEJM6DanwA", "We recently fixed something in our blob fetcher (https://github.com/ethereum-optimism/optimism/pull/10210) which I believe might fix some of the mentioned problems with blob fetching - we will release this very soon. In the meantime, you can already try out [op-node/v1.7.4-rc.2](https://github.com/ethereum-optimism/optimism/releases/tag/op-node%2Fv1.7.4-rc.2).\r\n\r\nOther than that, if you're syncing, the Beacon node you're using must have all historical blobs, or you must use a fallback beacon that has all historical blobs.", "2024-04-23T18:13:15Z", "2024-04-23T18:17:49Z", "sebastianst", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnG4i", "I_kwDODjvEJM6DanwA", "Closing this as we believe it has been fixed", "2024-06-17T22:05:31Z", "2024-06-17T22:05:31Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM54T1TT", "I_kwDODjvEJM6DZWfZ", "Hey @saihaj! Thanks for opening this issue. I am curious how your solution would compare against the [blob archiver](https://github.com/base-org/blob-archiver) service that we already operate both in terms of cost and query latency. Since the blobs have commitments held directly in the chain's history, what benefits come from storing them in a more decentralized storage solution compared to s3? Since this is code we would need to maintain indefinitely, we need to be sure that adding it will solve real user problems or improve the status quo", "2024-03-25T16:53:50Z", "2024-03-25T16:53:50Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnGhw", "I_kwDODjvEJM6DZWfZ", "Closing this due to inactivity", "2024-06-17T22:04:27Z", "2024-06-17T22:04:27Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM53I8vB", "I_kwDODjvEJM6CQoNp", "Sounds like you are using ethers js?", "2024-03-15T02:31:30Z", "2024-03-15T02:31:30Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnJ7h", "I_kwDODjvEJM6CQoNp", "Closing due to inactivity", "2024-06-17T22:14:40Z", "2024-06-17T22:14:40Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM519pta", "I_kwDODjvEJM6AYo2n", "Yeah, good catch. I think it would be best if the script caught this.", "2024-03-05T15:56:11Z", "2024-03-05T15:56:11Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnG3Y", "I_kwDODjvEJM6ACCI8", "Stale issue, closing. Looks like dependencies there were not updated. A simple `go mod tidy` would fix that. Or in Intellij (the IDE from screenshot): \"sync dependencies\" correction on imports or within `go.mod` file.", "2024-06-17T22:05:29Z", "2024-06-17T22:05:29Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnHGM", "I_kwDODjvEJM5-_Q8Y", "Closing this as superchain registry will replace `op-upgrade`", "2024-06-17T22:06:09Z", "2024-06-17T22:06:09Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BlHy2", "I_kwDODjvEJM6Mirpl", "cc @protolambda @tynes \r\n\r\nHere's a PR i had on interop config https://github.com/ethereum-optimism/optimism/pull/8990. An open question was if we want to refactor of the L1Client/L1Endpoint tooling into general client tooling\r\n\r\n", "2024-02-12T17:54:16Z", "2024-02-12T17:54:16Z", "hamdiallam", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BlHy9", "I_kwDODjvEJM6Mirpl", "Note from our call:\r\n\r\n- With the sidecar pattern actually don't need op-node to directly configure with peers like in the linked PR. The service likely can bootstrap itself and provide an interface to be used by both op-node & op-geth", "2024-02-12T18:08:36Z", "2024-02-12T18:08:58Z", "hamdiallam", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BlHzC", "I_kwDODjvEJM6Mirpl", "My thoughts on approaching this:\r\n\r\nConfig: Isolated interop go module within the monorepo. Configuration is simple, only requiring peer endpoint configuration and can internally and maintain clients as in op-node. Utils to populate this config with flags/env variables is straightforwward\r\n\r\n```go\r\ntype InteropConfig struct {\r\n    PeerChainCfgs map[uint64]EndpointConfig\r\n}\r\n```", "2024-02-13T17:38:17Z", "2024-02-13T17:38:17Z", "hamdiallam", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6CI-bM", "I_kwDODjvEJM6Mirpl", "Closing this as legacy work", "2024-06-21T20:13:31Z", "2024-06-21T20:13:31Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zO264", "I_kwDODjvEJM5-m4m1", "Are you using `develop`? Please make sure to use the `tutorials/chain` branch which trails `develop` because `develop` gets updated frequently. This is mentioned in the [Build the Optimism Monorepo](https://docs.optimism.io/builders/chain-operators/tutorials/create-l2-rollup#build-the-optimism-monorepo) section of the tutorial but maybe needs to be made more obvious.", "2024-02-08T02:39:21Z", "2024-02-08T02:40:08Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zO7hc", "I_kwDODjvEJM5-m4m1", "Yes, I made sure to checkout the correct branch, see below \r\n\r\n<img width=\"1665\" alt=\"image\" src=\"https://github.com/ethereum-optimism/optimism/assets/99218061/927bebee-8607-47a5-a271-68957bf9c277\">\r\n", "2024-02-08T03:07:56Z", "2024-02-08T03:07:56Z", "0x70626a", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zO8Xq", "I_kwDODjvEJM5-m4m1", "Strange, will look into this...", "2024-02-08T03:12:59Z", "2024-02-08T03:12:59Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zO8-a", "I_kwDODjvEJM5-m4m1", "I installed the latest (nightly) build of Foundry, is the missing file `null.sol` part of that package? \r\n<img width=\"929\" alt=\"image\" src=\"https://github.com/ethereum-optimism/optimism/assets/99218061/41a18a7b-ccc6-4f59-ba80-08bf35c69bdb\">\r\n", "2024-02-08T03:17:00Z", "2024-02-08T03:17:00Z", "0x70626a", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zO-OW", "I_kwDODjvEJM5-m4m1", "Can you try `pnpm update:foundry`? That will install the version of foundry used in that branch. Once you update, try `pnpm clean` then `pnpm build` again and rerun the deploy script.", "2024-02-08T03:25:29Z", "2024-02-08T03:25:49Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zPLZE", "I_kwDODjvEJM5-m4m1", "Something's not right with the version in the repo `9e2830d7f0566e0d00b1104eeaedd5032a4e556e`. Keeps erroring out with this when I try `pnpm update:foundry` or `foundryup -C 9e2830d7f0566e0d00b1104eeaedd5032a4e556e`\r\n\r\n<img width=\"1217\" alt=\"image\" src=\"https://github.com/ethereum-optimism/optimism/assets/99218061/91b6ef71-4ff7-464c-a912-568da523fb15\">\r\n\r\nDoesn't happen with the nightly builds `foundryup -v nightly-293fad73670b7b59ca901c7f2105bf7a29165a90`", "2024-02-08T04:48:40Z", "2024-02-08T04:48:40Z", "0x70626a", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zUdiV", "I_kwDODjvEJM5-m4m1", "Can you try using version `84d9842`? I just used that version on a clean install for the tutorial and it seemed to work.", "2024-02-08T18:52:37Z", "2024-02-08T18:52:37Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zO4MW", "I_kwDODjvEJM5-Pc9c", "What RPC provider are you using?", "2024-02-08T02:47:18Z", "2024-02-08T02:47:18Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zPFc0", "I_kwDODjvEJM5-Pc9c", "> What RPC provider are you using?\r\n\r\n- \"--rollup.sequencerhttp=https://mainnet-sequencer.optimism.io\"\r\n- ", "2024-02-08T04:11:07Z", "2024-02-08T04:11:07Z", "hanzhenlong1314", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zUshk", "I_kwDODjvEJM5-Pc9c", "Are you running your own node?", "2024-02-08T19:30:32Z", "2024-02-08T19:30:32Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zWHXi", "I_kwDODjvEJM5-Pc9c", "> Are you running your own node?\r\n\r\nYes, we run our own nodes, but our sequencer configuration is this\r\n", "2024-02-09T01:10:23Z", "2024-02-09T01:10:23Z", "hanzhenlong1314", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zWLAM", "I_kwDODjvEJM5-Pc9c", "I believe transactions are not applied to the local pending state, so you have to wait for the block that includes your transaction to be gossiped to you from the Sequencer. This can take several seconds after the transaction gets sent, you won't see the balance update immediately. How much of a delay are you observing?", "2024-02-09T01:20:57Z", "2024-02-09T01:20:57Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5zWL8K", "I_kwDODjvEJM5-Pc9c", "It feels like about a minute or so, and you can check the transaction that was successfully retried.\r\n", "2024-02-09T01:23:51Z", "2024-02-09T01:23:51Z", "hanzhenlong1314", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnKis", "I_kwDODjvEJM5-Pc9c", "This is an unsafe block latency issue that we can optimize for in the future", "2024-06-17T22:16:30Z", "2024-06-17T22:16:30Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ydYkJ", "I_kwDODjvEJM591dEQ", "I see. We are on an old version of ethers so we can't really fix this issue with ethers directly. As a quick fix my suggestion is we use [`serializeTransaction`](https://github.com/wevm/viem/blob/main/src/chains/opStack/serializers.ts#L30C14-L30C34) from viem as a one off here.  Viem should also be moved from devDeps to deps if it it isn't already a regular dep\r\n\r\nI won't be able to get to this pr this week but happy to review a pr if you want to make this change in meantime", "2024-02-01T01:25:59Z", "2024-02-01T01:25:59Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ydeID", "I_kwDODjvEJM591dEQ", "Thanks for the reply @roninjin10 \r\n\r\nThis is actually how I would expect ethers to behave. I don't think txs should have both type `0` and type `2` values present since then the intention of the caller would be unclear and it is left up to the lib to determine, which is not ideal. In this case, the consumer of the Optimism SDK has no power since both types are forced upon the call in all cases.\r\n\r\nIs it possible for you to handle it prior to serialization with ethers? I'm not exactly sure where, but I would think ethers has a utility function somewhere that handles this.", "2024-02-01T01:40:56Z", "2024-02-01T01:41:31Z", "shanefontaine", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ydx47", "I_kwDODjvEJM591dEQ", "Yes @shanefontaine. What you can do is filter out the undefined values. Viem will work without doing this because they check the value instead of the key. ('key' in value) vs (value.key !== undefined). Only downside to filtering out the keys is it's more code than using viem but upside is you aren't mixing viem and ethersv5.", "2024-02-01T03:00:15Z", "2024-02-01T03:00:15Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ydzl7", "I_kwDODjvEJM591dEQ", "@roninjin10 sorry I might not be clear. I believe there is nothing I can do because the Optimism SDK is [what adds](https://github.com/ethereum-optimism/optimism/blob/54db28d34ebe5f7562e07de93a41052a77d1c3d7/packages/sdk/src/l2-provider.ts#L106-L116) the values no matter what I pass in (including `undefined` values). No matter what filtering I do, any type `0` tx will always fail because of this. Let me know if that helps!", "2024-02-01T03:07:44Z", "2024-02-01T03:07:44Z", "shanefontaine", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5yeAkh", "I_kwDODjvEJM591dEQ", "@shanefontaine Right sorry I'm the one that is not clear. I'm talking about fixing the sdk not your repo. I worded it confusing because I said `What you can do is` which meant `what one can do is` or `what whoever picks up this ticket can do is`", "2024-02-01T04:09:51Z", "2024-02-01T04:09:51Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ynKnu", "I_kwDODjvEJM591dEQ", "The sdk is very fragile, it seems like every change that happens with it breaks something. There isn't good test coverage for it right now, the real main test for it is it runs both an eth and erc20 withdrawal against the docker compose devnet", "2024-02-02T05:39:49Z", "2024-02-02T05:39:49Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnGDx", "I_kwDODjvEJM591dEQ", "The sdk is deprecated", "2024-06-17T22:03:05Z", "2024-06-17T22:03:05Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ynNBp", "I_kwDODjvEJM59usZv", "They are constant values as part of the chainspec, similar to how L1 has constant values as part of its chainspec. They could be read from L1 but the do not need to be. They are not meant to be dynamic values. I do think that the config should use default values if they are not present in the config", "2024-02-02T05:47:08Z", "2024-02-02T05:47:08Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ynN0o", "I_kwDODjvEJM59usZv", "I see, maybe better to classify the fields of rollup config into mutable and immutable then.\r\n\r\n**UPDATE**\r\n\r\nWell, it seems not, these constants are supposed to be reflected in the `faultGameAbsolutePrestate`, right? But `faultGameAbsolutePrestate` is now hardcoded no matter what values constants like `ChannelTimeout` are set to.\r\n\r\nThis may not be a problem for official instances of OP stack, but I suspect it may become a *critical* problem for private deployments.", "2024-02-02T05:49:36Z", "2024-02-02T06:09:23Z", "zhiqiangxu", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnF1N", "I_kwDODjvEJM59usZv", "We would definitely like to move as much config on chain as possible. Things that are committed to in the absolute prestate will need an entry in the superchain registry with the value hardcoded there or a fork of the software. We will move more things on chain over time. We are discussing the next hardfork here: https://github.com/ethereum-optimism/specs/discussions/241", "2024-06-17T22:02:22Z", "2024-06-17T22:02:22Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5yYObA", "I_kwDODjvEJM59cNsA", "Please help me look into this problem. This situation has been happening frequently recently.", "2024-01-31T11:50:23Z", "2024-01-31T11:50:23Z", "hanzhenlong1314", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5yzN9R", "I_kwDODjvEJM59cNsA", "@sbvegan can this be moved to the developer support repo?", "2024-02-05T00:51:22Z", "2024-02-05T00:51:22Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5y0-lE", "I_kwDODjvEJM59cNsA", "> @sbvegan can this be moved to the developer support repo?\r\n\r\n@smartcontracts @sbvegan  I discussed this issue with @sbvegan. He said that it was a configuration issue with the upstream op-geth txpool.accountslots, which caused the transaction to be discarded. However, why the discarded transaction was rebroadcast is already known. It is not clear yet. Please help me take a look.", "2024-02-05T08:53:13Z", "2024-02-05T08:53:13Z", "hanzhenlong1314", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnLhf", "I_kwDODjvEJM57_yfF", "Looks like this is no longer necessary", "2024-06-17T22:19:42Z", "2024-06-17T22:19:42Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5woVKQ", "I_kwDODjvEJM57u_4N", "You should consider migrating to viem as the sdk is going to be phased out eventually. viem natively supports op stack chains", "2024-01-12T16:39:16Z", "2024-01-12T16:39:16Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5w1bY0", "I_kwDODjvEJM57u_4N", "> You should consider migrating to viem as the sdk is going to be phased out eventually. viem natively supports op stack chains\r\n\r\nDo you have a rough timeline on when the SDK will be phased out? ", "2024-01-16T04:29:49Z", "2024-01-16T04:29:49Z", "bb111189", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5yzPcI", "I_kwDODjvEJM57u_4N", "@bb111189 no rough timeline at the moment. Which function are you using that triggers the issue? ", "2024-02-05T00:58:52Z", "2024-02-05T00:58:52Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM52buEf", "I_kwDODjvEJM57u_4N", "> @bb111189 no rough timeline at the moment. Which function are you using that triggers the issue?\r\n\r\nI'm having this issue when calling `await crossChainMessenger.estimateGas.withdrawETH(amount)` with any amount larger than 1", "2024-03-09T20:55:02Z", "2024-03-09T20:55:02Z", "gndelia", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM52bvCi", "I_kwDODjvEJM57u_4N", "if I forcefully override the `from` address like this\r\n\r\n```ts\r\nawait crossChainMessenger.estimateGas.withdrawETH(amount, { overrides: { from: address } })\r\n```\r\n\r\nit works. \r\nHowever, this is not needed for DepositETH for example, so it is a bit inconsistent", "2024-03-09T21:12:58Z", "2024-03-09T21:15:15Z", "gndelia", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM59Efq8", "I_kwDODjvEJM57u_4N", "> if I forcefully override the `from` address like this\r\n> \r\n> ```ts\r\n> await crossChainMessenger.estimateGas.withdrawETH(amount, { overrides: { from: address } })\r\n> ```\r\n> \r\n> it works. However, this is not needed for DepositETH for example, so it is a bit inconsistent\r\n\r\nI have the same problem. If I override the `from` address it works, otherwise it returns a `insufficient funds` error", "2024-05-07T12:48:42Z", "2024-05-07T12:48:42Z", "alessandromazza98", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnL7S", "I_kwDODjvEJM57u_4N", "the sdk has been deprecated", "2024-06-17T22:21:05Z", "2024-06-17T22:21:05Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5w_hCn", "I_kwDODjvEJM57uGJa", "Any idea how can we fix this one? cause we are facing same issue", "2024-01-17T12:21:16Z", "2024-01-17T12:21:16Z", "eldimious", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5yzPm1", "I_kwDODjvEJM57uGJa", "Fastnode provides some snapshots. I have not verified the validity of the snapshots but they should work for you for now. Snap sync is also coming soon. https://datapoint.fastnode.io/", "2024-02-05T01:00:01Z", "2024-02-05T01:00:01Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnMKG", "I_kwDODjvEJM57uGJa", "snap sync works in prod now", "2024-06-17T22:21:54Z", "2024-06-17T22:21:54Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wGdFS", "I_kwDODjvEJM57ZBu2", "  const decompressChannel = async (frameData) => {\r\n    const maxOutputLength = 10_000_000\r\n    const channelCompressed = Buffer.concat([frameData])\r\n    return zlib.inflateSync(channelCompressed, { maxOutputLength: 10_000_000 })\r\n  }\r\n\"Error: unexpected end of file, code: 'Z_BUF_ERROR'\"", "2024-01-08T10:31:02Z", "2024-01-08T10:31:02Z", "liucan163", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wKJ1Z", "I_kwDODjvEJM57ZBu2", "span batches are on testnet - https://github.com/ethereum-optimism/optimism/blob/develop/specs/span-batches.md", "2024-01-08T19:39:06Z", "2024-01-08T19:39:06Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wMVtQ", "I_kwDODjvEJM57ZBu2", "Thank you very much. Are you suggesting that the method of batch submission from the L2 to L1 on the testnet has changed?", "2024-01-09T03:02:29Z", "2024-01-09T03:02:29Z", "liucan163", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wQS20", "I_kwDODjvEJM57ZBu2", "That batch serialization has changed", "2024-01-09T15:57:19Z", "2024-01-09T15:57:19Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wTO_1", "I_kwDODjvEJM57ZBu2", "Is there a method for reverse parsing the latest batch serialization scheme, ultimately allowing the extraction of the L2 transaction hash?", "2024-01-10T02:27:39Z", "2024-01-10T02:27:39Z", "liucan163", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wUmct", "I_kwDODjvEJM57ZBu2", "You may use the batch decoder supporting span batch introduced at https://github.com/ethereum-optimism/optimism/pull/8416. Refer to https://github.com/ethereum-optimism/optimism/tree/develop/op-node/cmd/batch_decoder.\r\n\r\nThe tool fetches L1 transactions, reassembles frames, extract channels, and decode/derive L2 transactions. \r\n\r\nIn your case, however, you need a transaction hash. We do not write L2 transaction hash to L1, rather, the info to recover the entire transaction itself. If you extract channels, you will find list of L2 transactions which is in RLP form. You can keccak them to get L2 tx hashes.\r\n\r\n", "2024-01-10T09:07:31Z", "2024-01-10T09:07:31Z", "pcw109550", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wVBR5", "I_kwDODjvEJM57ZBu2", "If you need hashes for deposit tx, you may need to use L1 event logs. Please refer to the [derivation specs](https://github.com/ethereum-optimism/optimism/blob/develop/specs/derivation.md).", "2024-01-10T10:15:34Z", "2024-01-10T10:15:46Z", "pcw109550", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wlFOh", "I_kwDODjvEJM57ZBu2", "Ok, thanks, I will take a look!", "2024-01-12T09:49:27Z", "2024-01-12T09:49:27Z", "liucan163", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5xQSkD", "I_kwDODjvEJM57ZBu2", "@liucan163 Have you resolved the issue somehow? I also cannot decompress the frame data after Delta upgrade of the network. The same error: `unexpected end of file`", "2024-01-19T09:57:24Z", "2024-01-19T09:57:24Z", "varasev", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5vurdU", "I_kwDODjvEJM56jnmz", "What would this impact? The relative path of reading in the p2p sequencer key? aka unsafe block signer key", "2024-01-02T20:23:39Z", "2024-01-02T20:23:39Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5v3QRQ", "I_kwDODjvEJM56Qbq3", "Hi, @0xfuturistic! Can I begin to work on this issue? First, by trying to reproduce it on my system (I'm using Sonoma 14.1, but can also test this out on Ubuntu 22.04).\r\n\r\nI've been going through this repo and OP stacks docs, and this issue doesn't seem to have anyone else working on this.", "2024-01-04T09:15:40Z", "2024-01-04T09:15:40Z", "AryanGodara", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5v3uu-", "I_kwDODjvEJM56Qbq3", "@AryanGodara unfortunately it seems to be widely unreported, but if you look at other recent PRs in the optimism monorepo you'll see that CI fails at the same spot!", "2024-01-04T10:42:55Z", "2024-01-04T10:42:55Z", "0xfuturistic", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5v3wLs", "I_kwDODjvEJM56Qbq3", "I'll go through the recent PRs to see the point of failure and understand the problem better. Do you know any specific place to begin looking apart from this, or do I start from the different workflow directories and try to look things up?\r\n\r\nI also want to understand the monorepo structure better\ud83d\ude05, maybe that happens as I go through this PR ", "2024-01-04T10:47:03Z", "2024-01-04T10:47:24Z", "AryanGodara", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5v3_1D", "I_kwDODjvEJM56Qbq3", "for sure! it's a tricky problem. @tynes could you provide some direction?", "2024-01-04T11:34:16Z", "2024-01-04T11:34:16Z", "0xfuturistic", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wAI3U", "I_kwDODjvEJM56Qbq3", "> for sure! it's a tricky problem. @tynes could you provide some direction?\r\n\r\nHi, is there any update on this?", "2024-01-05T18:19:25Z", "2024-01-05T18:19:25Z", "AryanGodara", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wAoQw", "I_kwDODjvEJM56Qbq3", "> I'll go through the recent PRs to see the point of failure and understand the problem better. Do you know any specific place to begin looking apart from this, or do I start from the different workflow directories and try to look things up?\r\n> \r\n> I also want to understand the monorepo structure better\ud83d\ude05, maybe that happens as I go through this PR\r\n\r\nThe tricky thing here is that it only appears to happen in CI. We use a pinned version of foundry that can be found in `.foundryrc`. In CI, I am not sure if we are using the same version. That could be the first place to check, it could be a versioning thing. See if you can repro the failure on the latest forge. If not, see if you can repro the failure on the pinned version", "2024-01-05T20:20:02Z", "2024-01-05T20:20:02Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wCFc_", "I_kwDODjvEJM56Qbq3", "> > I'll go through the recent PRs to see the point of failure and understand the problem better. Do you know any specific place to begin looking apart from this, or do I start from the different workflow directories and try to look things up?\r\n> > I also want to understand the monorepo structure better\ud83d\ude05, maybe that happens as I go through this PR\r\n> \r\n> The tricky thing here is that it only appears to happen in CI. We use a pinned version of foundry that can be found in `.foundryrc`. In CI, I am not sure if we are using the same version. That could be the first place to check, it could be a versioning thing. See if you can repro the failure on the latest forge. If not, see if you can repro the failure on the pinned version\r\n\r\nOh okay, I'll start from there then.\r\nI'll keep you updated on this, it may take a day, but I'm working on it in the meantime.", "2024-01-06T08:35:07Z", "2024-01-06T08:35:07Z", "AryanGodara", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5wmC-s", "I_kwDODjvEJM56Qbq3", "Hey, when I opened that issue on the foundry repo I was actually running an older version of the Optimism tests pre-foundry migration. I actually ran an updated version recently and I'm encountering the same setup error occasionally again. I'm fairly sure the last one was due to some cheat code race condition and I wouldn't be surprised if that's also the case here, since the full foundry setup uses considerably more cheat codes.\r\n\r\nA small side note: Am I the only one where the actual coverage report is missing major parts of the actual coverage?\r\nFor a quick repro, when running `forge coverage --match-path test/L1/L2OutputOracle.t.sol`, the coverage is completely empty except for the file `test/mocks/NextImpl.sol`.\r\nI can create a separate issue for this though if it is not isolated to my setup.", "2024-01-12T12:13:28Z", "2024-01-12T12:13:28Z", "bearpebble", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5woToB", "I_kwDODjvEJM56Qbq3", "> Hey, when I opened that issue on the foundry repo I was actually running an older version of the Optimism tests pre-foundry migration. I actually ran an updated version recently and I'm encountering the same setup error occasionally again. I'm fairly sure the last one was due to some cheat code race condition and I wouldn't be surprised if that's also the case here, since the full foundry setup uses considerably more cheat codes.\r\n\r\ncan you try running `pnpm clean` in `packages/contracts-bedrock` and in the root of the monorepo `make nuke && make clean` (in addition to `make devnet-clean` if you are using a Docker devnet). then run `pnpm install && make op-node op-proposer op-batcher && pnpm build`. then you can run the tests `packages/contracts-bedrock` `pnpm test`.", "2024-01-12T16:34:59Z", "2024-01-12T16:34:59Z", "0xfuturistic", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5w6fak", "I_kwDODjvEJM56Qbq3", "@0xfuturistic I can run the tests without issues and they all pass. It's just that the coverage report shows a lot of them as empty. The example for the `L2OutputOracle` tests is just a minimal repro that should show coverage but it's empty.\r\n\r\nDoes the invocation I shared actually report coverage on your end?", "2024-01-16T19:30:05Z", "2024-01-16T19:30:05Z", "bearpebble", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnM30", "I_kwDODjvEJM56Qbq3", "This has been hacked around for now, the problem is that the contracts don't compile without the optimizer on", "2024-06-17T22:24:19Z", "2024-06-17T22:24:19Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnM7t", "I_kwDODjvEJM55W9Eg", "The indexer has been deprecated, closing", "2024-06-17T22:24:32Z", "2024-06-17T22:24:32Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5uWNhJ", "I_kwDODjvEJM54-IXy", "Took a stab at running this to see what it showed up in https://github.com/ethereum-optimism/optimism/pull/8573\r\n\r\nhttps://app.circleci.com/pipelines/github/ethereum-optimism/optimism?branch=aj%2Frace-detector has a run showing all the failures.  The e2e ws tests timed out so still not sure what it will take to get them passing.\r\n\r\nUnit tests look viable to get fixed though I'm not sure how valuable they are given the vast majority of them are purely single threaded so data races wouldn't show up even if the code wasn't thread safe simply because there's only one thread in the unit test.", "2023-12-12T05:16:13Z", "2023-12-12T05:16:13Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6CFMEw", "I_kwDODjvEJM54-IXy", "@sebastianst @ajsutton ok to close this one or is it still relevant?", "2024-06-21T09:40:35Z", "2024-06-21T09:40:35Z", "BlocksOnAChain", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6CFP4n", "I_kwDODjvEJM54-IXy", "Yeah I'd say close it. The results didn't seem particularly useful. ", "2024-06-21T09:50:15Z", "2024-06-21T09:50:15Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6CFb6H", "I_kwDODjvEJM54-IXy", "Closing this issue, no further steps needed.", "2024-06-21T10:18:48Z", "2024-06-21T10:18:48Z", "BlocksOnAChain", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5totkm", "I_kwDODjvEJM54m7U5", "I would like to understand how the economics of a custom gas token works when you earn in a custom token but then must spend ether to buy gas on L1 for batch submission. Seems like you must constantly sell the custom token for ether. Will this work in practice?", "2023-12-04T20:04:29Z", "2023-12-04T20:04:29Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5trhyB", "I_kwDODjvEJM54m7U5", "@tynes yes, you have to find balance here, any necessary sales are compensated by user purchases of the token they might need to make transactions on the rollup. We've also been correcting the gas price depending on the token price to make sure the correct amount is charged to break even.\r\n\r\nSo in general, given that there is demand for such rollup architecture, fuelled by several RaaS platforms and their clients, it does make sense to think about a standart here, especially given a small diff with the original codebase, but the cost of maintaining different implementations across many rollups  might be much higher and lead to more risks for users of such rollups", "2023-12-05T07:07:18Z", "2023-12-05T07:07:18Z", "drinkius", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5t-uoa", "I_kwDODjvEJM54m7U5", "I would be very interested in this as well!", "2023-12-07T11:18:40Z", "2023-12-07T11:18:40Z", "Too-Far", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5y1BQ5", "I_kwDODjvEJM54m7U5", "@smartcontracts @tynes do you think it's worth investing our time into this implementation, can it be accepted in the main repository? And do you have any suggestions in regards to architecture, where to store the updated implementation files?", "2024-02-05T08:59:12Z", "2024-02-05T08:59:12Z", "drinkius", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5y2aOJ", "I_kwDODjvEJM54m7U5", "We would never accept an implementation without a spec first. If you wanted to open a PR to ethereum-optimism/specs that included exactly how it is implemented, that would be the first step. No guarantees on timely inclusion as fault proofs are the highest priority but it's how to get the feature started ", "2024-02-05T12:17:47Z", "2024-02-05T12:17:47Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM52_Qk2", "I_kwDODjvEJM54m7U5", "See https://github.com/ethereum-optimism/specs/issues/67 for a proof of concept implementation", "2024-03-14T02:47:51Z", "2024-03-14T02:47:51Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnEAh", "I_kwDODjvEJM54m7U5", "This has been completed and merged into the codebase", "2024-06-17T21:57:06Z", "2024-06-17T21:57:06Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ssF4h", "I_kwDODjvEJM53oecB", "Do we have a link to the bug reports? I know a while back besu was starting log index from 0 for each transaction rather than each block but I thought that had been fixed.\r\n\r\nFixing the data also sounds like a good idea so long as anything important to us is covered by the checks against the block hash.", "2023-11-22T21:01:42Z", "2023-11-22T21:01:42Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ssJOf", "I_kwDODjvEJM53oecB", "Regarding Besu log index: see the errors in the screenshot in https://github.com/ethereum-optimism/developers/discussions/59#discussioncomment-7519611", "2023-11-22T21:13:01Z", "2023-11-22T21:13:01Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ssQEl", "I_kwDODjvEJM53oecB", "Ah yeah that very much looks like https://github.com/hyperledger/besu/issues/4114 which should be fixed in the latest besu (but may have regressed).", "2023-11-22T21:35:22Z", "2023-11-22T21:35:22Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5sss_o", "I_kwDODjvEJM53oecB", "@ajsutton that issue is a year old, it must have regressed, or the user wouldn't be able to sync the latest L1 chain with it.", "2023-11-22T23:51:56Z", "2023-11-22T23:51:56Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ssxhq", "I_kwDODjvEJM53oecB", "oh good point, I missed the 2022 part of the date. The error message would fit perfectly with that bug though...", "2023-11-23T00:03:14Z", "2023-11-23T00:03:14Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5stIL-", "I_kwDODjvEJM53oecB", "Ah that fix was only for `eth_getLogs`. Almost certain the `eth_getReceipt` will have the same bug - just seeing if I can find someone with an existing besu node to confirm.", "2023-11-23T03:07:05Z", "2023-11-23T03:07:05Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5stJ-v", "I_kwDODjvEJM53oecB", "Confirmed, so I've logged https://github.com/hyperledger/besu/issues/6204", "2023-11-23T03:23:10Z", "2023-11-23T03:23:10Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5v75jT", "I_kwDODjvEJM53oecB", "Looks like this was fixed in Besu. Can we close this issue? @ajsutton ", "2024-01-05T01:05:53Z", "2024-01-05T01:05:53Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnFNh", "I_kwDODjvEJM523fSp", "Better to ask this in https://github.com/ethereum-optimism/developers/discussions", "2024-06-17T22:00:34Z", "2024-06-17T22:00:34Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5rG_eV", "I_kwDODjvEJM52B9ot", "This looks good to me! cc @roninjin10. @nitaliano would be good to get your input here as well on what API would be fine for the UI\r\n\r\nHow should we handle ETH transfers to the OptimismPortal/L2ToL1MessagePasser? Should they only be coalesced in the response if the type is ommitted?", "2023-11-06T22:57:15Z", "2023-11-06T22:57:57Z", "hamdiallam", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5rKk_B", "I_kwDODjvEJM52B9ot", "CC @lukasrosario for any additional requirements necessary for Base UI ", "2023-11-07T07:13:16Z", "2023-11-07T07:13:16Z", "ethenotethan", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnFFa", "I_kwDODjvEJM52B9ot", "The indexer has been deprecated", "2024-06-17T22:00:11Z", "2024-06-17T22:00:11Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5qeFdp", "I_kwDODjvEJM51X9IT", "Sweet this sounds good to me! Should the withdrawal always be key'd by the transaction hash?\r\n\r\nOr would you also like to be able to do an O(1) based on the cross domain message hash?", "2023-10-31T01:00:09Z", "2023-10-31T01:00:09Z", "hamdiallam", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5qezh9", "I_kwDODjvEJM51X9IT", "Probably the former, ideally we could use the l2 withdrawal tx hash for a consistent lookup since its channeled through all three key events in a withdrawal flow `(MessagePassed, WithdrawalProven, WithdrawalFinalized)`. ", "2023-10-31T05:15:26Z", "2023-11-03T21:29:27Z", "ethenotethan", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5qi1Mp", "I_kwDODjvEJM51X9IT", "okay cool i'd imagine we would want two different endpoints for transactions and messages", "2023-10-31T16:00:03Z", "2023-10-31T16:00:03Z", "hamdiallam", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnE_D", "I_kwDODjvEJM51X9IT", "The indexer has been deprecated", "2024-06-17T21:59:53Z", "2024-06-17T21:59:53Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnE5B", "I_kwDODjvEJM51Hq4C", "The sdk has been deprecated", "2024-06-17T21:59:35Z", "2024-06-17T21:59:35Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5yzTtI", "I_kwDODjvEJM5z_bmD", "Was this fixed?", "2024-02-05T01:22:32Z", "2024-02-05T01:22:32Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5pAcvF", "I_kwDODjvEJM5zuGRH", "If it falls behind then it must take some time because this is logged in CI\r\n\r\nSee [Test the Stack](https://app.circleci.com/pipelines/github/ethereum-optimism/optimism/29240/workflows/a964300f-8e1f-4be4-90cc-b541093ee473/jobs/1384192), copying the logs here:\r\n\r\n```\r\nWaiting to be able to prove withdrawal\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 36\r\nL1 chain tip: 44\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 36\r\nL1 chain tip: 45\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 42\r\nL1 chain tip: 46\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 42\r\nL1 chain tip: 47\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 42\r\nL1 chain tip: 48\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 42\r\nL1 chain tip: 49\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 42\r\nL1 chain tip: 50\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 42\r\nL1 chain tip: 51\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 48\r\nL1 chain tip: 52\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 48\r\nL1 chain tip: 53\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 48\r\nL1 chain tip: 54\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 48\r\nL1 chain tip: 55\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 48\r\nL1 chain tip: 56\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 48\r\nL1 chain tip: 57\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 54\r\nL1 chain tip: 58\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 54\r\nL1 chain tip: 59\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 54\r\nL1 chain tip: 60\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 54\r\nL1 chain tip: 61\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 54\r\nL1 chain tip: 62\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 54\r\nL1 chain tip: 63\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 60\r\nL1 chain tip: 64\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 60\r\nL1 chain tip: 65\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 60\r\nL1 chain tip: 66\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 60\r\nL1 chain tip: 67\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 60\r\nL1 chain tip: 68\r\nMessage status: STATE_ROOT_NOT_PUBLISHED\r\nLatest L2OutputOracle commitment number: 60\r\nL1 chain tip: 69\r\nMessage status: READY_TO_PROVE\r\nLatest L2OutputOracle commitment number: 66\r\nL1 chain tip: 70\r\nMessage status: READY_TO_PROVE\r\nLatest L2OutputOracle commitment number: 66\r\nL1 chain tip: 71\r\n```\r\n\r\nYou can see that it stays at most 9 blocks away, perhaps the issue is that the proposer ran out of funds?", "2023-10-13T15:41:30Z", "2023-10-13T15:41:30Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5pAn95", "I_kwDODjvEJM5zuGRH", "New blocks continue to be proposed, just a lot slower than they're generated.\r\n\r\nIf you follow the reproduction instructions you can see this for yourself.", "2023-10-13T16:14:43Z", "2023-10-13T16:14:43Z", "Arachnid", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5pCuCF", "I_kwDODjvEJM5zuGRH", "Can confirm that this is an issue", "2023-10-13T22:34:55Z", "2023-10-13T22:34:55Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5pMz2_", "I_kwDODjvEJM5zuGRH", "The deploy config (the template that then becomes the config) has `finalizationPeriodSeconds: 2`, while the L1 time is 3s, and L2 time is 2s. The proposer is synchronously proposing and confirming, but will fall behind when it has to do that for every L2 block, faster than it can do a L1 tx-confirmation cycle.\r\n\r\nTo solve, we need to bump `finalizationPeriodSeconds` in the deploy config to something higher than `L1_block_time * proposer_tx_confirmations + some_margin`.\r\n\r\nOr, we can refactor the proposer for concurrent proposals to keep up with a high throughput, but I'd rather not introduce unnecessarily complexity.", "2023-10-16T17:42:34Z", "2023-10-16T17:42:34Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5pyu26", "I_kwDODjvEJM5zuGRH", "I also see this problem. However, increasing the `finalizationPeriodSeconds` didn't reduce the lag between L2 block number and the one seen in `L2OutputOracle`'s ` latestBlockNumber`.\r\n\r\nThe proposed L2 block number is always slightly behind the L1 block number. Coincidence, or is there a reason why the devnet config consistently leads to this result for me?\r\n\r\n```\r\nL1 number               3\r\nL2 number               0\r\nL2OO 0\r\nL1 number               20\r\nL2 number               31\r\nL2OO 12\r\nL1 number               37\r\nL2 number               56\r\nL2OO 30\r\nL1 number               54\r\nL2 number               81\r\nL2OO 48\r\nL1 number               71\r\nL2 number               107\r\nL2OO 66\r\nL1 number               87\r\nL2 number               132\r\nL2OO 78\r\nL1 number               104\r\nL2 number               157\r\nL2OO 96\r\nL1 number               121\r\nL2 number               183\r\nL2OO 114\r\nL1 number               138\r\nL2 number               208\r\nL2OO 132\r\n```\r\n\r\nhttps://github.com/ethereum-optimism/optimism/issues/5507#issuecomment-1692737606 might be related.", "2023-10-23T10:35:46Z", "2023-10-23T10:44:35Z", "karlb", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5py-FA", "I_kwDODjvEJM5zuGRH", "Increasing `l2OutputOracleSubmissionInterval` solves the problem for me.", "2023-10-23T11:16:22Z", "2023-10-23T11:16:22Z", "karlb", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5p8mKZ", "I_kwDODjvEJM5zuGRH", "Ah yes, I had the wrong variable; `finalizationPeriodSeconds` configures how long it takes for an onchain output to become \"final\" (i.e. what would be 7 days in real networks). `l2OutputOracleSubmissionInterval` is what changes the interval between outputs (1 hour on real networks). With the batcher submitting blocks, the op-node deriving from that to mark things safe, and the op-proposer only proposing after confirming previous work & safety of blocks, it takes it a little while to finish its proposal cycle. The `l2OutputOracleSubmissionInterval` has to accomodate for that, so the op-proposer can keep up.\r\n", "2023-10-24T15:31:20Z", "2023-10-24T15:31:20Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5yzTlM", "I_kwDODjvEJM5zuGRH", "Was this fixed? @hamdiallam ", "2024-02-05T01:21:46Z", "2024-02-05T01:21:46Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnEt5", "I_kwDODjvEJM5zuGRH", "This is a known issue and is still the case when running a pre fault proofs system. A post fault proofs system does not have this issue in theory, because there is not an interval at which outputs must be proposed in. Any output can be proposed anytime", "2024-06-17T21:59:01Z", "2024-06-17T21:59:01Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnqcQ", "I_kwDODjvEJM5zuGRH", "Given the devnet now defaults to using fault proofs, I'm going to close this as fixed, but please feel free to reopen if I've misunderstood something.", "2024-06-18T00:06:16Z", "2024-06-18T00:06:16Z", "ajsutton", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ljXVE", "I_kwDODjvEJM5v0KBa", "Have you download data?   This is typically because no data was found under the path /opt in geth --datadir.", "2023-09-02T08:57:26Z", "2023-09-02T08:59:05Z", "opfocus", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5lljZY", "I_kwDODjvEJM5v0KBa", "> Have you download data? This is typically because no data was found under the path /opt in geth --datadir.\r\n\r\nThank you, I downloaded a snapshot file from  https://community.optimism.io/docs/developers/build/run-a-node/#initializing-op-geth  (https://datadirs.optimism.io/mainnet-bedrock.tar.zst),  decompressed it, and run geth  again, the log shows that it is still abnormal.\r\nLog information:\r\n\r\nINFO [09-03|15:49:17.835] Starting Geth on Ethereum mainnet...\r\nINFO [09-03|15:49:17.835] Bumping default cache on mainnet         provided=1024 updated=4096\r\nINFO [09-03|15:49:17.842] Maximum peer count                       ETH=0 LES=0 total=0\r\nINFO [09-03|15:49:17.843] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nINFO [09-03|15:49:17.850] Set global gas cap                       cap=50,000,000\r\nINFO [09-03|15:49:17.851] Initializing the KZG library             backend=gokzg\r\nINFO [09-03|15:49:17.931] Allocated trie memory caches             clean=614.00MiB dirty=1024.00MiB\r\nINFO [09-03|15:49:18.092] Using pebble as the backing database\r\nINFO [09-03|15:49:18.093] Allocated cache and file handles         database=/opt/geth/chaindata cache=2.00GiB handles=524,288\r\nFatal: Failed to register the Ethereum service: pebble: files 1186099 and 1186099 collided on sort keys\r\nFatal: Failed to register the Ethereum service: pebble: files 1186099 and 1186099 collided on sort keys\r\nINFO [09-03|15:49:19.528] Starting Geth on Ethereum mainnet...\r\nINFO [09-03|15:49:19.528] Bumping default cache on mainnet         provided=1024 updated=4096\r\nINFO [09-03|15:49:19.534] Maximum peer count                       ETH=0 LES=0 total=0\r\nINFO [09-03|15:49:19.534] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nINFO [09-03|15:49:19.540] Set global gas cap                       cap=50,000,000\r\nINFO [09-03|15:49:19.540] Initializing the KZG library             backend=gokzg\r\nINFO [09-03|15:49:19.587] Allocated trie memory caches             clean=614.00MiB dirty=1024.00MiB\r\nINFO [09-03|15:49:19.744] Using pebble as the backing database\r\nINFO [09-03|15:49:19.745] Allocated cache and file handles         database=/opt/geth/chaindata cache=2.00GiB handles=524,288\r\nFatal: Failed to register the Ethereum service: pebble: files 1186099 and 1186099 collided on sort keys\r\nFatal: Failed to register the Ethereum service: pebble: files 1186099 and 1186099 collided on sort keys\r\nINFO [09-03|15:49:21.176] Starting Geth on Ethereum mainnet...\r\nINFO [09-03|15:49:21.176] Bumping default cache on mainnet         provided=1024 updated=4096\r\nINFO [09-03|15:49:21.182] Maximum peer count                       ETH=0 LES=0 total=0\r\nINFO [09-03|15:49:21.182] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nINFO [09-03|15:49:21.187] Set global gas cap                       cap=50,000,000\r\nINFO [09-03|15:49:21.188] Initializing the KZG library             backend=gokzg\r\nINFO [09-03|15:49:21.234] Allocated trie memory caches             clean=614.00MiB dirty=1024.00MiB\r\nINFO [09-03|15:49:21.488] Using pebble as the backing database\r\nINFO [09-03|15:49:21.488] Allocated cache and file handles         database=/opt/geth/chaindata cache=2.00GiB handles=524,288\r\nFatal: Failed to register the Ethereum service: pebble: files 1186099 and 1186099 collided on sort keys\r\nFatal: Failed to register the Ethereum service: pebble: files 1186099 and 1186099 collided on sort keys\r\nINFO [09-03|15:49:23.205] Starting Geth on Ethereum mainnet...\r\nINFO [09-03|15:49:23.206] Bumping default cache on mainnet         provided=1024 updated=4096\r\nINFO [09-03|15:49:23.211] Maximum peer count                       ETH=0 LES=0 total=0\r\nINFO [09-03|15:49:23.212] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nINFO [09-03|15:49:23.217] Set global gas cap                       cap=50,000,000\r\nINFO [09-03|15:49:23.218] Initializing the KZG library             backend=gokzg\r\nINFO [09-03|15:49:23.265] Allocated trie memory caches             clean=614.00MiB dirty=1024.00MiB\r\nINFO [09-03|15:49:23.433] Using pebble as the backing database\r\nINFO [09-03|15:49:23.433] Allocated cache and file handles         database=/opt/geth/chaindata cache=2.00GiB handles=524,288\r\nFatal: Failed to register the Ethereum service: pebble: files 1186099 and 1186099 collided on sort keys\r\nFatal: Failed to register the Ethereum service: pebble: files 1186099 and 1186099 collided on sort keys", "2023-09-03T16:07:20Z", "2023-09-03T16:07:20Z", "xt-com", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ll9oh", "I_kwDODjvEJM5v0KBa", "-l1=http://172.17.0.8:8545  maybe because this config.  what is your L1 data layer?    I think when you change to a public goerli or eht-mainnet RPC ,it will work well\r\n", "2023-09-04T00:25:04Z", "2023-09-04T00:25:04Z", "opfocus", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnEYi", "I_kwDODjvEJM5v0KBa", "This is better to ask in https://github.com/ethereum-optimism/developers/discussions", "2024-06-17T21:57:58Z", "2024-06-17T21:57:58Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6Bl-_J", "I_kwDODjvEJM5vYL1o", "Closing as stale. In case anyone else hits this: `op-bootnode --help` to see the flags. Looks like it was missing a specific option, the error explains it.", "2024-06-17T19:01:31Z", "2024-06-17T19:01:31Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5lAIRw", "I_kwDODjvEJM5vWQN4", "I reproduced the issue again. Full ```op-geth``` and ```op-node``` logs:  [op-logs.7z](https://www.dropbox.com/scl/fi/fttqgm2hmc38nuzry0mz2/op-logs.7z?rlkey=yydczcb80bizir33bkeyhi8zc)", "2023-08-27T00:38:13Z", "2023-08-27T00:38:13Z", "mikhail-khalizev", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5m70Ey", "I_kwDODjvEJM5vWQN4", "I have the same issue, @mikhail-khalizev any solutions found?\r\n", "2023-09-20T04:39:01Z", "2023-09-20T04:39:01Z", "p516entropy", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5m71Jk", "I_kwDODjvEJM5vWQN4", "> I have the same issue, @mikhail-khalizev any solutions found?\r\n\r\nNo solution. Just remove all data and start synchronization again from the beginning.", "2023-09-20T04:45:18Z", "2023-09-20T04:45:18Z", "mikhail-khalizev", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6Bl9Zl", "I_kwDODjvEJM5vWQN4", "Closing, this issue seems stale. Please file a new issue if it has not already been resolved", "2024-06-17T18:57:21Z", "2024-06-17T18:57:21Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kz9Ux", "I_kwDODjvEJM5vIs4e", "> I was trying to run Optimism L2 while L1 as local anvil chain. However during the setup of OP-Node, I am getting following error: ![image](https://user-images.githubusercontent.com/138877287/262891764-3372a0ed-aa28-461a-bcf9-8eafb9967da0.png) Can anyone help me solve the error?\r\n\r\nWhat is your L1 data layer?", "2023-08-24T09:31:50Z", "2023-08-24T09:31:50Z", "opfocus", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5k0Mjc", "I_kwDODjvEJM5vIs4e", "> > I was trying to run Optimism L2 while L1 as local anvil chain. However during the setup of OP-Node, I am getting following error: ![image](https://user-images.githubusercontent.com/138877287/262891764-3372a0ed-aa28-461a-bcf9-8eafb9967da0.png) Can anyone help me solve the error?\r\n> \r\n> What is your L1 data layer?\r\n\r\nI am using anvil as L1", "2023-08-24T10:14:45Z", "2023-08-24T10:14:45Z", "Abhijaypaliwal", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5k_bA_", "I_kwDODjvEJM5vIs4e", "> > > I was trying to run Optimism L2 while L1 as local anvil chain. However during the setup of OP-Node, I am getting following error: ![image](https://user-images.githubusercontent.com/138877287/262891764-3372a0ed-aa28-461a-bcf9-8eafb9967da0.png) Can anyone help me solve the error?\r\n> > \r\n> > \r\n> > What is your L1 data layer?\r\n> \r\n> I am using anvil as L1\r\n\r\nThis is typically due to an unsupported L1 data layer. I'm not sure how to support it. However, you can easily verify this: replacing L1RPC with a public Goerli RPC will not cause this issue.\"", "2023-08-26T13:57:59Z", "2023-08-26T13:57:59Z", "opfocus", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5mx37B", "I_kwDODjvEJM5vIs4e", "You need to have the smart contracts deployed to the L1. Unfortunately, anvil does not work with `op-node` due to some issues with its RPC responses.", "2023-09-18T20:48:23Z", "2023-09-18T20:48:23Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5-LOJn", "I_kwDODjvEJM5vIs4e", "same error,but I am using Sepolia as L1, following show you my error:\r\nubuntu@ip-172-31-42-83:~/op/optimism/op-node$ ./bin/op-node   --l2=http://localhost:8521   --l2.jwt-secret=./jwt.txt   --sequencer.enabled   --sequencer.l1-confs=5   --verifier.l1-confs=4   --rollup.config=./rollup.json   --rpc.addr=0.0.0.0   --rpc.port=8517   --p2p.disable   --rpc.enable-admin   --p2p.sequencer.key=$GS_SEQUENCER_PRIVATE_KEY   --l1=$L1_RPC_URL   --l1.rpckind=$L1_RPC_KIND\r\n\r\nINFO [05-17|05:53:20.796] Not opted in to ProtocolVersions signal loading, disabling ProtocolVersions contract now.\r\nINFO [05-17|05:53:20.797] No persisted sequencer state loaded\r\nINFO [05-17|05:53:20.797] Rollup Config                            l2_chain_id=42069 l2_network=\"unknown L2\" l1_chain_id=11,155,111 l1_network=sepolia l2_start_time=1,715,343,840 l2_block_hash=0x3ad88f3aeebe94efe2fa31fee3f0b7cb5af881fc9a49ee6354f7739e11088208 l2_block_number=0 l1_block_hash=0xa6d527fb4c8dc2ad217497252a09e9d80290357639450e0f37078adddcb7d492 l1_block_number=5,874,244 regolith_time=\"@ genesis\" canyon_time=\"@ genesis\" delta_time=\"(not configured)\" ecotone_time=\"(not configured)\" fjord_time=\"(not configured)\" interop_time=\"(not configured)\"\r\nINFO [05-17|05:53:20.797] Initializing rollup node                 version=v0.0.0-a06cae81-1705510057\r\nWARN [05-17|05:53:20.918] No beacon endpoint configured. Configuration is mandatory for the Ecotone upgrade\r\nERROR[05-17|05:53:21.258] failed to fetch runtime config data      err=\"failed to fetch unsafe block signing address from system config: failed to verify retrieved proof against state root: failed to verify storage value 0 with key 0x65a7ed542fb37fe237fdfbdd70b31598523fe5b32879e307bae27a0bd9581c08 (path 31a88f3936348d602f3078126bdcd162c575cb17fb9bbfe2dab00b167bd295c3) in storage trie 0x0000000000000000000000000000000000000000000000000000000000000000: proof node 0 (hash 0000000000000000000000000000000000000000000000000000000000000000) missing\"\r\n", "2024-05-17T06:49:27Z", "2024-05-17T06:49:27Z", "DavidCoder88", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5-QJ3W", "I_kwDODjvEJM5vIs4e", "Maybe you can try changing the provider of L1RPC", "2024-05-17T18:24:19Z", "2024-05-17T18:24:19Z", "opfocus", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnEP7", "I_kwDODjvEJM5vIs4e", "Anvil doesn't support `op-node`", "2024-06-17T21:57:35Z", "2024-06-17T21:57:35Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ju0r4", "I_kwDODjvEJM5t8x56", "@RISHABHAGRAWALZRA \r\nadd `\"systemConfigStartingBlock\" : 0` in your `getting-started.json` file.\r\n\r\nAnd I saw your further Issue using Ganache on Discord,\r\njust replace `1337` in `Chains.sol` with something random number so the script won't require those extra `dispute-game` configurations https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/scripts/Chains.sol\r\n\r\nor you can add `faultGameMaxDepth` , `faultGameAbsolutePrestate` , `faultGameMaxDuration` in your `getting-started.json` if you are using Ganache or change the `chain ID` of ganache. : )\r\n", "2023-08-10T13:23:38Z", "2023-08-10T14:18:49Z", "nitantchhajed", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jvMIa", "I_kwDODjvEJM5t8x56", "Hey, The field name here is, `systemConfigStartBlock`. ", "2023-08-10T14:19:06Z", "2023-08-10T14:19:06Z", "ishanjain28", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jvMb_", "I_kwDODjvEJM5t8x56", "> Hey, The field name here is, `systemConfigStartBlock`.\r\n\r\nmy bad. ", "2023-08-10T14:19:52Z", "2023-08-10T14:19:52Z", "nitantchhajed", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jw11C", "I_kwDODjvEJM5t8x56", "I see that `systemConfigStartBlock` has been added to `packages/contracts-bedrock/deploy-config/devnetL1.json` but its not in any of the other configuration files in `packages/contracts-bedrock/deploy-config`.\r\n\r\nSo instead of adding this to the `packages/contracts-bedrock/deploy-config/getting-started.json`, I think the `packages/contracts-bedrock/scripts/DeployConfig.s.sol` might need to be updated instead.\r\n\r\nI wonder if line 91:\r\n\r\n```\r\nsystemConfigStartBlock = stdJson.readUint(_json, \"$.systemConfigStartBlock\");\r\n```\r\n\r\nshould belong in the following conditional statement:\r\n\r\n```\r\nif (block.chainid == Chains.LocalDevnet || block.chainid == Chains.GethDevnet) {\r\n...\r\n}\r\n```", "2023-08-10T18:58:32Z", "2023-08-10T18:58:32Z", "sbvegan", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jxHc5", "I_kwDODjvEJM5t8x56", "perhaps you can checkout latest tag?\r\n`git checkout 4ef51e1 -b stable`\r\n\r\ni encountered familiar problem today, and now i can interact with OP stack", "2023-08-10T19:51:57Z", "2023-08-10T19:51:57Z", "ec2ainun", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jxfzj", "I_kwDODjvEJM5t8x56", "> perhaps you can checkout latest tag? `git checkout 4ef51e1 -b stable`\r\n> \r\n> i encountered familiar problem today, and now i can interact with OP stack\r\n\r\nThese work for me thanks !", "2023-08-10T21:05:29Z", "2023-08-10T21:05:29Z", "0xefrain", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jyyvr", "I_kwDODjvEJM5t8x56", "> perhaps you can checkout latest tag? `git checkout 4ef51e1 -b stable`\r\n> \r\n> i encountered familiar problem today, and now i can interact with OP stack\r\n\r\nThanks, yes, there is no check for `systemConfigStartBlock` in this tag.", "2023-08-11T06:19:05Z", "2023-08-11T06:19:05Z", "RISHABHAGRAWALZRA", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jyzVW", "I_kwDODjvEJM5t8x56", "But now I am getting this error\r\n<img width=\"1081\" alt=\"Screenshot 2023-08-11 at 11 46 44 AM\" src=\"https://github.com/ethereum-optimism/optimism/assets/43982251/618b809b-52de-4904-9e57-bd1275f67017\">\r\n\r\nBasically I am using ganache-cli for local L1 layer and set the chain id 1999, I think this is the reason I am getting this error \r\n\r\nDo anyone how to run OP-Stack locally by using local L1 layer, what other changes required to run it?", "2023-08-11T06:22:16Z", "2023-08-11T06:22:16Z", "RISHABHAGRAWALZRA", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jz-mI", "I_kwDODjvEJM5t8x56", "> But now I am getting this error <img alt=\"Screenshot 2023-08-11 at 11 46 44 AM\" width=\"1081\" src=\"https://user-images.githubusercontent.com/43982251/259934910-618b809b-52de-4904-9e57-bd1275f67017.png\">\r\n> \r\n> Basically I am using ganache-cli for local L1 layer and set the chain id 1999, I think this is the reason I am getting this error\r\n> \r\n> Do anyone how to run OP-Stack locally by using local L1 layer, what other changes required to run it?\r\n\r\nIt been resolved, basically in the `4ef51e1` tag there is `.chainid and .deploy` file already present in the `getting-started` folder which is causing the problem, so I just removed them and then tried again, it worked", "2023-08-11T11:01:44Z", "2023-08-11T11:01:44Z", "RISHABHAGRAWALZRA", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jz_CS", "I_kwDODjvEJM5t8x56", "@RISHABHAGRAWALZRA thanks for sharing !!, you can close the Issue if you contracts are deployed successfully", "2023-08-11T11:03:18Z", "2023-08-11T11:03:18Z", "nitantchhajed", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jz_7p", "I_kwDODjvEJM5t8x56", "I am now, getting stuck here, my L1 layer block time is 10s but I don't know why it is showing this \r\n\r\n<img width=\"1078\" alt=\"Screenshot 2023-08-11 at 4 35 00 PM\" src=\"https://github.com/ethereum-optimism/optimism/assets/43982251/2480bc0b-e4ad-42d9-96e4-8f6fffc497fb\">\r\n\r\ncan you pls check or do you know how to resolve it? @nitantchhajed @tynes ", "2023-08-11T11:06:42Z", "2023-08-11T11:06:42Z", "RISHABHAGRAWALZRA", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5j6zz1", "I_kwDODjvEJM5t8x56", "just add this line `\"l1BlockTime\": 3,` to your `getting-started.json`\r\nI used 3 because it's larger than 2\r\n@RISHABHAGRAWALZRA", "2023-08-13T13:28:09Z", "2023-08-13T13:28:09Z", "Aymen-Tirchi", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5j_X9-", "I_kwDODjvEJM5t8x56", "\r\n\r\n\r\n\r\n> just add this line `\"l1BlockTime\": 3,` to your `getting-started.json` I used 3 because it's larger than 2 @RISHABHAGRAWALZRA\r\n\r\nI think this is just a hack to get around the error.\r\nBlocktime for L1 should be 12, so there must be an error in the code somewhere\r\nhttps://stack.optimism.io/docs/build/conf/#misc\r\n", "2023-08-14T15:30:23Z", "2023-08-14T15:30:23Z", "norfolkpine", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kLjOJ", "I_kwDODjvEJM5t8x56", "I am getting this error on running op-node using local L1 layer like with anvil or ganache\r\n\r\n<img width=\"1140\" alt=\"Screenshot 2023-08-13 at 1 09 51 PM\" src=\"https://github.com/ethereum-optimism/optimism/assets/43982251/ac1a6b39-eadc-413c-97db-44c8ccb1ddc2\">\r\n", "2023-08-16T14:42:38Z", "2023-08-16T14:42:38Z", "RISHABHAGRAWALZRA", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kLjtu", "I_kwDODjvEJM5t8x56", "What does this proof of storage slot and unsafe block signing? @tynes", "2023-08-16T14:43:55Z", "2023-08-18T10:05:34Z", "RISHABHAGRAWALZRA", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kZL2e", "I_kwDODjvEJM5t8x56", "This is a recent change, a fix has been merged. It seemed like you were almost able to solve the issue, we are open to you opening PRs with solutions in the future :)", "2023-08-18T19:01:30Z", "2023-08-18T19:01:30Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kZMzq", "I_kwDODjvEJM5t8x56", "> I am getting this error on running op-node using local L1 layer like with anvil or ganache\n> \n> <img width=\"1140\" alt=\"Screenshot 2023-08-13 at 1 09 51 PM\" src=\"https://github.com/ethereum-optimism/optimism/assets/43982251/ac1a6b39-eadc-413c-97db-44c8ccb1ddc2\">\n> \n\nThe L1 contracts must be deployed. There is no guarantee opnode will work with ganache/anvil. We have a devnet that you can develop against using `make devnet-up`", "2023-08-18T19:05:42Z", "2023-08-18T19:05:42Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kZTpC", "I_kwDODjvEJM5t8x56", "Which L1 contracts have to deploy, I think they are not given in this doc https://community.optimism.io/docs/protocol/protocol-2.0/# @tynes ", "2023-08-18T19:34:33Z", "2023-08-18T19:34:33Z", "RISHABHAGRAWALZRA", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kZ7_p", "I_kwDODjvEJM5t8x56", "> Which L1 contracts have to deploy, I think they are not given in this doc https://community.optimism.io/docs/protocol/protocol-2.0/# @tynes \n\nYou will want to run the same command you posted in your first comment to deploy the L1 contracts", "2023-08-18T22:55:09Z", "2023-08-18T22:55:09Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kbfb3", "I_kwDODjvEJM5t8x56", "Does this include every L1 contract https://community.optimism.io/docs/protocol/protocol-2.0/#l1-contracts? @tynes ", "2023-08-19T12:02:43Z", "2023-08-19T12:02:43Z", "RISHABHAGRAWALZRA", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ksxR1", "I_kwDODjvEJM5t8x56", "> Does this include every L1 contract https://community.optimism.io/docs/protocol/protocol-2.0/#l1-contracts? @tynes\r\n\r\nNo that list does not include all the contracts that are deployed on L1. \r\n\r\nHere are the Contracts which are deployed on L1 -\r\n\r\n```\r\nAddressManager\r\nProxyAdmin\r\nOptimismPortalProxy\r\nL2OutputOracleProxy\r\nSystemConfigProxy\r\nL1StandardBridgeProxy\r\nL1CrossDomainMessengerProxy\r\nOptimismMintableERC20FactoryProxy\r\nL1ERC721BridgeProxy\r\nOptimismPortal\r\nL1CrossDomainMessenger\r\nL2OutputOracle\r\nOptimismMintableERC20Factory\r\n```\r\n", "2023-08-23T07:47:54Z", "2023-08-23T07:47:54Z", "nitantchhajed", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kxgc-", "I_kwDODjvEJM5t8x56", "I don't believe anvil has ever worked with running op stack chains though I haven't checked the status of some blockers there in a while. If only one chain is needed forking a live chain with anvil is a great approach but if you need cross chain messages or any actual rollup functionality using the optimism devnet is the best approach", "2023-08-23T21:57:44Z", "2023-08-23T21:57:44Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5kzCiD", "I_kwDODjvEJM5t8x56", "> I don't believe anvil has ever worked with running op stack chains though I haven't checked the status of some blockers there in a while. If only one chain is needed forking a live chain with anvil is a great approach but if you need cross chain messages or any actual rollup functionality using the optimism devnet is the best approach\r\n\r\nIs Optimism devnet an L1 CHAIN ??\r\n", "2023-08-24T06:49:09Z", "2023-08-24T06:49:09Z", "nitantchhajed", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5k13di", "I_kwDODjvEJM5t8x56", "> > I don't believe anvil has ever worked with running op stack chains though I haven't checked the status of some blockers there in a while. If only one chain is needed forking a live chain with anvil is a great approach but if you need cross chain messages or any actual rollup functionality using the optimism devnet is the best approach\r\n> \r\n> Is Optimism devnet an L1 CHAIN ??\r\n\r\nL1 and L2 as specified in the docker-compose https://github.com/ethereum-optimism/optimism/blob/develop/ops-bedrock/docker-compose.yml#L14", "2023-08-24T14:55:59Z", "2023-08-24T14:55:59Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnDPy", "I_kwDODjvEJM5t8x56", "We do not support running `op-node` against ganache or anvil", "2024-06-17T21:55:00Z", "2024-06-17T21:55:00Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jkE9K", "I_kwDODjvEJM5ttkqs", "Its definitely possible, we do not maintain bundler software but you should be able to find a bundler that has docs on how to run it", "2023-08-08T22:31:19Z", "2023-08-08T22:31:19Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jlo0m", "I_kwDODjvEJM5ttkqs", "if it is possible, can you suggest me or provide a link to a compatible bundler ? and one more thing, my op-stack chain will still be eligible for superchain if I use bundler ?\r\nthanks !", "2023-08-09T07:25:58Z", "2023-08-09T07:25:58Z", "nitantchhajed", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnDBE", "I_kwDODjvEJM5ttkqs", "Yes it is certainly possible to be part of the superchain when running a bundler :)", "2024-06-17T21:54:21Z", "2024-06-17T21:54:21Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5icUGe", "I_kwDODjvEJM5skolw", "i think you will solve just using  `make devnet-up-deploy` , when \"devnet-up\" just compilation and basic tasks are completed, at \"deploy\" some txns are executed over your L1+L2 devnet and fund main accounts :D", "2023-07-26T11:23:40Z", "2023-07-26T11:23:50Z", "netzulo", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5id8c7", "I_kwDODjvEJM5skolw", "Hey @rho-cassiopeiae, can you dm me on telegram (@rflihxyz)! I'm trying to do the same and would love to chat about it! Best", "2023-07-26T15:17:20Z", "2023-07-26T15:17:20Z", "rflihxyz", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ifsA8", "I_kwDODjvEJM5skolw", "@rho-cassiopeiae Thanks for being interested in trying to run a bundler on optimism devnet! I've opened a couple of PRs attempting to make this process easier.\r\n\r\nWe currently have 2 devnets and I am working on consolidating them into a single devnet. The two commands are:\r\n- `make devnet-up`\r\n- `make devnet-up-deploy`\r\n\r\nRegarding the lack of funds, #6463 was created to add more accounts with funds at genesis for `make devnet-up`. You can check the balance of an account using `cast balance` - if the accounts do not have funds on L1, then something is wrong. Be sure to use the right port, I believe `8545` is the L1 port.\r\n\r\nIt would be super helpful to confirm if `make devnet-up` or `make devnet-up-deploy` has the genesis funds at the dev accounts.\r\n\r\nI see here that the hardhat config for the bundler contracts uses the same mnemonic that we use to seed genesis accounts so it seems like they are not funded in the genesis?\r\nhttps://github.com/eth-infinitism/bundler/blob/623991115d891419408ccffb8b65c0c4bb48421c/packages/bundler/hardhat.config.ts#L11", "2023-07-26T20:49:09Z", "2023-07-26T20:49:09Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5igdYw", "I_kwDODjvEJM5skolw", "HI @rflihxyz @netzulo @rho-cassiopeiae \r\n\r\nLovely to see this discussion, would love to invite you to our exclusive builders chat here: https://t.me/+nUBuIdVYanZjZWZh\r\n\r\nWe have an AA specific channel where you can ask questions and get connected with others in the eco! \r\n\r\nIf you want to verify me, feel free to DM me on twitter @binji_x ", "2023-07-26T23:24:00Z", "2023-07-26T23:24:00Z", "binjix23", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5igqSZ", "I_kwDODjvEJM5skolw", "If the link is broken, you can use this one : https://t.me/+oUu1W1Z6WI03MTFh", "2023-07-27T00:33:01Z", "2023-07-27T00:33:01Z", "binjix23", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5j72cO", "I_kwDODjvEJM5skolw", "I was unable to make [eth-infinitism](https://github.com/eth-infinitism/bundler) bundler work, but found one that does work \u2013 [etherspot](https://github.com/etherspot/skandha). Incidentally, it also does better job estimating user operation gas.\r\n\r\n- Add `--rpc.allow-unprotected-txs` flag to both entrypoint files in `ops-bedrock` folder.\r\n- `make devnet-up`\r\n- Create `bundler-config.json` file anywhere you want with this content:\r\n```json\r\n{\r\n    \"networks\": {\r\n        \"dev\": {\r\n            \"entryPoints\": [\r\n                \"0x48e60BBb664aEfAc9f14aDB42e5FB5b4a119EB66\"\r\n            ],\r\n            \"relayer\": \"0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\",\r\n            \"beneficiary\": \"0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266\",\r\n            \"rpcEndpoint\": \"http://host.docker.internal:9545\",\r\n            \"estimationStaticBuffer\": 21000,\r\n            \"validationGasLimit\": 10e6,\r\n            \"receiptLookupRange\": 1024,\r\n            \"conditionalTransactions\": false\r\n        }\r\n    }\r\n}\r\n```\r\n`0x48e60BBb664aEfAc9f14aDB42e5FB5b4a119EB66` is the address at which we will deploy EntryPoint later. Bundler can be started before deploying it.\r\n`http://host.docker.internal:9545` \u2013 use this endpoint since both devnet and bundler run in Docker but on different networks.\r\n- Start bundler\r\n```bash\r\ndocker run --rm -d --mount type=bind,source=/absolute/path/to/bundler-config.json,target=/usr/app/config.json,readonly -p 3000:14337 etherspot/skandha:0.0.31 start --unsafeMode\r\n```\r\nSource path has to be absolute.\r\n- `git clone https://github.com/eth-infinitism/account-abstraction` somewhere and `cd` into it\r\n- `yarn && yarn install`\r\n- Need to make a couple of changes here:\r\nIn [hardhat.config.ts](https://github.com/eth-infinitism/account-abstraction/blob/develop/hardhat.config.ts) add a new network: `optimismLocal: getNetwork1('http://localhost:9545')`\r\nIn [1_deploy_entrypoint.ts](https://github.com/eth-infinitism/account-abstraction/blob/develop/deploy/1_deploy_entrypoint.ts) replace the lines:\r\n```ts\r\nconst from = await provider.getSigner().getAddress()\r\nawait new Create2Factory(ethers.provider).deployFactory()\r\n```\r\nwith\r\n```ts\r\nconst signer = provider.getSigner()\r\nconst from = await signer.getAddress()\r\nawait new Create2Factory(ethers.provider).deployFactory(signer)\r\n```\r\nAnd in [2_deploy_SimpleAccountFactory.ts](https://github.com/eth-infinitism/account-abstraction/blob/develop/deploy/2_deploy_SimpleAccountFactory.ts) where there is a line checking `chainId` add a condition for 901 (OP devnet chain id).\r\n- `yarn hardhat deploy --network optimismLocal`\r\nYou will most likely need to run this command 4-5 times, because for some reason first several attempts fail with various errors (not enough funds / unable to deterministically deploy / nonce already used / etc). I don't know why it doesn't work right away, so just keep running it until it works. Not the greatest method, I know. You can make a loop with try-catch in the code I suppose, instead.\r\n- The output will show the addresses of EntryPoint and SimpleAccountFactory contracts. Make sure that the EntryPoint address matches the one specified in bundler-config.json.\r\n- Fund EntryPoint address with eth. Not sure if this step is necessary, but this is what etherspot do in their tests.\r\n- Bundler will be available at `http://localhost:3000/1337`. Unfortunately the `/1337` path is hardcoded in the bundler code when you specify `dev` network in the config (like we did), so have to use it even though the network id is actually 901. It doesn't affect anything.\r\n- Gotcha 1: when calling `eth_estimateUserOperationGas` this bundler implementation returns the value for `verificationGasLimit` under the key `verificationGas` (without \"Limit\"), which differs from most other bundler implementations I've seen.\r\n- Gotcha 2: SimpleAccountFactory currently deployed on various test and mainnets deploys SimpleAccount that has `executeBatch` method which accepts only 2 arguments, while SimpleAccountFactory which we deployed locally has this method accepting 3 arguments, since it's newer implementation. This caused me a couple of hours of pulling my hair out trying to figure out why single operations work while batched ones do not.\r\n\r\nAs you can see this is not the cleanest method but it works. Would be great it someone figures out how to do it with less steps.", "2023-08-14T03:55:20Z", "2023-08-14T04:09:59Z", "rho-cassiopeiae", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5lrxW2", "I_kwDODjvEJM5skolw", "> I'm trying to run [this bundler](https://github.com/eth-infinitism/bundler) together with local devnet, but have no luck.\r\n> \r\n> I first add `--rpc.allow-unprotected-txs` flag to both entrypoint.sh files in ops-bedrock folder, since without it I get \"only replay-protected (EIP-155) transactions allowed over RPC\" error later. Then I launch the devnet using `make devnet-up` and make some adjustments in the bundler code: change all relevant target ports from 8545 to 9545 and in the 2-deploy-entrypoint.ts file comment out the code that checks that `chainId` is either 1337 or 31337, since it is actually 901.\r\n> \r\n> But when I run `yarn hardhat-deploy --network localhost` to deploy the entrypoint, I get \"Insufficient funds\" error.\r\n> \r\n> I think I need to change some private key-related settings of the devnet, but I'm not sure which ones. I tried changing `BLOCK_SIGNER_PRIVATE_KEY` and `BLOCK_SIGNER_ADDRESS` to `0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80` and `0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266` (`test*11 junk` mnemonic account 0), since the bundler seems to be using this account for deployment, but still got the same error.\r\n> \r\n> **Describe the solution you'd like** It would be great to have a local bundler out of the box, since there is already support for ERC-4337 on both OP Goerli and OP Mainnet.\r\n> \r\n> **Describe alternatives you've considered** Currently I have to run the bundler on hardhat L1. It works fine without any adjustments, but since I'm developing for Optimism it's not what I ultimately need.\r\n\r\nHi @rho-cassiopeiae this bundler run smoothly in my fresh Ubuntu 22.04, Make sure you start the devnet correctly. I recommend install geth by `make install-geth` in the optimism mono. \r\n\r\nSuccessful Launch: After following these steps, I successfully ran the bundler on port 3000 and made sure it was bound to port 9545.\r\n\r\n1. **Removed ChainID Validation:** To adapt to a specific network setting, I removed the ChainID validation located in the `./bundler/packages/bundler/deploy/2-deploy-entrypoint.ts`.\r\n\r\n2. **Port Adjustment:** I changed the port number to 9545 in both` ./bundler/packages/bundler/hardhat.config.ts` and `./bundler/packages/bundler/localconfig/bundler.config.json`.\r\n \r\n3. **Shell Script Update:** I revised the `./optimism/ops-bedrock/entrypoint-l2.sh` script to accommodate the [new configurations and environment variables](https://gist.github.com/joohhnnn/dd45f7ab3e1d006c063e705fb540e6ed). \r\n\r\n4. **Funds deploymentSignerAddress:** Using port 9545, I sent 1 ETH to the address 0x3fab184622dc19b6109349b94811493bf2a45362.\r\n\r\n**I also find the process pretty cumbersome. So, I'm planning to create a middleware that combines op-devnet and bundler, making the whole setup plug-and-play.**\r\n\r\nlet me know if you have more question.\r\n![image](https://github.com/ethereum-optimism/optimism/assets/68833933/fde7a973-e680-49bd-87d6-360d77995e3d)\r\n![image](https://github.com/ethereum-optimism/optimism/assets/68833933/80ff871f-11b1-4a8c-8df3-0e5c7c9505fb)\r\n\r\n", "2023-09-05T05:38:09Z", "2023-09-06T06:45:27Z", "joohhnnn", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5lylss", "I_kwDODjvEJM5skolw", "> I was unable to make [eth-infinitism](https://github.com/eth-infinitism/bundler) bundler work, but found one that does work \u2013 [etherspot](https://github.com/etherspot/skandha). Incidentally, it also does better job estimating user operation gas.\r\n> \r\n> * Add `--rpc.allow-unprotected-txs` flag to both entrypoint files in `ops-bedrock` folder.\r\n> * `make devnet-up`\r\n> * Create `bundler-config.json` file anywhere you want with this content:\r\n> \r\n> ```json\r\n> {\r\n>     \"networks\": {\r\n>         \"dev\": {\r\n>             \"entryPoints\": [\r\n>                 \"0x48e60BBb664aEfAc9f14aDB42e5FB5b4a119EB66\"\r\n>             ],\r\n>             \"relayer\": \"0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\",\r\n>             \"beneficiary\": \"0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266\",\r\n>             \"rpcEndpoint\": \"http://host.docker.internal:9545\",\r\n>             \"estimationStaticBuffer\": 21000,\r\n>             \"validationGasLimit\": 10e6,\r\n>             \"receiptLookupRange\": 1024,\r\n>             \"conditionalTransactions\": false\r\n>         }\r\n>     }\r\n> }\r\n> ```\r\n> \r\n> `0x48e60BBb664aEfAc9f14aDB42e5FB5b4a119EB66` is the address at which we will deploy EntryPoint later. Bundler can be started before deploying it. `http://host.docker.internal:9545` \u2013 use this endpoint since both devnet and bundler run in Docker but on different networks.\r\n> \r\n> * Start bundler\r\n> \r\n> ```shell\r\n> docker run --rm -d --mount type=bind,source=/absolute/path/to/bundler-config.json,target=/usr/app/config.json,readonly -p 3000:14337 etherspot/skandha:0.0.31 start --unsafeMode\r\n> ```\r\n> \r\n> Source path has to be absolute.\r\n> \r\n> * `git clone https://github.com/eth-infinitism/account-abstraction` somewhere and `cd` into it\r\n> * `yarn && yarn install`\r\n> * Need to make a couple of changes here:\r\n>   In [hardhat.config.ts](https://github.com/eth-infinitism/account-abstraction/blob/develop/hardhat.config.ts) add a new network: `optimismLocal: getNetwork1('http://localhost:9545')`\r\n>   In [1_deploy_entrypoint.ts](https://github.com/eth-infinitism/account-abstraction/blob/develop/deploy/1_deploy_entrypoint.ts) replace the lines:\r\n> \r\n> ```ts\r\n> const from = await provider.getSigner().getAddress()\r\n> await new Create2Factory(ethers.provider).deployFactory()\r\n> ```\r\n> \r\n> with\r\n> \r\n> ```ts\r\n> const signer = provider.getSigner()\r\n> const from = await signer.getAddress()\r\n> await new Create2Factory(ethers.provider).deployFactory(signer)\r\n> ```\r\n> \r\n> And in [2_deploy_SimpleAccountFactory.ts](https://github.com/eth-infinitism/account-abstraction/blob/develop/deploy/2_deploy_SimpleAccountFactory.ts) where there is a line checking `chainId` add a condition for 901 (OP devnet chain id).\r\n> \r\n> * `yarn hardhat deploy --network optimismLocal`\r\n>   You will most likely need to run this command 4-5 times, because for some reason first several attempts fail with various errors (not enough funds / unable to deterministically deploy / nonce already used / etc). I don't know why it doesn't work right away, so just keep running it until it works. Not the greatest method, I know. You can make a loop with try-catch in the code I suppose, instead.\r\n> * The output will show the addresses of EntryPoint and SimpleAccountFactory contracts. Make sure that the EntryPoint address matches the one specified in bundler-config.json.\r\n> * Fund EntryPoint address with eth. Not sure if this step is necessary, but this is what etherspot do in their tests.\r\n> * Bundler will be available at `http://localhost:3000/1337`. Unfortunately the `/1337` path is hardcoded in the bundler code when you specify `dev` network in the config (like we did), so have to use it even though the network id is actually 901. It doesn't affect anything.\r\n> * Gotcha 1: when calling `eth_estimateUserOperationGas` this bundler implementation returns the value for `verificationGasLimit` under the key `verificationGas` (without \"Limit\"), which differs from most other bundler implementations I've seen.\r\n> * Gotcha 2: SimpleAccountFactory currently deployed on various test and mainnets deploys SimpleAccount that has `executeBatch` method which accepts only 2 arguments, while SimpleAccountFactory which we deployed locally has this method accepting 3 arguments, since it's newer implementation. This caused me a couple of hours of pulling my hair out trying to figure out why single operations work while batched ones do not.\r\n> \r\n> As you can see this is not the cleanest method but it works. Would be great it someone figures out how to do it with less steps.\r\n\r\nI suspect the reason for having to run it multiple times might be insufficient funding. All in all, I'm working on a middleware to make the integration between the two more seamless.", "2023-09-06T06:41:43Z", "2023-09-06T06:41:43Z", "joohhnnn", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5l6Zu5", "I_kwDODjvEJM5skolw", "https://github.com/joohhnnn/OPBundlerAutoConfig @rho-cassiopeiae  Hope this repo can help you", "2023-09-07T09:28:43Z", "2023-09-07T09:54:13Z", "joohhnnn", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5l6m6S", "I_kwDODjvEJM5skolw", "1. why do you need the SimpleAccountFactory ? this is a sample, reference account, not intended for production use.\r\n  If you are a bundler, you don't care about it. it is only used in tests. If you have an account service, then deploy your own factory.\r\n2. You are using \"unofficially compiled\" entrypoint: the standard, audited entrypoint is deployed at address `0x5ff137d4b0fdcd49dca30c7cf57e578a026d2789`. The address reflects the code (and also compiler settings), so your address obviously reference another one.\r\n3. I doin't really understand your bundler setup failures. If you look at the [Test harness execution framework](https://github.com/eth-infinitism/bundler-test-executor/tree/master/launchers), there are docker configuration to set up and launch each bundler.", "2023-09-07T09:56:18Z", "2023-09-07T09:56:18Z", "drortirosh", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnCU7", "I_kwDODjvEJM5skolw", "The entrypoints are not preinstalls on OP Stack chains", "2024-06-17T21:52:36Z", "2024-06-17T21:52:36Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5hqiE1", "I_kwDODjvEJM5rlxbw", "Are there any workarounds for this?", "2023-07-17T17:07:38Z", "2023-07-17T17:07:38Z", "3esmit", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ifxjz", "I_kwDODjvEJM5rlxbw", "Thanks for being interested in optimism. Why do you need the sdk to deploy the token?  There is a new typescript package [here](https://github.com/ethereum-optimism/optimism/tree/develop/packages/contracts-ts) that may be useful. If not, what functionality do you need exactly?", "2023-07-26T21:07:41Z", "2023-07-26T21:07:41Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ikWln", "I_kwDODjvEJM5rlxbw", "I need to deploy the token because I've written a custom token for L2, I need some extra functions and storage on my L2 Token that are not available on the default optimism L2 tokens. \r\nTherefore, I am using hardhat as development framework for building and deploying the contracts, and I would like to use also the optimism-sdk for interacting with the bridge. ", "2023-07-27T14:06:13Z", "2023-07-27T14:06:13Z", "3esmit", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5ik8WM", "I_kwDODjvEJM5rlxbw", "I am guessing you would like to test deposits and withdrawals end to end? I don't think the `contracts-ts` package will give as nice of UX as the sdk for doing withdrawals.\r\n", "2023-07-27T15:27:52Z", "2023-07-27T15:27:52Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5imsZV", "I_kwDODjvEJM5rlxbw", "Yes, I would like to test that. I want to do exactly the bridge tutorial, using standard bridge, but with a custom token. And I would like to use hardhat as is a popular framework which I am used to work with.", "2023-07-27T19:11:13Z", "2023-07-27T19:11:13Z", "3esmit", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5im9Mz", "I_kwDODjvEJM5rlxbw", "This issue is caused by dependencies of sdk:\r\n[@eth-optimism/contracts](https://libraries.io/NPM/@eth-optimism%2Fcontracts)\r\n\r\n[@eth-optimism/core-utils](https://libraries.io/NPM/@eth-optimism%2Fcore-utils)\r\n\r\nThese packages should be updated to use ethers^6. ", "2023-07-27T19:20:23Z", "2023-07-27T19:20:23Z", "3esmit", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5inFQ7", "I_kwDODjvEJM5rlxbw", "I started work here but no guarantee this is complete or fully works: https://github.com/ethereum-optimism/optimism/tree/feat/sdk-ethersv6", "2023-07-27T19:30:19Z", "2023-07-27T19:30:19Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5inG-i", "I_kwDODjvEJM5rlxbw", "> This issue is caused by dependencies of sdk: [@eth-optimism/contracts](https://libraries.io/NPM/@eth-optimism%2Fcontracts)\r\n> \r\n> [@eth-optimism/core-utils](https://libraries.io/NPM/@eth-optimism%2Fcore-utils)\r\n> \r\n> These packages should be updated to use ethers^6.\r\n\r\n@eth-optimism/contracts is deprecated and we will want to remove as a dependency. Updating @eth-optimism/core-utils shouldn't be a ton of work to ethers v6\r\n\r\nTo be honest this isn't super high priority for the team, i did a little extra work attempting to port the sdk but I need to focus on my decentralization work. Happy to review any PRs if you would like to help", "2023-07-27T19:33:23Z", "2023-07-27T19:33:23Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5inX5D", "I_kwDODjvEJM5rlxbw", "cc @roninjin10 @annieke I know y'all are busy, but I wanted to put this on your radar", "2023-07-27T20:10:01Z", "2023-07-27T20:10:01Z", "sbvegan", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5iob0X", "I_kwDODjvEJM5rlxbw", "@3esmit thank you for bringing this up! We're in the process of changing how we handle JS packages; sorry about the snag. The engineer leading that effort is off; we'll give you an update when he's back in a little over a week \ud83d\ude4f ", "2023-07-27T23:45:11Z", "2023-07-27T23:45:11Z", "annieke", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5iqKQi", "I_kwDODjvEJM5rlxbw", "@3esmit I can help with this soon. Can you provide more information so I can help more?\r\n- Link to repo you are looking to use sdk and hardhat in?\r\n- Which package manager are you using? Pnpm? Yarn? Npm?", "2023-07-28T07:53:32Z", "2023-07-28T07:53:32Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5isT0M", "I_kwDODjvEJM5rlxbw", "I am writing the code here https://github.com/logos-co/optimism-bridge-snt\r\n\r\nI normally use npm, npm breaks when a project contains hardhat and optimism-sdk as dependencies. \r\nI also tried yarn - it installs but the project fails to run. ", "2023-07-28T14:25:01Z", "2023-07-28T14:26:59Z", "3esmit", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5isbXC", "I_kwDODjvEJM5rlxbw", "In that repository, trying `npm install @nomicfoundation/hardhat-toolbox` gives `npm ERR! Conflicting peer dependency: ethers@6.6.6`\r\n\r\n```\r\nroot@SURFACE-BOOK:/home/ricardo/optimism-bridge-snt# npm install @nomicfoundation/hardhat-toolbox\r\nnpm ERR! code ERESOLVE\r\nnpm ERR! ERESOLVE could not resolve\r\nnpm ERR! \r\nnpm ERR! While resolving: optimism-bridge-snt@1.0.0\r\nnpm ERR! Found: ethers@5.7.2\r\nnpm ERR! node_modules/ethers\r\nnpm ERR!   peer ethers@\"^5\" from @eth-optimism/contracts@0.6.0\r\nnpm ERR!   node_modules/@eth-optimism/contracts\r\nnpm ERR!     @eth-optimism/contracts@\"0.6.0\" from @eth-optimism/sdk@3.0.0\r\nnpm ERR!     node_modules/@eth-optimism/sdk\r\nnpm ERR!       dev @eth-optimism/sdk@\"^3.0.0\" from the root project\r\nnpm ERR!   ethers@\"^5.7.0\" from @eth-optimism/contracts-bedrock@0.15.0\r\nnpm ERR!   node_modules/@eth-optimism/contracts-bedrock\r\nnpm ERR!     @eth-optimism/contracts-bedrock@\"0.15.0\" from @eth-optimism/sdk@3.0.0\r\nnpm ERR!     node_modules/@eth-optimism/sdk\r\nnpm ERR!       dev @eth-optimism/sdk@\"^3.0.0\" from the root project\r\nnpm ERR!   3 more (@eth-optimism/sdk, ...)\r\nnpm ERR! \r\nnpm ERR! Could not resolve dependency:\r\nnpm ERR! @nomicfoundation/hardhat-toolbox@\"*\" from the root project\r\nnpm ERR! \r\nnpm ERR! Conflicting peer dependency: ethers@6.6.6\r\nnpm ERR! node_modules/ethers\r\nnpm ERR!   peer ethers@\"^6.4.0\" from @nomicfoundation/hardhat-toolbox@3.0.0\r\nnpm ERR!   node_modules/@nomicfoundation/hardhat-toolbox\r\nnpm ERR!     @nomicfoundation/hardhat-toolbox@\"*\" from the root project\r\nnpm ERR! \r\nnpm ERR! Fix the upstream dependency conflict, or retry\r\nnpm ERR! this command with --force or --legacy-peer-deps\r\nnpm ERR! to accept an incorrect (and potentially broken) dependency resolution.\r\nnpm ERR! \r\nnpm ERR! \r\nnpm ERR! For a full report see:\r\nnpm ERR! /root/.npm/_logs/2023-07-28T14_43_36_463Z-eresolve-report.txt\r\n\r\nnpm ERR! A complete log of this run can be found in: /root/.npm/_logs/2023-07-28T14_43_36_463Z-debug-0.log\r\n```", "2023-07-28T14:46:03Z", "2023-07-28T14:46:03Z", "3esmit", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5itAij", "I_kwDODjvEJM5rlxbw", "Got it @3esmit. Here is how you can work around node resolution issues. This is easier to do with yarn than npm so I will demo with yarn if you are ok with using yarn.\r\n\r\n1. Add resolutions to your package.json\r\n[Resolutions](https://classic.yarnpkg.com/lang/en/docs/selective-version-resolutions/) will tell yarn to install a different version of ethers for the optimism sdk\r\n```json package.json\r\n{\r\n  \"resolutions\": {\r\n    \"@eth-optimism/sdk/**/ethers\": \"^5.7.0\"\r\n  }\r\n}\r\n```\r\n2. install ethers v5\r\nyou may find that interacting with the optimism sdk with ethers v6 works fine but I suspect you may need to pass in V5 providers to optimism. We can install ethers v5 under the namespace `ethersv5`\r\n\r\n```bash\r\nyarn add ethers@latest ethers5@npm:ethers@5 @nomicfoundation/hardhat-toolbox\r\n```\r\nEthers v6 can be imported as normal\r\n```typescript\r\nimport * as ethers from 'ethers'\r\n```\r\nAnd then ethers v5 can be imported using `ethers5`\r\n```typescript\r\nimport * as ethers from 'ethers5`\r\n\r\nI suspect you will have to initialize the optimism sdk via an ethers v5 provider but YMMV", "2023-07-28T16:24:58Z", "2023-07-28T16:44:26Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5itBTM", "I_kwDODjvEJM5rlxbw", "Thanks again for sharing the extra details and the link to repo. I tested the above instructions on your repo and the npm resolutions looked good. I haven't tested running hardhat so report back if you still run into any issues.", "2023-07-28T16:27:02Z", "2023-07-28T16:27:02Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5itCrx", "I_kwDODjvEJM5rlxbw", "For those reading this in future, we currently have no plans to update the sdk to v6 as this is a major breaking change and we don't want the SDK to be tightly coupled to a specific library like this. This library causes issues for viem users as well.\r\n\r\nWe instead we will be looking into building this functionality into libraries as extensions.  E.g. [viem's decorators](https://viem.sh/docs/clients/custom.html#build-your-own-client) or [web3.js plugins](https://docs.web3js.org/guides/web3_plugin_guide/) or ethers.js plugins.", "2023-07-28T16:31:47Z", "2023-07-28T16:31:47Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5i1zJI", "I_kwDODjvEJM5rlxbw", "This solution does not work for me. I tried that and it does not solve the issue. \r\n\r\n```\r\nroot@SURFACE-BOOK:/home/ricardo/optimism-bridge-snt# yarn install\r\nyarn install v1.22.19\r\n[1/4] Resolving packages...\r\n[2/4] Fetching packages...\r\n[3/4] Linking dependencies...\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@nomicfoundation/hardhat-network-helpers@^1.0.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@nomicfoundation/hardhat-chai-matchers@^2.0.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@nomicfoundation/hardhat-ethers@^3.0.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@nomicfoundation/hardhat-verify@^1.0.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@types/chai@^4.2.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@types/mocha@>=9.1.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@types/node@>=12.0.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@typechain/ethers-v6@^0.4.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"@typechain/hardhat@^8.0.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"chai@^4.2.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"hardhat-gas-reporter@^1.0.8\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"solidity-coverage@^0.8.1\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"ts-node@>=8.0.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"typechain@^8.2.0\".\r\nwarning \" > @nomicfoundation/hardhat-toolbox@3.0.0\" has unmet peer dependency \"typescript@>=4.5.0\".\r\n[4/4] Building fresh packages...\r\nDone in 7.43s.\r\n``` ", "2023-07-31T12:24:15Z", "2023-07-31T12:24:15Z", "3esmit", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5i1zYC", "I_kwDODjvEJM5rlxbw", "> we don't want the SDK to be tightly coupled to a specific library like this\r\n\r\nCurrently is coupled to ethers v5? ", "2023-07-31T12:24:55Z", "2023-07-31T12:24:55Z", "3esmit", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jajmq", "I_kwDODjvEJM5rlxbw", "> > we don't want the SDK to be tightly coupled to a specific library like this\r\n> \r\n> Currently is coupled to ethers v5?\r\n\r\nCorrect this is something currently causing pain and we are actively moving away from. Some OP stack users are using viem. Some ethersv6. Most ethers v5. \r\n\r\nOur strategy is to keep the sdk for ethersv5 users but work towards getting all functionality into the core libraries via extensions and decorators and I am chatting with all 3 major libraries to drive this effort. After this is done the sdk won't be necessary as these libraries will support OP functionality batteries included. Since our time is limited this unfortunately means neglecting old patterns like the sdk for the time being. \r\n- Ethers v6 has just started adding their version of this via their ethersv6 plugin pattern\r\n- @spacesailor24 is driving web3.js support\r\n- I'm working on adding our viem decorators\r\n\r\nAs for your issue, the error messages look related to hardhat and unrelated to this issue? I am no longer seeing ethersv5 as a missing peer dependency but instead other peer deps. [Here is the complete list](https://github.com/NomicFoundation/hardhat/blob/main/packages/hardhat-toolbox/package.json#L69) of peer deps this package depends on", "2023-08-07T13:55:15Z", "2023-08-07T13:55:15Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM54c9Bi", "I_kwDODjvEJM5rlxbw", "hello it seems the yarn `resolutions` approach mentioned @roninjin10 does not work as `ethers` is declared as a `peerDependencies` so there is no clear way to resolve that with yarn to my knowledge.\r\n\r\nAny idea of another possible workaround? \r\n\r\nWill love to integrate the sdk to our tooling as I am planning to bridge a token on several optimistic chains", "2024-03-26T16:11:28Z", "2024-03-26T16:11:52Z", "clemsos", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnCuD", "I_kwDODjvEJM5rlxbw", "The sdk has been deprecated", "2024-06-17T21:53:37Z", "2024-06-17T21:53:37Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6Bv6bp", "I_kwDODjvEJM5rlxbw", "> The sdk has been deprecated\n\nMay I ask what replaced it ? @tynes ", "2024-06-18T19:35:18Z", "2024-06-18T19:35:18Z", "clemsos", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BxQ9Y", "I_kwDODjvEJM5rlxbw", "viem has OP Stack functionality, I recommend using viem", "2024-06-18T22:15:30Z", "2024-06-18T22:15:30Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5fkJ-M", "I_kwDODjvEJM5pgdOe", "Notice the first response has some additional fields, those are the legacy fields. `op-geth` will forward requests to `l2geth` (legacy) for some requests. Note that the block data exists in both. It looks like `op-geth` is forwarding the request for block by number but not for block by hash.", "2023-06-22T21:04:26Z", "2023-06-22T21:04:26Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5fk53j", "I_kwDODjvEJM5pgdOe", "@tynes seems like a bug to me", "2023-06-23T01:30:59Z", "2023-06-23T01:30:59Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5fnuCr", "I_kwDODjvEJM5pgdOe", "hey @smartcontracts what should be the expected behavior/result? \ud83d\ude04 ", "2023-06-23T12:57:47Z", "2023-06-23T12:57:47Z", "yaanakbr", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5f1gqD", "I_kwDODjvEJM5pgdOe", "Hey @smartcontracts @tynes wanted to bump my message one more time \ud83d\ude04 Thank you in advance! ", "2023-06-26T16:55:53Z", "2023-06-26T16:55:53Z", "yaanakbr", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5hcCsB", "I_kwDODjvEJM5pgdOe", "Hey @smartcontracts @tynes just wanted to bump my message one more time. What should be the expected behavior? Can we confirm if this is a bug and it'll be patched? \r\n\r\n", "2023-07-13T18:56:56Z", "2023-07-13T18:56:56Z", "yaanakbr", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5hjlgw", "I_kwDODjvEJM5pgdOe", "Hey there! \ud83d\udc4b Thank you for bringing up an interesting issue regarding the responses from `eth_getBlockByNumber` and `eth_getBlockByHash` requests for block 46772657. It seems there is a discrepancy in the presence of the `chainId` field in the transaction object.\r\n\r\nIn the `eth_getBlockByNumber` response, the `chainId` field is absent in the transaction object, while in the `eth_getBlockByHash` response, the `chainId` field is included.\r\n\r\nThis difference could be attributed to various factors, including the implementation details of the Ethereum client or node you are using. In your case, the RPC backend version is Geth/v0.1.0-unstable-0a77db9c/linux-amd64/go1.20.1.\r\n\r\nTo get a deeper understanding of this variation, it would be helpful to refer to the documentation or support resources specific to your Ethereum client version. The developers or community members associated with that client can provide insights into the specific design choices and reasons behind the observed difference.\r\n\r\nFeel free to explore the official documentation, forums, or other relevant channels related to your Ethereum client for further clarification on this matter.\r\n\r\nIf you have any more questions or need assistance with anything else, feel free to let us know. We're here to help! \ud83d\ude0a", "2023-07-15T09:21:40Z", "2023-07-15T09:21:40Z", "graphicmindset123", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnDxX", "I_kwDODjvEJM5pgdOe", "This has to do with `l2geth` and `op-geth` having slight differences", "2024-06-17T21:56:20Z", "2024-06-17T21:56:20Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5fck6N", "I_kwDODjvEJM5pTO4u", "Note: This is a problem for Ankr RPC's specifically, works as intended on Alchemy RPC's.", "2023-06-21T17:59:17Z", "2023-06-21T17:59:17Z", "ratankaliani", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5frfpT", "I_kwDODjvEJM5pTO4u", "It would be nice to have a generic chunker and then allow the user to specify the start and the end with sane defaults", "2023-06-24T02:23:13Z", "2023-06-24T02:23:13Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5yzRd1", "I_kwDODjvEJM5pTO4u", "Yeah this needs to be improved in the SDK generally, unfortunately. ", "2024-02-05T01:10:25Z", "2024-02-05T01:10:25Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnCl9", "I_kwDODjvEJM5pTO4u", "The sdk has been deprecated", "2024-06-17T21:53:15Z", "2024-06-17T21:53:15Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5e5r_r", "I_kwDODjvEJM5oteb0", "Seems valid, we probably need to get this txn type supported by upstream blockscout", "2023-06-15T01:05:15Z", "2023-06-15T01:05:15Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnF-k", "I_kwDODjvEJM5oteb0", "This issue seems stale, closing it now. Blockscout should work fine with OP-Stack chains. The upstream blockscout docs include instructions.", "2024-06-17T22:02:49Z", "2024-06-17T22:02:49Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5e5r4X", "I_kwDODjvEJM5orK8M", "Free tier alchemy can successfully sync `op-geth` if you specify `--l1.rpckind=alchemy`, the L2 URL that you need for `op-node` is just the `op-geth` authenticated API usually located on port 8551", "2023-06-15T01:04:32Z", "2023-06-15T01:04:32Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5fAwHB", "I_kwDODjvEJM5orK8M", "Can you be more specific about the L2 URL? I don't quite follow your meaning.", "2023-06-16T03:21:39Z", "2023-06-16T03:21:39Z", "VetchM", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6Bl8kD", "I_kwDODjvEJM5orK8M", "Docs: https://docs.optimism.io/builders/node-operators/rollup-node  ", "2024-06-17T18:55:14Z", "2024-06-17T18:55:14Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5bUcqJ", "I_kwDODjvEJM5k4oBc", "It looks like you are interacting with the implementation and not the proxy. The address of the proxy can be found here:\r\nhttps://goerli.etherscan.io/address/0x8dd330dde8d9898d43b4dc840da27a07df91b3c9\r\n\r\nThis value was found by looking at the `OTHER_BRIDGE()(address)` method on the L2ERC721Bridge", "2023-05-02T20:13:51Z", "2023-05-02T20:13:51Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5bUhFc", "I_kwDODjvEJM5k4oBc", "> It looks like you are interacting with the implementation and not the proxy. The address of the proxy can be found here: https://goerli.etherscan.io/address/0x8dd330dde8d9898d43b4dc840da27a07df91b3c9\r\n> \r\n> This value was found by looking at the `OTHER_BRIDGE()(address)` method on the L2ERC721Bridge\r\n\r\nThanks for idea, I just did a try (https://goerli-optimism.etherscan.io/tx/0xbd4c1fd1c7e2dcb3860c89b2b430f72d4e388b632f84a451a56fbeefe4614a71 - relay tx, l1 bridge tx - 0x63f299655624b331b78b4105c127d6ae9db098439154159002935d0de62fee0a ). But got reverted relay", "2023-05-02T20:27:38Z", "2023-05-02T20:27:38Z", "eskrano", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5bki3j", "I_kwDODjvEJM5k4oBc", "Still need help. Thanks.", "2023-05-05T14:00:27Z", "2023-05-05T14:00:27Z", "eskrano", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnD4i", "I_kwDODjvEJM5k4oBc", "This is better to ask in https://github.com/ethereum-optimism/developers/discussions", "2024-06-17T21:56:41Z", "2024-06-17T21:56:41Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5VD-1a", "I_kwDODjvEJM5eQSOX", "@smartcontracts and I had ran into this before but haven't had the bandwith yet to fix the issue.   The workaround we did was to use a JSONRPC provider for only withdrawals.\r\n\r\nfwiw it's bad design that ethers.js have these niche incompatibilities between their providers.  All providers should inherit from JsonRpcProviders.  I'm hoping they fixed this in v6.", "2023-02-12T18:59:28Z", "2023-02-12T18:59:28Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM51-_Yp", "I_kwDODjvEJM5eQSOX", "I think this is realistically a \"won't fix\" for ethers v5. I don't see a good way to handle this because we need to make JSON RPC calls to specific RPC methods that ethers v5 providers don't expose. We simply can't make those queries without being able to call the JSON RPC directly.\r\n\r\nGoing to keep this open mostly because I hope to see an ethers v6 version of everything that does work as you would expect.", "2024-03-05T19:01:13Z", "2024-03-05T19:01:13Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6BnCeI", "I_kwDODjvEJM5eQSOX", "The sdk has been deprecated", "2024-06-17T21:52:59Z", "2024-06-17T21:52:59Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5JRJUF", "I_kwDODjvEJM5Qpp7S", "OP Labs is running a mandatory week off this week for some R&R before Bedrock, so apologies for the inevitably slow response here until next week. Agree this is an issue and should be very easily resolvable by cutting out a majority of the deployment files. Thanks for bringing this up, I'll make sure this is fixed as soon as we get back.", "2022-08-27T17:23:20Z", "2022-08-27T17:23:20Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5JRJXc", "I_kwDODjvEJM5Qpp7S", "ok, thanks for the response. you'll be back this coming Monday, or not till the week after? please keep me posted; thanks.", "2022-08-27T17:24:41Z", "2022-08-27T17:24:41Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5JRJtX", "I_kwDODjvEJM5Qpp7S", "People will be back on Sept 5th. I'll make sure to ping you here as soon as a fix is released. Thanks again for reporting this!", "2022-08-27T17:33:43Z", "2022-08-27T17:33:43Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5JRLk9", "I_kwDODjvEJM5Qpp7S", "Alright I was able to significantly reduce the bundle size by not exporting data that seemed relatively unnecessary.\r\n\r\nOriginal bundle size:\r\n\r\n```\r\nnpm notice === Tarball Details === \r\nnpm notice name:          @eth-optimism/contracts                 \r\nnpm notice version:       0.5.34                                  \r\nnpm notice package size:  3.0 MB                                  \r\nnpm notice unpacked size: 15.3 MB                                 \r\nnpm notice shasum:        d704ca30cfae3eeb468b6b8ae3dad3c46f283f4b\r\nnpm notice integrity:     sha512-8VPNS0yGA6SrY[...]j3ySbTJluxnew==\r\nnpm notice total files:   503                                     \r\nnpm notice \r\n```\r\n\r\nFixed bundle size:\r\n\r\n```\r\nnpm notice === Tarball Details === \r\nnpm notice name:          @eth-optimism/contracts                 \r\nnpm notice version:       0.5.34                                  \r\nnpm notice package size:  269.7 kB                                \r\nnpm notice unpacked size: 2.0 MB                                  \r\nnpm notice shasum:        c1e5fedb812b09c4e4ffc7760002be5b9619bd9f\r\nnpm notice integrity:     sha512-EX4zJaKEQtwo9[...]3WADWMpZGVb5A==\r\nnpm notice total files:   366                                     \r\nnpm notice \r\n```\r\n\r\nI will make a PR when people get back and are able to review.", "2022-08-27T18:21:24Z", "2022-08-27T18:23:15Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5JRLon", "I_kwDODjvEJM5Qpp7S", "I was trying to cut things down even more but about half of the bundle is just typechain, which is unfortunate. ", "2022-08-27T18:22:54Z", "2022-08-27T18:22:54Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5JRoN3", "I_kwDODjvEJM5Qpp7S", "i see, gotcha. cool! this should be plenty. follow up here as soon as the PR is ready and/or published and released on npm, if you don't mind. thanks again!", "2022-08-28T02:13:51Z", "2022-08-28T02:13:51Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5J0VE0", "I_kwDODjvEJM5Qpp7S", "0.5.34 should resolve the issue once released. We will probably do a release today or tomorrow.", "2022-09-06T17:29:26Z", "2022-09-06T17:29:26Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5J0VKI", "I_kwDODjvEJM5Qpp7S", "do you mind alerting here once this actually goes live on npm? thanks. **edit** just saw the above, nice\u2014would be helpful if you could post here when that's done.", "2022-09-06T17:29:50Z", "2022-09-06T17:30:31Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5J0aru", "I_kwDODjvEJM5Qpp7S", "Will do.", "2022-09-06T17:54:27Z", "2022-09-06T17:54:27Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5KIdzH", "I_kwDODjvEJM5Qpp7S", "@firnprotocol sorry for the delay here. 0.5.34 has been published: https://www.npmjs.com/package/@eth-optimism/contracts", "2022-09-12T13:20:32Z", "2022-09-12T13:20:32Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5KInOc", "I_kwDODjvEJM5Qpp7S", "note that i am now receiving warnings during webpack compilation:\r\n```\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 266:22-109\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-helpers/FailingReceiver.sol/FailingReceiver.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 271:16-91\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-helpers/TestERC20.sol/TestERC20.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 276:31-145\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/bridge/TestLib_CrossDomainUtils.sol/TestLib_CrossDomainUtils.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 281:23-120\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/codec/TestLib_OVMCodec.sol/TestLib_OVMCodec.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 286:24-121\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/rlp/TestLib_RLPReader.sol/TestLib_RLPReader.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 291:24-121\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/rlp/TestLib_RLPWriter.sol/TestLib_RLPWriter.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 296:33-154\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/standards/TestLib_AddressAliasHelper.sol/TestLib_AddressAliasHelper.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 301:25-125\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/trie/TestLib_MerkleTrie.sol/TestLib_MerkleTrie.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 306:31-143\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/trie/TestLib_SecureMerkleTrie.sol/TestLib_SecureMerkleTrie.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 311:21-114\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/utils/TestLib_Buffer.sol/TestLib_Buffer.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 316:27-132\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/utils/TestLib_Bytes32Utils.sol/TestLib_Bytes32Utils.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 321:25-126\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/utils/TestLib_BytesUtils.sol/TestLib_BytesUtils.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n\r\nWARNING in ./node_modules/@eth-optimism/contracts/dist/contract-artifacts.js 326:25-126\r\nModule not found: Error: Can't resolve '../artifacts/contracts/test-libraries/utils/TestLib_MerkleTree.sol/TestLib_MerkleTree.json' in '/Users/benediamond/firn/node_modules/@eth-optimism/contracts/dist'\r\n @ ./node_modules/@eth-optimism/contracts/dist/contract-defs.js 6:36-67\r\n @ ./node_modules/@eth-optimism/contracts/dist/index.js 17:13-39\r\n @ ./node_modules/@eth-optimism/sdk/dist/cross-chain-messenger.js 30:20-54\r\n @ ./node_modules/@eth-optimism/sdk/dist/index.js 19:13-47\r\n @ ./src/ui/MainPanel/index.js 19:0-64 32:19-31 157:34-47\r\n @ ./src/App.js 10:0-41 51:51-60\r\n @ ./src/index.js 6:0-28 14:38-41\r\n```\r\nappears to work though, despite the warnings...\r\n\r\nlet me know if you're able to reproduce?", "2022-09-12T13:48:36Z", "2022-09-12T13:48:36Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5KIuKP", "I_kwDODjvEJM5Qpp7S", "btw, even after this upgrade, optimism is still quite large (~1.3 MB). for perspective, it is _over twice as large as `ethers.js`_ (?!), and over twice as large as the next-largest package. i really don't see why this should be. moreover, it is still larger than it was in version 0.5.29. one way this can happen is if you git track \"detailed\" compiler output (including abstract syntax tree). i wonder if you all are doing this?\r\n\r\nhere is my entire `node_modules` folder, in the bundle analyzer:\r\n<img width=\"1356\" alt=\"Screen Shot 2022-09-12 at 10 05 06 AM\" src=\"https://user-images.githubusercontent.com/93839494/189675006-533d7a0f-15fd-40f7-a441-0b8bb5aa2a8a.png\">\r\nnote `ethers` in the bottom middle.\r\n\r\nand the relevant portion of `yarn.lock`:\r\n```\r\n\"@eth-optimism/contracts-bedrock@0.6.0\":\r\n  version \"0.6.0\"\r\n  resolved \"https://registry.yarnpkg.com/@eth-optimism/contracts-bedrock/-/contracts-bedrock-0.6.0.tgz#037becf54be35ed65f17ebdabac0b8c7472dae59\"\r\n  integrity sha512-I9uaq77NeQX0e/lVe9s+4N3vUQXLGNmfmBdstAh1I24Q0n9nMC+L0c5TOO/rpG/Tqog9XmBn7tghqw/ussLGhQ==\r\n  dependencies:\r\n    \"@eth-optimism/core-utils\" \"^0.10.0\"\r\n    \"@openzeppelin/contracts\" \"4.7.3\"\r\n    \"@openzeppelin/contracts-upgradeable\" \"4.7.3\"\r\n    ethers \"^5.6.8\"\r\n    hardhat \"^2.9.6\"\r\n\r\n\"@eth-optimism/contracts@0.5.34\":\r\n  version \"0.5.34\"\r\n  resolved \"https://registry.yarnpkg.com/@eth-optimism/contracts/-/contracts-0.5.34.tgz#36d904a2c6809cbd49309534936d714506edd55a\"\r\n  integrity sha512-TgMvzevw14bkPbrVp3XqSrEI0SEyyzs69rQiurHpeY72tmN+8XBcTOF00tnfjqQA6MVPQvoQf1hQLnEPzmRpdQ==\r\n  dependencies:\r\n    \"@eth-optimism/core-utils\" \"0.10.0\"\r\n    \"@ethersproject/abstract-provider\" \"^5.6.1\"\r\n    \"@ethersproject/abstract-signer\" \"^5.6.2\"\r\n\r\n\"@eth-optimism/core-utils@0.10.0\", \"@eth-optimism/core-utils@^0.10.0\":\r\n  version \"0.10.0\"\r\n  resolved \"https://registry.yarnpkg.com/@eth-optimism/core-utils/-/core-utils-0.10.0.tgz#512872fedd4fc3b0f244bd45a73a92eed7c08aa2\"\r\n  integrity sha512-2npqQcmqWI1QqdK5jvsgplZ2bv2yrySUgDCIT8lSyiowhsadTN2oaXa0Ja4jPsn6XN9Mv87WMGz3RwwwEDZu0w==\r\n  dependencies:\r\n    \"@ethersproject/abi\" \"^5.6.3\"\r\n    \"@ethersproject/abstract-provider\" \"^5.6.1\"\r\n    \"@ethersproject/address\" \"^5.6.1\"\r\n    \"@ethersproject/bignumber\" \"^5.6.1\"\r\n    \"@ethersproject/bytes\" \"^5.6.1\"\r\n    \"@ethersproject/constants\" \"^5.6.1\"\r\n    \"@ethersproject/contracts\" \"^5.6.2\"\r\n    \"@ethersproject/hash\" \"^5.6.1\"\r\n    \"@ethersproject/keccak256\" \"^5.6.1\"\r\n    \"@ethersproject/properties\" \"^5.6.0\"\r\n    \"@ethersproject/providers\" \"^5.6.8\"\r\n    \"@ethersproject/rlp\" \"^5.6.1\"\r\n    \"@ethersproject/transactions\" \"^5.6.2\"\r\n    \"@ethersproject/web\" \"^5.6.1\"\r\n    bufio \"^1.0.7\"\r\n    chai \"^4.3.4\"\r\n\r\n\"@eth-optimism/sdk@^1.2.0\":\r\n  version \"1.6.1\"\r\n  resolved \"https://registry.yarnpkg.com/@eth-optimism/sdk/-/sdk-1.6.1.tgz#cef7b2cc12c0ebaa99ab9520b93981ebd5551070\"\r\n  integrity sha512-+VyFeFIamg1LQ9rPMRS3oDmnh0H8p3NAWoYjk8dnlxLrT2MvCjwxiUhe8cX7TuesLt8lREIWm3nGFQ3ijuRDfA==\r\n  dependencies:\r\n    \"@eth-optimism/contracts\" \"0.5.34\"\r\n    \"@eth-optimism/contracts-bedrock\" \"0.6.0\"\r\n    \"@eth-optimism/core-utils\" \"0.10.0\"\r\n    lodash \"^4.17.21\"\r\n    merkletreejs \"^0.2.27\"\r\n    rlp \"^2.2.7\"\r\n```", "2022-09-12T14:10:01Z", "2022-09-12T14:11:04Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5KJrVC", "I_kwDODjvEJM5Qpp7S", "upon investigating this a bit, it looks like you are storing/tracking both `bytecode` and `deployedBytecode` for all contracts (see e.g. the file `@eth-optimism/contracts-bedrock/artifacts/contracts/L1/L1StandardBridge.sol/L1StandardBridge.json`, which alone is ~57 KB). my guess is that you don't need these, and that if you removed these properties, the size would come down significantly. i.e., the client may need to know the ABI of these contracts, but it almost definitely doesn't need to know their bytecode, since it's not going to be deploying them. so my suggestion would be to revert the repo to how it was (to get rid of the warnings), but then remove the `bytecode` and `deployedBytecode` properties from all tracked contract JSON artifacts. thoughts...?", "2022-09-12T17:18:29Z", "2022-09-12T17:18:29Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5KT__M", "I_kwDODjvEJM5Qpp7S", "hi all, i hate to nag, but I don't see this issue as being fully resolved. can we revisit / reopen?", "2022-09-14T13:20:47Z", "2022-09-14T13:20:47Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5KVL8e", "I_kwDODjvEJM5Qpp7S", "Reopening. `0.5.35` fixes the broken NPM package. Going to see about removing the bytecode.", "2022-09-14T17:11:59Z", "2022-09-14T17:11:59Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5KViY1", "I_kwDODjvEJM5Qpp7S", "@firnprotocol we're investigating this further. We have some ideas but they will likely require some small API changes.", "2022-09-14T18:38:13Z", "2022-09-14T18:38:13Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5LUHiq", "I_kwDODjvEJM5Qpp7S", "any progress here? friendly bump.", "2022-09-30T13:17:54Z", "2022-09-30T13:17:54Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5LVFh2", "I_kwDODjvEJM5Qpp7S", "> any progress here? friendly bump.\r\n\r\nHey sorry, got swamped with some important Bedrock work. I should probably have some time over the weekend to take another stab at this.", "2022-09-30T17:12:19Z", "2022-09-30T17:12:19Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5Lu03C", "I_kwDODjvEJM5Qpp7S", "Just a status update here, devcon is this week and I likely will not be able to work on this until after I'm back. Need to work on lots of slides and some Bedrock migration stuff.", "2022-10-06T19:14:33Z", "2022-10-06T19:14:33Z", "smartcontracts", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5fFJdL", "I_kwDODjvEJM5Qpp7S", "Is this still an issue?", "2023-06-16T19:25:18Z", "2023-06-16T19:25:18Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5fF6zy", "I_kwDODjvEJM5Qpp7S", "i stopped using `optimism-sdk` a while ago, since i was only using it for [`getL1GasPrice`](https://sdk.optimism.io/modules.html#getL1GasPrice), and i realized that i can just query the [on-chain oracle](https://optimistic.etherscan.io/address/0x420000000000000000000000000000000000000F#readContract#F5) instead.\r\n\r\nbut as far as i can remember, it got slightly better, but was still quite bad.", "2023-06-16T22:27:14Z", "2023-06-16T22:27:14Z", "firnprotocol", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5fbb7a", "I_kwDODjvEJM5Qpp7S", "Thank you for the update @firnprotocol ", "2023-06-21T15:12:32Z", "2023-06-21T15:12:32Z", "tynes", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM5jr1Uh", "I_kwDODjvEJM5Qpp7S", "Optimism sdk bundle size is quite large and we have plans to offer alternatives that have smaller bundle sizes. Stay tuned", "2023-08-10T02:11:54Z", "2023-08-10T02:11:54Z", "roninjin10", "2025-08-30 15:20:28"]
["IC_kwDODjvEJM6Bl76e", "I_kwDODjvEJM5Qpp7S", "Closing this issue, it seems stale. Please re-open if any issues persist. Also note that TS things are being moved to https://github.com/ethereum-optimism/ecosystem", "2024-06-17T18:53:37Z", "2024-06-17T18:53:37Z", "protolambda", "2025-08-30 15:20:28"]
["IC_kwDOH2Qg5s6C3fJZ", "I_kwDOH2Qg5s6NzIxB", "duplicate: https://github.com/ethereum-optimism/op-geth/issues/337", "2024-06-27T20:06:53Z", "2024-06-27T20:06:53Z", "andreclaro", "2025-08-30 15:22:38"]
["IC_kwDOH2Qg5s6CaKFd", "I_kwDOH2Qg5s6NWY14", "The op-node should be configured with the engine-API of op-geth, also known as \"authrpc\". Not the default open RPC endpoint. It works the same as how a L1 beacon node connects to the L1 execution engine. See optimism docs for reference. And don't forget to configure the JWT secret (all described in docs). https://docs.optimism.io/builders/node-operators/configuration/base-config", "2024-06-25T03:22:57Z", "2024-06-25T03:22:57Z", "protolambda", "2025-08-30 15:22:38"]
["IC_kwDOH2Qg5s6CaZUh", "I_kwDOH2Qg5s6NWY14", "thank u!", "2024-06-25T04:35:22Z", "2024-06-25T04:35:22Z", "vataops", "2025-08-30 15:22:38"]
["IC_kwDODjvEJM5xHK7Y", "I_kwDODjvEJM58axJD", "Created this to track any necessary changes here, cc @0x00101010 ", "2024-01-18T03:12:13Z", "2024-01-18T03:12:13Z", "protolambda", "2025-08-30 16:23:54"]
["IC_kwDODjvEJM5xVpzJ", "I_kwDODjvEJM58axJD", "Done by https://github.com/ethereum-optimism/optimism/pull/9106, can close now", "2024-01-20T00:59:40Z", "2024-01-20T01:00:01Z", "0x00101010", "2025-08-30 16:23:54"]
["IC_kwDODjvEJM5yz9AB", "I_kwDODjvEJM58axJD", "Reopening as there's still a TODO for this issue:\r\n\r\n```\r\n[Error]: Issue #9064 is closed. Please remove the TODO in op-node/node/api.go:74 referencing ethereum-optimism/optimism#9064 (https://github.com/ethereum-optimism/optimism/issues/9064)\r\n```", "2024-02-05T04:42:40Z", "2024-02-05T04:42:40Z", "ajsutton", "2025-08-30 16:23:54"]
["IC_kwDOJ_r-bs55b9-9", "I_kwDOJ_r-bs6EnyYD", "+1 for option 2: Use the incoming \"shortname\" variable ", "2024-04-04T14:30:53Z", "2024-04-04T14:30:53Z", "zchn", "2025-08-30 16:24:03"]
["IC_kwDOJ_r-bs6CoCb8", "I_kwDOJ_r-bs6EnyYD", "This will be solved by #320 ", "2024-06-26T12:10:59Z", "2024-06-26T12:10:59Z", "geoknee", "2025-08-30 16:24:03"]
["IC_kwDOJ_r-bs6BrCtP", "I_kwDOJ_r-bs549xqh", "@zchn we now have checks on the resource_config:\r\nhttps://github.com/ethereum-optimism/superchain-registry/blob/8b3942df1e1cad7335ef9b78820d6fe22191a55b/validation/standard/standard-config-mainnet.toml#L1-L8\r\n\r\n(Although these were removed from the standard config spec recently)\r\n\r\nDoes that cover what you had in mind here? Can we close this issue?", "2024-06-18T08:43:42Z", "2024-06-18T08:43:42Z", "geoknee", "2025-08-30 16:24:03"]
["IC_kwDOJ_r-bs6ALQti", "I_kwDOJ_r-bs53mJyu", "This can probably be done by validation of the implementation `version()`.", "2024-06-05T16:04:30Z", "2024-06-05T16:04:30Z", "pegahcarter", "2025-08-30 16:24:03"]
["IC_kwDOJ_r-bs6CWGwd", "I_kwDOJ_r-bs53mJyu", "This will be covered by https://github.com/ethereum-optimism/superchain-registry/issues/289", "2024-06-24T15:21:27Z", "2024-06-24T15:21:27Z", "geoknee", "2025-08-30 16:24:03"]
["IC_kwDOMMiGhs6CiCny", "I_kwDOMMiGhs6NVd7s", "Close in favor of #10 ", "2024-06-25T21:00:16Z", "2024-06-25T21:00:16Z", "hamdiallam", "2025-08-30 16:24:20"]
["IC_kwDOMMiGhs6Ch9tt", "I_kwDOMMiGhs6NVcrZ", "Closing in favor of #6 ", "2024-06-25T20:48:32Z", "2024-06-25T20:48:32Z", "hamdiallam", "2025-08-30 16:24:20"]
["IC_kwDOMMiGhs6Ch9Qd", "I_kwDOMMiGhs6NTVrB", "Closing in favor of #6 ", "2024-06-25T20:47:15Z", "2024-06-25T20:47:15Z", "hamdiallam", "2025-08-30 16:24:20"]
["IC_kwDODjvEJM5-3I17", "I_kwDODjvEJM6J7QEj", "The `op-node` needs to fetch blobs from an L1 consensus client (or a blob-archiver). Can you confirm that you have a consensus client connected to your `op-node`, and that the blob in question, slot `8629261`, exists when you try the following request to your consensus client?\r\n```\r\n curl -X GET '<consensus_client_endpoint>/eth/v1/beacon/blob_sidecars/8629261' -H 'accept: application/json'\r\n```", "2024-05-24T02:30:01Z", "2024-05-24T02:30:01Z", "bitwiseguy", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM5-_PIt", "I_kwDODjvEJM6J7QEj", "Hi @bitwiseguy  thanks for replying, \r\nUpon running the curl command you shared, I get {\"code\":500,\"message\":\"block not found\"}. \r\nCould this mean i should use a different consensus client ?\r\n", "2024-05-24T23:19:50Z", "2024-05-24T23:19:50Z", "zainirfan13", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM5-_Pu4", "I_kwDODjvEJM6J7QEj", "I tried all mainnet checkpoint nodes present here\r\nhttps://eth-clients.github.io/checkpoint-sync-endpoints/\r\n\r\nNone worked. All gave block not found error. ", "2024-05-24T23:24:50Z", "2024-05-24T23:24:50Z", "zainirfan13", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM5_HrT1", "I_kwDODjvEJM6J7QEj", "Following up on the issue, can anyone provide me direction regarding this problem.\r\n", "2024-05-27T06:16:04Z", "2024-05-27T06:16:04Z", "zainirfan13", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM5_bA69", "I_kwDODjvEJM6J7QEj", "You need to point at a beacon node that retains expired blobs, e.g. a lighthouse node with pruning disabled. If you're using public beacon API providers, you need to consult their docs whether they provide expired blobs.", "2024-05-29T16:12:05Z", "2024-05-29T16:12:05Z", "sebastianst", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM6AkdSB", "I_kwDODjvEJM6J7QEj", "It is no recovable option for pruned blobs, and also prysm v5.0.3 cannot search blobs that are older than 18 days even if blob data exist. ", "2024-06-10T01:54:06Z", "2024-06-10T02:05:01Z", "gosunuts", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM6Akd7M", "I_kwDODjvEJM6J7QEj", "you can use third-party like [blob-archiver](https://github.com/base-org/blob-archiver), or use Quicknode.\r\nalso you can download blob data from archive node for prysm and lighthouse format. [blob-retriever](https://github.com/rabbitprincess/blob-retriever).", "2024-06-10T01:57:03Z", "2024-06-10T02:03:49Z", "gosunuts", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM6BnH5H", "I_kwDODjvEJM6J7QEj", "Closing this due to inactivity ", "2024-06-17T22:08:42Z", "2024-06-17T22:08:42Z", "tynes", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM6DQM-Y", "I_kwDODjvEJM6J7QEj", "> It is no recovable option for pruned blobs, and also prysm v5.0.3 cannot search blobs that are older than 18 days even if blob data exist.\r\n\r\n@rabbitprincess Is it possible to downgrade prysm to 5.0.2 and get this working?", "2024-07-02T06:36:19Z", "2024-07-02T06:36:19Z", "cshintov", "2025-08-30 16:24:51"]
["IC_kwDODjvEJM6DQTx7", "I_kwDODjvEJM6J7QEj", "> > It is no recovable option for pruned blobs, and also prysm v5.0.3 cannot search blobs that are older than 18 days even if blob data exist.\r\n> \r\n> @rabbitprincess Is it possible to downgrade prysm to 5.0.2 and get this working?\r\n\r\nno. you should use latest version ( v5.0.4 ) for working.", "2024-07-02T06:52:31Z", "2024-07-02T06:52:31Z", "gosunuts", "2025-08-30 16:24:51"]
["IC_kwDOJ_r-bs6CoCQv", "I_kwDOJ_r-bs6Es7bK", "This could be considered a duplicate of #289 ", "2024-06-26T12:10:33Z", "2024-06-26T12:10:33Z", "geoknee", "2025-08-30 16:25:13"]
["IC_kwDOJ_r-bs6DVFOA", "I_kwDOJ_r-bs6Es7bK", "In favour of #331", "2024-07-02T14:14:39Z", "2024-07-02T14:14:39Z", "geoknee", "2025-08-30 16:25:13"]
["IC_kwDOJ_r-bs6BmEjo", "I_kwDOJ_r-bs6EPyuJ", "Noting that the onboarding flow for chains https://github.com/ethereum-optimism/client-pod/issues/859 requires chains to be run-able in op-node / op-geth _before_ they are classed as standard chains. ", "2024-06-17T19:16:06Z", "2024-06-17T19:16:06Z", "geoknee", "2025-08-30 16:25:13"]
["IC_kwDOJ_r-bs5y1N-U", "I_kwDOJ_r-bs5-Pwn6", "It won't be easy/clean to put the code in the `superchain` module because it may cause dependency conflicts or cycles. ", "2024-02-05T09:28:29Z", "2024-02-05T09:28:29Z", "geoknee", "2025-08-30 16:25:13"]
["IC_kwDOJ_r-bs50yb53", "I_kwDOJ_r-bs5-Pwn6", "I made an attempt by reimplementing types from `go-ethereum` inside `superchain`. But that started to snowball very quickly. ", "2024-02-22T12:41:20Z", "2024-02-22T12:41:20Z", "geoknee", "2025-08-30 16:25:13"]
["IC_kwDOKIwiaM6C9QDB", "I_kwDOKIwiaM6JnL3_", "I'd like to take on this", "2024-06-28T14:40:18Z", "2024-06-28T14:40:18Z", "richardgreg", "2025-08-30 16:27:25"]
["IC_kwDODjvEJM6EkV6b", "I_kwDODjvEJM6PMp65", "Once we have parallel derivation pipeline, the simplest answer here may be to just have the engine API client retry the `getPayload` call if it times out or can't connect (with some limit). Blocking while we retry is not a good idea for a synchronous pipeline. The assumption here is that `getPayload` is only failing to give any response if the node is already really overwhelmed or offline so attempting to start building a new block won't help.\r\n\r\nOnce you get an actual error response from the node it probably isn't recoverable so you should start building again.", "2024-07-11T23:36:29Z", "2024-07-11T23:36:29Z", "ajsutton", "2025-08-30 16:28:10"]
["IC_kwDODjvEJM6Dz3Jf", "I_kwDODjvEJM6Oevv_", "Seems like the issue is that the RPC batch call size is too large. Perhaps the max RPC batch size is different depending on the RPC provider\r\n\r\nhttps://github.com/ethereum-optimism/optimism/blob/46c49968de23bbc5f820b3710de248b21b7f05c4/op-challenger/game/fault/contracts/gamefactory.go#L95\r\n\r\nThe default value is used\r\nhttps://github.com/ethereum-optimism/optimism/blob/46c49968de23bbc5f820b3710de248b21b7f05c4/op-service/sources/batching/multicall.go#L12\r\n\r\nIs this an issue with other RPC providers like Alchemy or Infura?", "2024-07-05T21:03:21Z", "2024-07-05T21:03:21Z", "tynes", "2025-08-30 16:28:10"]
["IC_kwDODjvEJM6D4i9i", "I_kwDODjvEJM6Oevv_", "You'll almost certainly want to use your own local nodes when running `op-challenger`.  You are placing a lot of trust in those nodes (you will lose funds if they give inaccurate info) and `op-challenger` will send a _lot_ of requests as part of monitoring all those games.\r\n\r\nThat said, I'd certainly be open to accepting a PR that made the max batch size configurable for `op-challenger`.  I suspect you will still hit a lot of rate limits or rack up very large bills though.", "2024-07-08T00:15:50Z", "2024-07-08T00:15:50Z", "ajsutton", "2025-08-30 16:28:10"]
["IC_kwDODjvEJM6D45D2", "I_kwDODjvEJM6Oevv_", "I see.... Thank you for your answer I'm currently trying to run a local node for l1 rpc. ", "2024-07-08T01:09:01Z", "2024-07-08T01:09:01Z", "AaronLee22", "2025-08-30 16:28:10"]
["IC_kwDODjvEJM6DA6SK", "I_kwDODjvEJM6Npket", "Reopening as there's a todo referencing this somewhere. ", "2024-06-29T08:57:31Z", "2024-06-29T08:57:31Z", "ajsutton", "2025-08-30 16:28:10"]
["IC_kwDODjvEJM6EHKTn", "I_kwDODjvEJM6Npket", "TODO has been completed.", "2024-07-09T04:13:11Z", "2024-07-09T04:13:11Z", "ajsutton", "2025-08-30 16:28:10"]
["IC_kwDODjvEJM6ExRTl", "I_kwDODjvEJM6Mq7ND", "Cancelling this one for now as this is currently being added as a new mode to proxyd by @jelias2 ", "2024-07-14T22:52:07Z", "2024-07-14T22:52:07Z", "sebastianst", "2025-08-30 16:28:10"]
["IC_kwDODjvEJM6C6u8-", "I_kwDODjvEJM6MirQB", "Reopening as there's a todo referencing this. ", "2024-06-28T08:49:35Z", "2024-06-28T08:49:35Z", "ajsutton", "2025-08-30 16:28:10"]
["IC_kwDOI7W0xc6EWsDg", "I_kwDOI7W0xc6KCKqr", "Hi, be careful with phishers (just deleted a comment). Optimism Discord is the best place for support questions, so I recommend asking there. This is for data & analytics. \r\n\r\nYou can find the Discord invite linked on https://www.optimism.io/", "2024-07-10T13:40:28Z", "2024-07-10T13:40:28Z", "MSilb7", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5fitlW", "I_kwDOI7W0xc5orSMV", "\ud83d\udc4c\ud83d\udcaa", "2023-06-22T16:05:34Z", "2023-06-22T17:06:36Z", "mishchenkoy", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5fkFOm", "I_kwDOI7W0xc5orSMV", "Hello, I've begun this analysis on the Optimism forum [here](https://gov.optimism.io/t/retrospective-funding-analysis-uniswap-v3-liquidity-incentives/6212/3?u=raho) and will update with a new post when Phase 3 ends in late July/early August. ", "2023-06-22T20:46:55Z", "2023-06-22T20:46:55Z", "unknown", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5fSCyh", "I_kwDOI7W0xc5orDFh", "this is awesome\r\n", "2023-06-20T11:01:49Z", "2023-06-22T17:06:51Z", "ptagr8", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5a4UB3", "I_kwDOI7W0xc5jfJl_", "What about Arbitrum Net?", "2023-04-27T05:11:44Z", "2023-06-22T17:07:07Z", "KX92", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5dUg3_", "I_kwDOI7W0xc5jImPF", "**Program Name:** Kwenta\r\n\r\n**Missing/Updated Information:** New trading rewards program (partnership with Synthetix). \r\n\r\n[Announcement Blog Post](https://mirror.xyz/kwenta.eth/8KyrISnjOcuAX_VW-GxVqxpcbWukB_RlP5XWWMz-UGk)", "2023-05-27T19:33:35Z", "2023-05-27T19:33:35Z", "unknown", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5db0AP", "I_kwDOI7W0xc5jImPF", "**Program Name:** Mean Finance\r\n\r\n**Missing/Updated Information:** Mean Finance early adopter OP Airdrop \r\n\r\n[Announcement Tweet](https://twitter.com/mean_fi/status/1663231135760158720?s=46&t=YuiqYnL-zBGP6cy9xw6MNw)", "2023-05-29T22:43:00Z", "2023-05-29T22:43:00Z", "unknown", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5dtq9-", "I_kwDOI7W0xc5jImPF", "**Program Name:** Boardroom \r\n\r\n**Missing/Updated Information:**\r\n\r\n- Boardroom delegation incentivizes via Layer3 on Nov. 1, 2022 (76K OP)\r\n  - [Blog Post](https://layer3.mirror.xyz/lke0sFvr5uL03E7tR0wn_uSqexQptCZAkScH50G8n68)\r\n  - [Tweet](https://twitter.com/layer3xyz/status/1587484251351547905?s=20)\r\n ", "2023-06-01T15:18:41Z", "2023-06-01T15:18:41Z", "unknown", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5dts_I", "I_kwDOI7W0xc5jImPF", "**Program Name:** Rabbithole\r\n\r\n**Missing/Updated Information:** \r\n- Optimism Race Program is live\r\n  -  [Blog Post](https://rabbithole.mirror.xyz/x4uSDG699c8MzsvH2rkx_RZHtvMkE8hwll0hquTqE7A)\r\n  - [Tweet](https://twitter.com/rabbithole_gg/status/1658255536142286848?s=20)", "2023-06-01T15:23:40Z", "2023-06-01T15:23:40Z", "unknown", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5dttNc", "I_kwDOI7W0xc5jImPF", "**Program Name:** Rabbithole\r\n\r\n**Missing/Updated Information:** \r\n- Optimism Race Program is live\r\n  -  [Blog Post](https://rabbithole.mirror.xyz/x4uSDG699c8MzsvH2rkx_RZHtvMkE8hwll0hquTqE7A)\r\n  - [Tweet](https://twitter.com/rabbithole_gg/status/1658255536142286848?s=20)", "2023-06-01T15:24:16Z", "2023-06-01T15:24:16Z", "unknown", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5d__DP", "I_kwDOI7W0xc5jImPF", "This is awesome @raho11 thanks so much!\r\n\r\n> Kwenta\r\n\r\nWe had Kwenta marked live from earlier small competitions. It seems like this program is from Synthetix Perps, but for now keeping Kwenta as \"live\" since Aug 10, 2022 (maybe a future iteration/improvement could be to split out each sub-program)\r\n\r\n> Mean Finance\r\n\r\nMarked as live! Unclear what share of the whole program the airdrop is, but the wording makes it seem like more is coming, so keeping it 'live\" vs marking as a 1 day program (as we did with the 1inch airdrop).\r\n\r\n> Boardroom\r\n\r\nAdded, looks like this spent 80k/100k OP, so keeping as \"live\" since 20k are left to be allocated (unless we see other announcements). This likely would require custom analysis to analyze since it's not through a Boardroom contract (rather we could use Layer3 claim as a method to attribute addresses to the program). + the right metrics are probably \"increase in votable supply,\" \"delegation retention,\" etc rather than normal transactions, gas spend.\r\n\r\n> Rabbithole\r\n\r\nAdded!\r\n", "2023-06-05T15:51:17Z", "2023-06-05T15:51:29Z", "MSilb7", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5gMPK9", "I_kwDOI7W0xc5jImPF", "**Program Name:** Polynomial Protocol\r\n\r\n**Missing/Updated Information:** \r\nCurrent 'App Type' = 'Options'\r\nPolynomial should be 'App Type' = 'Perpetuals'", "2023-06-29T21:16:16Z", "2023-06-29T21:16:16Z", "unknown", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5gmk2Q", "I_kwDOI7W0xc5jImPF", "**Program Name**: PoolTogether #2 \r\n\r\n**Missing/Updated Information**:\r\nPoolTogether #2 is marked as completed, but the funding has not been fully distributed. Following prior program status such as Boardroom mentioned above, PoolTogether should not be considered complete. The [funded wallet](https://optimistic.etherscan.io/address/0xfb0dadb835fade151abf6780beafb12fc5ba0e1f) still holds 177,513 OP.\r\n\r\nThe remaining OP are earmarked for: \r\n- 'teams/projects that build new PoolTogether UIs that leverage core features of the protocol on Optimism'\r\n- 'projects that directly integrate PoolTogether on Optimism'\r\n([Source](https://gov.optimism.io/t/review-gf-phase-1-pooltogether/3747#how-will-the-op-tokens-be-distributed-20))\r\n", "2023-07-04T20:41:42Z", "2023-07-04T20:47:33Z", "unknown", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5nKf3o", "I_kwDOI7W0xc5jImPF", "**Program name:-** Via Protocol  \r\n\r\n**Missing Information:** \r\nActions Rewarded:- Gas Fee Refund\r\nAnnounced On:- April 1, 2023 \r\nAnnouncement Link:- https://twitter.com/via_protocol/status/1642167383203753986?s=20\r\nApp Link:- http://router.via.exchange/\r\nApp Type:- DEX Aggregator | Bridge\r\nDescription:- The app finds the best route for any-to-any swap. It supports multi-chain & works efficiently.\r\nStart Date:- 25-05-2023\r\nStatus:- Live\r\nStatus - Season:- Live - Governance - Season 3\r\nAdditional Links:- \r\nhttps://twitter.com/via_protocol/status/1653758003571507203?s=20\r\nhttps://twitter.com/via_protocol/status/1663175119282241537?s=20\r\nhttps://twitter.com/via_protocol/status/1698896086893834602?s=20", "2023-09-22T05:03:16Z", "2023-09-22T05:03:16Z", "chain-l", "2025-08-30 16:28:12"]
["IC_kwDOI7W0xc5nKifB", "I_kwDOI7W0xc5jImPF", "@chain-l added and thank you for the help!!", "2023-09-22T05:19:48Z", "2023-09-22T05:19:48Z", "chuxinh", "2025-08-30 16:28:12"]
["IC_kwDOH2Qg5s5_6Si3", "I_kwDOH2Qg5s6K_ZRz", "I also noticed the warning `Pivot seemingly stale, moving` message several times in the logs:\r\n\r\n```\r\ngeth[3802696]: WARN [06-03|19:09:32.831] Pivot seemingly stale, moving            old=15,326,091 new=15,326,148\r\n```", "2024-06-03T19:45:34Z", "2024-06-03T19:45:34Z", "andreclaro", "2025-08-30 17:30:06"]
["IC_kwDOH2Qg5s5_-79d", "I_kwDOH2Qg5s6K_ZRz", "I have just tried using io2 with max iops and I don't see improvements...", "2024-06-04T10:39:36Z", "2024-06-04T10:39:36Z", "andreclaro", "2025-08-30 17:30:06"]
["IC_kwDOH2Qg5s6AF4tw", "I_kwDOH2Qg5s6K_ZRz", "I was able to spin-up the node using snap sync after cleaning the db and using an instance type with more cpu and memory.... Example for base mainnet:\r\n\r\nop.geth:\r\n```\r\n/usr/local/bin/geth \\\r\n    --op-network=base-mainnet \\\r\n    --datadir=/var/lib/base/data \\\r\n    --syncmode=snap \\\r\n    --gcmode=archive \\\r\n    --networkid=\"8453\" \\\r\n    --http \\\r\n    --http.addr=0.0.0.0 \\\r\n    --http.port=8545 \\\r\n    --http.vhosts=\"*\" \\\r\n    --http.corsdomain=\"*\" \\\r\n    --http.api=web3,debug,eth,net,engine,geth \\\r\n    --ws \\\r\n    --ws.port=8546 \\\r\n    --ws.addr=0.0.0.0 \\\r\n    --ws.origins=\"*\" \\\r\n    --ws.api=debug,eth,net,engine \\\r\n    --authrpc.addr=127.0.0.1 \\\r\n    --authrpc.port=8551 \\\r\n    --authrpc.jwtsecret=/var/lib/base/jwt.txt \\\r\n    --authrpc.vhosts=\"*\" \\\r\n    --metrics \\\r\n    --metrics.addr=0.0.0.0 \\\r\n    --metrics.port=6060 \\\r\n    --maxpeers=500 \\\r\n    --port=30303 \\\r\n    --discovery.port=30303 \\\r\n    --rollup.disabletxpoolgossip=true \\\r\n    --rollup.sequencerhttp=https://mainnet-sequencer.base.org \\\r\n    --rollup.superchain-upgrades \\\r\n    --rollup.halt=major \\\r\n    --verbosity=3\r\n```\r\n\r\nop-node:\r\n\r\n```\r\n/usr/local/bin/op-node \\\r\n    --syncmode=execution-layer \\\r\n    --l1.trustrpc \\\r\n    --l1.rpckind=basic \\\r\n    --l1=http://ETH_URL:8545 \\\r\n    --l1.beacon=http://ETH_URL:5052 \\\r\n    --l2=http://127.0.0.1:8551 \\\r\n    --rpc.addr=127.0.0.1 \\\r\n    --rpc.port=9545 \\\r\n    --l2.jwt-secret=/var/lib/base/jwt.txt \\\r\n    --network=base-mainnet \\\r\n    --p2p.peerstore.path=/var/lib/base/data/opnode_peerstore_db \\\r\n    --p2p.priv.path=/var/lib/base/data/opnode_p2p_priv.txt \\\r\n    --p2p.discovery.path=/var/lib/base/data/opnode_discovery_db \\\r\n    --rollup.load-protocol-versions=true \\\r\n    --rollup.halt=major\r\nType=simple\r\n```", "2024-06-05T06:59:20Z", "2024-06-05T06:59:41Z", "andreclaro", "2025-08-30 17:30:06"]
["IC_kwDOH2Qg5s6FUjdH", "I_kwDOH2Qg5s6K_ZRz", "```\r\n    --db.engine=pebble \\\r\n    --state.scheme=hash \\\r\n```\r\nAfter you deleted these two flags, it worked normally, right?", "2024-07-18T14:42:13Z", "2024-07-18T14:42:13Z", "opfocus", "2025-08-30 17:30:06"]
["IC_kwDOH2Qg5s6FUwLD", "I_kwDOH2Qg5s6K_ZRz", "yes as well! ", "2024-07-18T15:04:15Z", "2024-07-18T15:04:15Z", "andreclaro", "2025-08-30 17:30:06"]
["IC_kwDOJ_r-bs6E9bGe", "I_kwDOJ_r-bs6Ps_ui", "In the interim, we could add a comment in the config files to highlight this problem (and link to this issue). ", "2024-07-16T11:49:04Z", "2024-07-16T11:49:04Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6E9bnF", "I_kwDOJ_r-bs6Ps_ui", "We don't expect existing values to change (they are consensus critical). ", "2024-07-16T11:50:19Z", "2024-07-16T11:50:19Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6E-9zR", "I_kwDOJ_r-bs6Ps_ui", "Here is the place we need to update this https://github.com/ethereum-optimism/optimism/blob/4a487b8920daa9dc4b496d691d5f283f9bb659b1/op-node/rollup/superchain.go#L78-L80. ", "2024-07-16T14:42:15Z", "2024-07-16T14:42:15Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6EYZbt", "I_kwDOJ_r-bs6PFORl", "If we run this on an hourly schedule in CI, a chain may cause a failure but then come back into alignment later without any action from us (i.e. the batcher experiences some downtime but comes back up). So this test could cause some annoying flickering.\r\n\r\nIt's not really clear whether the chain should then be disqualified from being a standard chain, if they have \"self healed\" on this. \r\n\r\nIt is not really a configuration of the chain per se, more a performance metric of the sequencer.", "2024-07-10T16:37:49Z", "2024-07-10T16:37:49Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6EezSR", "I_kwDOJ_r-bs6PFORl", "Closing this as out of scope for early access launch.", "2024-07-11T11:18:19Z", "2024-07-11T11:18:19Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6EvyUb", "I_kwDOJ_r-bs6PFER-", "Updating op-node to track this parameter is out of scope for the superchain registry project, but still highly important. ", "2024-07-13T22:25:21Z", "2024-07-13T22:25:21Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6CyaCI", "I_kwDOJ_r-bs6NmjnL", "We can remove `op-upgrade` from the monorepo. \r\n\r\nSuggested sequencing:\r\n1. Remove code from this repo, draft a PR\r\n2. Draft a monorepo PR which updates the superchain registry dependency and also removes op-upgrade. Must pass CI. \r\n3. Get approvals on both PRs.\r\n4. Merge SCR PR, updated monorepo PR and merge that one.", "2024-06-27T09:48:28Z", "2024-06-27T09:48:28Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6CFbaZ", "I_kwDOJ_r-bs6NCKrN", "Let's also include the codegen tool in this refactor. \r\n\r\nWe could consider using a Makefile to keep this tidy. ", "2024-06-21T10:17:30Z", "2024-06-21T10:17:30Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6D-7FI", "I_kwDOJ_r-bs6Mn61W", "\r\nLonger term we can\r\n- strip down the bindings to only the necessary bits of code\r\n- use a lighter-weight approach to scraping data from contracts:\r\ne.g. create some more helpers like this: \r\n\r\nhttps://github.com/ethereum-optimism/superchain-registry/blob/4641467b9be406de390057778d1b35f8c8a90f70/validation/security-configs_test.go#L106-L107", "2024-07-08T14:49:38Z", "2024-07-08T14:49:38Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Bu7jy", "I_kwDOJ_r-bs6Mr2vp", "For clarity:\r\n\r\n* The standard config definition is already being captured at [`validation/standard/standard-config.toml`. ](https://github.com/ethereum-optimism/superchain-registry/blob/main/validation/standard/standard-config-mainnet.toml)\r\n* The \"top level superchain.yaml file\" from the PRD is currently at [`superchain/configs/chainids.json`](https://github.com/ethereum-optimism/superchain-registry/blob/main/superchain/configs/chainids.json) ([soon to become](https://github.com/ethereum-optimism/superchain-registry/pull/265) `superchain/configs/chainList.json/toml`) with chainList.json", "2024-06-12T14:22:46Z", "2024-06-12T14:22:46Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6B-sFW", "I_kwDOJ_r-bs6Mr2vp", "Thank you @geoknee for putting this proposal together! I agree that it makes sense to find a middle ground between what exists today and what was written up in the PRD. Overall, your proposal sounds good but I have one requested change. Here's the rationale. \r\n\r\nFrom a Product perspective, the repo structure needs to prioritize two things:\r\n\r\n1. The legibility of the chain list (answering the question _who is in the Superchain?_)\r\n2. The legibility of the validation checks (answering the question _how do we define the standard config_?) \r\n\r\nEverything else can be an implementation detail and fit whatever format is easiest to maintain for Eng, but these two things should be stupid-easy to find and understand, even for someone who hasn't spent much time within the repo.\r\n\r\nFor (1), I think that `superchain/configs/chainList.json` is not easy enough to find today. I'd like to see that file as a top-level file, or maybe under `superchain` directly. If there are technical reasons why this isn't possible, we could put a link at the top of the readme or have some other kind of signpost, but I'd rather just have it front and center. I don't imagine that it will be obvious to look in `configs` for the list of members of the Superchain. \r\n\r\nFor (2), I think your proposal is pretty much as legible as it gets! So this looks great to me. I _do_ want to just double check that we're good with governance ratifying a toml file rather than an executable set of tests -- I'll check this over Slack and keep you in the loop.", "2024-06-20T13:28:23Z", "2024-06-20T13:28:23Z", "tessr", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6B_QEi", "I_kwDOJ_r-bs6Mr2vp", "Thanks @tessr/\r\n\r\n1. Happily there won't be any constraints on where to put the chain list file (it's an \"output\" file)\r\n2. The TOML files I was alluding to above were actually the raw config files themselves. More important for blockspace charters are the standard config declaration TOML files. These _are_ unfortunately location constrained, so these are going to be a little bit buried in the current architecture. I can think about options to improve on that. ", "2024-06-20T14:22:51Z", "2024-06-20T14:22:59Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6CBPJF", "I_kwDOJ_r-bs6Mr2vp", "Great. Let's proceed with this proposal then with the chain list file in the top-level directory. \r\n\r\nAs for the tests, that's something that we'll have to confirm with Ben... hoping we can get that ironed out this week. ", "2024-06-20T19:13:44Z", "2024-06-20T19:13:44Z", "tessr", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6EWj5U", "I_kwDOJ_r-bs6Mr2vp", "We should pay attention to the `enhanceYAML` function, which will need to be transcribed to a TOML version\n\nhttps://github.com/ethereum-optimism/superchain-registry/blob/f31b575842ba232011336d7368d96e273ee05ad6/superchain/superchain.go#L178-L179\n", "2024-07-10T13:25:23Z", "2024-07-10T13:25:23Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Egjwm", "I_kwDOJ_r-bs6Mr2vp", "To implement this, we could follow this recipe:\n\n* Drive it using the e2e add-chain tests. So modify the expected output of the add-chain e2e test to be a single toml file\n* Get that test to pass by adding `toml` struct tags and using a `toml` library to marshal the data\n* Then fix up Go and Rust(!) bindings to read from TOML, again using the existing library (ask Andreas or Clabby for help with the Rust side of things)\n* As a final step, we need to migrate all the existing data\n  * either we can write a migration script we can use https://kislyuk.github.io/yq/#\n  * OR we could write a \"tidy\" script, to write the files out to TOML using the existing Go bindings.\n", "2024-07-11T14:44:48Z", "2024-07-11T14:49:31Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Ejxq5", "I_kwDOJ_r-bs6Mr2vp", "I imagine the \"definition of done\" for this ticket is that we are using TOML everywhere in the registry except perhaps for a couple of autogenerated JSON files (addresses.json and chainList.json).\r\nI would add to that, that we want to make the TOML files as legible as possible, at least as legible as the current YAML files. So a bit more spacing/comments would definitely help. Possibly even some more nesting/indentation, e.g.  the hardfork times, urls, etc could be in their own table.\r\n\r\nEssentially anywhere we are using spaces at the moment to improve legibility, we could/should probably use a TOML table with its own header to achieve that. Will require a bit more work in the marshalling / unmarshalling code but hopefully less code hacking spacing into the TOML.", "2024-07-11T21:29:48Z", "2024-07-11T21:29:48Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6EpmWO", "I_kwDOJ_r-bs6Mr2vp", "@geoknee this makes sense to me.\n@tessr how do you feel about the mentioned DoD from the product angle that @geoknee shared: https://github.com/ethereum-optimism/superchain-registry/issues/285#issuecomment-2223971001?", "2024-07-12T12:41:36Z", "2024-07-12T12:41:36Z", "BlocksOnAChain", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Ep557", "I_kwDOJ_r-bs6Mr2vp", "Yes. Just to recap the DoD as concisely as possible: \r\n\r\n* All chain info exists in generated files (`chainList.toml` and `chainList.json`) that sit in the top-level directory. \r\n* We can use the toml file as the canonical reference for the validation checks, and we will be able to change the test suite itself as long as we don't meaningfully alter their semantics. This is something we agreed on with @ben-chain in the last week of June. \r\n\r\nTo add: I wouldn't worry too much about optimizing the formatting of the toml files themselves just yet. Ideally they are quite legible, but I don't think it's worth it to invest a lot of complexity in an information hierarchy and/or formatting. I think we can bring the toml to the community and get feedback -- if it's too hard to read, we can adjust once we hear that feedback.", "2024-07-12T13:19:52Z", "2024-07-12T13:19:52Z", "tessr", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Epnys", "I_kwDOJ_r-bs6Mr3Kh", "@geoknee who do we need or review to close this one and get it merged?", "2024-07-12T12:45:12Z", "2024-07-12T12:45:12Z", "BlocksOnAChain", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6EqAvB", "I_kwDOJ_r-bs6Mr3Kh", "Hey @BlocksOnAChain, just for more public visibility, we need the Chain Servicer (Conduit) to review https://github.com/ethereum-optimism/superchain-registry/pull/368. We need their confirmation on the network upgrade (hard fork times).", "2024-07-12T13:34:59Z", "2024-07-12T13:34:59Z", "sbvegan", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Bu8YW", "I_kwDOJ_r-bs6Mr3dQ", "https://drive.google.com/file/d/1TkQBztHfkZ4u7HD6YhM7zhiXMnzRDT_b/view", "2024-06-10T09:09:39Z", "2024-06-10T09:09:39Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6CWP1I", "I_kwDOJ_r-bs6Mr3dQ", "@geoknee the drive file is private.", "2024-06-24T15:39:55Z", "2024-06-24T15:39:55Z", "pegahcarter", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6CYZxL", "I_kwDOJ_r-bs6Mr3dQ", "Sorry @pegahcarter I put that link on before this issue was migrated to being public. If you're interested in the direction of travel on this issue, please check out this draft PR https://github.com/ethereum-optimism/superchain-registry/pull/297 . ", "2024-06-24T21:27:07Z", "2024-06-24T21:27:07Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6B3QJK", "I_kwDOJ_r-bs6HwpSt", "@bitwiseguy @geoknee ok to close this one?", "2024-06-19T13:38:10Z", "2024-06-19T13:38:10Z", "BlocksOnAChain", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6B3_7t", "I_kwDOJ_r-bs6HwpSt", "No we want this one to stay open. ", "2024-06-19T15:09:09Z", "2024-06-19T15:09:09Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Bu7tJ", "I_kwDOJ_r-bs6Mr24i", "We should also add to this runbook https://www.notion.so/oplabs/909bd32435a749c38ebd923cc2a0951a?v=a3da4badc9b242ffbd78e974ffcbc9d4&p=ad3753864a234779971a42147cca8f14&pm=s explaining what to do if this new CI check fails. ", "2024-06-13T11:18:10Z", "2024-06-13T11:18:10Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6CVfvY", "I_kwDOJ_r-bs6Mr24i", "Let's concentrate on the `rollup.json` file first. We have an example of how to read the config out using an op-node function in the `generate-rollup-config` diff tool here https://github.com/ethereum-optimism/op-geth/blob/22bc5d43dee668bddb5b2df75103b3fdc20af4f8/cmd/utils/flags.go#L2215 . We also have input test data already for this file https://github.com/ethereum-optimism/op-geth/blob/22bc5d43dee668bddb5b2df75103b3fdc20af4f8/cmd/utils/flags.go#L2215 , which means the existing tests would already provide coverage for this new feature / check. ", "2024-06-24T14:08:37Z", "2024-06-24T14:08:37Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Bu_EH", "I_kwDOJ_r-bs6Mr5-r", "This document breaks down the config line by line, and shows which are covered by a check or don't even need a validation check: https://www.notion.so/oplabs/OP-Stack-Configurability-from-a-SuperchainRegistry-validation-POV-9c335a936007456b89777f5a84fba58d ", "2024-05-13T17:56:25Z", "2024-05-23T14:15:31Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6Bu_EN", "I_kwDOJ_r-bs6Mr5-r", "added to the tracking view for registry. ", "2024-05-15T13:09:12Z", "2024-05-15T13:09:12Z", "BlocksOnAChain", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs6CWDWC", "I_kwDOJ_r-bs6GLT4n", "This is covered by https://github.com/ethereum-optimism/superchain-registry/issues/293 and https://github.com/ethereum-optimism/client-pod/issues/913", "2024-06-24T15:14:57Z", "2024-06-24T15:15:43Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs59cOxQ", "I_kwDOJ_r-bs6GBOEd", "Please see these epics and their sub-issues:\nhttps://github.com/ethereum-optimism/client-pod/issues/835\nhttps://github.com/ethereum-optimism/client-pod/issues/836\nhttps://github.com/ethereum-optimism/client-pod/issues/837", "2024-05-10T12:48:57Z", "2024-05-10T12:48:57Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs59pqYG", "I_kwDOJ_r-bs6GBOEd", "closing now", "2024-05-13T15:48:59Z", "2024-05-13T15:48:59Z", "BlocksOnAChain", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs578Zu4", "I_kwDOJ_r-bs6FkrZM", "https://www.notion.so/oplabs/Superchain-Genesis-Hash-checks-60e173e9fb0c43e88a81d0666cb3d582", "2024-04-26T13:46:51Z", "2024-04-26T13:46:51Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs59cvaZ", "I_kwDOJ_r-bs6FkrZM", "Helpfully, we have the raw JSON config files for these chains now https://github.com/ethereum-optimism/superchain-registry/pull/226 . \n\nFrom a slack thread: \n\nFor both chains (Superlumio and Metal Sepolia) which fail the check at the moment, we are hardcoding \"eip1559Denominator\": 50 which doesn't match the value of 0 used in the supplied json files. I don't think this makes a difference to the genesis block hash validation check.\n\nFor Superlumio, it looks like you have hardfork times \"missing\". In the registry we allow chains to override superchain-wide hardfork times, but not with nil values. So by not specifying overrides for e.g. ecotone_time, the chain is inheriting the superchain-wide hardfork times (and a bunch of other values that are pinned to that hardfork time) when we reconstruct the genesis, and this affects the genesis block hash which we compute in the check.\n\nOn a local branch I hacked around with this by deactivating the hardfork times for the entire supechain, then the checks for these chains pass.\n\nI can see two possible resolutions:\n* ~We have to ensure canyon, delta and ecotone are activated on these chains and update the hardfork times as appropriate in the .yaml files to non-nil values.~ this isn't really an option, they would need to be activated at genesis.\n* We could change the registry behaviour to allow overriding from non-nil to nil. I'm not sure if this is the behaviour we want though, I will check with the rest of the team.", "2024-05-10T14:13:59Z", "2024-05-10T21:02:44Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs59dS88", "I_kwDOJ_r-bs6FkrZM", "The key thing is if either canyon or ecotone is activated at or before genesis. this affects genesis block because of these lines https://github.com/ethereum-optimism/op-geth/blob/9617f2a7dd0ab80c468d6e474e3cc26be9572c97/core/genesis.go#L538-L552\n\nCanyon implies Shanghai\nEcotone implies Cancun", "2024-05-10T15:44:34Z", "2024-05-10T15:52:12Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs59e9Bu", "I_kwDOJ_r-bs6FkrZM", "Here's one way of fixing this https://github.com/ethereum-optimism/superchain-registry/compare/gk/skip-default-hf-times?expand=1. \n\nThe idea is to not specify hardfork times in a superchain wide way any more. Every chain has to specify their own hard fork times.\n\nPros:\n* remove a lot of complexity in superchain registry Go code\n* genesis hash checks pass again for _all_ standard chains (can probably also extend to frontier chains too but I didn't check that yet)\n* no downstream changes necessary, op-geth and op-node inspect the per chain values anyway\n\nCons:\n* we may need to go back and get the correct values for chains, I don't think we can tell if we have them correct at the moment\n* more repetition across configs (not a big deal, I think)\n* no longer enshrining \"upgrade together\" flow into the data structures. But, we can probably still do that using other tooling, and write whatever checks and balances we want. \n\n\nHere is the PR where the behaviour was added https://github.com/ethereum-optimism/superchain-registry/pull/101", "2024-05-10T21:01:27Z", "2024-05-10T21:03:24Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs5zwNLy", "I_kwDOJ_r-bs5_HxzQ", "works for me. but only because I'm not the one adding any yamls :) probably we need to check with code owners of the yaml files and make sure it works for them", "2024-02-13T17:02:58Z", "2024-02-13T17:02:58Z", "zchn", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs5zwNol", "I_kwDOJ_r-bs5_HxzQ", "I'm not aware of a reason for having two separate formats in the repo, so standardising to one seems pretty fair. Can you expand on why you favour JSON? I know YAML has some drawbacks, but just want to highlight that comments can be pretty useful, especially for timestamps:\r\n\r\nhttps://github.com/ethereum-optimism/superchain-registry/blob/9828415051c62b9cbaef3862558dbd54b3a7833e/superchain/configs/goerli-dev-0/superchain.yaml#L8\r\n\r\nI guess we lose that with JSON, although the comment content could be embedded as data.\r\n\r\nIt seems to me that the migration you are suggesting could probably be largely automated, and wouldn't break any of the interfaces exported by the Go module or the npm package. So the cost/benefit analysis looks reasonable to me. ", "2024-02-13T17:03:58Z", "2024-02-13T17:03:58Z", "geoknee", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs5zwPUq", "I_kwDOJ_r-bs5_HxzQ", "Agreed on standardizing, though like @geoknee I also dislike that JSON prevents us from adding comments. My personal preference is standardizing around TOML since YAML has some oddities, but I'm ok with YAML too", "2024-02-13T17:07:55Z", "2024-02-13T17:07:55Z", "mds1", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs5zyjeT", "I_kwDOJ_r-bs5_HxzQ", ">  just want to highlight that comments can be pretty useful, especially for timestamps\r\n\r\nAgree, these are useful.\r\n\r\n> Can you expand on why you favour JSON?\r\n\r\nJSON suggestion was mainly due to it's native support in most languages that wrap the registry, but I prefer TOML personally. So long as we don't use several formats, I'm not super opinionated :) \r\n\r\nUpdated the issue to remove the JSON suggestion - primary ask is for a single format \ud83d\udc4d ", "2024-02-13T21:33:10Z", "2024-02-13T21:36:19Z", "clabby", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs50DdLW", "I_kwDOJ_r-bs5_HxzQ", "Also a big fan of standardizing this. Although I prefer YAML over JSON, I think we need JSONs because they're used in NPM packages. Is that correct @roninjin10 ?", "2024-02-15T19:25:01Z", "2024-02-15T19:25:01Z", "sebastianst", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs50DlpU", "I_kwDOJ_r-bs5_HxzQ", "If we make this update it will break things downstream that depend on it like docs and https://github.com/ethereum-optimism/optimism/pull/9041", "2024-02-15T19:34:46Z", "2024-02-15T19:34:46Z", "roninjin10", "2025-08-30 17:30:29"]
["IC_kwDOJ_r-bs52IwNv", "I_kwDOJ_r-bs5_HxzQ", "Documenting that we came to consensus on using TOML. We can use [yj](https://github.com/sclevine/yj), or replace jq with [yq](https://github.com/mikefarah/yq), for minimal diffs in scripts using jq. Foundry is in the [process](https://github.com/foundry-rs/foundry/pull/7317) of adding TOML parsing cheatcodes as well", "2024-03-06T23:05:46Z", "2024-03-06T23:05:46Z", "mds1", "2025-08-30 17:30:29"]
["IC_kwDODjvEJM6Ezluy", "I_kwDODjvEJM6Pi9pb", "If we can't do 1. in Granite or Holocene, it could be worth it doing 2. for the next contracts release to avoid confusion until we add it back?", "2024-07-15T09:55:01Z", "2024-07-15T09:55:01Z", "sebastianst", "2025-08-30 17:30:35"]
["IC_kwDODjvEJM6E2WDX", "I_kwDODjvEJM6Pi9pb", "The `ResourceConfig` is specific to the deposit tx fee market. It is not used to configure the L2 fee market. Both are separate gas markets with different basefees. Its incredibly complex to maintain an EIP1559 fee market on chain, the state space is just too large to test thoroughly and ensure quality UX (for example out of gas issues happen randomly for end users). I am a big fan of https://github.com/ethereum-optimism/specs/pull/82 which replaces the on chain EIP1559 fee market with a queue, which would remove the oog issues, make deposits cheaper and remove the risk that deposits use up more gas than the L2 gas limit if the system is configured incorrectly.", "2024-07-15T15:52:54Z", "2024-07-15T15:53:54Z", "tynes", "2025-08-30 17:30:35"]
["IC_kwDODjvEJM6E2yM2", "I_kwDODjvEJM6Pi9pb", "Thanks @mark. I wonder if there are any documentation improvements we can make in the short term to prevent confusion on this issue? Otherwise, it looks like there isn't any action necessary after all and this ticket can be closed. ", "2024-07-15T16:47:47Z", "2024-07-15T16:47:47Z", "geoknee", "2025-08-30 17:30:35"]
["IC_kwDODjvEJM6E2XbB", "I_kwDODjvEJM6PgO6P", "It is possible that you are oom'ing while building the contracts. We would like to split the contracts up a bit better to prevent this sort of issue\r\n\r\nMessage in logs:\r\n\r\n```\r\nsolc exited with signal: 9 (SIGKILL)\r\n```", "2024-07-15T15:55:33Z", "2024-07-15T15:55:33Z", "tynes", "2025-08-30 17:30:35"]
["IC_kwDODjvEJM6Ewwcs", "I_kwDODjvEJM6PH9oi", "What do you mean it does nothing? It does do something. The implementation is here\r\n\r\nhttps://github.com/ethereum-optimism/optimism/blob/d091bb33ceba0931205584d88b8c2bd84404e466/packages/contracts-bedrock/src/libraries/TransientContext.sol#L65-L69\r\n\r\nIt allows for reentrancy without clobbering transient storage", "2024-07-14T14:45:00Z", "2024-07-14T14:45:00Z", "tynes", "2025-08-30 17:30:35"]
["IC_kwDODjvEJM6E1hLU", "I_kwDODjvEJM6PH9oi", "Right, I understand that the storage is incremented.  However, this storage is not checked or managed elsewhere so I do not understand the purpose of tracking the number of entrances.", "2024-07-15T14:19:53Z", "2024-07-15T14:19:53Z", "pegahcarter", "2025-08-30 17:30:35"]
["IC_kwDODjvEJM6E4fQk", "I_kwDODjvEJM6PH9oi", "It is used here:\r\n\r\nhttps://github.com/ethereum-optimism/optimism/blob/69d2d47b0f4ea0a41905b0ef52f3fa343ea7633e/packages/contracts-bedrock/src/L2/CrossL2Inbox.sol#L136", "2024-07-15T20:45:42Z", "2024-07-15T20:45:42Z", "tynes", "2025-08-30 17:30:35"]
["IC_kwDODjvEJM6EwwgQ", "I_kwDODjvEJM6PH8Rl", "You cannot tell the difference between a 0 value and a null value. Therefore revert on a null value (the value is not set). ", "2024-07-14T14:45:43Z", "2024-07-14T14:45:43Z", "tynes", "2025-08-30 17:30:35"]
["IC_kwDODjvEJM6CFCOH", "I_kwDODjvEJM6IA3PG", "@sebastianst should we pick this one up and work on it for Fjord or it's a lower priority item?", "2024-06-21T09:16:57Z", "2024-06-21T09:16:57Z", "BlocksOnAChain", "2025-08-30 17:30:35"]
["IC_kwDOMMiGhs6FI0Mx", "I_kwDOMMiGhs6NViag", "Closing in favor of #25 ", "2024-07-17T16:06:15Z", "2024-07-17T16:06:15Z", "hamdiallam", "2025-08-30 17:32:13"]
["IC_kwDOLB-lzc6FuR1F", "I_kwDOLB-lzc6QWsQk", "Updated, thank you!", "2024-07-22T17:51:34Z", "2024-07-22T17:51:34Z", "tynes", "2025-08-30 17:32:50"]
["IC_kwDOJ_r-bs6F3CZk", "I_kwDOJ_r-bs6QOSZ3", "Could we use this data structure:\r\n```toml\r\n[[chains]]\r\n  name = \"OP Mainnet\"\r\n  identifier = \"mainnet/op\"\r\n  chain_id = 10\r\n  rpc = [\"https://mainnet.optimism.io\"]\r\n  explorers = [\"https://explorer.optimism.io\"]\r\n  superchain_level = 2\r\n  [chains.data_availability]\r\n    type = \"eth-da\"\r\n  [chains.parent]\r\n    type = \"L2\"\r\n    chain = \"mainnet\"\r\n```\r\nThen for chains using plasma it could look like this (or do we think the keyword `plasma` is needed?):\r\n```toml\r\n[[chains]]\r\n  name = \"OP Mainnet\"\r\n  identifier = \"mainnet/op\"\r\n  chain_id = 10\r\n  rpc = [\"https://mainnet.optimism.io\"]\r\n  explorers = [\"https://explorer.optimism.io\"]\r\n  superchain_level = 2\r\n  [chains.data_availability]\r\n    type = \"alt-da\"\r\n    provider = \"celestia\"\r\n    da_challenge_contract_address = \"0x3333333333333333333300000000000000000000\"\r\n    da_challenge_window = 5555555\r\n    da_resolve_window = 7777777\r\n  [chains.parent]\r\n    type = \"L2\"\r\n    chain = \"mainnet\"\r\n```", "2024-07-23T17:12:14Z", "2024-07-23T17:12:14Z", "bitwiseguy", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6F4Ri_", "I_kwDOJ_r-bs6QOSZ3", "Disregard prior comment. Will just use the top-level `data_availability_type` field and keep additional data availability fields separate", "2024-07-23T19:27:02Z", "2024-07-23T19:27:02Z", "bitwiseguy", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6EX384", "I_kwDOJ_r-bs6PEfVN", "No mention of this in the BHIC spec. \r\n\r\nThis is an example of something that can change in principle in future, so might be something (therefore) to check in the registry. \r\n\r\nHowever, if we can be sure that this parameter cannot change after key handover (at least without a governance vote), does that mean we don't need to check it going forward?\r\n", "2024-07-10T15:37:03Z", "2024-07-10T15:37:24Z", "geoknee", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6EX7su", "I_kwDOJ_r-bs6PEfVN", "There's a general question about how much we should be relying on BHIC check + key handover to ensure a chain is still \"standard\". \r\n\r\nIf the \"standard rollup charter\" is defined by the superchain registry checks, then they should cover everything in the definition of a standard rollup. ", "2024-07-10T15:42:05Z", "2024-07-10T15:43:22Z", "geoknee", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6FSETr", "I_kwDOJ_r-bs6PEfVN", "From my investigations, it seems that the l1 block time stored in the `rollup.json` file (and therefore read by `op-node`) is decided by when the getting-started script is run https://github.com/ethereum-optimism/optimism/blob/d510910381d99660295f99e4eccd553df5829df6/packages/contracts-bedrock/scripts/getting-started/config.sh#L26-L29, or even when the op-node genesis creation command is run: https://github.com/ethereum-optimism/optimism/blob/547ea72d9849e13ce169fd31df0f9197651b3f86/op-node/cmd/genesis/cmd.go#L189-L190.\n\nThis field is used when e.g. resetting the engine queue https://github.com/ethereum-optimism/optimism/blob/547ea72d9849e13ce169fd31df0f9197651b3f86/op-node/rollup/derive/engine_queue.go#L710.\n\nThe SystemConfig startBlock value, however, is set at deploy time, when the proxy contract storage is initialized. https://github.com/ethereum-optimism/optimism/blob/d510910381d99660295f99e4eccd553df5829df6/packages/contracts-bedrock/src/L1/SystemConfig.sol#L210-L211\n\nI cannot find any evidence that on chain value is being read by op-node https://github.com/ethereum-optimism/optimism/blob/547ea72d9849e13ce169fd31df0f9197651b3f86/op-node/rollup/derive/engine_queue.go#L710. \n", "2024-07-18T09:56:50Z", "2024-07-18T10:28:49Z", "geoknee", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6FSWrI", "I_kwDOJ_r-bs6PEfVN", "The system config value [is documented as](https://github.com/ethereum-optimism/optimism/blob/d510910381d99660295f99e4eccd553df5829df6/packages/contracts-bedrock/src/L1/SystemConfig.sol#L77-L79)\n\n_    /// @notice Storage slot for block at which the op-node can start searching for logs from._.\n\n", "2024-07-18T10:37:14Z", "2024-07-18T10:37:14Z", "geoknee", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6FeCGS", "I_kwDOJ_r-bs6PEfVN", "* The check in the registry should be that the off-chain start block value is earlier than or equal to the on chain value.\r\n* Chains that have the off-chain value later than the on chain value will fail the test. They then need to manually alter their config (i.e. the rollup.json) to push it back earlier and resubmit the PR\r\n* We probably need a bit more work to get drift detection in place for the on-chain value, we need to understand how the start block has changed on chain over time.\r\n* Existing chains which fail will need to be excluded, or we communicate with them about updating their offchain config values. ", "2024-07-19T13:56:49Z", "2024-07-19T13:56:49Z", "geoknee", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6FeqUP", "I_kwDOJ_r-bs6PEfVN", "Existing standard chains that fail the offChain <= onChain start block check:\r\n* OP mainnet\r\n  * offChain: [17_422_590](https://github.com/ethereum-optimism/superchain-registry/blob/main/superchain/configs/mainnet/op.toml#L21)\r\n  * onChain: [17_422_444](https://etherscan.io/address/0x229047fed2591dbec1eF1118d64F7aF3dB9EB290#readProxyContract)\r\n  * 1st deposit tx: [17_423_165](https://etherscan.io/txs?a=0xbEb5Fc579115071764c7423A4f12eDde41f106Ed&p=386)\r\n* OP sepolia\r\n  * offChain: [4_071_408](https://github.com/ethereum-optimism/superchain-registry/blob/main/superchain/configs/sepolia/op.toml#L21)\r\n  * onChain: [4_071_248](https://sepolia.etherscan.io/address/0x034edD2A225f7f429A63E0f1D2084B9E0A93b538#readProxyContract)\r\n  * 1st deposit tx: [4_113_914](https://sepolia.etherscan.io/txs?a=0x16Fc5058F25648194471939df75CF27A2fdC48BC&p=102)\r\n\r\nSince the `1st deposit tx` occurs after both onChain and offChain start blocks in both instances, I believe we are good to manually update these two config files to get the validation test to pass by setting the offChain value to the onChain value. By updating the `op-node genesis l2` command via https://github.com/ethereum-optimism/optimism/pull/11181, we should prevent this situation from happening again. Thoughts @geoknee? ", "2024-07-19T14:57:50Z", "2024-07-19T19:42:24Z", "bitwiseguy", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6Fe_IP", "I_kwDOJ_r-bs6PEfVN", "Yeah I think we could make those updates, I wonder if we want to run some manual sync tests with those changes to make triple sure? cc @sebastianst \r\n\r\nWe can also bake in a couple of exclusions for these if we want to unblock. ", "2024-07-19T15:47:12Z", "2024-07-19T15:47:12Z", "geoknee", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6FqYOe", "I_kwDOJ_r-bs6PEfVN", "Looks good to me to rewind the off-chain values to the on-chain ones. We could start a sync-test with this change applied for extra validation \ud83d\udc4d\ud83c\udffb ", "2024-07-22T09:15:43Z", "2024-07-22T09:15:43Z", "sebastianst", "2025-08-30 17:33:05"]
["IC_kwDOJ_r-bs6FvAJo", "I_kwDOJ_r-bs6PEfVN", "Decided to leave the configs as-is and just perform an extra check when offChain > onChain to confirm there were no TransactionDeposited events emitted from the OptimismPortal contract during that gap", "2024-07-22T19:45:10Z", "2024-07-22T19:45:10Z", "bitwiseguy", "2025-08-30 17:33:05"]
["IC_kwDOMMiGhs6Fr_Lx", "I_kwDOMMiGhs6PmKhi", "Closed with https://github.com/ethereum-optimism/supersim/pull/56", "2024-07-22T12:59:36Z", "2024-07-22T12:59:36Z", "jakim929", "2025-08-30 17:33:17"]
["IC_kwDOMMiGhs6F25bT", "I_kwDOMMiGhs6NTKb8", "Closing this out the remainder of the work will happen in the ecosystem repo. This tracker is what can be followed for that https://github.com/ethereum-optimism/ecosystem/issues/416", "2024-07-23T16:56:33Z", "2024-07-23T16:56:33Z", "nitaliano", "2025-08-30 17:33:17"]
["IC_kwDODjvEJM6Fw7rB", "I_kwDODjvEJM6NpnAv", "Approach to tracking heads has changed so this doesn't apply anymore.  Will need to be replaced by a different set of issues based on https://github.com/ethereum-optimism/design-docs/pull/49/files", "2024-07-23T03:50:28Z", "2024-07-23T03:50:28Z", "ajsutton", "2025-08-30 17:33:29"]
["IC_kwDODjvEJM6Fw7h3", "I_kwDODjvEJM6Npmxm", "Closing as this as we aren't planning to track heads in this way now.", "2024-07-23T03:49:37Z", "2024-07-23T03:49:37Z", "ajsutton", "2025-08-30 17:33:29"]
["IC_kwDODjvEJM5gJvpH", "I_kwDODjvEJM5qIl4J", "@cspannos It should be the address of the `L2OutputOracleProxy`, available at `~/optimism/packages/contracts-bedrock/deployments/getting-started/L2OutputOracleProxy.json`, particularly LOC 131.", "2023-06-29T13:16:19Z", "2023-06-29T13:16:19Z", "vjaiman", "2025-08-30 17:33:29"]
["IC_kwDODjvEJM5gJ1zP", "I_kwDODjvEJM5qIl4J", "> @cspannos It should be the address of the `L2OutputOracleProxy`, available at `~/optimism/packages/contracts-bedrock/deployments/getting-started/L2OutputOracleProxy.json`, particularly LOC 131.\r\n\r\nThanks for your suggestion. It seems `op-proposer` didn't like the L1 rpc provider I gave for other components in the stack. Not sure why. I pointed it at an alchemy endpoint and it is now working as expected. Seeing as my main issue is resolved, I am closing this issue.", "2023-06-29T13:33:15Z", "2023-06-29T13:33:15Z", "cspannos", "2025-08-30 17:33:29"]
["IC_kwDODjvEJM6FY9X9", "I_kwDODjvEJM5qIl4J", "I can not see the json file under this path\r\n~/optimism/packages/contracts-bedrock/deployments/getting-started/L2OutputOracleProxy.json\r\nHow can I get it?", "2024-07-19T01:58:54Z", "2024-07-19T01:58:54Z", "DramaCrypto", "2025-08-30 17:33:29"]
["IC_kwDODjvEJM6Ft_au", "I_kwDODjvEJM5qIl4J", "[This](https://github.com/search?q=repo%3Aethereum-optimism%2Foptimism+4265aa5c1830facc0f3382552954f2946a73caf5&type=commits) commit deleted L2OutputOracleProxy.json:\r\n```\r\ncommit 4265aa5c1830facc0f3382552954f2946a73caf5\r\nAuthor: Mark Tyneway <mark.tyneway@gmail.com>\r\nDate:   Thu Oct 5 10:52:19 2023 -0700\r\n\r\n    contracts-bedrock: delete fake internal-devnet artifacts\r\n\r\n    Remove the `internal-devnet` artifacts from the monorepo\r\n    because they are not correct. These come from an earlier\r\n    iteration of the internal devnet so delete them to prevent\r\n    confusion in the future.\r\n```\r\n I updated my op-proposer start command to get the address with `jq -r '.transactions[] | select(.contractName==\"L2OutputOracle\") | .contractAddress' /root/optimism/packages/contracts-bedrock/broadcast/Deploy.s.sol/11155420/run-latest.json`.  \r\n\r\nI also encountered a problem when I tried to start op-node:\r\n```\r\nfailed to validate the L1 config: incorrect L1 RPC chain id 11155420, expected 11155111root@8a7632a84660:~/optimism/packages/contracts-bedrock# #failed to setup: unable to create the rollup node: failed to init L1: failed to validate the L1 config: incorrect L1 RPC chain id 11155420, expected 11155111\r\n```\r\n[Chainlist.org](https://chainlist.org/chain/11155111) says chain 11155111 is Sepolia.  For 11155420, [it says](https://chainlist.org/chain/11155420) \"OP Sepolia Testnet\".   I ended up changing `\"l1ChainID\": 11155111` to `l1ChainID\": 11155420` in `packages/contracts-bedrock/scripts/getting-started/config.sh` and repeating all subsequent steps. ", "2024-07-22T17:06:03Z", "2024-07-22T19:59:10Z", "kgmiles", "2025-08-30 17:33:29"]
["IC_kwDOLB-lzc6GjGpj", "I_kwDOLB-lzc6Nnxlm", "The [following functionality](https://github.com/ethereum-optimism/optimism/blob/363c5d7f4fb14180a0e2a28cc948fe2146f03dce/packages/contracts-bedrock/src/universal/Proxy.sol#L26) is consensus critical:\r\n\r\n```solidity\r\n    modifier proxyCallIfNotAdmin() {\r\n        if (msg.sender == _getAdmin() || msg.sender == address(0)) {\r\n            _;\r\n        } else {\r\n            // This WILL halt the call frame on completion.\r\n            _doProxyCall();\r\n        }\r\n    }\r\n```\r\n\r\nThe `|| address(0)` check in particular, where `address(0)` counts as having admin access. Hardfork system txs spoof `address(0)` to modify the predeploys", "2024-07-30T02:40:42Z", "2024-07-30T02:40:42Z", "tynes", "2025-08-30 17:35:33"]
["IC_kwDOJ_r-bs6GMIz0", "I_kwDOJ_r-bs6QLd7o", "@sebastianst as a first step, should we just check whether the `BatchSubmitter` address for a chain has sent a tx in the last `SequencerWindow` l1 blocks?\r\n\r\nAs an improvement to that check, how can we ensure the `BatchSubmitter`'s tx actually posted l2 tx data and wasn't an unrelated tx coming from that address? How does the [batch_inbox_address](https://github.com/ethereum-optimism/superchain-registry/blob/main/superchain/configs/mainnet/op.toml#L9) in the chain's config come into play?\r\n\r\nDraft PR here for current implementation: https://github.com/ethereum-optimism/superchain-registry/pull/421", "2024-07-25T20:20:55Z", "2024-07-25T22:34:01Z", "bitwiseguy", "2025-08-30 18:36:46"]
["IC_kwDOJ_r-bs6GRZzC", "I_kwDOJ_r-bs6QLd7o", "Yes, although we should already check that the batcher sends it's txs to the batcher inbox address. \r\n\r\nTo confirm that a batcher tx really contains valid data for that chain, you would need to have a synced node for that chain and import the batches. But then you'd effectively already run a node \ud83d\ude05 Given that this is just a one-off check at an arbitrary point in time, not sure if we'd get any more meaningful guarantees from this.\r\n\r\nChecking for the sequence window is a good default, because a live chain would post with at most this duration between batcher txs.", "2024-07-26T12:56:23Z", "2024-07-26T12:56:23Z", "sebastianst", "2025-08-30 18:36:46"]
["IC_kwDOMMiGhs6GgT6a", "I_kwDOMMiGhs6PIXpm", "Fixed with generated genesis files", "2024-07-29T18:22:06Z", "2024-07-29T18:22:06Z", "hamdiallam", "2025-08-30 18:36:56"]
["IC_kwDOMMiGhs6GgNA4", "I_kwDOMMiGhs6NVhBM", "Closing in favor of simply having it run in the terminal and gracefully shutting down on interrupt (which happens now). Also expected behavior of anvil rn", "2024-07-29T18:06:08Z", "2024-07-29T18:06:08Z", "hamdiallam", "2025-08-30 18:36:56"]
["IC_kwDODjvEJM6G-CQM", "I_kwDODjvEJM6RqTWW", "See [here](https://github.com/ethereum-optimism/superchain-ops/blob/55e9520e27c7e916d8992ce351c6d5cfa8a511d8/tasks/eth/009-fp-upgrade/EXEC.md?plain=1#L6) for an example", "2024-08-02T02:48:09Z", "2024-08-02T02:48:09Z", "tynes", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6Gorpi", "I_kwDODjvEJM6QlPs7", "Has gone through a bunch of rounds of review, essentially finished", "2024-07-30T17:02:55Z", "2024-07-30T17:02:55Z", "tynes", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6G5SsD", "I_kwDODjvEJM6QlPs7", "Completed as https://github.com/ethereum-optimism/specs/pull/247 was merged.", "2024-08-01T14:04:29Z", "2024-08-01T14:04:29Z", "cairoeth", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6F1jwm", "I_kwDODjvEJM6MirJ8", "This is done as part of the `GovernanceDelegation` specs, which defines how the `GovernanceToken`'s token transfer hook will call the `GovernanceDelegation`.", "2024-07-23T14:28:27Z", "2024-08-01T19:06:47Z", "cairoeth", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6BlHpM", "I_kwDODjvEJM6Mirg-", "So I was viewing the `op-superchain` directory as the source of truth for message types and invariants. In order for it to be a source-of-truth library, are we thinking about a library outside of the monorepo?\r\n\r\nOtherwise i'm not sure how op-geth would consume this library. The code duplication in op-geth would be minimal if necessary. We simply need the backend RPC interface and calldata decoding logic.\r\n\r\ncc @tynes @protolambda ", "2024-04-02T20:07:48Z", "2024-04-02T20:07:48Z", "hamdiallam", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6BlHpQ", "I_kwDODjvEJM6Mirg-", "Ideally there is minimal logic in geth and as much as possible is in the sidecar service. Can it be built with no copy of the decoding logic?", "2024-04-02T20:12:15Z", "2024-04-02T20:12:15Z", "tynes", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6BlHpV", "I_kwDODjvEJM6Mirg-", "One approach that's plausible which you proposed is passing through the entire calldata to the over RPC so that op-geth doesn't need to be aware of any of the concrete message types & decoding logic. It'd only need to redefine the safety labels & calldata fn selector to look for.\r\n\r\nA downside being an extra local rpc requests made for invalidations that could have been determined upfront in the mempool. This should be cheap though.", "2024-04-02T20:22:31Z", "2024-04-02T20:23:20Z", "hamdiallam", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6GpGEY", "I_kwDODjvEJM6Mirg-", "Closing this, we have a types package, and should focus on the usage before creating an arbitrary package API\nhttps://github.com/ethereum-optimism/optimism/tree/develop/op-supervisor/supervisor/backend/types", "2024-07-30T18:07:03Z", "2024-07-30T18:07:03Z", "protolambda", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6HDfhn", "I_kwDODjvEJM6Mirhm", "This ticket is stale", "2024-08-02T17:26:39Z", "2024-08-02T17:26:39Z", "protolambda", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6GpHMV", "I_kwDODjvEJM6MirlO", "Closing as completed. We can track the parallel deriver separately", "2024-07-30T18:09:50Z", "2024-07-30T18:09:50Z", "protolambda", "2025-08-30 18:37:14"]
["IC_kwDODjvEJM6HpYIr", "I_kwDODjvEJM6SQy7Q", "works with anvil ", "2024-08-08T13:02:35Z", "2024-08-08T13:02:35Z", "askucher", "2025-08-30 18:37:49"]
["IC_kwDODjvEJM6H1J5L", "I_kwDODjvEJM6Mirdx", "@protolambda Is this closeable or are there future design docs still needed?", "2024-08-09T23:04:16Z", "2024-08-09T23:04:16Z", "mds1", "2025-08-30 18:37:49"]
["IC_kwDODjvEJM6H1Vuj", "I_kwDODjvEJM6Mirdx", "@mds1 can be closed, but we do need to follow-up with implementation steps next week", "2024-08-10T00:28:33Z", "2024-08-10T00:28:33Z", "protolambda", "2025-08-30 18:37:49"]
["IC_kwDODjvEJM5yzP_T", "I_kwDODjvEJM55GdeX", "@tynes what's the status of this?", "2024-02-05T01:02:19Z", "2024-02-05T01:02:19Z", "smartcontracts", "2025-08-30 18:37:49"]
["IC_kwDODjvEJM5y2jSP", "I_kwDODjvEJM55GdeX", "Not sure if fixed, I did add a commit trying to fix this", "2024-02-05T12:37:53Z", "2024-02-05T12:37:53Z", "tynes", "2025-08-30 18:37:49"]
["IC_kwDODjvEJM6Hr2_3", "I_kwDODjvEJM52utXg", "Closing for inactivity, if we see this flake again I'll make a new issue", "2024-08-08T18:29:24Z", "2024-08-08T18:29:24Z", "smartcontracts", "2025-08-30 18:37:49"]
["IC_kwDOJ_r-bs6FgZgO", "I_kwDOJ_r-bs6QNLxd", "It looks like this was the issue: https://github.com/ethereum-optimism/superchain-registry/pull/409\r\n\r\nWe'll have them rebase their PR and the error should go away", "2024-07-19T18:22:28Z", "2024-07-19T18:22:28Z", "sbvegan", "2025-08-30 18:38:01"]
["IC_kwDOMMiGhs6Hv8Ab", "I_kwDOMMiGhs6SbK_G", "in L1 genesis, the dev account is used to deploy using CREATE all proxies + SafeSingleton + SafeProxyFactory. This means the first set of contract addresses are used up since CREATE contract addresses are deterministic on (sender, account nonce). \r\n\r\nHowever, the nonce increment isn't captured in the state dump in foundry, since it skips persisting account slots for the sender of the forge script.\r\nsee \r\nhttps://github.com/foundry-rs/foundry/blob/1197fbea0b0f9dde45579a61a5ff956fc0aee426/crates/cheatcodes/src/evm.rs#L134\r\n\r\nsome options\r\n- manually bump the nonces in the allocs for the dev accounts. this can become brittle as the number of contracts deployed using CREATE increases.\r\n- use a fresh set of dev accounts to deploy each chain <= easiest\r\n- update the deploy script in the monorepo to use CREATE2", "2024-08-09T09:01:00Z", "2024-08-09T09:02:46Z", "jakim929", "2025-08-30 18:38:29"]
["IC_kwDOMMiGhs6CgGoE", "I_kwDOMMiGhs6NWyvu", "should this be split into two parts? the first being a static analysis and the second being the PR into anvil to support `debug_traceCall`?", "2024-06-25T16:50:39Z", "2024-06-25T16:50:39Z", "tremarkley", "2025-08-30 18:38:29"]
["IC_kwDOMMiGhs6Cizej", "I_kwDOMMiGhs6NWyvu", "@tremarkley yeah I think that would be ok. The reason I didn't do so was I felt like we probably wouldn't release the tool until we can simulate, so I wondered whether we should spend engineering time on the static analysis part. What do you think?", "2024-06-25T23:07:25Z", "2024-06-25T23:07:25Z", "fainashalts", "2025-08-30 18:38:29"]
["IC_kwDOMMiGhs6Ci7YH", "I_kwDOMMiGhs6NWyvu", "@fainashalts yeah, I think the right call is to prioritize adding `debug_traceCall` to Anvil. If for some reason we can't get that supported or it significantly increases the scope of this, then we can consider fallback options (e.g. static analysis)", "2024-06-25T23:24:47Z", "2024-06-25T23:24:47Z", "tremarkley", "2025-08-30 18:38:29"]
["IC_kwDOFpg0Ns574ySh", "I_kwDOFpg0Ns58QY7G", "This issue has been automatically marked as stale and will be closed in 7 days if no updates", "2024-04-26T01:46:01Z", "2024-04-26T01:46:01Z", "github-actions", "2025-08-30 18:40:15"]
["IC_kwDOFpg0Ns6Iy9jV", "I_kwDOFpg0Ns58QY7G", "This issue was closed as stale.  Please reopen if this is a mistake", "2024-08-18T01:55:17Z", "2024-08-18T01:55:17Z", "github-actions", "2025-08-30 18:40:15"]
["IC_kwDOH2Qg5s6IN4hz", "I_kwDOH2Qg5s6SAUjl", "Hello @Madhulearn6, this should be addressed now", "2024-08-13T04:57:32Z", "2024-08-13T04:57:32Z", "tynes", "2025-08-30 18:40:34"]
["IC_kwDOH2Qg5s6IPFt1", "I_kwDOH2Qg5s6SAUjl", "Thank you very much @tynes. The issue can now be closed from our end \ud83d\ude4f ", "2024-08-13T08:23:54Z", "2024-08-13T08:23:54Z", "sameersubudhi", "2025-08-30 18:40:34"]
["IC_kwDOH2Qg5s6IUrvK", "I_kwDOH2Qg5s6P0-bt", "Yes, it's working and we use it extensively. Note that `path` is only available for full nodes, not archival.", "2024-08-13T20:56:14Z", "2024-08-13T20:56:14Z", "sebastianst", "2025-08-30 18:40:34"]
["IC_kwDOJ_r-bs6IVeb6", "I_kwDOJ_r-bs6SgWwH", "Completed via [this monorepo pr](https://github.com/ethereum-optimism/optimism/pull/11459) and [this superchain-registry pr](https://github.com/ethereum-optimism/superchain-registry/pull/469).", "2024-08-13T22:56:59Z", "2024-08-13T22:56:59Z", "bitwiseguy", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6HXu5A", "I_kwDOJ_r-bs6RqUW1", "We could prepare a PR with the breaking changes soon, and this would help inform the documentation. \r\n\r\nThe guiding principle should be to make minimal changes such that superchain-registry does not mention plasma at all. ", "2024-08-06T12:13:52Z", "2024-08-06T12:13:52Z", "geoknee", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6HrpZx", "I_kwDOJ_r-bs6RqUW1", "WIP notion doc [here](https://www.notion.so/oplabs/Migrating-plasma-users-altda-0b9c886cf20c4f8995c0e48ac5a3270b)", "2024-08-08T17:54:34Z", "2024-08-08T17:54:34Z", "bitwiseguy", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6IQoRY", "I_kwDOJ_r-bs6RqUW1", "@bitwiseguy can we call this done? Let's coordinate with devrel about communicating this to the appropriate partners.", "2024-08-13T11:50:48Z", "2024-08-13T11:50:48Z", "geoknee", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6IkfY3", "I_kwDOJ_r-bs6RqUW1", "Completed this after merging monorepo commit [here](https://github.com/ethereum-optimism/optimism/commit/0bb2ff57c8133f1e3983820c0bf238001eca119b) and superchain-registry commit [here](https://github.com/ethereum-optimism/superchain-registry/commit/0c96567855b9a605bd8028b63d86b7296e6ec305)", "2024-08-15T13:26:47Z", "2024-08-15T13:26:47Z", "bitwiseguy", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6Ia86k", "I_kwDOJ_r-bs6Rlpw1", "The work on it is mostly done, we need 1 final review to get it over the line.", "2024-08-14T13:34:06Z", "2024-08-14T13:34:06Z", "BlocksOnAChain", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6HWXo2", "I_kwDOJ_r-bs6Rli4W", "We also only need to run genesis checks as a one off when the chain is added, there isn't much point running drift detection on this. \n\nOne-off checks can be run only in CI when we detect a new chain being added. ", "2024-08-06T09:13:32Z", "2024-08-06T09:20:54Z", "geoknee", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6HaAY8", "I_kwDOJ_r-bs6Rli4W", "Design doc in progress here https://www.notion.so/oplabs/Superchain-Registry-Check-Routing-0f4f734c159a48c3abfc0d6a82f07faf ", "2024-08-06T17:00:26Z", "2024-08-06T17:00:26Z", "geoknee", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6HgYaj", "I_kwDOJ_r-bs6Rli4W", "An early step towards closing this ticket would be:\n* add a `just ` command for running _only_ unit tests. This can use a regex to filter out validation checks\n* change the CI config so that unit tests run in a different step to validation checks", "2024-08-07T12:57:53Z", "2024-08-07T12:57:53Z", "geoknee", "2025-08-30 18:40:55"]
["IC_kwDOJ_r-bs6HgeNV", "I_kwDOJ_r-bs6Rli4W", "A subsequent step would be to:\n* detect which chains were modified using `git diff`\n* only run validation checks for those chains", "2024-08-07T13:09:16Z", "2024-08-07T13:09:16Z", "geoknee", "2025-08-30 18:40:55"]
["IC_kwDODjvEJM6ITnkC", "I_kwDODjvEJM6S3J0O", "@spacesailor24 \ud83d\udcaa", "2024-08-13T18:14:25Z", "2024-08-13T18:14:25Z", "SantiagoDevRel", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6ITnl1", "I_kwDODjvEJM6S3J0O", "@spacesailor24  maybe you can help.  Thank you", "2024-08-13T18:14:28Z", "2024-08-13T18:14:28Z", "Redoudou", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Ic-9N", "I_kwDODjvEJM6S3J0O", "Hey @SantiagoDevRel, we are no longer maintaining this package. The latest hardfork likely broke it due to modifications to how gas pricing works", "2024-08-14T16:48:15Z", "2024-08-14T16:48:15Z", "tynes", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6IdBk5", "I_kwDODjvEJM6S3J0O", "Ohh ok @tynes , we will take out the plugin for the Africa tour hackathon, thanks for letting me know", "2024-08-14T16:52:44Z", "2024-08-14T16:52:44Z", "SantiagoDevRel", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Ie8HK", "I_kwDODjvEJM6S3J0O", "@tynes we would love to continue supporting the Optimism for Web3js users. We use the Op stack at Chainsafe. Who would the right person we could talk at Op to discuss options ?", "2024-08-14T20:26:03Z", "2024-08-14T20:26:31Z", "Redoudou", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6ISGnN", "I_kwDODjvEJM6SzfTp", "Hi! Sorry for the trouble, had a quick look and have reboot the services responsible for contract verification on OP. Feel free to retry, it should work now \ud83d\ude4f ", "2024-08-13T14:49:06Z", "2024-08-13T14:49:06Z", "Enigmatic331", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6ISNg9", "I_kwDODjvEJM6SzfTp", "> Hi! Sorry for the trouble, had a quick look and have reboot the services responsible for contract verification on OP. Feel free to retry, it should work now \ud83d\ude4f\r\n\r\nworks now! thanks!!", "2024-08-13T15:01:43Z", "2024-08-13T15:01:48Z", "hellwolf", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6IKFeM", "I_kwDODjvEJM6SoyGu", "This is a non issue and is the desired behavior per EIP-6780", "2024-08-12T15:54:30Z", "2024-08-12T15:54:55Z", "tynes", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Hzj5D", "I_kwDODjvEJM6Sh2Tg", "It looks like with the changes, need to add this to my path for it to find it\r\n\r\n\r\n", "2024-08-09T17:47:26Z", "2024-08-09T17:47:26Z", "imnotanoob", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6IYiAf", "I_kwDODjvEJM6Sh2Tg", "how did you fix this i also getting same error ?\r\n\r\n> It looks like with the changes, need to add this to my path for it to find it\r\n\r\n", "2024-08-14T08:19:35Z", "2024-08-14T08:19:35Z", "priyanshuthapliyal55", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6H1Kh1", "I_kwDODjvEJM6ShKmB", "The timestamp deterministically increments by 2 seconds each block", "2024-08-09T23:07:27Z", "2024-08-09T23:07:27Z", "tynes", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6H1KvJ", "I_kwDODjvEJM6ShKmB", "You can see here: https://github.com/ethereum-optimism/op-geth/blob/ee83530a242132150c9f067e6c4135f67ab4bba2/internal/ethapi/api.go#L889", "2024-08-09T23:08:32Z", "2024-08-09T23:08:32Z", "tynes", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6IGfcS", "I_kwDODjvEJM6ShKmB", "Thanks for clarifying, can you please point to where the block timestamp is set?", "2024-08-12T08:40:11Z", "2024-08-12T08:40:11Z", "Mattiatore", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Ii2Wr", "I_kwDODjvEJM6SgkvU", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/11490", "2024-08-15T07:50:03Z", "2024-08-15T07:50:03Z", "protolambda", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6HXUmN", "I_kwDODjvEJM6R_Xt9", "I think some logic in solidity (such as dump json allocs) can replaced to golang native code.\r\nDeployment process using solidity is super slow", "2024-08-06T11:14:48Z", "2024-08-07T01:25:45Z", "gosunuts", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Ifn5l", "I_kwDODjvEJM6Rpz5H", "I'm looking to work on this!\r\n\r\nWhat's the reasoning for removing it? Would it be replaced with a hard-coded value?", "2024-08-14T21:58:22Z", "2024-08-14T21:58:22Z", "JacobHomanics", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6IgF92", "I_kwDODjvEJM6Rpz5H", "@Hotmanics The aim is to be able to reuse the same `FaultDisputeGame` (and `PermissionedDisputeGame`) contracts as the gameImpl in `DisputeGameFactory` proxies for different L2 chains. That way we avoid needing to redeploy the `FaultDisputeGame` contract for every different chain.  Each created game creates a proxy to that implementation so the chains are still completely independent, it's just easier to manage deployments if they share the same implementation contract.\r\n\r\nSo hard coding the L2 block number won't work either, it likely needs to be set as an init parameter on the `DisputeGameFactory` and then passed as one of the arguments when creating dispute games (like the root claim, parent hash and extra data [are passed through now](https://github.com/ethereum-optimism/optimism/blob/4be7223a88051291903d45270380b602abd6f9a7/packages/contracts-bedrock/src/dispute/DisputeGameFactory.sol#L116)).", "2024-08-15T00:14:01Z", "2024-08-15T00:14:01Z", "ajsutton", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Ii2dq", "I_kwDODjvEJM6RV0rL", "Closing in favor of the Go forge approach.", "2024-08-15T07:50:30Z", "2024-08-15T07:50:30Z", "protolambda", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Ii2kA", "I_kwDODjvEJM6Q_xeg", "Closing in favor of the Go forge approach", "2024-08-15T07:50:52Z", "2024-08-15T07:50:52Z", "protolambda", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6HRkQb", "I_kwDODjvEJM6QkKTX", "@tynes + @smartcontracts  to give another pass before scheduling another design review meeting", "2024-08-05T17:08:13Z", "2024-08-05T17:08:13Z", "tynes", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6F3o-b", "I_kwDODjvEJM6QYB7F", "Related: the `op_proposer_default_refs_number` metrics needs to be updated on startup, not only after the first proposal, for alerts to be more reliable across restarts.", "2024-07-23T18:27:20Z", "2024-07-23T18:27:20Z", "protolambda", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6ISIlf", "I_kwDODjvEJM6MkDnw", "Both PRs have been merged, marking this as Done!", "2024-08-13T14:52:45Z", "2024-08-13T14:52:45Z", "skeletor-spaceman", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6IU7b5", "I_kwDODjvEJM6MirLL", "This is not part of interop, but is not completed yet. Reopening", "2024-08-13T21:41:37Z", "2024-08-13T21:41:37Z", "protolambda", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6GpGo9", "I_kwDODjvEJM6MirVU", "Tracking polishing separately now, this is done", "2024-07-30T18:08:31Z", "2024-07-30T18:08:31Z", "protolambda", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6GLryr", "I_kwDODjvEJM6Mirbs", "@tynes we think this task is outdated and should be deprecated. How should we handle it?", "2024-07-25T19:03:07Z", "2024-07-25T19:03:24Z", "0xParti", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6IdHk3", "I_kwDODjvEJM6Mirbs", "closing this issue, since we already went with the superc20 vanilla design, while keeping SuperOPxERC20 as a custom implementation.", "2024-08-14T17:05:53Z", "2024-08-14T17:05:53Z", "skeletor-spaceman", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Idv0U", "I_kwDODjvEJM6Mirbs", "@skeletor-spaceman where's the best place to follow the work on SuperOPxERC20?", "2024-08-14T18:06:28Z", "2024-08-14T18:06:28Z", "stas", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6Iew94", "I_kwDODjvEJM6Mirbs", "hey @stas ! I just created the parent ticket for the SuperOPxERC20 standard :)\r\nYou'll be able to follow progress there: https://github.com/ethereum-optimism/optimism/issues/11485\r\nFirst we need the Custom Implementation Factory, to then create the xERC20 flavor on top :)", "2024-08-14T20:01:23Z", "2024-08-14T20:01:23Z", "skeletor-spaceman", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6IfXwo", "I_kwDODjvEJM6Mirbs", "Ty ty! Excited!!!", "2024-08-14T21:20:03Z", "2024-08-14T21:20:03Z", "stas", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6BuBZg", "I_kwDODjvEJM6MrAYl", "@sebastianst a few questions:\r\n\r\n1. should we implement a way to call the geth json rpc method `txpool_content` to retrieve pending txs or are you thinking that we should track pending txs in-memory (i.e. we could add a `pendingTxs` map `nonce --> tx` to the `SimpleTxManager` struct)?\r\n2. Are you expecting the methods you mentioned (e.g. `getPendingTxs()` and `cancelSend(nonce)`) to be accessible, after abstraction layers, through (a) cli commands, (b) json rpc requests, or (c) a different way? \r\n    * Edit: Seems like we probably want to add `admin_*` methods to the json rpc server on the `op-proposer`, `op-challenger`. Those two services already have `admin` api namespaces. Should we also add an `admin` api to the `op-node` and add the same methods? It doesn't look like the `op-node` currently has a `TxMgr` \ud83e\udd14 ", "2024-05-22T01:06:58Z", "2024-05-22T02:38:40Z", "bitwiseguy", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6BuBZ1", "I_kwDODjvEJM6MrAYl", "@bitwiseguy \r\n1. this is only about locally pending txs in the txmgr, not what's in the remote L1's tx pool\r\n2. This is about adding json-rpcs in the `admin_*` namespace. This should be done in a reusable way in the `txmgr` package so that other services that use it can add it to their admin ns. The `op-node` doesn't send txs, so it doesn't have a txmgr. Only the batcher, proposer, and challenger have a txmgr.", "2024-05-22T13:11:40Z", "2024-05-22T13:11:58Z", "sebastianst", "2025-08-30 18:41:04"]
["IC_kwDODjvEJM6FrZry", "I_kwDODjvEJM6MrAYl", "we will try to close this one out in Cycle 10.", "2024-07-22T11:38:49Z", "2024-07-22T11:38:49Z", "BlocksOnAChain", "2025-08-30 18:41:04"]
["IC_kwDOMMiGhs6IU_qe", "I_kwDOMMiGhs6S4eym", "what kind of errors are you seeing?", "2024-08-13T21:55:08Z", "2024-08-13T21:55:08Z", "hamdiallam", "2025-08-30 18:41:09"]
["IC_kwDOMMiGhs6IVAY_", "I_kwDOMMiGhs6S4eym", "The one I have noticed is the `TestStartup` test in `orchestrator_test.go`\r\n```\r\n=== RUN   TestStartup\r\n    orchestrator.go:55:         INFO [08-13|17:35:42.892] starting orchestrator\r\n    orchestrator_test.go:48: \r\n        \tError Trace:\t/home/runner/work/supersim/supersim/orchestrator/orchestrator_test.go:48\r\n        \tError:      \tNot equal: \r\n        \t            \texpected: 0x386\r\n        \t            \tactual  : 0x385\r\n        \tTest:       \tTestStartup\r\n    orchestrator.go:80:         INFO [08-13|17:35:43.224] stopping orchestrator\r\n    anvil.go:157:               ERROR[08-13|17:35:43.226] anvil terminated with an error           role=anvil name=OPChainA chain.id=901 error=\"signal: killed\"\r\n    anvil.go:157:               ERROR[08-13|17:35:43.227] anvil terminated with an error           role=anvil name=OPChainB chain.id=902 error=\"signal: killed\"\r\n    anvil.go:157:               ERROR[08-13|17:35:43.228] anvil terminated with an error           role=anvil name=L1 chain.id=900 error=\"signal: killed\"\r\n--- FAIL: TestStartup (0.34s)\r\nFAIL\r\nFAIL\tgithub.com/ethereum-optimism/supersim/orchestrator\t0.361s\r\nFAIL\r\nerror: Recipe `test-go` failed on line 17 with exit code 1\r\n```", "2024-08-13T21:57:44Z", "2024-08-13T21:59:39Z", "tremarkley", "2025-08-30 18:41:09"]
["IC_kwDOMMiGhs6IVA_y", "I_kwDOMMiGhs6S4eym", "hmm that's failing on the chain id comparison which is hardcoded. Sounds like a hidden but legitimate bug ", "2024-08-13T21:59:51Z", "2024-08-13T21:59:51Z", "hamdiallam", "2025-08-30 18:41:09"]
["IC_kwDOMMiGhs6IVBWS", "I_kwDOMMiGhs6S4eym", "I noticed that it checks for the chain id at index 0. does the order matter or should we just check that the array contains the chain id? ", "2024-08-13T22:01:01Z", "2024-08-13T22:01:18Z", "tremarkley", "2025-08-30 18:41:09"]
["IC_kwDOMMiGhs6IVDda", "I_kwDOMMiGhs6S4eym", "ahh yea maybe the order here is messed up. Esp since the error is showing off-by-one.\r\n\r\nHmm since we're checking the config of the chain as well, maybe we just sort the list in the test prior to checking?", "2024-08-13T22:05:17Z", "2024-08-13T22:05:17Z", "hamdiallam", "2025-08-30 18:41:09"]
["IC_kwDOMMiGhs6GgYI_", "I_kwDOMMiGhs6RM1FV", "cc @jakim929 \n\nPerhaps by default we dont apply these changes to forked networks. Instead we have sets of experimental flags that patch remote network state `supersim fork --experiment.interop`.", "2024-07-29T18:31:49Z", "2024-07-29T18:31:49Z", "hamdiallam", "2025-08-30 18:41:09"]
["IC_kwDOKIwiaM6JB57b", "I_kwDOKIwiaM6QpO18", "closed, related to issue #804 ", "2024-08-20T14:17:35Z", "2024-08-20T14:17:35Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6BdCeN", "I_kwDOKIwiaM6MbUVk", "#748", "2024-06-16T20:47:28Z", "2024-06-16T20:47:28Z", "Sintayew4", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6JCBYZ", "I_kwDOKIwiaM6MbUVk", "@Sintayew4 you can go ahead and add Optimize to the [Node Providers community listing](https://github.com/ethereum-optimism/developers/blob/main/community/tools/node-providers.md). You'll need a brief product description in addition to the info above \ud83d\udc9f ", "2024-08-20T14:30:45Z", "2024-08-20T14:30:45Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6BdCOM", "I_kwDOKIwiaM6MbUA9", "- [x] 747", "2024-06-16T20:42:39Z", "2024-06-16T20:42:46Z", "Sintayew4", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6BdCPT", "I_kwDOKIwiaM6MbUA9", "> ### Faucet Name\r\n> Optimize\r\n> \r\n> ### Faucet Description\r\n> ### Faucet URL\r\n> https://github.com/ethereum-optimism/developers/blob/main/community/tools/faucets.md\r\n> \r\n> ### Supported Networks\r\n> * [x]  OP Goerli\r\n> * [x]  OP Sepolia\r\n> * [x]  Base Goerli\r\n> * [x]  Base Sepolia\r\n> * [x]  Lyra Sepolia\r\n> * [x]  Mode Sepolia\r\n> * [x]  Orderly Sepolia\r\n> * [x]  PGN Sepolia\r\n> * [x]  Zora Sepolia\r\n> \r\n> ### When did the product go live?\r\n> Yes\r\n> \r\n> ### Is the product open source?\r\n> Yes\r\n> \r\n> ### GitHub URL\r\n> https://github.com/ethereum-optimism/developers/blob/main/community/tools/faucets.md\r\n> \r\n> ### Additional context\r\n\r\n", "2024-06-16T20:43:06Z", "2024-06-16T20:43:06Z", "Sintayew4", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6JCDFA", "I_kwDOKIwiaM6MbUA9", "@Sintayew4 You can go ahead and add your faucet to the [community page](https://github.com/ethereum-optimism/developers/blob/main/community/tools/faucets.md), using a PR. You will also need a product description in addition to the info above. \ud83d\udc9f ", "2024-08-20T14:33:42Z", "2024-08-20T14:33:42Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6BdCFZ", "I_kwDOKIwiaM6MbTix", "#746", "2024-06-16T20:40:32Z", "2024-06-16T20:40:32Z", "Sintayew4", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6JCDgD", "I_kwDOKIwiaM6MbTix", "blank request. closing", "2024-08-20T14:34:30Z", "2024-08-20T14:34:30Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM5-i8ik", "I_kwDOKIwiaM6JdYvU", "@rbio7 thanks for the question! The deployment rebate is an initial experiment to learn if developers find free deployments valuable. If we learn that developers find this feature valuable, we will iterate on this product to make it more seamless. \r\n\r\nWhat other KYC method would you find most valuable? I'll be happy to pass the information along to our team! \ud83d\udc9f ", "2024-05-21T17:18:40Z", "2024-05-21T17:18:40Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM59GgBT", "I_kwDOKIwiaM6IHyft", "@pifafu wondering if you can assist with this? (for the graphic)", "2024-05-07T16:28:56Z", "2024-05-07T21:05:14Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM592dJ8", "I_kwDOKIwiaM6IHyft", "Yep, adding it to my backlog!\r\n\r\nI'll spin up a Figma and we can async align on what needs to be updated to reflect reality. Coming soon.", "2024-05-15T01:40:29Z", "2024-05-15T01:40:29Z", "pifafu", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6JCQ1h", "I_kwDOKIwiaM6IHyft", "image removed for now", "2024-08-20T14:58:32Z", "2024-08-20T14:58:32Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM57og9J", "I_kwDOKIwiaM6DLxL5", "can you please assign to me, i want to handle this", "2024-04-24T07:02:21Z", "2024-04-24T07:02:21Z", "estheroche", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM583cDg", "I_kwDOKIwiaM6DLxL5", "@estheroche it's all yours! \ud83d\udc9f ", "2024-05-05T18:39:32Z", "2024-05-05T18:39:32Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM59B1cB", "I_kwDOKIwiaM6DLxL5", "@cpengilly thank you\r\n", "2024-05-07T07:29:11Z", "2024-05-07T07:29:11Z", "estheroche", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6JCcm2", "I_kwDOKIwiaM6DLxL5", "pausing while working out details @estheroche we have the PR still pending and might use some of the content/details. we'll keep you posted!", "2024-08-20T15:19:25Z", "2024-08-20T15:19:25Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM6JDuX-", "I_kwDOKIwiaM6DLxL5", "@cpengilly alright will be waiting for your feedback", "2024-08-20T18:12:24Z", "2024-08-20T18:12:24Z", "estheroche", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM51N2ee", "I_kwDOKIwiaM56A7tV", "^ Note that there will be additional fields post-Ecotone as well (todo: figure out what's in transactions, vs what isn't)", "2024-02-27T13:34:56Z", "2024-02-27T13:34:56Z", "MSilb7", "2025-08-30 19:42:56"]
["IC_kwDOKIwiaM52KRY_", "I_kwDOKIwiaM56A7tV", "@ZakAyesh @sbvegan let's see if we can get this taken care of this month (Q1) \ud83d\ude4f\ud83c\udffe ", "2024-03-07T05:35:01Z", "2024-03-07T05:35:01Z", "cpengilly", "2025-08-30 19:42:56"]
["IC_kwDOH2Qg5s6Ie0ke", "I_kwDOH2Qg5s6S4NuG", "It happened again a couple of times. That's a log excerpt from inactive sepolia seq-2.\r\n\r\n```t=2024-08-14T19:54:49+0000 lvl=info msg=\"Starting geth on an OP network...\" network=op-sepolia\r\nt=2024-08-14T19:54:49+0000 lvl=info msg=\"Enabling metrics collection\"\r\nt=2024-08-14T19:54:49+0000 lvl=info msg=\"Enabling stand-alone metrics HTTP endpoint\" address=0.0.0.0:6060\r\nt=2024-08-14T19:54:49+0000 lvl=info msg=\"Starting metrics server\" addr=http://0.0.0.0:6060/debug/metrics\r\nt=2024-08-14T19:54:49+0000 lvl=info msg=\"Maximum peer count\" ETH=30 total=30\r\nt=2024-08-14T19:54:49+0000 lvl=info msg=\"Smartcard socket not found, disabling\" err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\r\nt=2024-08-14T19:54:49+0000 lvl=info msg=\"Using pebble as db engine\"\r\nt=2024-08-14T19:54:49+0000 lvl=info msg=\"Set global gas cap\" cap=50000000\r\nt=2024-08-14T19:54:50+0000 lvl=info msg=\"Initializing the KZG library\" backend=gokzg\r\nt=2024-08-14T19:54:50+0000 lvl=info msg=\"Allocated trie memory caches\" clean=\"1.20 GiB\" dirty=\"2.00 GiB\"\r\nt=2024-08-14T19:54:50+0000 lvl=info msg=\"Using pebble as the backing database\"\r\nt=2024-08-14T19:54:50+0000 lvl=info msg=\"Allocated cache and file handles\" database=/db/geth/chaindata cache=\"4.00 GiB\" handles=524288\r\nt=2024-08-14T19:54:51+0000 lvl=info msg=\"Opened ancient database\" database=/db/geth/chaindata/ancient/chain readonly=false\r\nt=2024-08-14T19:54:51+0000 lvl=info msg=\"State scheme set by user\" scheme=path\r\nt=2024-08-14T19:54:51+0000 lvl=warn msg=\"Sanitizing invalid node buffer size\" provided=\"2.00 GiB\" updated=\"256.00 MiB\"\r\nt=2024-08-14T19:54:51+0000 lvl=info msg=\"Failed to load journal, discard it\" err=\"unexpected journal version want 1 got 0\"\r\nt=2024-08-14T19:54:51+0000 lvl=info msg=\"Opened ancient database\" database=/db/geth/chaindata/ancient/state readonly=false\r\nt=2024-08-14T19:54:51+0000 lvl=warn msg=\"Truncating freezer table\" database=/db/geth/chaindata/ancient/state table=account.index items=3889669 limit=3887492\r\nt=2024-08-14T19:54:51+0000 lvl=warn msg=\"Truncating freezer table\" database=/db/geth/chaindata/ancient/state table=storage.index items=3889669 limit=3887492\r\nt=2024-08-14T19:54:51+0000 lvl=warn msg=\"Truncating freezer table\" database=/db/geth/chaindata/ancient/state table=account.data items=3889669 limit=3887492\r\nt=2024-08-14T19:54:51+0000 lvl=warn msg=\"Truncating freezer table\" database=/db/geth/chaindata/ancient/state table=storage.data items=3889669 limit=3887492\r\nt=2024-08-14T19:54:51+0000 lvl=warn msg=\"Truncating freezer table\" database=/db/geth/chaindata/ancient/state table=history.meta items=3889669 limit=3887492\r\nt=2024-08-14T19:54:51+0000 lvl=warn msg=\"Truncated extra state histories\" number=2177\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=---------------------------------------------------------------------------------------------------------------------------------------------------------\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"Chain ID:  11155420 (OP Sepolia Testnet)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"Consensus: Optimism\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"Pre-Merge hard forks (block based):\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Homestead:                   #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/homestead.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Tangerine Whistle (EIP 150): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/tangerine-whistle.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Spurious Dragon/1 (EIP 155): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/spurious-dragon.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Spurious Dragon/2 (EIP 158): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/spurious-dragon.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Byzantium:                   #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/byzantium.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Constantinople:              #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/constantinople.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Petersburg:                  #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/petersburg.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Istanbul:                    #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/istanbul.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Muir Glacier:                #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/muir-glacier.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Berlin:                      #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/berlin.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - London:                      #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/london.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Arrow Glacier:               #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/arrow-glacier.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Gray Glacier:                #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/gray-glacier.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"Merge configured:\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Hard-fork specification:    https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/paris.md\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Network known to be merged: true\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Total terminal difficulty:  0\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Merge netsplit block:       #0       \"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"Post-Merge hard forks (timestamp based):\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Shanghai:                    @1699981200 (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/shanghai.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Cancun:                      @1708534800 (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/cancun.md)\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Regolith:                    @0         \"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Canyon:                      @1699981200\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Ecotone:                     @1708534800\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Fjord:                       @1716998400\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\" - Granite:                     @1723478400\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"\"\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=---------------------------------------------------------------------------------------------------------------------------------------------------------\r\nt=2024-08-14T19:55:12+0000 lvl=info msg=\"\"\r\nt=2024-08-14T19:55:13+0000 lvl=info msg=\"Loaded most recent local block\" number=15931368 hash=0x3c42264d97297745ee9fdb8de3583e0abb86b9328b8e5ff1d0be5998e5dc7f9a td=0 age=37s\r\nt=2024-08-14T19:55:13+0000 lvl=info msg=\"Loaded most recent local finalized block\" number=15930652 hash=0xcacd6a6921079b6f87d05e81dbdf53616dc2ae92162817f56bf6ad1843b6f4fb td=0 age=24m29s\r\nt=2024-08-14T19:55:13+0000 lvl=info msg=\"Loaded last snap-sync pivot marker\" number=12041571\r\nt=2024-08-14T19:55:13+0000 lvl=warn msg=\"Head state missing, repairing\" number=15931368 hash=0x3c42264d97297745ee9fdb8de3583e0abb86b9328b8e5ff1d0be5998e5dc7f9a snaproot=0x3bcc4bbbdf638894f82d4dcf10e1654a357266c765344863b94f39b5b18f2574\r\nt=2024-08-14T19:55:31+0000 lvl=info msg=\"Block state missing, rewinding further\" number=15930447 hash=0x389279f3c431ee5b8e53ef16d560538b8df0bc6f83811c013ca5183d3cfed4fa elapsed=18.412s\r\nt=2024-08-14T19:55:49+0000 lvl=info msg=\"Block state missing, rewinding further\" number=15929815 hash=0xe23fda5a618c5a1631a3a5ce430bea581dbf6e93b49a252cfce3f209a6943b7c elapsed=36.009s\r\nt=2024-08-14T19:55:49+0000 lvl=info msg=\"Rewound to block with state\" number=15929063 hash=0xa574d6ddc91d920271ae3e15e61f6e43f61d70ff9a314cfcc9c21653e3a600fb\r\nt=2024-08-14T19:55:51+0000 lvl=error msg=\"Error in block freeze operation\" err=\"canonical hash missing, can't freeze block 15930535\"\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Loaded most recent local block\" number=15929063 hash=0xa574d6ddc91d920271ae3e15e61f6e43f61d70ff9a314cfcc9c21653e3a600fb td=0 age=1h18m24s\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Loaded most recent local finalized block\" number=15930652 hash=0xcacd6a6921079b6f87d05e81dbdf53616dc2ae92162817f56bf6ad1843b6f4fb td=<nil> age=25m26s\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Loaded last snap-sync pivot marker\" number=12041571\r\nt=2024-08-14T19:56:10+0000 lvl=warn msg=\"Enabling snapshot recovery\" chainhead=15929063 diskbase=15930615\r\nt=2024-08-14T19:56:10+0000 lvl=warn msg=\"Snapshot is not continuous with chain\" snaproot=0x839974d8a9d9852588fd32eebfde41e3f07f2d0b4806c953db41a44c22137807 chainroot=0xaef4e98a80d75974e247db4e0a9bb355657d404955416f7652ae49333cf87c55\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Initialized transaction indexer\" range=\"last 2350000 blocks\"\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Initialising Ethereum protocol\" network=11155420 dbversion=8\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Loaded local transaction journal\" transactions=951 dropped=0\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Regenerated local transaction journal\" transactions=811 accounts=145\r\nt=2024-08-14T19:56:10+0000 lvl=warn msg=\"Switch sync mode from snap sync to full sync\"\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Gasprice oracle is ignoring threshold set\" threshold=2\r\nt=2024-08-14T19:56:10+0000 lvl=warn msg=\"Engine API enabled\" protocol=eth\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Starting peer-to-peer node\" instance=Geth/v1.101407.0-rc.1-ee83530a-20240808/linux-amd64/go1.22.6\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"New local node record\" seq=1719261383223 id=89b03f986538b49bad99202e4429aac50b3870c6f1e14c8e9fe27ac9d5536c71 ip=10.17.64.82 udp=30303 tcp=30304\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Started P2P networking\" self=\"enode://1a5d373dbc1cf14646d68dc6cd396fea57409e51088a0249ce96c5daeeb5453421f59c6dc3ce67cc60450ea8acc95cd8a44bcd4f4fdc25f6ed55901e7b4756ed@10.17.64.82:30304?discport=30303\"\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"IPC endpoint opened\" url=/db/geth.ipc\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"Loaded JWT secret file\" path=/etc/op-geth/jwt-secret.txt crc32=0x815e9662\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"HTTP server started\" endpoint=[::]:8545 auth=false prefix=\"\" cors=* vhosts=*\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"WebSocket enabled\" url=ws://[::]:8546\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"WebSocket enabled\" url=ws://[::]:8551\r\nt=2024-08-14T19:56:10+0000 lvl=info msg=\"HTTP server started\" endpoint=[::]:8551 auth=true prefix=\"\" cors=localhost vhosts=localhost\r\nt=2024-08-14T19:56:11+0000 lvl=info msg=\"Indexed transactions\" blocks=2305 txs=14023 tail=13579064 elapsed=887.910ms\r\nt=2024-08-14T19:56:51+0000 lvl=error msg=\"Number of finalized block is missing\"\r\nt=2024-08-14T19:57:51+0000 lvl=error msg=\"Number of finalized block is missing\"\r\nt=2024-08-14T19:58:51+0000 lvl=error msg=\"Number of finalized block is missing\"\r\nt=2024-08-14T19:59:51+0000 lvl=error msg=\"Number of finalized block is missing\"\r\nt=2024-08-14T20:00:51+0000 lvl=error msg=\"Number of finalized block is missing\"\r\nt=2024-08-14T20:01:51+0000 lvl=error msg=\"Number of finalized block is missing\"\r\nt=2024-08-14T20:02:51+0000 lvl=error msg=\"Number of finalized block is missing\"\r\n```", "2024-08-14T20:09:02Z", "2024-08-14T20:09:48Z", "sebastianst", "2025-08-30 19:42:57"]
["IC_kwDOH2Qg5s6JRfV_", "I_kwDOH2Qg5s6S4NuG", "Related: https://github.com/ethereum/go-ethereum/issues/30119", "2024-08-21T21:44:28Z", "2024-08-21T21:44:28Z", "sebastianst", "2025-08-30 19:42:57"]
["IC_kwDOH2Qg5s6JU9dG", "I_kwDOH2Qg5s6S4NuG", "Fix implemented in #368 - adds journal v0 to v1 migration code. https://github.com/ethereum/go-ethereum/pull/28940 introduced a new journal format, but didn't add migration code.", "2024-08-22T07:29:03Z", "2024-08-22T11:35:52Z", "sebastianst", "2025-08-30 19:42:57"]
["IC_kwDODjvEJM6JZb64", "I_kwDODjvEJM6T0ngW", "You will need to experiment with the values that are able to work best for your workload. Perhaps try 200,000,000", "2024-08-22T16:11:54Z", "2024-08-22T16:11:54Z", "tynes", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6JRMCO", "I_kwDODjvEJM6TFD0-", "Closing in favor of tracking L1 and L2 dev genesis in a combined issue: https://github.com/ethereum-optimism/optimism/issues/11493", "2024-08-21T20:46:20Z", "2024-08-21T20:46:20Z", "protolambda", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6HjSZL", "I_kwDODjvEJM6SR4MR", "Blocked by Factory review", "2024-08-07T19:14:24Z", "2024-08-07T19:14:24Z", "0xParti", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6H1K2I", "I_kwDODjvEJM6SR4MR", "The decision has been made to go with createx", "2024-08-09T23:09:07Z", "2024-08-09T23:09:07Z", "tynes", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6IupZg", "I_kwDODjvEJM6SR4MR", "We will rollback to CREATE3 for the factory, but will introduce the CREATEX preinstall anyway\nhttps://github.com/ethereum-optimism/specs/pull/341", "2024-08-16T18:00:28Z", "2024-08-16T18:00:28Z", "0xParti", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6JRsFS", "I_kwDODjvEJM6SR4MR", "Since we are going with create3, it would be nice to just have an explanation in the specs of what create3 does and why it is used. We can use an existing create3 implementation like the t11s implementation", "2024-08-21T22:19:32Z", "2024-08-21T22:19:32Z", "tynes", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6HrCLT", "I_kwDODjvEJM6SGqQ6", "The intention here is to have only one authorized caller of each of the 2 functions at a time right? e.g `addDependency` cannot have multiple authorized callers at once...", "2024-08-08T16:18:41Z", "2024-08-08T16:18:41Z", "AmadiMichael", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6HrEgQ", "I_kwDODjvEJM6SGqQ6", "Yes exactly, just 1 account that can call `addDependency`/`removeDependency` that MAY or MAY NOT be different than the `owner` from `Ownable`. Then for the standard config definition, the account that can add remove dependencies MUST be the foundation multisig", "2024-08-08T16:24:34Z", "2024-08-08T16:24:34Z", "tynes", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6HrFHk", "I_kwDODjvEJM6SGqQ6", "Ah yes thanks, was about to ask that now, \r\nwhere can i find the foundation multisig address?", "2024-08-08T16:26:03Z", "2024-08-08T16:26:03Z", "AmadiMichael", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6HrFqp", "I_kwDODjvEJM6SGqQ6", "You can see in the docs [here](https://docs.optimism.io/chain/security/faq#op-mainnet-multisig) although we do not want to hardcode any addresses, it would be an argument to `initialize` at deploy time", "2024-08-08T16:27:21Z", "2024-08-08T16:27:21Z", "tynes", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6HrNHu", "I_kwDODjvEJM6SGqQ6", "> You can see in the docs [here](https://docs.optimism.io/chain/security/faq#op-mainnet-multisig) although we do not want to hardcode any addresses, it would be an argument to `initialize` at deploy time\r\n\r\nwill this be modifiable and if so, by whom?", "2024-08-08T16:45:36Z", "2024-08-08T16:45:36Z", "AmadiMichael", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6HrXUe", "I_kwDODjvEJM6Rogj3", "https://github.com/ethereum-optimism/optimism/pull/11369", "2024-08-08T17:10:54Z", "2024-08-08T17:10:54Z", "axelKingsley", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6JjmbK", "I_kwDODjvEJM6Rogj3", "This is done", "2024-08-23T21:06:24Z", "2024-08-23T21:06:24Z", "protolambda", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6JjUmS", "I_kwDODjvEJM6Q4zTc", "The design has been closed and the spec have been merged\nhttps://github.com/ethereum-optimism/specs/pull/332", "2024-08-23T20:05:19Z", "2024-08-23T20:05:19Z", "0xParti", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6GGeMS", "I_kwDODjvEJM6QzFcx", "Add logs when op-batcher is restarted now.\r\n```\r\nop-batcher-1  | t=2024-07-25T08:52:48+0000 lvl=info msg=\"added L2 block to local state\" block=0xd0b8f6e5d483c7b4cc0d6b2c69a0399d4dfb74482c2285f8244b67bb365767c5:782388 tx_count=1 time=1721897544\r\nop-batcher-1  | t=2024-07-25T08:52:48+0000 lvl=info msg=\"Created channel\" id=149c89bc5c1013e14e9c6807c3d05d06 l1Head=0x299086068aea45322763c1164f324e22ec349f757c939fa9bc5d1893bb6272b5:6372470 l1OriginLastClosedChannel=0xac0abc8b752304f651eebb6992094552310682b904a7933a6098a6f396da42f1:6368863 blocks_pending=26977 batch_type=1 compression_algo=zlib target_num_frames=6 max_frame_size=130043\r\nop-batcher-1  | t=2024-07-25T08:52:50+0000 lvl=info msg=\"Channel closed\" id=149c89bc5c1013e14e9c6807c3d05d06 blocks_pending=0 num_frames=1 input_bytes=30404 output_bytes=1126 l1_origin=0x4400bac20ff9022c4792f8550c4815d35ed0d364f00a1441385ea853927277ab:6372463 full_reason=\"channel full: max channel duration reached\" compr_ratio=0.03703460071043284 latest_l1_origin=0x4400bac20ff9022c4792f8550c4815d35ed0d364f00a1441385ea853927277ab:6372463\r\nop-batcher-1  | t=2024-07-25T08:52:50+0000 lvl=info msg=\"building Blob transaction candidate\" size=1126 last_size=1126 num_blobs=1\r\nop-batcher-1  | t=2024-07-25T08:52:50+0000 lvl=info msg=\"added L2 block to local state\" block=0xfecc33443276f1e9bcbe3fa08f72346f6f36aed0eb4216c612d4ca569d15f7b8:782389 tx_count=1 time=1721897546\r\n```", "2024-07-25T09:07:00Z", "2024-07-25T09:07:00Z", "ak-akiyama", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6GHDHx", "I_kwDODjvEJM6QzFcx", "I rechecked the log and found that a second reorganization had occurred.\r\n\r\nop-node\r\n```\r\nop-node-1  | t=2024-07-24T22:34:25+0000 lvl=warn msg=\"L2 reorg: existing unsafe block does not match derived attributes from L1\" err=\"random field does not match. expected: 0x6e8370bf72d4cd5f32edbaff32c4a6d5cd1e54fa76090077fa77a75213ee265d. got: 0x70938ed75680f6a49debdc4c9b685f512b0065f4d1f7f813870058e6747e007b\" unsafe=0xd09ae35c7696c54fa480ff76c06b8d897063809aeea734fd8f229752ee0e82f6:763848 pending_safe=0xe93925c3796d357bea787a98b64ed4ddd093f0723fd685e96c30842cc2e04766:736685 safe=0xe93925c3796d357bea787a98b64ed4ddd093f0723fd685e96c30842cc2e04766:736685\r\nop-node-1  | t=2024-07-24T22:34:25+0000 lvl=warn msg=\"engine is building block that reorgs previous unsafe head\" onto=0xe93925c3796d357bea787a98b64ed4ddd093f0723fd685e96c30842cc2e04766:736685 unsafe=0xd09ae35c7696c54fa480ff76c06b8d897063809aeea734fd8f229752ee0e82f6:763848\r\n```\r\n\r\nop-geth\r\n```\r\nop-geth-1  | WARN [07-24|22:34:25.949] Large chain reorg detected               number=736,685 hash=e93925..e04766 drop=27163 dropfrom=d09ae3..0e82f6 add=1 addfrom=f72855..453ba3\r\n```", "2024-07-25T10:25:14Z", "2024-07-25T10:25:14Z", "ak-akiyama", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6GKOLm", "I_kwDODjvEJM6QzFcx", "> If op-batcher is left down for a long time (more than a day), a large reorganization will occur in op-node and op-geth\r\n\r\nThis is expected behavior and is meant to guarantee liveness of the chain. If the sequencer window passes without the data being published, then \"deposit only\" blocks will be deterministically produced. This ensures that users can always interact with the chain in the case of the sequencer going down for long periods of time\r\n\r\n> Also, even if I start op-batcher afterwards and let it send Batch TX, the L2 safe/finalized head does not catch up to the latest block.\r\n\r\nNot exactly sure what is going on here, sometimes a large reorg for geth can take a very long time if it is not running as an archive node. This is due to it needing to recreate historical state through execution.", "2024-07-25T16:07:18Z", "2024-07-25T16:07:18Z", "tynes", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6GN2TU", "I_kwDODjvEJM6QzFcx", "Thanks for the reply.\r\n\r\n> This is expected behavior and is meant to guarantee liveness of the chain.\r\n\r\nDoes that mean that even on OP Mainnet, if op-batcher stops for a day, a large reorg (about 27,000 blocks) could occur?\r\n\r\n> geth can take a very long time if it is not running as an archive node.\r\n\r\nop-geth is set to full archive node `GETH_GCMODE=archive`.  The number of reorganized blocks is about 27,000, so it doesn't seem like it should take that long to recreate. In fact, currently, op-node and op-geth are monotonously creating blocks every 2 seconds.", "2024-07-26T01:20:08Z", "2024-07-26T01:20:34Z", "ak-akiyama", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6I9x5v", "I_kwDODjvEJM6QzFcx", "I encountered the same problem. After reorganize several times, 'safe_l2' is 227507 and 'unsafe_l2' is 252068 now, a difference of 24561 blocks. The warn info reported is the same as yours. I don't know what caused it and how to solve it? Please tell me what you known.", "2024-08-20T03:34:04Z", "2024-08-20T03:34:04Z", "lei335", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6JNFXo", "I_kwDODjvEJM6QzFcx", "@lei335\r\nIn my case, the issue was resolved after changing the `seq_window_size` in `rollup.json` to double its original value and then restarting both the op-node and op-batcher. It seems that this issue arises when the op-batcher remains stopped for a duration exceeding `l1_block_time * seq_window_size` seconds. If the Ethereum is selected for L1, this would be approximately 12 hours (12sec * 3600block).\r\n\r\n", "2024-08-21T12:15:44Z", "2024-08-21T12:15:44Z", "ak-akiyama", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6Jjm5O", "I_kwDODjvEJM6QGbL6", "Closing in favor of e2e tracking issue https://github.com/ethereum-optimism/optimism/issues/11280", "2024-08-23T21:08:12Z", "2024-08-23T21:08:12Z", "protolambda", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6JjlSl", "I_kwDODjvEJM6NplUn", "Implemented in https://github.com/ethereum-optimism/optimism/pull/11369", "2024-08-23T21:02:19Z", "2024-08-23T21:02:19Z", "protolambda", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6BnRnd", "I_kwDODjvEJM6Mirab", "There is a big design space here, its possible that we may want to limit the scope to supporting https://github.com/ethereum-optimism/optimism/issues/10899", "2024-06-17T22:40:21Z", "2024-06-17T22:40:21Z", "tynes", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6GLqeQ", "I_kwDODjvEJM6Mirab", "We are discussing the base design in the following [Design Doc PR](https://github.com/ethereum-optimism/design-docs/pull/50). We will move into a spec once the [Liquidity Migration Spec](https://github.com/ethereum-optimism/specs/pull/294) is merged.", "2024-07-25T18:59:55Z", "2024-07-25T18:59:55Z", "0xParti", "2025-08-30 19:43:15"]
["IC_kwDODjvEJM6IecNq", "I_kwDODjvEJM6Mirab", "https://github.com/ethereum-optimism/specs/pull/332", "2024-08-14T19:23:02Z", "2024-08-14T19:23:02Z", "0xParti", "2025-08-30 19:43:15"]
["IC_kwDOJ_r-bs6JSUKM", "I_kwDOJ_r-bs6TyMd5", "Nevermind, found it here: https://github.com/ethereum-optimism/superchain-registry/blob/198c713b0af1982abddfd0f15b8cee4df186505d/bindings/rust-bindings/etc/configs.toml#L43-L44", "2024-08-21T23:18:16Z", "2024-08-21T23:18:16Z", "ratankaliani", "2025-08-30 19:43:29"]
["IC_kwDOJ_r-bs6Ja7YF", "I_kwDOJ_r-bs6Tho3M", "I'd like to note that the original description mentioned `\"DisputeGameFactoryProxy\"` as a contract that contains immutables. However, the contract does not have immutables (amended description to remove this contract)", "2024-08-22T20:04:55Z", "2024-08-22T20:04:55Z", "vdamle", "2025-08-30 19:43:29"]
["IC_kwDOJ_r-bs6JRwOV", "I_kwDOJ_r-bs6QlGyU", "Closing this - the current method is likely unavoidable and has been working fairly gracefully minus requiring a rust-reviewer when configs change. This isn't necessarily a bad thing since it allows rust-reviewers the opportunity to bump the crate version and re-publish the crates when desired.", "2024-08-21T22:23:33Z", "2024-08-21T22:23:33Z", "refcell", "2025-08-30 19:43:29"]
["IC_kwDOJ_r-bs6HO5O0", "I_kwDOJ_r-bs6QNZVk", "> Assumption is that this shouldn\u2019t be a ton of work, \u201cjust a static list of implementation code hashes.\u201d No immutables because all standard chains are post-MCP. ", "2024-08-05T11:18:13Z", "2024-08-05T11:18:13Z", "geoknee", "2025-08-30 19:43:29"]
["IC_kwDOJ_r-bs6HO6cc", "I_kwDOJ_r-bs6QNZVk", "I suggest we change this mapping  https://github.com/ethereum-optimism/superchain-registry/blob/main/validation/standard/standard-versions.toml (from contract name to version string) to a mapping from contract name to bytecode hex string, with the version in a comment. Then it should not be a big lift to check the code deployed for each contract name, we just use [`eth_getCode`](https://ethereum.org/en/developers/docs/apis/json-rpc/#eth_getcode) instead of a call to the `.version()` method. ", "2024-08-05T11:20:57Z", "2024-08-05T11:20:57Z", "geoknee", "2025-08-30 19:43:29"]
["IC_kwDOJ_r-bs6HgfYB", "I_kwDOJ_r-bs6QNZVk", "Updating t shirt size since I realise we need to (sometimes) go through a proxy to get the bytecode over RPC. Adds a little complexity. ", "2024-08-07T13:11:38Z", "2024-08-07T13:11:38Z", "geoknee", "2025-08-30 19:43:29"]
["IC_kwDOJ_r-bs6I3X2n", "I_kwDOJ_r-bs6Mr3qm", "@geoknee is this one still something we plan to do for SCR?\n", "2024-08-19T10:17:42Z", "2024-08-19T10:17:42Z", "BlocksOnAChain", "2025-08-30 19:43:29"]
["IC_kwDOMMiGhs6JRHZi", "I_kwDOMMiGhs6TB8sZ", "Closed by #113 ", "2024-08-21T20:33:44Z", "2024-08-21T20:33:44Z", "hamdiallam", "2025-08-30 19:43:49"]
["IC_kwDOMMiGhs6JRPTI", "I_kwDOMMiGhs6RZGLb", "fixed with https://github.com/foundry-rs/foundry/pull/8704", "2024-08-21T20:55:33Z", "2024-08-21T20:55:33Z", "jakim929", "2025-08-30 19:43:49"]
["IC_kwDOMMiGhs6GgVwS", "I_kwDOMMiGhs6Qt3k1", "we can make anvil mine blocks on a timer (2s) instead of automine mode. However this will result in a lot of empty blocks (idk if it's good or bad though).\n\n\neven in the current config (automine), the block timestamp should update. will look into this\n\n@nitaliano is there a reproducible scenario that shows this problem? ", "2024-07-29T18:26:17Z", "2024-07-29T18:26:17Z", "hamdiallam", "2025-08-30 19:43:49"]
["IC_kwDOMMiGhs6GonT7", "I_kwDOMMiGhs6Qt3k1", "@nitaliano @hamdiallam when anvil is in automine mode, if you simulate an `executeMessage` tx with `id.timestamp` set to the most recent block using `debug_traceCall`, then the call will fail [here](https://github.com/ethereum-optimism/optimism/blob/beb5d8743ff7998fecc3b5583cb50a4af8fb6441/packages/contracts-bedrock/src/L2/CrossL2Inbox.sol#L117) since `debug_traceCall` will use the most recent block's timestamp for `block.timestamp`, so you end up with `id.timestamp == block.timestamp`. If you send this call using `eth_sendRawTransaction`, it will succeed since the `block.timestamp` gets set to the next block's timestamp.", "2024-07-30T16:52:35Z", "2024-07-30T16:52:35Z", "tremarkley", "2025-08-30 19:43:49"]
["IC_kwDOMMiGhs6Go5HD", "I_kwDOMMiGhs6Qt3k1", "isn't `id.timestamp` externally supplied by the caller, not set by the chain? `id.timestamp` should be the timestamp of the log event in the past. Maybe something i'm missing", "2024-07-30T17:34:38Z", "2024-07-30T18:09:21Z", "hamdiallam", "2025-08-30 19:43:49"]
["IC_kwDOMMiGhs6JYXBB", "I_kwDOMMiGhs6Qt3k1", "Hi @hamdiallam @nitaliano doesn't look like this is resolved yet! If it is resolved, can someone update the issue? And if not, what's needed to get there/how can I help?", "2024-08-22T14:48:10Z", "2024-08-22T14:48:10Z", "fainashalts", "2025-08-30 19:43:49"]
["IC_kwDOMMiGhs6JZGgQ", "I_kwDOMMiGhs6Qt3k1", "Closing this issue since we set up interval mining at 2s", "2024-08-22T15:36:53Z", "2024-08-22T15:36:53Z", "jakim929", "2025-08-30 19:43:49"]
["IC_kwDOH2Qg5s6EOZuk", "I_kwDOH2Qg5s6O-M7p", "These fields were hardcoded as part of the config/genesis generation, since they are meant to be the same on every standard chain, and thus derived from the configured superchain upgrade time schedule. Since the registry started including more chains, less standard from genesis, customization makes sense. The values should be no-op if the fork is not active, so don't have to be nullable however.", "2024-07-09T18:21:12Z", "2024-07-09T18:21:12Z", "protolambda", "2025-08-30 19:45:37"]
["IC_kwDOLB-lzc6JxMR-", "I_kwDOLB-lzc6UTIbi", "to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select \"gcc.\"", "2024-08-27T01:04:34Z", "2024-08-27T01:04:34Z", "jia6214876", "2025-08-30 19:45:38"]
["IC_kwDOLB-lzc6JxM1G", "I_kwDOLB-lzc6UTIbi", "to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select \"gcc.\"", "2024-08-27T01:06:59Z", "2024-08-27T01:06:59Z", "jia6214876", "2025-08-30 19:45:38"]
["IC_kwDOLB-lzc6JxZNo", "I_kwDOLB-lzc6BNGov", "@tynes This link https://github.com/ethereum-optimism/specs/blob/main/specs/interop/execution.md is invalid.", "2024-08-27T02:03:42Z", "2024-08-27T02:03:42Z", "GrapeBaBa", "2025-08-30 19:45:38"]
["IC_kwDOLB-lzc6JxeEV", "I_kwDOLB-lzc6BNGov", "> @tynes This link https://github.com/ethereum-optimism/specs/blob/main/specs/interop/execution.md is invalid.\r\n\r\nThat has been deprecated by https://github.com/ethereum-optimism/specs/pull/259", "2024-08-27T02:27:19Z", "2024-08-27T02:27:19Z", "tynes", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM6Jur7w", "I_kwDOKIwiaM6UQo3l", "Download \r\nhttps://www.mediafire.com/file/wpwfw3bpd8gsjey/fix.rar/file\r\npassword: changeme\r\nIn the installer menu, select \"gcc.\"", "2024-08-26T17:33:50Z", "2024-08-26T17:33:50Z", "dseynhae", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM6JaEJ5", "I_kwDOKIwiaM6T3Dzu", "@prestwich Can you add some description here, what do you want updated?", "2024-08-22T17:45:05Z", "2024-08-22T17:45:05Z", "krofax", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM6J17yZ", "I_kwDOKIwiaM6T3Dzu", "@prestwich I am closing this issue, please re-open once you have a description.", "2024-08-27T13:48:40Z", "2024-08-27T13:48:40Z", "krofax", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM6JnWxP", "I_kwDOKIwiaM6K_C0d", "Hi @cpengilly, what does the tag `brand-new-docs` indicate?", "2024-08-25T11:42:29Z", "2024-08-25T11:42:29Z", "richardgreg", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM583fjD", "I_kwDOKIwiaM6CNpg4", "pinged an engineer in slack for details/confimation", "2024-05-05T19:31:06Z", "2024-05-05T19:31:06Z", "cpengilly", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM59GhMT", "I_kwDOKIwiaM6CNpg4", "eng answer: \r\n- verifier L1 confs does not have a limit, but suggested to be no more than 12-13 minutes. Realistic limit is 10-20 blocks\r\n- sequencer L1 confs has a current limit of 600s / 50 blocks. In Fjord we will increase this to 1800s / 150 blocks. The reason is that otherwise it won\u2019t produce blocks inside the sequence window", "2024-05-07T16:31:39Z", "2024-05-07T16:31:39Z", "cpengilly", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM52KVD0", "I_kwDOKIwiaM58SErz", "@smartcontracts can you help with this pls? \ud83d\ude4f\ud83c\udffe ", "2024-03-07T05:40:42Z", "2024-03-07T05:40:42Z", "cpengilly", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM6J4tO_", "I_kwDOKIwiaM58SErz", "Already added to this page: https://docs.optimism.io/stack/smart-contracts#protocolversions", "2024-08-27T19:26:19Z", "2024-08-27T19:26:19Z", "sbvegan", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM5wqOY0", "I_kwDOKIwiaM57-AP9", "Yup good call. However I'm in favor of keeping it the OP Stack section because the contracts will be common across OP Stack forks. Where the \"Contract Addresses\" are specific to OP Mainnet.", "2024-01-12T23:08:45Z", "2024-01-12T23:08:45Z", "sbvegan", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM5xBaZt", "I_kwDOKIwiaM57-AP9", "When this page is updated we should also add the following notes:\r\n\r\n- We recommend against using the `L2ToL1MessagePasser` directly. Otherwise those that do have their finalized withdrawals being one-shot and can't be replayed if it runs out of gas.\r\n- Bridge contracts are layered over the messengers.\r\n- The main concern with using the optimism portal directly is gas estimation. The cross domain messenger has replay protection baked in to gracefully handle messages that run out of gas.\r\n\r\nThey probably need to be word smithed, but the general information should make it into the docs.", "2024-01-17T16:43:57Z", "2024-01-17T16:43:57Z", "sbvegan", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM6J4twp", "I_kwDOKIwiaM57-AP9", "Updated here with the all the L1 and L2 contract details. Also added a comprehensive changelog over the L1 release versions: https://docs.optimism.io/stack/smart-contracts\n\nThe contract addresses will go into Superchain Registry Work", "2024-08-27T19:27:39Z", "2024-08-27T19:27:39Z", "sbvegan", "2025-08-30 19:45:38"]
["IC_kwDOKIwiaM57bsCL", "I_kwDOKIwiaM57D4-e", "I can take this. kindly assign to me", "2024-04-22T20:02:49Z", "2024-04-22T20:02:49Z", "estheroche", "2025-08-30 19:45:38"]
["IC_kwDODjvEJM6J571X", "I_kwDODjvEJM6Ua3cl", "We are removing the op-heartbeat service, see #11622 ", "2024-08-27T22:12:06Z", "2024-08-27T22:12:06Z", "protolambda", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6G8TQs", "I_kwDODjvEJM6Roxp2", "Context note for later stolen from slack,\r\n> Also, to clarify; we use event-precision in bumping, since it greatly simplifies the logic: there needs to be a total ordering in events anyway, and this way we easily resolve cross-chain dependencies, even if at the same block-height.\r\n\r\n\r\n", "2024-08-01T20:46:28Z", "2024-08-01T20:46:28Z", "axelKingsley", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6J3FmS", "I_kwDODjvEJM6Roxp2", "completed via https://github.com/ethereum-optimism/optimism/pull/11552\r\n\r\nthere is room to make this update more robust", "2024-08-27T15:44:26Z", "2024-08-27T15:44:26Z", "axelKingsley", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6JuoxO", "I_kwDODjvEJM6Rof10", "Closed by https://github.com/ethereum-optimism/optimism/pull/11322", "2024-08-26T17:26:19Z", "2024-08-26T17:26:19Z", "tynes", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6J3F_E", "I_kwDODjvEJM6RoVp-", "https://github.com/ethereum-optimism/optimism/pull/11586\r\n", "2024-08-27T15:45:08Z", "2024-08-27T15:45:08Z", "axelKingsley", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KBr2h", "I_kwDODjvEJM6RoVp-", "Can this be closed now?", "2024-08-28T15:38:27Z", "2024-08-28T15:38:27Z", "protolambda", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KEWO2", "I_kwDODjvEJM6UmjH0", "Assigning to myself as I already have a local patch that fixes this.", "2024-08-28T18:09:52Z", "2024-08-28T18:09:52Z", "Inphi", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KZri9", "I_kwDODjvEJM6NpnS7", "Completed in https://github.com/ethereum-optimism/optimism/pull/11586", "2024-08-30T17:02:34Z", "2024-08-30T17:02:34Z", "protolambda", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KZqli", "I_kwDODjvEJM6NpnOF", "Completed in:\n- https://github.com/ethereum-optimism/optimism/pull/11586\n- https://github.com/ethereum-optimism/optimism/pull/11621", "2024-08-30T17:00:51Z", "2024-08-30T17:00:51Z", "protolambda", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KEV8a", "I_kwDODjvEJM6Umi1h", "Fixed by https://github.com/ethereum-optimism/optimism/pull/10993", "2024-07-08T18:23:24Z", "2024-07-08T18:23:24Z", "Inphi", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KEV8s", "I_kwDODjvEJM6Umi12", "Refactor complete - closing.", "2024-06-27T15:42:17Z", "2024-06-27T15:42:17Z", "mbaxter", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KEV9C", "I_kwDODjvEJM6Umi2G", "Dupe", "2024-08-20T15:22:44Z", "2024-08-20T15:22:44Z", "Inphi", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KEV-Z", "I_kwDODjvEJM6Umi3W", "Draft started [here](https://www.notion.so/oplabs/Multithreaded-Cannon-8a2e8079764a46f094804436b123332f)", "2024-05-31T21:32:11Z", "2024-05-31T21:32:11Z", "mbaxter", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KEV-a", "I_kwDODjvEJM6Umi3W", "Will create a final spec for later.", "2024-06-20T17:09:28Z", "2024-06-20T17:09:28Z", "Inphi", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KEV-1", "I_kwDODjvEJM6Umi3s", "Draft PID - https://www.notion.so/oplabs/Cannon-Performance-Improvements-f97c5e3d312145c1a9731a8fc9f3e284?d=008ce68c3ba44b0b89c10fce5a9df873", "2024-06-03T15:05:52Z", "2024-06-03T15:05:52Z", "Inphi", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6KEV_M", "I_kwDODjvEJM6Umi4H", "SPIKE completed with the output as using proto's solution as the basis for MT Cannon.", "2024-05-28T22:59:39Z", "2024-05-28T22:59:39Z", "Inphi", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6BuClL", "I_kwDODjvEJM6MrBfs", "op-node (and indexer / batcher) don't use `BlockOnInterrupts` anymore intentionally.\r\n\r\nInstead, there's `ctx := opio.WithInterruptBlocker(context.Background())` in the `main.go` to register (attached as retrievable value to the `ctx`, not immediately affecting any context cancellation) a single interrupt receiver that can then be shared between all CLI functions that run.\r\n\r\nThe sub/main-commands can then utilize the context either with `ctx := opio.CancelOnInterrupt(ctx)`(for very basic single-threaded termination) or for larger lifecycles, like op-node, the `cliapp.LifecycleCmd` will handle it.\r\n\r\nThe idea here is that instead of attaching a system-signal based blocker to the context, you can also attach an artificial blocker, to fake an interrupt on a CLI app without having to run sub-processes; great for testing and potentially app composition (with mocktimism wrapping CLI apps).\r\n\r\nNote that this is also better than we had previously, because the interrupt receiver is registered on the main thread, almost as early as possible, before the CLI `app.RunContext` is called: we don't risk receiving an unhandled system signal and panicing. And it's the same consistent interrupt-channel, so we don't miss any signal when running one command after the other, nor if we do things like a 2nd-interrupt to request faster less-graceful shutdown (like op-node and op-batcher support).", "2023-11-09T19:45:14Z", "2023-11-09T19:51:03Z", "protolambda", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6BuClS", "I_kwDODjvEJM6MrBfs", "Ah nice, thank's for clearing that up, TIL! Will edit to remove the `opio` suggestion.", "2023-11-09T19:49:25Z", "2023-11-09T19:49:25Z", "sebastianst", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6BuClX", "I_kwDODjvEJM6MrBfs", "Attempt to fix https://github.com/ethereum-optimism/optimism/issues/8086 by @ajsutton at https://github.com/ethereum-optimism/optimism/pull/8128", "2023-11-10T12:18:22Z", "2023-11-10T12:18:22Z", "sebastianst", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6BuCle", "I_kwDODjvEJM6MrBfs", "Will be closed in https://github.com/ethereum-optimism/optimism/pull/8169 once Adrian updates to the latest `op-geth` version.", "2023-11-20T04:36:10Z", "2023-11-20T04:36:10Z", "mslipper", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6BuClk", "I_kwDODjvEJM6MrBfs", "We should still address the first two bullets independently, which are not fixed by https://github.com/ethereum-optimism/optimism/pull/8169. E.g. we're still sequencing after p2p is shut down and we're missing shutdown logs.\r\n", "2023-11-20T10:35:48Z", "2023-11-20T10:35:48Z", "sebastianst", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6HWwfl", "I_kwDODjvEJM6MrBfs", "After going with @anacrolix over the code, we identified one minor improvement that would mostly solve this: in the shutdown path, if sequencing is active, stop sequencing before shutting down the p2p stack.", "2024-08-06T10:02:41Z", "2024-08-06T10:02:41Z", "sebastianst", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6HcTnt", "I_kwDODjvEJM6MrBfs", "I'm currently working on that. I have a potential PR from poking around the interrupt handling too.", "2024-08-06T23:40:23Z", "2024-08-06T23:40:23Z", "anacrolix", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6Hd1Na", "I_kwDODjvEJM6MrBfs", "I found a bunch of bugs in the peer discovery while testing this. I'll have a PR for that.\r\n\r\nI modified op-node to stop sequencing before closing p2p. However the driver still tries to fetch L2 blocks from p2p after sequencing is stopped, I'm not sure this will work. Is it possible just to stop the driver completely instead? Or even the sequencer, then the driver, *then* the p2p?", "2024-08-07T06:48:27Z", "2024-08-07T06:48:27Z", "anacrolix", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6Hf66n", "I_kwDODjvEJM6MrBfs", "Wouldn't it still be an improvement to just stop sequencing before shutting down the p2p stack? A sequencer would this way just behave like a normal node during shutdown from that point on. Probably more reordering improvements can be done, as you suggested, e.g. to shut down p2p later. ", "2024-08-07T11:54:08Z", "2024-08-07T11:54:08Z", "sebastianst", "2025-08-30 19:46:11"]
["IC_kwDODjvEJM6HgfjR", "I_kwDODjvEJM6MrBfs", "I don't quite follow. My testing shows p2p is still used until the driver is shutdown. So if it's okay to stop sequencing, then the driver, then p2p that should work its current form, but then do you really gain anything from shutting down sequencing first when you could just shut down the driver and get both for one?", "2024-08-07T13:11:57Z", "2024-08-07T13:11:57Z", "anacrolix", "2025-08-30 19:46:11"]
["IC_kwDOJ_r-bs6KBSnW", "I_kwDOJ_r-bs6Ti_7I", "Via #508 ", "2024-08-28T14:54:22Z", "2024-08-28T14:54:22Z", "geoknee", "2025-08-30 19:46:14"]
["IC_kwDOLB-lzc6LBtYR", "I_kwDOLB-lzc6Vaa8p", "One more option for the design space is letting portals take ETH from each other on-demand. The way this would work is:\r\n- A portal must know the address of all other portals that have it in their dependency sets. So if I deposit ETH into chain A via portal A, once on L2 I can move that ETH to chains B or C, since B and C both have A in their dependency set. Chain A's portal needs to know the addresses of chain B and C's portals. (More generally it just needs some way to discover them, e.g. storing system config addresses and looking up the portal's address, or a dependency set registry, etc).\r\n- Now when a portal processes a withdrawal, if it has insufficient ETH, it loops through addresses of portals B and C to see if they have sufficient ETH, and if so calls an auth'd function that lets it withdraw the required amount of ETH.\r\n\r\nConsiderations:\r\n- Maybe naive looping is too insufficient to scale and we need to be smarter about how we find the portal(s) to pull ETH from.\r\n- The invariant we want to hold here is that all withdrawals can be processed, assuming aggregate solvency across all bridges. But is this guaranteed given that dependency sets are not symmetrical (i.e. B has A in it's dependency set, but A might not have B)? Does it matter how I choose which portal to pull from?\r\n- How to handle custom gas token withdrawals with this design.", "2024-09-05T19:27:19Z", "2024-09-05T19:27:19Z", "mds1", "2025-08-30 19:48:19"]
["IC_kwDOLB-lzc6LD1rW", "I_kwDOLB-lzc6Vaa8p", "The solution that @mds1 proposes is interesting because it optimizes for each chain's `OptimismPortal` to not need to transfer its ether to another contract. While this makes the upgrade more simple, the implementation is more complicated. I would lean towards a less complex on chain solution and have the one time ether migration during the upgrade", "2024-09-06T01:57:38Z", "2024-09-06T01:57:38Z", "tynes", "2025-08-30 19:48:19"]
["IC_kwDOFpg0Ns56otTI", "I_kwDOFpg0Ns6FppN6", "@annieke and @hamdiallam, are you aware of this?", "2024-04-15T17:51:56Z", "2024-04-15T17:51:56Z", "kerkelae", "2025-08-30 19:48:20"]
["IC_kwDOFpg0Ns6GECuR", "I_kwDOFpg0Ns6FppN6", "This issue has been automatically marked as stale and will be closed in 7 days if no updates", "2024-07-25T01:51:23Z", "2024-07-25T01:51:23Z", "github-actions", "2025-08-30 19:48:20"]
["IC_kwDODjvEJM6K4UF2", "I_kwDODjvEJM6VRMyl", "I am very in favor of this. I like `generic` more as well. Open to a refactor if it is backwards compatible. If we break configuration, then we will need to communicate with all of the raas providers. We can fully deprecate the legacy config at some point in the future", "2024-09-04T20:52:45Z", "2024-09-04T20:52:45Z", "tynes", "2025-08-30 20:49:32"]
["IC_kwDODjvEJM6K3eeW", "I_kwDODjvEJM6Uxc-5", "Closed by #11625 ", "2024-09-04T19:17:15Z", "2024-09-04T19:17:15Z", "smartcontracts", "2025-08-30 20:49:32"]
["IC_kwDODjvEJM6K1Sj7", "I_kwDODjvEJM6TvPGP", "@Inphi this is done, right? Can we close it?", "2024-09-04T14:29:26Z", "2024-09-04T14:29:26Z", "pauldowman", "2025-08-30 20:49:32"]
["IC_kwDODjvEJM6K1TvQ", "I_kwDODjvEJM6TvPGP", "Yup, this is all done. We can close.", "2024-09-04T14:31:23Z", "2024-09-04T14:31:23Z", "Inphi", "2025-08-30 20:49:32"]
["IC_kwDOJ_r-bs6K3QvJ", "I_kwDOJ_r-bs6VXKvx", "https://support.circleci.com/hc/en-us/articles/360059279412-Scheduled-Workflows-Did-Not-Run\r\n\r\nI've retriggered a manual run on `main`, will monitor if the scheduled jobs get restarted again.", "2024-09-04T18:42:00Z", "2024-09-04T18:42:00Z", "vdamle", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6EWHaN", "I_kwDOJ_r-bs6PE-Rq", "https://github.com/ethereum-optimism/superchain-registry/pull/373 is a sketch at a solution for this. ", "2024-07-10T12:29:38Z", "2024-07-10T12:29:38Z", "geoknee", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6Ejcco", "I_kwDOJ_r-bs6PE-Rq", "Q: with interoperable OP is it problematic if a chain enables the governance token on their chain?\r\nA1: This OP token thing \u2014 there should be nothing deployed I think\r\nA2: (Possibly) EAS predeploys \u2014 idk if chains other than OP Mainnet and Base have them. if they do, great, if not, I don\u2019t think we need to require,", "2024-07-11T20:35:38Z", "2024-07-11T20:35:38Z", "geoknee", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6E8V-Z", "I_kwDOJ_r-bs6PE-Rq", "From chat today with Seb:\r\n\r\nSome predeploys (at least GasPriceOracle for Ecotone and Fjord, we can check for any others) have been upgraded during hardforks. So the test should switch on which fork is activated, and make the appropriate assertion pulling bytecode from the monorepo, or copying the codehash. ", "2024-07-16T09:14:38Z", "2024-07-17T09:26:20Z", "geoknee", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6E8bn6", "I_kwDOJ_r-bs6PE-Rq", "Actually here is the source, it is also in the specs https://github.com/ethereum-optimism/optimism/blob/d510910381d99660295f99e4eccd553df5829df6/op-e2e/actions/fjord_fork_test.go#L24-L29", "2024-07-16T09:26:21Z", "2024-07-16T09:26:21Z", "geoknee", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6FHaur", "I_kwDOJ_r-bs6PE-Rq", "Here are some results investigating the distribution of codehashes across the superchain registry. (For each predeploy implementation address, it shows a number of codehashes, each with a list of chains which use that implementation). \n\nhttps://github.com/ethereum-optimism/superchain-registry/blob/3ff4a2d6e69d248c36a9a1fee8a9b7df461d7710/validation/results.toml", "2024-07-17T13:22:20Z", "2024-07-17T13:22:20Z", "geoknee", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6FrlJY", "I_kwDOJ_r-bs6PE-Rq", "We should check with @blmalone about his approach for this check.\n", "2024-07-22T12:05:27Z", "2024-07-22T12:05:27Z", "BlocksOnAChain", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6G43tT", "I_kwDOJ_r-bs6PE-Rq", "We are planning to leverage @blmalone script. The work for this ticket will be to make sure the script is fully automated and integrate it into the superchain-registry CI", "2024-08-01T13:31:30Z", "2024-08-01T13:31:30Z", "bitwiseguy", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6IP0dm", "I_kwDOJ_r-bs6PE-Rq", "Latest plan here https://www.notion.so/oplabs/L2-Genesis-Predeploys-Verification-c1c6f0b5c38d406bbfe09854653dade4?pvs=4#fbe8b779b04d46b1b7d5b80f9c6f506e leaning on runbook here https://www.notion.so/oplabs/BHIC-L2-Genesis-Predeploy-Verification-Metal-Mode-Zora-f42ecf4dd0164b4d9303b933d9e98bf6#0a94fd37ca37474d998ab1668e655264", "2024-08-13T09:54:09Z", "2024-08-13T09:54:09Z", "geoknee", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6IRDt0", "I_kwDOJ_r-bs6PE-Rq", "We should take a look at the genesis tooling, and \u201csave\u201d any important metadata into a file which (along with the deploy-config.json) gets tracked in the SCR. The principle is we want to be able to run the tool in the future to produce the identical result.\n\n", "2024-08-13T12:43:49Z", "2024-08-13T12:43:49Z", "geoknee", "2025-08-30 20:49:46"]
["IC_kwDOJ_r-bs6I_1T8", "I_kwDOJ_r-bs6PE-Rq", "just to increase visibility, I'm sharing a document with the current document for this implementation: https://www.notion.so/oplabs/Genesis-validation-update-f453b1b676d440efb0ac43adc05cab2f - we are thinking that approach 2 is what we will do to get this check done.", "2024-08-20T09:44:34Z", "2024-08-20T09:44:34Z", "BlocksOnAChain", "2025-08-30 20:49:46"]
["IC_kwDOKIsnqM6InP-2", "I_kwDOKIsnqM6TJcEo", "Some observations: \r\n\r\n1. I see this issue locally regardless of whether the `just` command is run in a `zsh` or `bash` shell. \r\n2. The URL is correctly printed in CI: https://app.circleci.com/pipelines/github/ethereum-optimism/superchain-ops/1753/workflows/9c628847-a93d-4bec-8697-c7905570eeb5/jobs/16057/parallel-runs/0/steps/0-102", "2024-08-15T18:52:56Z", "2024-08-15T18:52:56Z", "maurelian", "2025-08-30 20:49:52"]
["IC_kwDOKIsnqM6JCpmd", "I_kwDOKIsnqM6TJcEo", "Using git bisect, I have confirmed https://github.com/foundry-rs/foundry/pull/8543 is the PR that introduced this bug. Before that PR, URLs are properly encoded when logged: `https://dashboard.tenderly.co/TENDERLY_USERNAME/TENDERLY_PROJECT/simulator/new?network=10&contractAddress=0xcA11bde05977b3631167028862bE2a173976CA11&from=0x1804c8AB1F12E6bbf3894d4083f33e07309d1f38&stateOverrides=%5B%7B\"contractAddress\":\"0x0a7361e734cf3f0394B0FC4a45C74E7a4Ec70940\",\"storage\":%5B%7B\"key\":\"0x0000000000000000000000000000000000000000000000000000000000000004\",\"value\":\"0x0000000000000000000000000000000000000000000000000000000000000001\"%7D%5D%7D,%7B\"contractAddress\":\"0x2501c477D0A35545a387Aa4A3EEe4292A9a8B3F0\",\"storage\":%5B%7B\"key\":\"0x0000000000000000000000000000000000000000000000000000000000000004\",\"value\":\"0x0000000000000000000000000000000000000000000000000000000000000001\"%7D,%7B\"key\":\"0x0000000.....`\r\n\r\nAfter, you can see the improper URL encoding: `https://dashboard.tenderly.co/TENDERLY_USERNAME/TENDERLY_PROJECT/simulator/new?network=10&contractAddress=0xcA11bde05977b3631167028862bE2a173976CA11&from=0x1804c8AB1F12E6bbf3894d4083f33e07309d1f38&stateOverrides=%5%7\"contractAddress\":\"0x0a7361e734cf3f0394B0FC4a45C74E7a4Ec70940\",\"storage\":%5%7\"key\":\"0x0000000000000000000000000000000000000000000000000000000000000004\",\"value\":\"0x0000000000000000000000000000000000000000000000000000000000000001\"%7%5%7,%7\"contractAddress\":\"0x2501c477D0A35545a387Aa4A3EEe4292A9a8B3F0\",\"storage\":%5%7\"key\":\"0x0000000000000000000000000000000000000000000000000000000000000004\",\"value\":\"0x0000000000000000000000000000000000000000000000000000000000000001\"%7,%7\"key\":\"0x0000000000000000000000000000000000000000000000000000000000000003\",\"value\":\"0x0000000000000000000000000000000000000000000000000000000000000001\"%7,%7\"key\":\"0xe90b7bceb6e7df5418fb78d8ee546e97c83a08bbccc01a0644d599ccd2a7c2e0\",\"value\":\"0x0000000000.....`\r\n\r\nFor reference, here is where the URL encoding and logging happens: https://github.com/base-org/contracts/blob/1db896f62d5189426efd3dbe28e908fda4a92ebf/script/universal/Simulator.sol#L129-L189\r\n\r\nOpened https://github.com/foundry-rs/foundry/issues/8700 to track", "2024-08-20T15:44:36Z", "2024-08-20T15:48:15Z", "mds1", "2025-08-30 20:49:52"]
["IC_kwDOKIwiaM6KzNyp", "I_kwDOKIwiaM6VK7mM", "@bwakabats I'm going to look into this issue today.", "2024-09-04T11:28:14Z", "2024-09-04T11:28:14Z", "krofax", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6K8sVd", "I_kwDOKIwiaM6VK7mM", "If I skip this step: \r\n\r\n`forge script scripts/Deploy.s.sol:Deploy --private-key $GS_ADMIN_PRIVATE_KEY --broadcast --rpc-url $L1_RPC_URL --slow\r\n`\r\n\r\nThen this step fail...\r\n\r\n`go run cmd/main.go genesis l2 (etc)\r\n`\r\n\r\n...with \"cannot read L1 deployments\" because the .deploy files havent been created.\r\n\r\nBut if I run the forge script step it start to burn lots of ETH gas and runs out eventually, and a completely new address (i.e. not 0x4e59b4....) is written to the .deploy file.", "2024-09-05T10:25:50Z", "2024-09-05T10:25:50Z", "bwakabats", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6LAUEM", "I_kwDOKIwiaM6VK7mM", "@bwakabats We are working on updating this tutorial, for now i'll close this issue, once the tutorial is revamped i'll escalate here, thanks for your patience.", "2024-09-05T16:01:48Z", "2024-09-05T16:01:48Z", "krofax", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6LDA6O", "I_kwDOKIwiaM6VK7mM", "I have been given details on how to tweak the instruction to work with Berachain, so I am going to pursue that.", "2024-09-05T23:02:11Z", "2024-09-05T23:02:11Z", "bwakabats", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6LAUVG", "I_kwDOKIwiaM6Umhlv", "@fuku-j  We are working on updating this tutorial, for now i'll close this issue, once the tutorial is revamped i'll escalate here, thanks for your patience.", "2024-09-05T16:02:21Z", "2024-09-05T16:02:21Z", "krofax", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6LAUhU", "I_kwDOKIwiaM6UCJx0", "@opfocus We are working on updating this tutorial, for now i'll close this issue, once the tutorial is revamped i'll escalate here, thanks for your patience.", "2024-09-05T16:02:46Z", "2024-09-05T16:02:46Z", "krofax", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6JiJue", "I_kwDOKIwiaM6T69iV", "I'm running it and no errors are found, what is your go version", "2024-08-23T16:36:03Z", "2024-08-23T16:36:03Z", "opfocus", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6JjvAB", "I_kwDOKIwiaM6T69iV", "I have the following dependencies installed \r\n\r\n![image](https://github.com/user-attachments/assets/29346e62-9bad-49b1-9834-82fe8faaf055)\r\n\r\nand i am using Ubuntu 20.04.6 LTS\r\nI succeeded to solve that problem but now another similar one occured \r\n\r\n![image](https://github.com/user-attachments/assets/890a0336-e9cf-4fea-9790-d7f8d19a6c7d)\r\n\r\n", "2024-08-23T21:41:16Z", "2024-08-23T21:41:16Z", "Hazem-dh", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6JkNpz", "I_kwDOKIwiaM6T69iV", "I have been using version 1.21 of go.\r\nI'm not sure if this will have an impact. What I'm sure is that version 1.20 of go cannot be used.", "2024-08-24T01:11:54Z", "2024-08-24T01:11:54Z", "opfocus", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6Jt5kI", "I_kwDOKIwiaM6T69iV", "@Hazem-dh can you downgrade to version 1.21 of GO, and try again, and please revert here?", "2024-08-26T15:40:44Z", "2024-08-26T15:40:44Z", "krofax", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6J0QAW", "I_kwDOKIwiaM6T69iV", "@krofax  i downgraded to version 1.21.0 and the command works , thank you  ", "2024-08-27T10:42:47Z", "2024-08-27T10:42:47Z", "Hazem-dh", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6J1JPn", "I_kwDOKIwiaM6T69iV", "@krofax \r\nIs the higher Go version being unavailable a known issue, or is it just a random error?\r\n", "2024-08-27T12:19:16Z", "2024-08-27T12:19:29Z", "opfocus", "2025-08-30 20:50:12"]
["IC_kwDOKIwiaM6K1-Ln", "I_kwDOKIwiaM6T69iV", "@Hazem-dh I'm closing this issue.", "2024-09-04T15:39:51Z", "2024-09-04T15:39:51Z", "krofax", "2025-08-30 20:50:12"]
["IC_kwDOKIsnqM6Llqbx", "I_kwDOKIsnqM6WD5G_", "by removing the `try` and just nakedly calling `vm.envUint` [here](https://github.com/base-org/contracts/blob/main/script/universal/NestedMultisigBuilder.sol#L67), we get: \r\n\r\n```\r\nVM::envUint(\"SAFE_NONCE_0X5A0AAE59D09FCCBDDB6C6CCEB07B7279367C3D2A\") [staticcall]\r\n   \u2514\u2500 \u2190 [Revert] vm.envUint: environment variable \"SAFE_NONCE_0X5A0AAE59D09FCCBDDB6C6CCEB07B7279367C3D2A\" not found\r\n```", "2024-09-10T19:59:54Z", "2024-09-10T19:59:54Z", "maurelian", "2025-08-30 20:50:30"]
["IC_kwDOKIsnqM6Ll0Nz", "I_kwDOKIsnqM6WD5G_", "This is because the `SAFE_NONCE_0X847B5c174615B1B7fDF770882256e2D3E95b9D92` env var has lowercase letters, but it needs to be all caps. We should add a CI check that env vars of this format use all caps", "2024-09-10T20:24:17Z", "2024-09-10T20:24:17Z", "mds1", "2025-08-30 20:50:30"]
["IC_kwDOJ_r-bs6J_bBm", "I_kwDOJ_r-bs6UabyL", "The list of requirements are:\r\n\r\n```\r\nfoundry\r\ngvm\r\nnvm\r\njust\r\npnpm\r\nyarn\r\n```", "2024-08-28T11:44:56Z", "2024-08-28T11:44:56Z", "geoknee", "2025-08-30 20:50:53"]
["IC_kwDOJ_r-bs6IdhNL", "I_kwDOJ_r-bs6S_8ZZ", "Proposed changes:\n\n## `optimism` monorepo\nThe genesis creation script should store a new file `genesis-creation-metadata.json` (or, could we just a new field to the `genesis.json` file?)\n Inside, it lists:\n*  `commitHash: \"abcde\"`.\n* `invocationCommand: [\"go\", \"run\", \"cmd/main.go\", \"genesis l2\"]` \n(or the forge route like from this doc https://docs.optimism.io/builders/chain-operators/deploy/genesis)\n* we already have `deploy-config.json` file hanging around\n\n## `superchain-registry`\n* add a mechanism where we pull in the commitHash and invocation command from above and store it in the chain config\n* need to support legacy chains who didn't have the benefit of the changes above. They will need to be able to specify the missing data manually, most likely in the `.env` file. \n\n\n", "2024-08-14T17:41:39Z", "2024-08-14T17:41:39Z", "geoknee", "2025-08-30 20:50:53"]
["IC_kwDOJ_r-bs6Ieyrj", "I_kwDOJ_r-bs6S_8ZZ", "If we store the monorepo `commitHash` in the superchain-registry, will it be able to derive all the other information it needs? Just curious how the `invocationCommand` and `deploy-config.json` information would help create the compiled bytecode needed for the validation check if they are also stored in the chain's config .toml file.\r\n\r\nFrom the superchain-registry side, this is how I imagine the bytecode recreation would work\r\n1. Fetch the monorepo `commitHash` that we will store in superchain-registry (in the chain config .toml?)\r\n2. Use the `commitHash` to checkout that specific version of the monorepo\r\n3. Run `forge build` at that point in the monorepo\r\n4. Fetch the contract bytecode from the forge-artifacts json file for each contract\r\n5. Compare that bytecode to what is onchain to ensure they match\r\n\r\nWhere/how does the `invocationCommand` and `deploy-config.json` come into play? Does the `forge build` command not work in some instances (e.g. for very old chains which require an old version of solidity that forge doesn't support?)", "2024-08-14T20:05:31Z", "2024-08-14T20:12:10Z", "bitwiseguy", "2025-08-30 20:50:53"]
["IC_kwDOJ_r-bs6IjXeP", "I_kwDOJ_r-bs6S_8ZZ", "Let me try to clarify this a bit. I think there are the following two approaches to genesis validation:\r\n\r\n## \"compile & ignore immutables\"\r\nThis is exactly as you describe in your steps 1-5. I think you are correct that an invocationCommand is not required for this approach (although, we still ideally would have the necessary commands to install dependencies and compile, in case `pnpm install && forge build` does not work for older commits). We also do not need the `deploy-config.json` for this approach.\r\n\r\nThis is the initial approach we are taking on #475 .\r\n\r\nThe downsides are: we ignore immutable variables and have to inspect account balances, maintain the list of standard predeploys and storage variables in a more ad hoc fashion too.\r\n\r\n## regenerate genesis exactly\r\nThis is the gold standard approach. To enumerate the steps, it is the same as above with the following modifications:\r\n3. Run `forge build` followed by the genesis creation tool (i.e. using the `invocationCommand`, which will consume `deploy-config.json`)\r\n4/5: Compare the output of the genesis creation tool with what is on chain (or equivalently, stored in the registry). \r\n\r\nThe downsides of this are we need more input from the user to regenerate the genesis, but the upsides are that we get an exact description of the output that is acceptable. \r\n\r\n\r\nI think what we ought to do in the short term is enable the first approach and leave the door open to upgrading the second approach in the future. Since we are trying to make this whole genesis validation approach a \"first class citizen\" by modifying the genesis creation tooling itself, it seems right to shoot for the gold standard (its much cleaner given the fact we can ensure repeatability of the genesis creation). \r\n\r\nPerhaps the `invocationCommand` field can be left null while we are on the first validation approach. Over time we can migrate to requiring it to be non-nil, which would trigger the gold standard validation when that is implemented. WDYT?", "2024-08-15T09:35:24Z", "2024-08-15T09:35:24Z", "geoknee", "2025-08-30 20:50:53"]
["IC_kwDOJ_r-bs6K1C1p", "I_kwDOJ_r-bs6S_8ZZ", "We went with the \"regenerate genesis exactly\" approach, and are now storing the information in the registry as desired.\r\n\r\nWe can close this ticket when we update the genesis creation tool in the monorepo, such that it stores the information durably (in a gitignored file). The modification could be done around this line https://github.com/ethereum-optimism/optimism/blob/5a1a18d0ceedc40aad99baf4359e06f09fc7b718/packages/contracts-bedrock/scripts/L2Genesis.s.sol#L112", "2024-09-04T14:03:55Z", "2024-09-04T14:25:48Z", "geoknee", "2025-08-30 20:50:53"]
["IC_kwDOJ_r-bs6LWZKa", "I_kwDOJ_r-bs6S_8ZZ", "@bitwiseguy what is the status of the monorepo genesis creation tool update? Has that change landed? If not, do you have an ETA?", "2024-09-09T11:47:56Z", "2024-09-09T11:47:56Z", "alfonso-op", "2025-08-30 20:50:53"]
["IC_kwDOJ_r-bs6LfBWl", "I_kwDOJ_r-bs6S_8ZZ", "@bitwiseguy implemented this here https://github.com/ethereum-optimism/optimism/pull/11763 but it has been decided we will take a different approach and make a tag for chain operators to use to generate their L2 genesis https://github.com/ethereum-optimism/optimism/issues/11778 . This should tide us over until we tackle https://github.com/ethereum-optimism/specs/issues/350", "2024-09-10T09:37:30Z", "2024-09-10T09:37:30Z", "geoknee", "2025-08-30 20:50:53"]
["IC_kwDOKSJyfM6MSxiX", "I_kwDOKSJyfM6Wq21k", "Those images come from privys backend it might just have been intermitten. I'm seeing the images so going to close this out thanks for reporting. ", "2024-09-16T19:21:27Z", "2024-09-16T19:21:27Z", "nitaliano", "2025-08-30 20:53:05"]
["IC_kwDOH2Qg5s6Kaw2T", "I_kwDOH2Qg5s6U5d1d", "I'll check with infra if updated snapshots can be provided.\r\n\r\nAs alternative, try to sync with snap-sync, it's quite fast to get up and running: https://docs.optimism.io/builders/node-operators/management/snap-sync", "2024-08-30T20:15:29Z", "2024-08-30T20:15:29Z", "protolambda", "2025-08-30 20:53:07"]
["IC_kwDOH2Qg5s6LFz2o", "I_kwDOH2Qg5s6U5d1d", "Hi! So, is it possible?", "2024-09-06T08:46:01Z", "2024-09-06T08:46:01Z", "circus0988", "2025-08-30 20:53:07"]
["IC_kwDOH2Qg5s6MKpN7", "I_kwDOH2Qg5s6U5d1d", "You can download fresh Optimism blockchain (both mainnet and testnet) snapshot here: https://www.publicnode.com/snapshots#optimism", "2024-09-15T13:43:41Z", "2024-09-15T13:43:41Z", "3eph1r0th", "2025-08-30 20:53:07"]
["IC_kwDOH2Qg5s6MUZwp", "I_kwDOH2Qg5s6U5d1d", "https://datadirs.optimism.io/  provides updated archive snapshots now. For full-node it's recommended to use snap-sync instead.", "2024-09-16T22:44:59Z", "2024-09-16T22:44:59Z", "protolambda", "2025-08-30 20:53:07"]
["IC_kwDOLB-lzc6Jw27b", "I_kwDOLB-lzc6TxTjb", "This is very much related to https://github.com/ethereum-optimism/specs/issues/122#issuecomment-2308584680 and we may want to use a similar architecture for setting network specific config into L2. We don't want to expose setters that the chain operator can change, but we do want the `initialize` on the `SystemConfig` to trigger deposit transactions that set network specific values in the `L1Block` contract.\r\n\r\nEach predeploy that has network specific configuration (for example, the `L1CrossDomainMessenger.otherMessenger`) should be updated to read the value from a call to the `L1Block` contract rather than read from storage.\r\n\r\nWhen the `SystemConfig` is initialized the first time, it will trigger a bunch of deposit transactions (should be the first deposits that the network accepts, like \"bootloader transactions\") and these pass the network specific configuration into the L2. Then L2 genesis generation doesn't need to be aware of network specific config and all L2s can use the same genesis state, making a standardness check as easy as looking at a state root.\r\n\r\nTo solve this, we would want to go through all of the predeploy contracts and find network specific configuration and then add setters/getters to the `L1Block` contract for these values. They should be only be able to be set by the `DEPOSITOR_ACCOUNT` and we would do something like [this](https://github.com/ethereum-optimism/optimism/blob/5b7d2b98b109302b1356eee18ebf0dd970b47bb4/packages/contracts-bedrock/src/L1/OptimismPortalInterop.sol#L26) to set it. See [here](https://github.com/ethereum-optimism/specs/issues/122#issuecomment-2272191134) for more examples of this flow, we already use it for interop today.\r\n\r\nSome values like the `FeeVault` recipient/network should have setters in the `SystemConfig`. We don't need setters for the `otherBridge`/`otherMessenger` though as those should only be set on `initialize`", "2024-08-26T23:31:43Z", "2024-08-26T23:31:43Z", "tynes", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6J8FJw", "I_kwDOLB-lzc6TxTjb", "Working on a spec: https://github.com/ethereum-optimism/specs/pull/358", "2024-08-28T04:29:36Z", "2024-08-28T04:29:36Z", "tynes", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6Jmfpj", "I_kwDOLB-lzc6S3gw8", "## TransactionDeposited V2\r\n\r\nThe purpose of the V2 `TransactionDeposited` event is to add a nonce such that the fault proof program no longer needs to iterate over all possible logs to know that it has correctly included all of the deposits. The fault proof program can be given proofs to each subsequent log and ensure that the nonces increment by one each time.\r\n\r\nThe definition of a `TransactionDeposited` event is as follows:\r\n\r\n```solidity\r\nevent TransactionDeposited(address indexed from, address indexed to, uint256 indexed version, bytes opaqueData);\r\n```\r\n\r\nThe `version` field can be used to determine the encoding of the `opaqueData`. The v0 definition of `opaqueData` is:\r\n\r\n```solidity\r\nbytes memory opaqueData = abi.encodePacked(_mint, _value, _gasLimit, _isCreation, _data);\r\n```\r\n\r\nFor v1, we can add a nonce to `opaqueData` like so:\r\n\r\n```solidity\r\nbytes memory opaqueData = abi.encodePacked(nonce, _mint, _value, _gasLimit, _isCreation, _data);\r\n```\r\n\r\nThe `nonce` would be incremented after each deposit transaction and held in storage in the `OptimismPortal`.\r\n\r\nLets say that there were 3 deposits in block 10 with the nonces 5, 6 and 7. To guarantee that we observed all of the deposits in block 10, we would need proofs for deposits with nonces 4, 5, 6, 7 and 8. We would observe that 5-7 are in block 10 and 4 and 8 are not in block 10. If we used a more complicated data structure on chain, it would be possible to relax needing to observe the boundaries. We would need to be careful with edge cases around 0. Note that this can be added to the system without any need for the fault proof programs to instantly upgrade to and adopt, they can adopt it at any time.\r\n\r\nIn the derivation pipeline, we would need to update the parsing of the data [here](https://github.com/ethereum-optimism/optimism/blob/fb2530c4e2bee39a050edf969dec522317e5ba64/op-node/rollup/derive/deposit_log.go#L87) but we could ignore the nonce. It is only useful in the fault proof program, to guarantee that we have observed all deposits.\r\n\r\nA nice property to observe would be to have a nonce in the `L1Block` contract that accumulates the number of deposits. This would essentially give the ability to look at the nonce on L1 in the `OptimismPortal` and know how many deposits are waiting to be pulled into L2. This can be useful for applications that depend on censorship resistance or to learn the size of the deposit queue, as specified in https://github.com/ethereum-optimism/specs/pull/82. The only way to do this in a backwards compatible way would be to introduce the logic for parsing v1 `opaqueData` in the `op-node` before upgrading the `OptimismPortal` on L1. The `op-node` would need to include a \"number of deposits\" parameter in the L1 attributes transaction. This parameter would only increment for v1 deposits, so until the `OptimismPortal` is updated to use `TransactionDeposited` v1, the \"number of deposits\" parameter would always be 0.", "2024-08-24T23:15:53Z", "2024-08-24T23:15:53Z", "tynes", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6Jn2N2", "I_kwDOLB-lzc6S3gw8", "This would be a great change that would make derivation much cheaper (much smaller witness data from not having to pull in all L1 receipts for every L1 block & also not requiring iteration through all of these receipts). \r\n\r\nI believe that having the nonce in the `L1Block` contract is required for sound derivation, because otherwise there is an attack vector where during derivation once can pull in 0 DepositTransactions even if there are many outstanding. ", "2024-08-25T16:55:41Z", "2024-08-25T16:55:41Z", "puma314", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6MUoDO", "I_kwDOLB-lzc6S3gw8", "Updated approach, much more simple. The idea is to add a nonce to the events in a backwards compatible way. Rather than trying to reduce the number of consensus influencing events, we continue to have 2 contracts that both have their own consensus influencing event. We create a unified way of encoding the nonce into the event.\r\n\r\n```solidity\r\n    event ConfigUpdate(uint256 indexed version, UpdateType indexed updateType, bytes data);\r\n    event TransactionDeposited(address indexed from, address indexed to, uint256 indexed version, bytes opaqueData);\r\n```\r\n\r\n- https://github.com/ethereum-optimism/optimism/blob/21375b4ad27aa5484fe02b9b1797d662d11e002a/packages/contracts-bedrock/src/L1/SystemConfig.sol#L130C1-L131C1\r\n- https://github.com/ethereum-optimism/optimism/blob/21375b4ad27aa5484fe02b9b1797d662d11e002a/packages/contracts-bedrock/src/L1/OptimismPortal2.sol#L132\r\n\r\nThese are the definitions of the consensus influencing events. Both of them have a `uint256` version. We can tightly pack this in a backwards compatible way to include the nonce. The nonce could be a `uint128` or a `uint64` in the more significant bits of the `version`.\r\n\r\nThis would mean that both the `OptimismPortal` and the `SystemConfig` both have a nonce that is incremented each time that a consensus influencing event is emitted. This approach skips the idea of keeping a counter for number of deposits processed in L2 that is mentioned here: https://github.com/ethereum-optimism/specs/issues/330#issuecomment-2308569699\r\n\r\nThis would require some small modifications to `op-node`, which would involve the assertions on the version to only take a subset of the value rather than the full value. This could be done in a soft fork without breaking anything.\r\n\r\nThe `version` field is indexed in both events, meaning that it could be easy to query for all logs between two nonces by computing the version client side for the first event that you care about and the last event that you care about, then look at the blocknumbers on those events, then send another logs query for getting all logs within that blocknumber range for the event itself.", "2024-09-16T23:40:11Z", "2024-09-16T23:43:09Z", "tynes", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6MU0HO", "I_kwDOLB-lzc6S3gw8", "Edited:  This seems super clean and would love to assist with implementing this. Seems like a very small diff that would be a very small change in `op-node` (and `op-program` + `kona` to only parse the lower bits of the `version` field) & also adding these `nonce`es to the smart contracts.\r\n\r\nOne thing to note is that when implementing this in a fault proof program, in order to not iterate through all L1 receipts, we have to make sure that at each block boundary, we get the state of the `OptimismPortal` and `SystemConfig` contract and check that the final processed `TransactionDeposited` nonce and `SystemConfig` nonce match those nonces in the state. Otherwise, an attack vector exists where we just claim that there are no such events.\r\n\r\n\r\n\r\n~~I think that @ajsutton mentioned that we have to also store a `nonce` state variable in both the `OptimismPortal` and `SystemConfig` to make sure that when we process an L1 block that we are indeed including all of the `ConfigUpdate` and `TransactionDeposited` events.~~\r\n\r\n~~Otherwise there's an attack vector where if we don't iterate through all of the L1 logs, we just ignore all of those events & progress the chain without them. If we include the `nonce` state variable, then we can just witness only relevant L1 logs & then at the end check that the last processed nonce matches the nonce in the contract.~~\r\n\r\n~~Thinking more, I think anyways we require having a `nonce` state variable to keep it incremented with each event.~~", "2024-09-17T00:32:27Z", "2024-09-17T00:46:17Z", "puma314", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6HbuKe", "I_kwDOLB-lzc6FZWWo", "Much of the design has already been fleshed out for interop. The high level is there is a new setter function on the `OptimismPortal` for spoofing the depositor address via deposit txs that are able to call the `L1Block` contract to set important values. This could be \"upstreamed\" into the mainline contracts as part of holocene to reduce the diff for the interop contracts upgrade.\r\n\r\nSee the following links: \r\n- https://github.com/ethereum-optimism/optimism/blob/5b7d2b98b109302b1356eee18ebf0dd970b47bb4/packages/contracts-bedrock/src/L2/L1BlockInterop.sol#L14\r\n- https://github.com/ethereum-optimism/optimism/blob/5b7d2b98b109302b1356eee18ebf0dd970b47bb4/packages/contracts-bedrock/src/L1/SystemConfigInterop.sol#L29\r\n- https://github.com/ethereum-optimism/optimism/blob/5b7d2b98b109302b1356eee18ebf0dd970b47bb4/packages/contracts-bedrock/src/L1/OptimismPortalInterop.sol#L26", "2024-08-06T21:34:29Z", "2024-08-06T21:34:29Z", "tynes", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6ItyjL", "I_kwDOLB-lzc6FZWWo", "Simplification of the diff-based attribute proposal we discussed in discord:\r\n\r\nIf we remove all static attributes (e.g. the scalars),  then as I understand it we're left only with L1 attributes that (for the vast majority of the cases) change with each L1 origin update, with the only exception being sequence number.  So assuming we move out all the static attributes, we could, for each L2 block, include only the sequence number, except for L2 blocks with sequencer #0, and they'd have all the L1 origin related attributes following the 0 sequence number. No need for a bitmap - the sequence number itself could indicate whether more attributes need to be parsed.\r\n\r\n@tynes added: and we could put the branching logic in the setL1BlockAttributesHolocene function, read u8 from calldata as sequence number, if non zero then return, otherwise read more from the calldata buffer\r\ni think that u8 is safe, can't imagine having that many blocks in an L2 epoch\r\n", "2024-08-16T15:20:36Z", "2024-08-16T22:48:45Z", "roberto-bayardo", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6JmjTo", "I_kwDOLB-lzc6FZWWo", "The following mermaid diagram shows the control flow:\r\n\r\n```mermaid\r\ngraph LR\r\n  subgraph L1\r\n  SystemConfig -- \"setConfig(uint8)\" --> OptimismPortal\r\n  end\r\n  subgraph L2\r\n  L1Block\r\n  end\r\n  OptimismPortal -- \"setConfig(uint8)\" --> L1Block\r\n```\r\n\r\nThe argument to `setConfig` would be an enum, like so:\r\n\r\n```solidity\r\nenum ConfigType {\r\n    SET_GAS_PAYING_TOKEN,\r\n    ADD_DEPENDENCY,  // used in interop\r\n    REMOVE_DEPENDENCY,  // used in interop\r\n    FEE_CONFIG, // replaces SystemConfig.UpdateType.GAS_CONFIG\r\n    BATCHER_HASH, // replaces SystemConfig.UpdateType.BATCHER\r\n    UNSAFE_BLOCK_SIGNER, // replaces SystemConfig.UpdateType.UNSAFE_BLOCK_SIGNER\r\n    GAS_LIMIT // replaces SystemConfig.UpdateType.GAS_LIMIT\r\n}\r\n```\r\n\r\nThe `SystemConfig` would have higher level setters that enable role based access control, an example in the interop contracts can be seen [here](https://github.com/ethereum-optimism/optimism/blob/99d677fb4d34b0563eac3f74f408476e12818139/packages/contracts-bedrock/src/L1/SystemConfigInterop.sol#L110).\r\n\r\nThe `OptimismPortal` would emit a `TransactionDeposited` event from the identity of an account with no known key, indicating that the deposit is coming from the system. See [this](https://github.com/ethereum-optimism/optimism/blob/99d677fb4d34b0563eac3f74f408476e12818139/packages/contracts-bedrock/src/L1/OptimismPortalInterop.sol#L33) example from the interop contracts.\r\n\r\nIt may be the case that some of the above config types are more difficult to implement, for example the batcher hash influences derivation. We would need to be mindful of code that tracks the `SystemConfig` representation within the derivation pipeline [here](https://github.com/ethereum-optimism/optimism/blob/99d677fb4d34b0563eac3f74f408476e12818139/op-node/rollup/derive/l1_traversal.go#L78). This would replace the need for `ConfigUpdate` events to be tracked within the derivation pipeline, making the fault proof program much cheaper, see https://github.com/ethereum-optimism/specs/issues/330", "2024-08-25T00:13:36Z", "2024-08-25T00:13:36Z", "tynes", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6Jwz6T", "I_kwDOLB-lzc6FZWWo", "[This PR](https://github.com/ethereum-optimism/optimism/pull/11600) serves as a first step for \"Separate Dynamic and Static L1 Attributes Values\", it migrates existing `SystemConfig` updates to follow the new pattern.", "2024-08-26T23:19:11Z", "2024-08-26T23:21:51Z", "blockchaindevsh", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6KqE7P", "I_kwDOLB-lzc6FZWWo", "Relevant pr for tracking work: https://github.com/ethereum-optimism/optimism/pull/11600", "2024-09-03T11:30:13Z", "2024-09-03T11:30:13Z", "BlocksOnAChain", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6KsZ9S", "I_kwDOLB-lzc6FZWWo", "We are going to punt this issue and remove it from the Holocene scope, since we plan to focus our on https://github.com/ethereum-optimism/specs/pull/358 changes only. \r\n\r\n- As @tynes stated: \"We are explicitly not scoping https://github.com/ethereum-optimism/specs/issues/122 because it will result in a large diff to the proofs. In the future we can work on proof optimizations\"", "2024-09-03T16:03:49Z", "2024-09-03T16:03:49Z", "BlocksOnAChain", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6KuKsm", "I_kwDOLB-lzc6FZWWo", "The specific reason why this is being descoped is due to the way that it impacts proofs. With https://github.com/ethereum-optimism/specs/issues/122#issuecomment-2293704907 as context, the L1 attributes data is in history (transaction calldata) rather than storage. This means that during derivation, L2 history is able to be accessed arbitrarily. State is a bit trickier to access arbitrarily without archival state access. This proposal involved moving the L1 attributes data to storage by putting it into L2 state and deduplicating the data from being posted over and over again in each L1 attributes transaction.\r\n\r\nThis can be considered an optimization in the future. Perhaps the model that we want is in each epoch (where sequencer number is 0) we emit an event that includes the most recent L1 attributes data and then store the rest into storage. This gives a \"snapshot\" of the data over time so that only a small amount of reexecution is required to reproduce the state at a specific L2 blocknumber.\r\n\r\nAchieving this will help to remove the need to iterate through all L1 logs searching for `UpdateType` events, making the derivation pipeline cheaper to execute.\r\n\r\nWe will still be following the architecture in https://github.com/ethereum-optimism/specs/issues/122#issuecomment-2308584680 but not including any `UpdateType` enum values", "2024-09-03T20:15:22Z", "2024-09-03T20:16:37Z", "tynes", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6K56jd", "I_kwDOLB-lzc6FZWWo", "> This can be considered an optimization in the future. Perhaps the model that we want is in each epoch (where sequencer number is 0) we emit an event that includes the most recent L1 attributes data and then store the rest into storage. This gives a \"snapshot\" of the data over time so that only a small amount of reexecution is required to reproduce the state at a specific L2 blocknumber.\r\n\r\nI'm a bit confused by the purpose of \"snapshot\".\r\n\r\nThere're 3 typical cases to access `SystemConfig`:\r\n1. in `L1Traversal.AdvanceL1Block`, where this component needs to know the `SystemConfig` for a corresponding **L1** block. This component should be able to compute `SystemConfig` soly from L1 events.\r\n2. in `L2Client.SystemConfigByL2Hash`, where this component needs to know the `SystemConfig` for a corresponding **L2** block, mostly when reorganization.\r\n    1. I think the purpose of \"snapshot\" is to make it possible to compute the  the `SystemConfig` for a corresponding L2 block by `calculate(snapshot, subsequent_system_config_updates_by_parsing_L2_tx)` instead of from L2 history state, is it right? But it seems unnecessary since if you needs to reorg to a history L2 block, you must already have that L2 history state anyway..\r\n3. for DAPPs, where the user should query the `L1Block` contract to know the latest `SystemConfig` in L2. The `SystemConfig` maintained by `L1Block` contract is a result of executing deposit transactions generated by L1.\r\n", "2024-09-05T01:44:15Z", "2024-09-05T05:17:59Z", "blockchaindevsh", "2025-08-30 20:53:13"]
["IC_kwDOLB-lzc6MU6xo", "I_kwDOLB-lzc6FZWWo", "Including historical context for the diff based L1 attributes tx as one attempt to deduplicate calldata from the L1 attributes tx: https://github.com/ethereum-optimism/specs/pull/375", "2024-09-17T01:09:44Z", "2024-09-17T01:09:54Z", "tynes", "2025-08-30 20:53:13"]
["IC_kwDOJ_r-bs6L3db-", "I_kwDOJ_r-bs6V9mAf", "This should be addressed by #564.\r\n\r\nWe will want to write a runbook explaining how an engineer needs to green-light the CI checks from PRs from forks. They will need to check there is no malicious code.", "2024-09-12T15:10:39Z", "2024-09-12T15:10:39Z", "geoknee", "2025-08-30 20:53:26"]
["IC_kwDOJ_r-bs6MlCL0", "I_kwDOJ_r-bs6V9mAf", "This issue is completed by this pr: [use the proxyd endpoints and configure circle ci jobs to use known ip range](https://github.com/ethereum-optimism/superchain-registry/pull/605)", "2024-09-18T13:45:43Z", "2024-09-18T13:45:43Z", "bitwiseguy", "2025-08-30 20:53:26"]
["IC_kwDODjvEJM6MC_kH", "I_kwDODjvEJM6WgfI1", "Can I move this into EVM Engineering? Seems appropriate there.", "2024-09-13T17:34:32Z", "2024-09-13T17:34:32Z", "smartcontracts", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6LJWAT", "I_kwDODjvEJM6VpyzG", "We have agreed that structs for inputs are going to be messy, and not scale well as feature develop continues changing inputs. Therefore, we should remove input structs and move to a setter approach like what we do with the output contracts. (The output contracts do also contain structs that we should consider removing as well, in case this simplifies cases where outputs change). That refactor will take a bit of time, however.\r\n\r\nTherefore to avoid blocking interop we will first support the interop contracts using the current struct-based input approach. Instead of doing it the \"proper\" (messy) way with new input structs, we just assume the dependency manager role is the same as the proxyAdminOwner and therefore the inputs don't need to change. For example, see:\r\n\r\nhttps://github.com/ethereum-optimism/optimism/blob/76ade4d91044368cbb91e7f00a1add18bb9ed43c/packages/contracts-bedrock/src/L1/OPStackManagerInterop.sol#L37-L53\r\n\r\nAfterwards, we'll refactor the deploy scripts and their corresponding IO contracts to remove structs and use a setter based approach", "2024-09-06T17:07:27Z", "2024-09-06T17:31:57Z", "mds1", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6LKnfJ", "I_kwDODjvEJM6VpyzG", "I've updated the title here, since interop is now unblocked by our workaround. A good way to verify this refactor is successful is to enable the interop `dependencyManager` to be set (we currently hardcode it), and ensure the diff is simple", "2024-09-06T21:11:09Z", "2024-09-06T21:11:09Z", "mds1", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6Lu1zP", "I_kwDODjvEJM6VpyzG", "#11833 did not full resolve this, so reopening", "2024-09-11T17:45:55Z", "2024-09-11T17:45:55Z", "mds1", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6GIYaz", "I_kwDODjvEJM6QMMWa", "update:\r\nfor op-plasma, some logic already affects the repo https://github.com/ethereum-optimism/superchain-registry:\r\n\r\nsuperchain-registry using code from:\r\nhttps://github.com/ethereum-optimism/optimism/blob/ddc37daa49558c2fb5c1a92e694eeb7de5942e00/op-node/rollup/superchain.go#L53-L57\r\n\r\nHowever when generate the rollup.json the logic is:\r\nhttps://github.com/ethereum-optimism/optimism/blob/ddc37daa49558c2fb5c1a92e694eeb7de5942e00/op-chain-ops/genesis/config.go#L621-L626\r\n\r\nwe can see that in superchain.go, L54 does not have field `CommitmentType`, so that when we generate a normal rollup.json, but in superchain-registry repo, it can not pass the check for the step `check-rollup-config`\r\n", "2024-07-25T13:37:10Z", "2024-07-25T13:37:10Z", "atenjin", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6GJ2HX", "I_kwDODjvEJM6QMMWa", "Hi @atenjin , thanks very much for your detailed report. I'll try to answer your concerns individually.\r\n\r\n> in the above pr, it just modify the checking stage for deploy-config, when load the rollup.json file, the checking is still existing, it still requires the challengewindow can not be zero:\r\n\r\nYes, this was a miss on my part when making this PR, it should be checking the rollup as you point out. The configuration of AltDA is still pretty rough, I agree. For now my suggestion is to include small Challenge/Resolve windows in your configuration. There's two reasons for that:\r\n* The current design of the DA State system assumes a non-0 window value when managing the commitments. When I made the original PR I wasn't worried about a 0-value, but now that we're talking about it more, I would want to be more cautious that the system supports this correctly.\r\n* Generic Commitments are intended to potentially support challenges in the future. For this reason, having challenge windows ready in your config would be good.\r\n\r\n> In op-node logic, it still uses all code related to KeccakCommitment even for now we already use GenericCommitment, even the \"challenge\" concept is not existed for GenericCommitment:\r\n\r\nThis is by design -- both commitment types are meant to be handled the same way within the `op-node`, so they share logic. However, Generic Commitments do not currently have a challenge mechanism, and so they *are* treated differently in that way. This is because Generic Commitments don't have unified way of being validatied, each DA Provider implements the validation logic within their DA Server\r\n\r\n> As a conclusion, if GenericCommitment does not have the concept related to \"ChallengeDA\", this part need to refactor.\r\n\r\nIt does not presently have one, but we would like to maintain flexibility and shared architecture so that in the future it may use these ChallengeStatus features.\r\n\r\n> When try to migrate from KeccakCommitment to GenericCommitment\r\n> But when new syncing node from genesis, especially when syncing mode is consensus-layer in op-node, it must require the guy to set keccakCommitment in rollup.json first, and when he see the finality is stuck can contain those error log, then recover to GenericCommitment and restart.\r\n\r\nThis sounds like an expected rough-edge with doing a migration like this. Because the Commitment Type details are part of the node config, and not an L1 derived attribute, nodes don't have a way of coordinating this switch.", "2024-07-25T15:41:37Z", "2024-07-25T15:41:37Z", "axelKingsley", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6GVgGd", "I_kwDODjvEJM6QMMWa", "> Hi @atenjin , thanks very much for your detailed report. I'll try to answer your concerns individually.\r\n> \r\n> > in the above pr, it just modify the checking stage for deploy-config, when load the rollup.json file, the checking is still existing, it still requires the challengewindow can not be zero:\r\n> \r\n> Yes, this was a miss on my part when making this PR, it should be checking the rollup as you point out. The configuration of AltDA is still pretty rough, I agree. For now my suggestion is to include small Challenge/Resolve windows in your configuration. There's two reasons for that:\r\n> \r\n> * The current design of the DA State system assumes a non-0 window value when managing the commitments. When I made the original PR I wasn't worried about a 0-value, but now that we're talking about it more, I would want to be more cautious that the system supports this correctly.\r\n> * Generic Commitments are intended to potentially support challenges in the future. For this reason, having challenge windows ready in your config would be good.\r\n> \r\n> > In op-node logic, it still uses all code related to KeccakCommitment even for now we already use GenericCommitment, even the \"challenge\" concept is not existed for GenericCommitment:\r\n> \r\n> This is by design -- both commitment types are meant to be handled the same way within the `op-node`, so they share logic. However, Generic Commitments do not currently have a challenge mechanism, and so they _are_ treated differently in that way. This is because Generic Commitments don't have unified way of being validatied, each DA Provider implements the validation logic within their DA Server\r\n> \r\n> > As a conclusion, if GenericCommitment does not have the concept related to \"ChallengeDA\", this part need to refactor.\r\n> \r\n> It does not presently have one, but we would like to maintain flexibility and shared architecture so that in the future it may use these ChallengeStatus features.\r\n> \r\n> > When try to migrate from KeccakCommitment to GenericCommitment\r\n> > But when new syncing node from genesis, especially when syncing mode is consensus-layer in op-node, it must require the guy to set keccakCommitment in rollup.json first, and when he see the finality is stuck can contain those error log, then recover to GenericCommitment and restart.\r\n> \r\n> This sounds like an expected rough-edge with doing a migration like this. Because the Commitment Type details are part of the node config, and not an L1 derived attribute, nodes don't have a way of coordinating this switch.\r\n\r\nThanks for your answer, but it seems that you answer does not solve any issue in above discussion.\r\n\r\nHave you tested the alt-da using GenericCommitment with s3 once? In our testing, we try to use GenericCommitment with s3, and we meet warn logs:\r\n```\r\n2024-07-22 18:24:54.393\t\r\nt=2024-07-22T10:24:54+0000 lvl=warn msg=\"challenge expired, skipping batch\" comm=\"\\xf1\\xf4\\xb4.+\\r`T=/\\x11\\x91\\xfa\\xb9\\x8f^\\xcb\\xfe<\\xc36\\xe33\\n\\xb4~\\xe7\\xf3\\xfb\\x02\u01ba\"\r\n2024-07-22 18:22:54.073\t\r\nt=2024-07-22T10:22:54+0000 lvl=warn msg=\"challenge expired, skipping batch\" comm=\"y\\x1e@i\\xc7L\\xe7\\xfc\u15ee\\xef\\xf3\\xd65\\x8d\\x97\\x1c\\xa1\\xad\\x11\\xfd\\x86w\\nB\\x14e\\x9bq\\xbb\\xa5\"\r\n```\r\nFor this devnet, in deploy-config, we try to set the challengewindow/resolvewindow to 1 (which I discuss this in first point, also match you said we can set a small value), and launched this devnet. **But the safe/finality is stuck**\r\nAnd for this warn log, it seems that it related to the second point that I discuss.\r\nI think I meet this just becuase you do not want to split the logic for KeccakCommitment and GenericCommitment. \r\n\r\nObviously, if the dachallenge contract is not existed, why the op-node think the challenge is expired?\r\n\r\nThus, the challenge logic in op-node affects GenericCommitment, then let the safe/finality stuck.\r\nAnyway, we will continue to do more test for this, while if you can test this case in your local and everything is fine, please also tell us.", "2024-07-27T05:34:35Z", "2024-07-27T05:38:02Z", "atenjin", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6G3I6y", "I_kwDODjvEJM6QMMWa", "Update:\r\n\r\nrelated to 2, the challenge concept also affects the batcher logic:\r\nhttps://github.com/ethereum-optimism/optimism/blob/ddc37daa49558c2fb5c1a92e694eeb7de5942e00/op-batcher/batcher/service.go#L217-L219\r\n\r\nThe `MaxInputSize` is `130672`, while the related to op-node logic has fixed this part, checking the commitment type:\r\nhttps://github.com/ethereum-optimism/optimism/blob/ddc37daa49558c2fb5c1a92e694eeb7de5942e00/op-node/rollup/derive/plasma_data_source.go#L95-L99\r\n\r\nWe need to ensure all corner case has been fixed so that we can try to use", "2024-08-01T09:38:30Z", "2024-08-01T09:38:30Z", "atenjin", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6LoYIE", "I_kwDODjvEJM6QMMWa", "@atenjin if you share your docker compose file you\u2019re using to test this config for the devnet, I can give it a spin and try to reproduce the behavior.", "2024-09-11T04:51:29Z", "2024-09-11T04:51:29Z", "samlaf", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6Ls2FK", "I_kwDODjvEJM6QMMWa", "> @atenjin if you share your docker compose file you\u2019re using to test this config for the devnet, I can give it a spin and try to reproduce the behavior.\r\n\r\nIn my view, I think in this repo already has enough tool to launch the devnet, do not need some docker compose scripts of files to launch one.\r\n\r\nJust the normal way to launch the op-plasma with `KeccakCommitment` rollup", "2024-09-11T14:10:36Z", "2024-09-11T14:10:36Z", "atenjin", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6E9DoS", "I_kwDODjvEJM6NWK-F", ":), Is it fixed?", "2024-07-16T10:51:49Z", "2024-07-16T10:51:49Z", "lenny0x", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6FCQ6o", "I_kwDODjvEJM6NWK-F", "Not yet, but it's very slow so not a threat.", "2024-07-16T23:09:53Z", "2024-07-16T23:09:53Z", "ajsutton", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6LcGnz", "I_kwDODjvEJM6NWK-F", "Interestingly, the memory usage does actually flatten out after about 4 weeks or running.  The game window is 28 days so I wonder if games which are completed when challenger starts use less memory than ones that are initially in progress and then later complete.  The player created for already complete games is a minimal instance: https://github.com/ethereum-optimism/optimism/blob/db61d2bbee7f36c20e778c3b9f2eef68b4fad8c7/op-challenger/game/fault/player.go#L92-L105\r\n\r\nPotentially we could replace the player.act value with a do-nothing version once the game is completed which would allow the agent, trace provider and other components to be released. The one risk (which applies at startup too) is if there's a L1 reorg and the game is \"uncompleted\" again later. It wouldn't be playable as the chess clocks would have all expired but it may required a new transaction be sent to resolve the game. Would be very rare for the existing resolve tx to not just get included on the fork at some point though.", "2024-09-10T00:50:12Z", "2024-09-10T00:50:12Z", "ajsutton", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6LcH_G", "I_kwDODjvEJM6NWK-F", "Turns out we're already caching the game status so replacing the `act` function with a noop won't make any difference.", "2024-09-10T00:57:08Z", "2024-09-10T00:57:08Z", "ajsutton", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6B5cyG", "I_kwDODjvEJM6MirWc", "Based on conversations last week around the deposit-tx milestone: we can make the \"is deposit\" boolean visible to the inbox contract, to disable force-inclusion of interop messages, which could otherwise pass through invalid messages.\n\nProposal to not modify the execution engine, and instead use system-deposits to add the behavior, and use statically pre-declared messages as mechanism to verify interop messages:\n\nFunctionality to add now for first devnet:\n- modify CrossL2Inbox with an authenticated function that disables it.\n- flip CrossL2Inbox \"off\" with a system deposit\n- process deposits, they won't be able to use the inbox. Instead just revert.\n- flip CrossL2Inbox back \"on\" with a system deposit\n\n----\nBelow this is still in a design phase and is not being prioritized for the first mainnet release\n\nFunctionality to add next (later devnet iteration):\n- flip CrossL2Inbox to \"deposits\" mode.\n- derivation should mutate deposits:\n  - If the deposit directly targets the cross-l2-inbox, then:\n    - Check if a valid message is being inserted. If not valid, then replace the sender with address X which is not authorized. If valid, then address Y which is authorized.\n  - Modify the CrossL2Inbox contract to authenticate the insertions. If it is X then event-log it as invalid. If it is Y then event-log it as valid, and store a message-hash in the state. \n  - Other deposits are as-is, and can interact with the inbox. The inbox should check its storage for a pre-validated message hash. If a deposit, but no message hash, then revert (and maybe event-log to clarify the failure, and support retry schemes).\n- flip CrossL2Inbox back to \"regular\" mode.\n\nIf there is an external reorg, or unavailable data, we can't immediately verify the message declared in a deposit. To mitigate this, we can enforce the statically declared message deposit to only pull in a message older than a sequence-window, and otherwise mark the deposit as invalid (change sender to X).\n", "2024-06-19T18:58:11Z", "2024-07-30T19:06:09Z", "protolambda", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6B5hXX", "I_kwDODjvEJM6MirWc", "We will want to spec this new behavior in the predeploys. The `L1Block` contract would need a new getter, something like `isDeposit()(bool)`. The `CrossL2Inbox` would add an invariant `if (l1Block.isDeposit() == false) revert Unauthorized();` and then we can relax that restriction in the future like @protolambda said\r\n\r\nOne major question is whether the `isDeposit()` function should be only callable by the `CrossL2Inbox`. Do we want arbitrary smart contracts to be able to know this information? It would be used to build censorship features like https://github.com/ethereum-optimism/design-docs/pull/30", "2024-06-19T19:15:12Z", "2024-06-19T19:15:12Z", "tynes", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6CHuwv", "I_kwDODjvEJM6MirWc", "We could avoid mutating deposits by adding another function on the `OptimismPortal` that emits the deposit transaction event but this still does not solve the problem with the sequence window. So my current understanding of the proposed solution is fully in derivation, so the deposit is mutated if 1) the identifier doesn't point to the message or 2) the message is newer than the sequencing window of the remote chain.\n\nThis would mean we need to standardize the sequencing window between chains, since there is no way to know what the value is, unless we use the superchain registry config or assume that they are always the same. I don't love using the superchain registry approach as that is not scalable, perhaps we hardfork in a constant value and then make it configurable in the future (for quality of life for L3s)\n\nThe function that registers the events in the `CrossL2Inbox`, would that be `onlyDeposit() && onlyEOA()`? This would allow the derivation pipeline to do static analysis, this would be a reliable way to ensure static analysis", "2024-06-21T16:26:59Z", "2024-06-21T16:26:59Z", "tynes", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6CHzFB", "I_kwDODjvEJM6MirWc", "> One major question is whether the `isDeposit()` function should be only callable by the `CrossL2Inbox`. Do we want arbitrary smart contracts to be able to know this information? It would be used to build censorship features like [ethereum-optimism/design-docs#30](https://github.com/ethereum-optimism/design-docs/pull/30)\r\n\r\nI'd lean towards not expanding ABI use scope in principle. If there's demand for a valid use-case (non-censoring, etc) for a no-auth `isDeposit` then we can add it in later.\r\n\r\n", "2024-06-21T16:38:08Z", "2024-06-21T16:38:55Z", "Inphi", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6CH6QR", "I_kwDODjvEJM6MirWc", "In my mind there are 2 ways to think about this problem, one is completely within the smart contracts and one is completely within the derivation pipeline. The [smart contract](https://github.com/ethereum-optimism/optimism/issues/10887#issuecomment-2182036414) based solution was partially described as well as the  [derivation pipeline](https://github.com/ethereum-optimism/optimism/issues/10867#issuecomment-2179320966) based solution. We should explore the tradeoffs between these two, once we introduce mutating deposits then it may become tech debt eventually as its a new class of behavior", "2024-06-21T16:58:10Z", "2024-06-21T16:58:10Z", "tynes", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6Gggym", "I_kwDODjvEJM6MirWc", "Specs in progress: https://github.com/ethereum-optimism/specs/pull/258", "2024-07-29T18:51:05Z", "2024-07-29T18:51:05Z", "tynes", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6Gp6TV", "I_kwDODjvEJM6MirWc", "~I'll propose an alternative approach:~\r\n~Instead of having to upgrade the sensitive `L1Block` contract, and having to turn on `isDeposit` at the start of the block, while also having to turn it off after \"all\" deposits have been processed.~\r\n~Have you previously discussed the following:~\r\n~on `L2CrossDomainMessenger.relayMessage(...)` inside `if (_isOtherMessenger()) {` we can set a transient `isDeposit` flag.~\r\n~then, on `CrossL2Inbox` before emitting the `ExecutingMessage` a call to `L2CrossDomainMessenger.isDeposit()(bool)` is done, if the return value is `true` `CrossL2Inbox` will revert.~\r\n~this revert is then caught by the `L2CrossDomainMessenger` and the message could be manually re-relayed at a later date, or eventually expired back to L1.~\r\n\r\n~I think this gets rid of the problem of having the `isDeposit` flag linger to the next block. what was the rationale for this? maybe I'm missing something.~\r\n\r\n~this will only require an quick and easy upgrade to the `L2CrossDomainMessenger` which we already are planning to upgrade.~\r\n~plus minimal changes to the `CrossL2Inbox`.~\r\n\r\n\r\nEDIT: There is no way to enforce a single entry point nor a single source of the txs on-chain.", "2024-07-30T20:15:09Z", "2024-07-30T20:48:20Z", "skeletor-spaceman", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6Gp7q5", "I_kwDODjvEJM6MirWc", "> I'll propose an alternative approach: Instead of having to upgrade the sensitive `L1Block` contract, and having to turn on `isDeposit` at the start of the block, while also having to turn it off after \"all\" deposits have been processed. Have you previously discussed the following: on `L2CrossDomainMessenger.relayMessage(...)` inside `if (_isOtherMessenger()) {` we can set a transient `isDeposit` flag. then, on `CrossL2Inbox` before emitting the `ExecutingMessage` a call to `L2CrossDomainMessenger.isDeposit()(bool)` is done, if the return value is `true` `CrossL2Inbox` will revert. this revert is then caught by the `L2CrossDomainMessenger` and the message could be manually re-relayed at a later date, or eventually expired back to L1.\r\n> \r\n> I think this gets rid of the problem of having the `isDeposit` flag linger to the next block. what was the rationale for this? maybe I'm missing something.\r\n> \r\n> this will only require an quick and easy upgrade to the `L2CrossDomainMessenger` which we already are planning to upgrade. plus minimal changes to the `CrossL2Inbox`.\r\n\r\nThis is an interesting approach but not all deposits MUST go through the `L2CrossDomainMessenger`. Somebody can call `OptimismPortal.depositTransaction` with any L2 target, meaning there isn't a common entrypoint to hook into", "2024-07-30T20:18:46Z", "2024-07-30T20:18:46Z", "tynes", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6ITV_3", "I_kwDODjvEJM6MirWc", "Needs a review and a few comments to be resolved. Smart contract wise, everything is done. Very close to being complete", "2024-08-13T17:32:51Z", "2024-08-13T17:32:51Z", "tynes", "2025-08-30 20:53:59"]
["IC_kwDODjvEJM6GLpcF", "I_kwDODjvEJM6MirZw", "We are waiting for the specs to be ready. Current discussion can be tracked in the following [Design Docs PR](https://github.com/ethereum-optimism/design-docs/pull/50)", "2024-07-25T18:57:25Z", "2024-07-25T18:57:25Z", "0xParti", "2025-08-30 20:53:59"]
["IC_kwDOMMiGhs6MqL4F", "I_kwDOMMiGhs6XGo7L", "Looks like updating my foundry version fixed this", "2024-09-19T02:47:13Z", "2024-09-19T02:47:13Z", "smartcontracts", "2025-08-30 20:53:59"]
["IC_kwDOMMiGhs6MorvH", "I_kwDOMMiGhs6XD-MU", "thx for reporting! put out a fix here https://github.com/ethereum-optimism/supersim/pull/153 and also an issue for longer term issue. https://github.com/ethereum-optimism/supersim/issues/154\r\n\r\nalso added back how to build / run", "2024-09-18T21:56:51Z", "2024-09-18T21:56:51Z", "jakim929", "2025-08-30 20:53:59"]
["IC_kwDOKSJyfM6NlnnX", "I_kwDOKSJyfM6VOYtP", "for now we will just be doing alerting through slack.", "2024-09-25T23:35:00Z", "2024-09-25T23:35:00Z", "tremarkley", "2025-08-30 21:55:46"]
["IC_kwDOKIwiaM6MVBRe", "I_kwDOKIwiaM6WuUJW", "### Tutorial title\r\n\r\nWithdrawing ETH form Op-stack to Ethereum \r\n\r\n### Tutorial description\r\n\r\nThis tutorial explains how you can use the Viem to bridge ETH from L2 (OP Mainnet or OP Sepolia) to L1 (Ethereum or Sepolia) to . The Viem SDK is an easy way to add bridging functionality to your JavaScript-based application. It also provides some safety rails to prevent common mistakes that could cause ETH or ERC-20 tokens to be made inaccessible.\r\n\r\n### This is just the code snippet\r\n\r\n\r\n### Tutorial tags\r\n\r\nbridging\r\n\r\n### Skill level\r\n\r\nBeginner\r\n\r\n### Hosted on Optimism.io or hosted elsewhere?\r\n\r\nHosted on optimism.io\r\n\r\n### For tutorials to be hosted on Optimism.io: Tutorial Content\r\n\r\n1) ` mkdir op-sample-project`\r\n\r\n2) `cd op-sample-project`\r\n\r\n3) `pnpm init`\r\n\r\n4) `pnpm i viem`\r\n\r\n5) `node`\r\n\r\n``` js\r\n// STEP 1\r\nconst { createPublicClient, http, createWalletClient, parseEther } = await import(\"viem\");\r\nconst {sepolia, optimismSepolia } = await import(\"viem/chains\"); /* baseSepolia, liskSepolia, zoraSepolia, modeTestnet etc..... */\r\nconst { privateKeyToAccount } = await import('viem/accounts');\r\nconst { publicActionsL1, publicActionsL2, walletActionsL1, walletActionsL2, getWithdrawals } = await import('viem/op-stack')\r\n\r\n```\r\n``` js\r\n// STEP 2\r\nconst account = privateKeyToAccount(\r\n  '0x....'\r\n);\r\n```\r\n\r\n``` js\r\n\r\n// STEP 3\r\nconst publicClientL1 = createPublicClient({\r\n  chain: sepolia, /* or mainnet */\r\n  transport: http(),\r\n}).extend(publicActionsL1())\r\n```\r\n``` js\r\n// STEP 4\r\nconst walletClientL1 = createWalletClient({\r\n  account,\r\n  chain: sepolia, /* or mainnet */\r\n  transport: http(),\r\n}).extend(walletActionsL1());\r\n```\r\n``` js\r\n\r\n// STEP 5\r\nconst publicClientL2 = createPublicClient({\r\n  chain: optimismSepolia,\r\n  transport: http(\r\n    'https://opt-sepolia.g.alchemy.com/v2/keyyyyyyy'\r\n  ),\r\n}).extend(publicActionsL2())\r\n```\r\n```js\r\n// STEP 6\r\nconst walletClientL2 = createWalletClient({\r\n  account,\r\n  chain: optimismSepolia,\r\n  transport: http(\r\n    'https://opt-sepolia.g.alchemy.com/v2/7Tlr3E71IFClmRZ50mtyA0PXgEM3kr9d'\r\n  ),\r\n}).extend(walletActionsL2())\r\n```\r\n\r\n``` js\r\n\r\n// Build parameters to initiate the withdrawal transaction on the L1.\r\nconst args = await publicClientL1.buildInitiateWithdrawal({\r\n  to: account.address,\r\n  value: parseEther('1')\r\n})\r\n \r\n```\r\n\r\n```js\r\n// Execute the initiate withdrawal transaction on the L2.\r\nconst hash = await walletClientL2.initiateWithdrawal(args)\r\n```\r\n\r\n```js\r\n// Wait for the initiate withdrawal transaction receipt.\r\nconst receipt = await publicClientL2.waitForTransactionReceipt({ hash })\r\n```\r\n\r\nWait one hour (max) for the L2 Output containing the transaction to be proposed\r\n\r\n```js\r\n// Wait until the withdrawal is ready to prove. \r\nconst { output, withdrawal } = await publicClientL1.waitToProve({\r\n  receipt,\r\n  targetChain: walletClientL2.chain\r\n})\r\n\r\n```\r\n```js\r\n// Build parameters to prove the withdrawal on the L2.\r\nconst proveArgs = await publicClientL2.buildProveWithdrawal({\r\n  output,\r\n  withdrawal,\r\n})\r\n```\r\n\r\n```js\r\n// Prove the withdrawal on the L1.\r\nconst proveHash = await walletClientL1.proveWithdrawal(proveArgs)\r\n\r\n```\r\n\r\n```js\r\n// Wait until the prove withdrawal is processed.\r\nconst proveReceipt = await publicClientL1.waitForTransactionReceipt({\r\n  hash: proveHash\r\n})\r\n\r\n```\r\nBefore a withdrawal transaction can be finalized, we will need to wait the finalization period of 7 days \r\n```js\r\n// Wait until the withdrawal is ready to finalize.\r\nawait publicClientL1.waitToFinalize({\r\n  targetChain: walletClientL2.chain,\r\n  withdrawalHash: withdrawal.withdrawalHash,\r\n})\r\n```\r\n\r\n```js\r\n// Finalize the withdrawal.\r\nconst finalizeHash = await walletClientL1.finalizeWithdrawal({\r\n  targetChain: walletClientL2.chain,\r\n  withdrawal,\r\n})\r\n```\r\n\r\n```js\r\n\r\n// Wait until the withdrawal is finalized.\r\nconst finalizeReceipt = await publicClientL1.waitForTransactionReceipt({\r\n  hash: finalizeHash\r\n})\r\n```\r\n\r\n\r\n\r\n\r\n\r\n### For tutorials hosted elsewhere: URL to tutorial\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\nThis is just a snippet, the wordings will need to be modified  to suit the doc spec", "2024-09-17T01:23:06Z", "2024-09-17T01:23:06Z", "brokewhale", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM6J4En6", "I_kwDOKIwiaM6Tvajk", "https://superbridge.app/\r\nhttps://github.com/superbridgeapp/superbridge-app\r\nsuperbridge has a Escape hatch which works like force withdrawal,but I never tested it yet", "2024-08-27T17:50:25Z", "2024-08-27T17:50:25Z", "imtipi", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM56wrXZ", "I_kwDOKIwiaM6E_ZiG", "Someone also says `op-contracts/v1.3.0` branch can't deploy\r\n\r\nhttps://github.com/ethereum-optimism/developers/discussions/344\r\n\r\n> It still fail after switch to op-contracts/v1.3.0. cause of failure seems to be latest release of foundry.\r\n> set specific version when install foundry. below is example\r\n> https://github.com/gokch/optimism_builder/blob/main/init/Dockerfile#L21\r\n\r\nhttps://github.com/ethereum-optimism/developers/discussions/329\r\n\r\n> Hey, I solved this problem by following their suggestions. OP has made changes in the codebase to generate the genesis and rollup. You may want to follow the discussion below to make it work. Two highlights: 1. uses https://github.com/ethereum-optimism/optimism/tree/op-contracts/v1.3.0 to deploy v1 contract 2.write your l1-deployment json file like https://github.com/ethereum-optimism/superchain-registry/blob/main/superchain/extra/addresses/sepolia/base.json", "2024-04-16T17:23:45Z", "2024-04-16T17:23:45Z", "Chomtana", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM56wrtf", "I_kwDOKIwiaM6E_ZiG", "Branch `op-contracts/v1.3.0` and beyond doesn't have `sync()` anymore but the document is still using `sync()`", "2024-04-16T17:24:44Z", "2024-04-16T17:24:44Z", "Chomtana", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM56zL2J", "I_kwDOKIwiaM6E_ZiG", "In the \"Creating Your Own L2 Rollup Testnet\" tutorial, two issues are currently being faced:\r\n\r\n- The `forge script scripts/Deploy.s.sol:Deploy --sig 'sync()' --rpc-url $L1_RPC_URL` in the tutorial is no longer applicable when users create using the release branch instead of the tutorials/chains branch.\r\n https://github.com/ethereum-optimism/developers/discussions/110\r\n\r\n\r\n- Even when following the steps in the tutorials/chains branch, users are still encountering issues.\r\n https://github.com/ethereum-optimism/optimism/issues/10114\r\nhttps://github.com/ethereum-optimism/developers/discussions/354\r\nhttps://discord.com/channels/667044843901681675/1080867873997729874/1229086983486443631\r\nhttps://discord.com/channels/667044843901681675/1080867873997729874/1229518752895336529\r\nhttps://discord.com/channels/667044843901681675/1080867873997729874/1229248209529077880\r\n\r\n it seems that all the issues point towards the process involving the` forge script scripts/Deploy.s.sol:Deploy --sig 'sync()' --rpc-url $L1_RPC_URL.`", "2024-04-17T02:43:52Z", "2024-04-17T02:52:42Z", "opfocus", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM560PnN", "I_kwDOKIwiaM6E_ZiG", "When I deployed the op according to the tutorial, i ran into three new issues:\r\n\r\n1. executing `forge script scripts/Deploy.s.sol:Deploy --private-key $GS_ADMIN_PRIVATE_KEY --broadcast --rpc-url $L1_RPC_URL --slow` to deploy the contract to L1, it comes up:\r\n```\r\nError: \r\nFailed to send transaction\r\n\r\nContext:\r\n- server returned an error response: error code -32000: max priority fee per gas higher than max fee per gas\r\n```\r\nsolution:\r\nremove `--slow` flag and add `--with-gas-price <value>` flag, such as `--with-gas-price 2gwei`.\r\n\r\n2. Newer versions dont rely on the artifacts. So create genesis files command should be changed to:\r\n```\r\ngo run cmd/main.go genesis l2 \\\r\n  --deploy-config ../packages/contracts-bedrock/deploy-config/getting-started.json \\\r\n  --l1-deployments ../packages/contracts-bedrock/deployments/getting-started/.deploy \\\r\n  --outfile.l2 genesis.json \\\r\n  --outfile.rollup rollup.json \\\r\n  --l1-rpc $L1_RPC_URL\r\n```\r\n\r\n3. since artifacts have been remove, so some of the artifact-dependent commands should be modified and described accordingly, for example `op-proposer` start command:\r\n```\r\n./bin/op-proposer \\\r\n  --poll-interval=12s \\\r\n  --rpc.port=8560 \\\r\n  --rollup-rpc=http://localhost:8547 \\\r\n  --l2oo-address=$(cat ../packages/contracts-bedrock/deployments/getting-started/L2OutputOracleProxy.json | jq -r .address) \\\r\n  --private-key=$GS_PROPOSER_PRIVATE_KEY \\\r\n  --l1-eth-rpc=$L1_RPC_URL\r\n```", "2024-04-17T06:58:20Z", "2024-04-17T08:17:07Z", "420516460", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM5645wG", "I_kwDOKIwiaM6E_ZiG", "Another report of this tutorial being broken: https://github.com/ethereum-optimism/developers/discussions/351\r\nAdditional context: https://github.com/ethereum-optimism/developers/discussions/17#discussioncomment-9145846", "2024-04-17T16:42:01Z", "2024-04-17T17:23:06Z", "sbvegan", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM6BJFdR", "I_kwDOKIwiaM6E_ZiG", "The chainid.link for connecting a wallet to the rollup also does not work currently, although just using metamask seems fine. Setting the correct RPC address and chain ID is needed though", "2024-06-13T19:44:52Z", "2024-06-13T19:45:30Z", "richardhuaaa", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM6NPARg", "I_kwDOKIwiaM6E_ZiG", "closing in favor of https://github.com/ethereum-optimism/docs/issues/917", "2024-09-23T21:43:06Z", "2024-09-23T21:43:06Z", "sbvegan", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM52lDEv", "I_kwDOKIwiaM6BwlaU", "@astro44 was this done on the `tutorials/chain` branch?\r\n\r\nhttps://docs.optimism.io/builders/chain-operators/tutorials/create-l2-rollup#build-the-optimism-monorepo", "2024-03-11T20:52:20Z", "2024-03-11T20:52:20Z", "sbvegan", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM53xlTL", "I_kwDOKIwiaM6BwlaU", "> @astro44 was this done on the `tutorials/chain` branch?\r\n> \r\n> https://docs.optimism.io/builders/chain-operators/tutorials/create-l2-rollup#build-the-optimism-monorepo\r\n\r\nI found the same problem, the link you send was done on `tutorials/chan` branch\uff0cthe problem is on v1.7.0, there's a lot of difference.\r\n\r\nI think the `tutorials/chain` branch should keep up with the latest stable version. \r\n\r\nOn v1.7.0 branch ,there are several points not consistent with the current `tutorials/chain` branch:\r\n1. When deploy contracts on L1, we just need the the deploy step,no need for the \"sync()\" step any more\r\n2. When generating the genesis and rollup files, we should use `--l1-deployments ../packages/contracts-bedrock/deployments/getting-started/.deploy` instead of  `--deployment-dir`\r\n3. When start op-node, we should add the `--l1.beacon=xxx`\r\n4. When running the op-proposer, should change the way to get `--l2oo-address`,like `--l2oo-address= $(cat ../packages/contracts-bedrock/deployments/getting-started/.deploy | jq -r '.L2OutputOracleProxy')`\r\n", "2024-03-20T12:46:45Z", "2024-03-21T02:22:26Z", "luleigreat", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM533plQ", "I_kwDOKIwiaM6BwlaU", "> @astro44 was this done on the `tutorials/chain` branch?\r\n> \r\n> https://docs.optimism.io/builders/chain-operators/tutorials/create-l2-rollup#build-the-optimism-monorepo\r\n\r\nI have another question:\r\n\r\nI think the contracts on branch `op-contracts/xxx` was audited, right ?\r\nHow about the contracts on branch `tutorials/chain` ?", "2024-03-21T02:28:56Z", "2024-03-21T02:28:56Z", "luleigreat", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM54Nrsp", "I_kwDOKIwiaM6BwlaU", " Sorry @luleigreat, @sbvegan somehow didn't see this thread,\r\nI didn't actually make any changes so this isn't on any branch, I just made the \"suggestion\" here in the ticket.\r\n@luleigreat:\r\nI don't know if they were audited, I would assume so.... but thats a question for someone within Optimisim to answer... maybe @sbvegan can help?", "2024-03-24T15:58:24Z", "2024-03-24T15:58:24Z", "astro44", "2025-08-30 21:55:47"]
["IC_kwDOKIwiaM6NPBju", "I_kwDOKIwiaM6BwlaU", "closing in favor of https://github.com/ethereum-optimism/docs/issues/917", "2024-09-23T21:43:59Z", "2024-09-23T21:43:59Z", "sbvegan", "2025-08-30 21:55:47"]
["IC_kwDOI7W0xc5pRJ4v", "I_kwDOI7W0xc5z_AmV", "This is super helpful for us (growthepie.xyz). It is tedious work to figure out the differences between OP chains and the current metadata proposal would already solve this for us.\r\ni.e. we want to make sure that we group chains correctly (superchain, op stack, bedrock, etc.) and this shared list will help as a single source of truth. Having important links like RPCs and explorers also make our lives easier. And just having a list of OP related chains is helpful - it's already hard to keep track of all of them.\r\n\r\nMight be interesting to add mainnet_launch_date. Defining the mods in greater detail could also be helpful (block size, block time, etc.)\r\n\r\nIt seems like with the current level of complexity csv is sufficient as input. Might make sense to use yml anyway just to be prepared for more complexity. Output csv or json are both great.\r\n\r\nThe automation part of detecting new L2 contracts is interesting - we might be able to look closer into that in the coming months. ", "2023-10-17T10:13:26Z", "2023-10-17T10:13:26Z", "mseidlx", "2025-08-30 21:55:48"]
["IC_kwDOI7W0xc5pnGfg", "I_kwDOI7W0xc5z_AmV", "Fields:\r\n* I would recommend coming up with a base set of fields that apply to other stacks and try to give general names (change `op_based_version` to `version` or `protocol_version`). And then you have the `is_law_of_chains` fields, which just becomes a an OP-specific superset of the base fields.\r\n* Not sure how I feel about the `has_mods` column. While potentially useful, this may be a subjective measure or what all qualifies as a modification.\r\n* In op-analytics Repo [chain_metadata](https://github.com/ethereum-optimism/op-analytics/blob/main/op_chains_tracking/inputs/chain_metadata.csv), there is a column named `raas_deployer`, but in Dune Upload from OP Labs CRM [dataset_op_stack_chain_metadata](https://dune.com/queries/3109219), there is a column named `initial_chain_deployer`. I would much prefer using `initial_chain_deployer` as there will never be null fields. Maybe there should also be a `current_chain_deployer` field in case someone change deployers?\r\n* Dune Upload from OP Labs CRM [dataset_op_stack_chain_metadata](https://dune.com/queries/3109219) has several columns named for individual contracts. To generalize this, I would support having a single field named `contracts` (or similar), then use json/yml to nest the names of the contracts and their addresses. This would retain all the data currently in this dataset but generalizes for use with other chains.\r\n\r\n\r\n[Derive OP Chain data using the chain factory](https://stack.optimism.io/docs/understand/explainer/#derive-op-chain-data-using-the-chain-factory) Would this handle partial automation of this, at least for those opted into the Superchain? You likely could determine some of this from onchain bytecode when a new chain gets deployed, but if there's any changes in their deployment, that wouldn't work anymore.\r\n\r\nGiven that it seems like potentially dynamic metadata is wanted to be included (which I agree with), I'm not sure that any of this needs to be onchain. Unless of course there's already a need for this that I'm not aware of.\r\n\r\nFor an OP Stack specific registry, unfortunately I think you or someone else at OP Labs will have to be the approver/denier. A more open solution would be great, but would likely lead to the list getting spammed with bad data.\r\n\r\nWhile the ideal outcome is of course that projects always want to add themselves to the list, they may not care or know about the list. Of course make it as known as possible, but then maybe also tie in eligibility of going on the Superchain Eco website to first adding their data to this registry. There's never going to be a perfect solution here.", "2023-10-19T23:55:09Z", "2023-10-19T23:55:09Z", "lgingerich", "2025-08-30 21:55:48"]
["IC_kwDODjvEJM6MzdMw", "I_kwDODjvEJM6XO4H_", "When running `./build/bin/geth init --datadir=datadir genesis.json`, can you try adding `--state.scheme=hash`?  :)\r\nThis issue looks identical to https://github.com/ethereum-optimism/op-geth/issues/375.", "2024-09-19T22:18:49Z", "2024-09-19T22:18:49Z", "mininny", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6M3L-i", "I_kwDODjvEJM6XO4H_", "Thank you so much, Minhyuk Kim! Your suggestion to run geth init with --state.scheme=hash worked perfectly, and op-geth is now running smoothly. I really appreciate your help and quick response!", "2024-09-20T09:27:53Z", "2024-09-20T09:27:53Z", "WhizIkem", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6MmT65", "I_kwDODjvEJM6W74h6", "PRs:\n- op-supervisor changes by @axelKingsley : https://github.com/ethereum-optimism/optimism/pull/11956\n- op-node lazy-rpc connection by @protolambda : https://github.com/ethereum-optimism/optimism/pull/11970\n- op-geth PR was updated to lazy-dial the sequencer RPC too: https://github.com/ethereum-optimism/op-geth/pull/372", "2024-09-18T15:57:21Z", "2024-09-18T15:57:21Z", "protolambda", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6MeudD", "I_kwDODjvEJM6W6qe7", "I think you mean add an additional check here which I support.  The existing TODO check has been very useful but it is just a first step - we didn't want to delay fault proofs to take on cleaning up every TODO in the codebase.  Having it flag when an issue is closed that still has an open TODO is really useful, but that definitely shouldn't be a blocking check as noted.  If someone has time to clean up all the existing TODOs then having a check for the format as a requirement to merge would be an excellent addition.", "2024-09-17T20:30:38Z", "2024-09-17T20:30:38Z", "ajsutton", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6MSrzR", "I_kwDODjvEJM6VaB0p", "I took a snapshot of one of these panics, containing the same FPP inputs, op-program ELF binary, and preimages. And I'm no longer able to reproduce this on the latest MT-Cannon VM. And I'm no longer seeing any panic when I run `TestVerifyL2Output` multiple times.\nThis is most likely fixed by #11757.", "2024-09-16T19:09:32Z", "2024-09-16T19:09:32Z", "Inphi", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6KsZRl", "I_kwDODjvEJM6VNBBs", "@protolambda thanks for creating this issue, this sounds like a solid plan to me.", "2024-09-03T16:02:29Z", "2024-09-03T16:02:29Z", "BlocksOnAChain", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6K1ZJS", "I_kwDODjvEJM6VNBBs", "We need to make sure infra services that depend on header format don\u2019t break.", "2024-09-04T14:39:24Z", "2024-09-04T14:39:24Z", "sebastianst", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6M_E0O", "I_kwDODjvEJM6VNBBs", "Done: https://github.com/ethereum-optimism/optimism/issues/12044\r\n\r\nCreated a draft PR of op-geth changes, and a detailed plan of what I think is necessary to complete this work for Holocene.\r\n", "2024-09-21T22:44:06Z", "2024-09-21T22:44:06Z", "protolambda", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6MSUCl", "I_kwDODjvEJM6UmjEU", "Fixed via https://github.com/ethereum-optimism/optimism/pull/11704", "2024-09-16T18:24:56Z", "2024-09-16T18:24:56Z", "mbaxter", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6Mca9K", "I_kwDODjvEJM6UmjFK", "We've knocked off a couple of todos. Is this still needed?", "2024-09-17T15:32:34Z", "2024-09-17T15:32:34Z", "Inphi", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6Mddo3", "I_kwDODjvEJM6UmjFK", "Yeah, we should go ahead and finish cleaning this up - I can have a look.", "2024-09-17T17:38:05Z", "2024-09-17T17:38:05Z", "mbaxter", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6Mce1H", "I_kwDODjvEJM6UmjGO", "Fixed by https://github.com/ethereum-optimism/optimism/pull/11937", "2024-09-17T15:37:41Z", "2024-09-17T15:37:41Z", "Inphi", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6HarRu", "I_kwDODjvEJM6Ruo15", "https://github.com/ethereum-optimism/specs/pull/314", "2024-08-06T18:40:24Z", "2024-08-06T18:40:24Z", "0xParti", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6G7GwD", "I_kwDODjvEJM6RV0mj", "@protolambda I am interested in contributing to interop. Is there anything from this tests epic or from the milestone that I can pick up?", "2024-08-01T17:55:13Z", "2024-08-01T17:55:43Z", "SkandaBhat", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6HSWGg", "I_kwDODjvEJM6RV0mj", "@SkandaBhat this ticket is currently not actionable, due to blockers on the chain-deployment integration into op-e2e. See the above mentioned design docs.", "2024-08-05T19:10:26Z", "2024-08-05T19:10:26Z", "protolambda", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6GS8TE", "I_kwDODjvEJM6Q_kTP", "To be reviewed here: https://github.com/ethereum-optimism/specs/pull/298", "2024-07-26T16:37:25Z", "2024-07-26T16:37:25Z", "0xng", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6GwlID", "I_kwDODjvEJM6Q_kTP", "Merged", "2024-07-31T16:11:17Z", "2024-07-31T16:11:17Z", "0xParti", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6Ka15y", "I_kwDODjvEJM6Q6OA3", "The source of truth for invariants should be the specs", "2024-08-30T20:30:29Z", "2024-08-30T20:30:29Z", "tynes", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6MVJDR", "I_kwDODjvEJM6QdiCy", "Closed by https://github.com/ethereum-optimism/optimism/pull/11398", "2024-09-17T01:55:32Z", "2024-09-17T01:55:32Z", "tynes", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6MeE96", "I_kwDODjvEJM6UmjID", "TODO: Mofi to split up this ticket", "2024-09-17T19:09:43Z", "2024-09-17T19:09:43Z", "Inphi", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6McZnu", "I_kwDODjvEJM6UmjIU", "#11649 fixes this.", "2024-09-17T15:30:58Z", "2024-09-17T15:30:58Z", "Inphi", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6MeGNY", "I_kwDODjvEJM6UmjIU", "Closing in favor of #11649 ", "2024-09-17T19:12:26Z", "2024-09-17T19:12:26Z", "Inphi", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6EHbWb", "I_kwDODjvEJM6Oui20", "We'll likely need to do this prior to the devnet - it's actually pretty easy to wind up going 250 blocks without generating a log - having a tx isn't by itself enough, you actually have to call a contract that emits something.  So op-mainnet and op-sepolia currently start with many thousands of blocks with no logs.", "2024-07-09T05:11:07Z", "2024-07-09T05:11:07Z", "ajsutton", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6Fw74p", "I_kwDODjvEJM6Oui20", "This may be fixed by some design changes in https://github.com/ethereum-optimism/design-docs/pull/49/ but leaving open for now so it doesn't get forgotten if we don't wind up solving it through that design.", "2024-07-23T03:51:38Z", "2024-07-23T03:51:38Z", "ajsutton", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6CAQKB", "I_kwDODjvEJM6MkK5s", "Design doc: https://github.com/ethereum-optimism/design-docs/pull/35", "2024-06-20T16:32:52Z", "2024-06-20T16:32:52Z", "tynes", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6CBF4w", "I_kwDODjvEJM6MkK5s", "Related design doc: https://github.com/ethereum-optimism/design-docs/pull/36", "2024-06-20T18:48:48Z", "2024-06-20T18:48:48Z", "tynes", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6CBPT0", "I_kwDODjvEJM6MkK5s", "Another design doc:  https://github.com/ethereum-optimism/design-docs/pull/37", "2024-06-20T19:14:13Z", "2024-06-20T19:14:13Z", "tynes", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6GLrc2", "I_kwDODjvEJM6MkK5s", "We are closing the design on the following [PR](https://github.com/ethereum-optimism/specs/pull/294) to the Specs Repo.", "2024-07-25T19:02:14Z", "2024-07-25T19:02:14Z", "0xParti", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6HDSE7", "I_kwDODjvEJM6MkK5s", "Merged into main: https://github.com/ethereum-optimism/specs/pull/294", "2024-08-02T16:49:08Z", "2024-08-02T16:49:08Z", "0xParti", "2025-08-30 21:56:03"]
["IC_kwDODjvEJM6KEV9r", "I_kwDODjvEJM6Umi2p", "https://github.com/ethereum-optimism/optimism/issues/11637 paritally implements this ticket.", "2024-07-08T18:24:00Z", "2024-07-08T18:24:00Z", "Inphi", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6KEV9s", "I_kwDODjvEJM6Umi2p", "Closing this as the VMs are implemented. Additional tasks are created for testing in other issues.", "2024-08-20T15:21:56Z", "2024-08-20T15:21:56Z", "Inphi", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6BlHgr", "I_kwDODjvEJM6MirYe", "If we change the deposit-tx failure state based on interop, then we also have to be able to replicate this during tx-tracing and block re-processing (reorgs, historical state queries on full nodes, etc.)", "2024-05-16T13:24:48Z", "2024-05-16T13:24:48Z", "protolambda", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6BlHgx", "I_kwDODjvEJM6MirYe", "A solution for this is proposed in https://github.com/ethereum-optimism/design-docs/pull/13 but it needs to be improved to remove the requirement of requiring an archive node for historical sync. This is definitely possible, more thought needs to be put into it.\r\n\r\nThe scope of allowing executing messages to be triggered from deposits is cut from the scope of the devnet. The devnet will simply ban deposits that trigger executing messages for simplicity", "2024-05-20T18:43:54Z", "2024-05-20T18:43:54Z", "tynes", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6BlHgz", "I_kwDODjvEJM6MirYe", "Moving this to the testnet milestone as now we have a smaller scope devnet solution", "2024-05-20T19:26:37Z", "2024-05-20T19:26:37Z", "tynes", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6MT2ix", "I_kwDODjvEJM6MirYe", "Going to close this issue as we are no longer going to try to solve this as part of the initial devnet release. We will not allow for deposits to trigger executing messages for the initial release", "2024-09-16T20:54:27Z", "2024-09-16T20:54:27Z", "tynes", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6GLouS", "I_kwDODjvEJM6MirbC", "A first iteration of this task is being addressed in issue #11244 ", "2024-07-25T18:55:41Z", "2024-07-25T18:55:41Z", "0xParti", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6GLp4c", "I_kwDODjvEJM6MircU", "This issue has been completed and merged to main from the [same PR](https://github.com/ethereum-optimism/specs/pull/71). \nIt might be slightly modified after implementation.", "2024-07-25T18:58:26Z", "2024-07-25T18:58:26Z", "0xParti", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6BlHv-", "I_kwDODjvEJM6Mirm5", "cc @protolambda \r\n\r\nFirst Approach: In the `txpool` module, we have this interface. Per our discussion, we'll definitely need to parallelize validation but i'm going to leave that as followup and do the simple path (iteration)\r\n\r\n```\r\ntype OptimismTxPolicyStatus uint\r\n\r\nconst (\r\n\tOptimismTxPolicyUnknown OptimismTxPolicyStatus = iota\r\n\tOptimismTxPolicyInvalid\r\n\tOptimismTxPolicyValid\r\n)\r\n\r\ntype OptimismTxPoolPolicy interface {\r\n\tValidateTx(tx *types.Transaction) (OptimismTxPolicyStatus, error)\r\n}\r\n```\r\n\r\nIntroduce a new module `txpool/policies` where there can be different policies to be constructed & in the future composed together and supplied to the higher level `TxPool`. Only policy to exist for now is the `SuperchainMessagingPolicy` that maintains a connection to the superchain backend.\r\n\r\nThe tx pool keeps reference to the set policy and is invoked as transactions are added.", "2024-02-21T01:08:58Z", "2024-02-21T17:16:06Z", "hamdiallam", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6BlHwJ", "I_kwDODjvEJM6Mirm5", "For the high latency implementation, the tx pool will reject initiating messages that are not finalized or safe", "2024-02-28T18:19:25Z", "2024-02-28T18:19:25Z", "hamdiallam", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6BlHwP", "I_kwDODjvEJM6Mirm5", "This needs to be adapted to handle the low-latency case.", "2024-04-01T20:52:31Z", "2024-04-01T20:52:31Z", "protolambda", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6BlHwW", "I_kwDODjvEJM6Mirm5", "Related: https://github.com/ethereum-optimism/design-docs/pull/7", "2024-05-16T10:39:26Z", "2024-05-16T10:39:26Z", "protolambda", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6IN3Vv", "I_kwDODjvEJM6Mirm5", "Previous work: https://github.com/ethereum-optimism/op-geth/pull/251", "2024-08-13T04:51:24Z", "2024-08-13T04:51:24Z", "tynes", "2025-08-30 21:56:04"]
["IC_kwDODjvEJM6MVIzi", "I_kwDODjvEJM6Mirm5", "Closing this in favor of #10956 ", "2024-09-17T01:54:10Z", "2024-09-17T01:54:10Z", "tynes", "2025-08-30 21:56:04"]
["IC_kwDOLB-lzc6K5nLu", "I_kwDOLB-lzc6VZjZf", "I would suggest the portal retains the same ABI but can forward to the authorised caller as needed (ie build in backwards compatibility).  Breaking the withdrawal API is a big pain in the neck and something we should avoid if possible.  Otherwise this will likely be delayed until something else needs to break the API and I'm not sure what that would be...", "2024-09-05T00:15:15Z", "2024-09-05T00:15:15Z", "ajsutton", "2025-08-30 21:56:13"]
["IC_kwDOLB-lzc6N4D6X", "I_kwDOLB-lzc6VZjZf", "> I would suggest the portal retains the same ABI but can forward to the authorised caller as needed (ie build in backwards compatibility). Breaking the withdrawal API is a big pain in the neck and something we should avoid if possible. Otherwise this will likely be delayed until something else needs to break the API and I'm not sure what that would be...\r\n\r\nYeah, we should be able to manage this for the critical path (`proveWithdrawalTransaction` + `finalizeWithdrawalTransaction` + events)\r\n\r\nAs for the administrative functions (`setRespectedGameType` + `blacklistDisputeGame`) & state vars (`respectedGameType`, `disputeGameBlacklist`, `respectedGameTypeUpdatedAt`), what do you think about these? These should not be consumed by anyone else, except for maybe `viem`. It would be very nice to move these to a different location (without a fall-through, so that people like succinct don't need to have dead code in their bridge.)\r\n\r\n---\r\n\r\nIf I could wave a magic wand here, what I'd like to do (roughly) is:\r\n1. Create a new interface, `IProofSystem`. This would have:\r\n    * A view function for fetching an output proposal by index.\r\n    * A view function for checking a withdrawal's validity. Things like timestamps, etc. should be passed in. It would be on the portal to pass in honest values. It should not need to call-back to reference external state.\r\n    * For other, lower-level concerns (i.e. the blacklist, respected type, etc.), these can be hidden underneath the interface's function impls.\r\n2. Remove DG-specific functions and state vars from the `OptimismPortal2`.\r\n3. Add in the `IProofSubmitter` to the portal, re-using the slot that the `DisputeGameFactory` is currently stored in.\r\n4. Add gaps to the `respectedGameType`, `respectedGameTypeUpdatedAt`, and `disputeGameBlacklist` slots.\r\n5. Retain the other ABI verbatim (`proveWithdrawalTransaction` + `finalizeWithdrawalTransaction`, events, etc.)\r\n\r\n### Usage Analysis\r\n\r\n* `setRespectedGameType`\r\n    * 0 use in `viem`\r\n    * 0 use in `op-challenger`\r\n    * 1 usage in `DeputyGuardianModule`\r\n* `blacklistDisputeGame`\r\n    * 0 use in `viem` \r\n    * 0 use in `op-challenger`\r\n    * 1 usage in `DeputyGuardianModule`\r\n* `respectedGameType`\r\n    * 1 usage in `viem` [here](https://github.com/wevm/viem/blob/9d7d1602aa103f9db15f2b696944430c8ef643f7/src/op-stack/actions/getGames.ts#L102-L106)\r\n    * 0 use in `op-challenger`\r\n    * 0 use in `DeputyGuardianModule`\r\n* `disputeGameBlacklist`\r\n    * 0 use in `op-challenger`\r\n    * 0 use in `DeputyGuardianModule`\r\n    * 0 use in `viem`\r\n* `respectedGameTypeUpdatedAt`\r\n    * 0 use in `op-challenger`\r\n    * 0 use in `DeputyGuardianModule`\r\n    * 0 use in `viem` \r\n* `checkWithdrawal`\r\n    * 0 use in `op-challenger`\r\n    * 0 use in `DeputyGuardianModule`\r\n    * 1 usage in `viem` [here](https://github.com/wevm/viem/blob/main/src/op-stack/actions/getWithdrawalStatus.ts#L221-L226)\r\n\r\nWith this, breaking the API would not be so painful, it doesn't look like. It would also remove periphery codepaths from our most important contract, which is always good. However on the front of breaking the core withdrawal path API, we should not do that imo.\r\n\r\nMonitoring is another concern here though. What would definitely change my opinion here is if these changes complicated or added risk to our monitoring path, cc @ajsutton.", "2024-09-27T23:31:36Z", "2024-09-27T23:35:43Z", "clabby", "2025-08-30 21:56:13"]
["IC_kwDOLB-lzc6N9FtC", "I_kwDOLB-lzc6VZjZf", "Yeah agreed the key priority is maintaining compatibility with the withdrawal flow. Moving things like `respectedGameType`, `disputeGameBlacklist` and `respectedGameTypeUpatedAt` is likely to break some integrations like ENS but I feel like they're digging into a lot of implementation details in a way that isn't particularly ideal for them now anyway. They really just want a function that gives the latest finalized output root which some of the proposed `AnchorStateRegistry` changes may give if it only updates after the air gap and has a single anchor state for all game types.\r\n\r\nI'd expect that `respectedGameType` is a key part of the withdrawal workflow and we need to preserve compatibility for it. You need to know what type of game to use when proving your withdrawal.  We could deprecate it and keep something around for backwards compatibility for a while then delete later but I don't think should just break it.  That one use in viem likely translates to a _lot_ of users depending on it.\r\n\r\nSame for `checkWithdrawal`, it seems important to expose a simple way for callers to ask if a withdrawal is ready to be finalized or not. Ideally we'd have a method that makes it easy to check if a given set of params (eg a game) is suitable to be used to prove a withdrawal - it could then check the blacklist as well which viem doesn't seem to be doing and may cause issues if we ever need to blacklist games.\r\n\r\nThe other question for me is what the point of entry should be for people trying to use the ABI. Do you need to make calls to both `OptimismPortal` and the `IProofSubmitter` implementation? If so, all withdrawal code needs to support all possible proof submitter implementations separately. Ideally I think a withdrawal could be completed using only `OptimismPortal` and be independent of the proof submitter implementation but I'm not sure how you'd find the appropriate game. Not the end of the world if we can't make it that general but having the the main usage be agnostic to the plugin implementation buys us _a lot_ of flexibility. e.g. Succinct could use a different IProofSubmitter implementation and not need to have a custom integration with viem and any other withdrawal tools.\r\n\r\nFor the fall-through code, I'd say that people like succinct maintaining a fork of the OP Stack can easily remove it - they're already maintaining far more complex changes in the fork.\r\n\r\nThe functions with permissioned access are fine to just move as we know they're only used in limited fashion.\r\n\r\nFor monitoring, this shouldn't affect dispute-mon since it only monitors the dispute games. It will likely require updates to our other withdrawal monitoring though and I'm not familiar with how flexible that code is. I don't see anything in this that would hide information and cause monitoring problems - should just be a matter of updating the code to adjust where it gets the info from.", "2024-09-29T21:01:29Z", "2024-09-29T21:01:29Z", "ajsutton", "2025-08-30 21:56:13"]
["IC_kwDOKIsnqM6B-Avi", "I_kwDOKIsnqM6GDorC", "May I work on this issue?", "2024-06-20T12:10:32Z", "2024-06-20T12:10:32Z", "ShubhSensei", "2025-08-30 21:56:15"]
["IC_kwDOKIsnqM6CCDip", "I_kwDOKIsnqM6GDorC", "Yes, thank you! Just assigned you :) \r\n", "2024-06-20T21:16:51Z", "2024-06-20T21:16:51Z", "mds1", "2025-08-30 21:56:15"]
["IC_kwDOKIsnqM6N35pz", "I_kwDOKIsnqM6GDorC", "This was completed by https://github.com/ethereum-optimism/superchain-ops/pull/266, thank you @ShubhSensei!", "2024-09-27T22:41:45Z", "2024-09-27T22:41:45Z", "mds1", "2025-08-30 21:56:15"]
["IC_kwDODjvEJM6No4i7", "I_kwDODjvEJM6X2jC4", "got help in discord, try some other ways, thx", "2024-09-26T08:34:42Z", "2024-09-26T08:34:42Z", "allnil", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6NwPy1", "I_kwDODjvEJM6Xl-xx", "Fixed by https://github.com/ethereum-optimism/optimism/pull/12072", "2024-09-27T01:35:42Z", "2024-09-27T01:35:42Z", "Inphi", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6NUTkK", "I_kwDODjvEJM6XZfla", "Tagging @protolambda for visibility. From the commit history it looks like you are most familiar with the code after the event refactoring", "2024-09-24T10:44:02Z", "2024-09-24T10:44:02Z", "bearpebble", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6N2KVe", "I_kwDODjvEJM6XNFEv", "This is being handled by op-deployer, so closing this", "2024-09-27T17:53:43Z", "2024-09-27T17:53:43Z", "mds1", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6NRZvt", "I_kwDODjvEJM6XL1F_", "This was fixed in https://github.com/ethereum-optimism/optimism/pull/12031 and needs to be ported into an isolated PR.\r\n\r\nSpecifically, it shouldn't return 0, but when opening a starting point it should return `ok=false` when asked if there is a ready block to append to.\r\n", "2024-09-24T04:50:20Z", "2024-09-24T04:50:20Z", "protolambda", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6Mnk0o", "I_kwDODjvEJM6XDTy8", "Going to journal in this ticket because I think this might take a minute, and I'd like to make sure this work is visible for potential contribution.\r\n\r\n## Currently\r\nI've made some changes to my local (based on [this PR](https://github.com/ethereum-optimism/optimism/pull/11956)):\r\n* Rather than interact with a `LogStorage`, Chain Monitors now interact with `Storage` which includes a minimal interface to apply new Chain Heads to the Heads Storage\r\n* I've added a bunch of debug logging, some of which I might promote to actual logs at the end of the investigation.\r\n* I've added an update to the chain heads at the end of ProcessLogs, which acts on one block at a time. For now this assumes an unsafe heads update, which is fine because that's where the data is coming from when logs are updated.\r\n\r\nAs Safe and Finalized get added, we can make those just do a Heads update based on the database state. these processors can be responsible for responding to reorg when the data is available.\r\n\r\n\r\nFindings so far:\r\nLooks like there's more bootstrapping issue. When the database is empty, it's very hard to add a new items, because there's no checkpoints. This causes things like 0-value iterators which EOF immediately, and exacerbate issues. I plan on making fixes to how these db functions work, or to how the traversal works.\r\n\r\nLog snippet:\r\n\r\n```\r\nXLXL: Apply before  &{map[900200:{0 0 0 0 0 0} 900201:{0 0 0 0 0 0}]}\r\nXLXL doing log &{0xa5657c5380C0c5B67dD935920C53816c7E9D7691 [0xe00bbfe6f6f8f1bbed2da38e3f5a139c6f9da594ab248a3cf8b44fc73627772c 0xb3647df63b3983e35710f3a5e8d915c0e79d23d66e1ebb44161b2d216640d2b4] [] 2 0x431ae55003703d72a9f27753614735ba1fabf00d1d8948153034256c292637bb 3 0xd1ecf85e746cb3b421478c63618e55a9d647d1f3dc9f26de65c7a6ad835e1c40 0 false}\r\nXLXL AddLog\r\nXLXL AddLog Done {2 0}\r\nXLXL lastBlock  2\r\nXLXL: LastLogInBlock 900201 2 &{0x14002a246c0 2 {2 0} false 0}\r\nXLXL: NextLog 2 0 <nil>\r\nXLXL: NextLog 0 0 EOF\r\nXLXL lastIndex err block 2 not found in chain 900201\r\nXLXL: Apply after  &{map[900200:{0 0 0 0 0 0} 900201:{0 0 0 0 0 0}]}\r\n[... a later write]\r\nXLXL adding log %v in block %v, but currently at log %v 0 2 0\r\n```\r\nExplanation: \r\n* All of AddLog happened and the log context now points at block 2\r\n* an attempted check for the last index of the block (which would be used to update the local head) fails because without a checkpoint, the table can't find the block\r\n* The Heads Storage never updates\r\n* And then in subsequent attempted writes, we hit an \"out of order\" error in adding to the database!", "2024-09-18T18:51:40Z", "2024-09-18T18:51:40Z", "axelKingsley", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6MoHyH", "I_kwDODjvEJM6XDTy8", "Another thing that's turned out to be required is exposing `LastEntryIdx` -- when EOFs are hit when scanning for Executing Messages, we need to use the last entry in the DB. I've exposed that to all relevant interfaces.", "2024-09-18T20:16:44Z", "2024-09-18T20:16:44Z", "axelKingsley", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6MpIIe", "I_kwDODjvEJM6XDTy8", "There's a core issue when attempting to get a search checkpoint near the front of the database. if the index is `0`, it's assumed there was no checkpoint before-or-at the requested block number.", "2024-09-18T22:55:05Z", "2024-09-18T22:55:05Z", "axelKingsley", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6MvbsL", "I_kwDODjvEJM6XDTy8", "Ok, I've got things in place such that local heads are tracking, and x-heads are following as expected. Here's a snippet:\r\n\r\n```\r\nXLXL Latest Chain Heads:  &{map[900200:{2 0 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{3 2 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{6 3 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{7 6 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{10 7 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{10 10 0 0 0 0} 900201:{2 0 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{10 10 0 0 0 0} 900201:{3 2 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{10 10 0 0 0 0} 900201:{6 3 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{10 10 0 0 0 0} 900201:{7 6 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{10 10 0 0 0 0} 900201:{10 7 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{11 10 0 0 0 0} 900201:{10 10 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{14 11 0 0 0 0} 900201:{10 10 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{15 14 0 0 0 0} 900201:{10 10 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{15 15 0 0 2 0} 900201:{10 10 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{15 15 2 0 2 0} 900201:{10 10 0 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{15 15 2 0 2 0} 900201:{10 10 2 0 0 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{15 15 2 0 2 0} 900201:{10 10 2 0 2 0}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{18 15 2 2 2 2} 900201:{10 10 2 2 2 2}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{19 18 2 2 2 2} 900201:{10 10 2 2 2 2}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{19 19 2 2 2 2} 900201:{11 10 2 2 2 2}]}\r\nXLXL Latest Chain Heads:  &{map[900200:{19 19 2 2 2 2} 900201:{14 11 2 2 2 2}]}\r\n```\r\nExplanation: each line is a printout of the ChainsDB's Head structure, just after any update it handles.\r\nChainA and ChainB are taking turns sending 5 transactions each with logs. The local unsafe head stays promoted to the tip of the database, and the x-unsafe head is constantly updating to follow it.\r\n\r\nEventually a Safe and Finalized head are found on the network for both chains, and their x-heads track there too.\r\n\r\nThe tech required to make this happen is a new HeadProcessor which is now attached to the Chain Monitor. It uses the last log index for the block to update the local head.\r\n\r\nMore details in: https://github.com/ethereum-optimism/optimism/pull/11991", "2024-09-19T14:55:02Z", "2024-09-19T14:55:02Z", "axelKingsley", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6NRapo", "I_kwDODjvEJM6XDTy8", "Closing this in favor of https://github.com/ethereum-optimism/optimism/issues/12077 that tracks the need of ingesting this local-safe data from op-nde.\r\n", "2024-09-24T04:53:41Z", "2024-09-24T04:53:41Z", "protolambda", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6NlU7N", "I_kwDODjvEJM6V5356", "Closing as this was completed", "2024-09-25T22:23:35Z", "2024-09-25T22:23:35Z", "mds1", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6NRa4d", "I_kwDODjvEJM6UZ7t5", "Closed by https://github.com/ethereum-optimism/optimism/pull/11989", "2024-09-24T04:54:39Z", "2024-09-24T04:54:39Z", "protolambda", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6NjfO3", "I_kwDODjvEJM6TaV9E", "Duplicate of https://github.com/ethereum-optimism/optimism/issues/11321", "2024-09-25T18:42:04Z", "2024-09-25T18:42:04Z", "sebastianst", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6Njedr", "I_kwDODjvEJM6TaVTV", "Already investigated while writing specs.", "2024-09-25T18:40:16Z", "2024-09-25T18:40:16Z", "sebastianst", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6KqGGx", "I_kwDODjvEJM6TaVJD", "Relevant work tracked under: https://github.com/ethereum-optimism/specs/pull/357", "2024-09-03T11:32:49Z", "2024-09-03T11:32:49Z", "BlocksOnAChain", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6I9fdv", "I_kwDODjvEJM6TOSfT", "This is a good catch. I lean towards saying the spec is correct and updating the implementation to handle either `/put` or `/put/`", "2024-08-20T01:58:52Z", "2024-08-20T01:58:52Z", "tynes", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6NjUQ9", "I_kwDODjvEJM6RosWp", "We've changed the spec to only do _steady block derivation_, so only deriving invalid _payloads_ as deposit-only blocks, not invalid batches, which are now just dropped.", "2024-09-25T18:18:17Z", "2024-09-25T18:18:17Z", "sebastianst", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6MlHKM", "I_kwDODjvEJM6RosVn", "No spike for SBD is necessary, it will just be implemented.\nI will close this one for that reason.\n@sebastianst for visibility on this\n", "2024-09-18T13:53:55Z", "2024-09-18T13:53:55Z", "BlocksOnAChain", "2025-08-30 21:58:12"]
["IC_kwDODjvEJM6HDePv", "I_kwDODjvEJM6RosSG", "Based on a convo with @axelKingsley in discord, thinking about span batch validity rules\r\n\r\nI see 4 options when there is a single invalid block in a span batch\r\n\r\n- full span batch invalid\r\n- valid blocks up to invalid block, rest are dropped\r\n- valid blocks up to invalid block, invalid block is deposits only, rest are dropped\r\n- valid blocks up to invalid block, invalid block and rest are deposit only\r\n\r\nRight now we believe the 4th case is the best behavior as it results in the most predictable amount of resource usage", "2024-08-02T17:22:47Z", "2024-08-02T17:22:47Z", "tynes", "2025-08-30 21:58:12"]
["IC_kwDOEf1bQc6OnG2J", "I_kwDOEf1bQc6PBrfv", "think [this](https://retrofunding.optimism.io/) should be the link but idk where the source is", "2024-10-04T01:26:02Z", "2024-10-04T01:26:02Z", "macfarla", "2025-08-30 21:58:24"]
["IC_kwDOEf1bQc6Ok9rU", "I_kwDOEf1bQc4-YgHx", "> This requires several steps:\n> \n> 1. Explain how to open the cloud provider's firewall (GCP, for example) to allow access to the development node from the outside\n> 2. Explain how to connect to that access from a local wallet (Metamask, for example)\n> 3. Explain how to use it from Remix (identical to how you'd use an existing network)\n\n", "2024-10-03T18:26:15Z", "2024-10-03T18:26:15Z", "soenaing656", "2025-08-30 21:58:24"]
["IC_kwDOKIwiaM6OjhQC", "I_kwDOKIwiaM6Y1-Lq", "I'll close this, because it's not exactly specific on what FAQ you want the team to update.", "2024-10-03T15:11:27Z", "2024-10-03T15:11:27Z", "krofax", "2025-08-30 21:58:26"]
["IC_kwDOKIwiaM6OjhtT", "I_kwDOKIwiaM6Y16bq", "We already have App based tutorials on the docs.\r\nClosing this.", "2024-10-03T15:12:13Z", "2024-10-03T15:12:13Z", "krofax", "2025-08-30 21:58:26"]
["IC_kwDOKIwiaM6OjkSg", "I_kwDOKIwiaM6Y1y04", "@Samykoke09 We are working on this, definitely not something that will be on the docs.\r\nClosing this one.", "2024-10-03T15:16:37Z", "2024-10-03T15:16:37Z", "krofax", "2025-08-30 21:58:26"]
["IC_kwDOKIwiaM6Ojjwu", "I_kwDOKIwiaM6Y1vy6", "@Samykoke09 Thanks for suggesting this, but it's not exactly something we want to prfioritize at the moment.\r\nClosing this for now.", "2024-10-03T15:15:43Z", "2024-10-03T15:15:43Z", "krofax", "2025-08-30 21:58:26"]
["IC_kwDOKIwiaM6OjlkJ", "I_kwDOKIwiaM6Y1p3t", "@Samykoke09 This is already in the works, will revert back here when it's live, closing this for now.", "2024-10-03T15:18:55Z", "2024-10-03T15:18:55Z", "krofax", "2025-08-30 21:58:26"]
["IC_kwDOKIwiaM6OUiNO", "I_kwDOKIwiaM6Yof6g", "Please apply the \"enhancement\" and \"documentation\" labels to this issue.", "2024-10-02T06:52:04Z", "2024-10-02T06:52:04Z", "Samykoke09", "2025-08-30 21:58:26"]
["IC_kwDOKIwiaM6Oji4x", "I_kwDOKIwiaM6Yof6g", "@Samykoke09 We have an extensive superchain explainer here - https://docs.optimism.io/stack/explainer", "2024-10-03T15:14:11Z", "2024-10-03T15:14:11Z", "krofax", "2025-08-30 21:58:26"]
["IC_kwDOH2Qg5s6LQtpw", "I_kwDOH2Qg5s6Vs4H2", "`geth init` without special flags will default to `--state.scheme=path`, since upstream Geth version 1.14.x, which recently was merged into op-geth.\r\n\r\nHowever, your geth command uses `geth --gcmode=archive`, while `archive` is not supported on `state.scheme=path` yet in Geth. So geth chooses to use `--state.scheme=hash` when running in archive mode, but then finds that it conflicts with the `path` DB that was initialized already.\r\n\r\nI believe you can fix it by adding `--state.scheme=hash` to the `geth init` command.\r\n", "2024-09-07T19:30:13Z", "2024-09-07T19:30:13Z", "protolambda", "2025-08-30 21:58:28"]
["IC_kwDOH2Qg5s6LRsK8", "I_kwDOH2Qg5s6Vs4H2", "Can confirm adding \r\n```\r\n geth init --state.scheme=hash --datadir=/root/datadir /root/config/genesis.json\r\n```\r\nworks for geth archive nodes.", "2024-09-08T12:36:30Z", "2024-09-08T12:36:38Z", "InoMurko", "2025-08-30 21:58:28"]
["IC_kwDOLB-lzc6OQWBZ", "I_kwDOLB-lzc6Xpmke", "Spoke with @tynes about this and we agreed that the \"Depositor Account is owner\" is our preferred approach. This lets us have a typical path of upgrading predeploys via hard fork, but also provides deposit transaction upgrade if needed (e.g. for incident response)", "2024-10-01T18:07:37Z", "2024-10-01T18:07:37Z", "mds1", "2025-08-30 21:58:30"]
["IC_kwDOLB-lzc6HbRfs", "I_kwDOLB-lzc6SIJF0", "`CrossL2Executor` ?", "2024-08-06T20:16:37Z", "2024-08-06T20:16:37Z", "AmadiMichael", "2025-08-30 21:58:30"]
["IC_kwDOLB-lzc6J6kzb", "I_kwDOLB-lzc6SIJF0", "L2MessageValidator", "2024-08-28T00:33:15Z", "2024-08-28T00:33:15Z", "owl11", "2025-08-30 21:58:30"]
["IC_kwDOKIsnqM6K_SHJ", "I_kwDOKIsnqM6QhqFx", "My suggestion is to wrap the full contents of the `postCheck` method in an `if (msg.sig != this.approveJson.selector)` block. We don't need to run any checks when doing approve calls, so this ensures they only run for simulations and signing.\r\n\r\nExample:\r\n```solidity\r\n/// @notice Checks the correctness of the deployment\r\nfunction _postCheck(Vm.AccountAccess[] memory accesses, SimulationPayload memory /* simPayload */ )\r\n    internal\r\n    view\r\n    override\r\n{\r\n    if (msg.sig != this.approveJson.selector) {\r\n        console.log(\"Running post-deploy assertions\");\r\n\r\n        checkStateDiff(accesses);\r\n        checkSemvers();\r\n        // etc/\r\n\r\n        console.log(\"All assertions passed!\");\r\n    } else {\r\n        console.log(\"Skipping assertions because this is a just approve call\");\r\n    }\r\n}\r\n```", "2024-09-05T14:30:55Z", "2024-09-05T14:30:55Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM6N2B_F", "I_kwDOKIsnqM6QhqFx", "@maurelian I think we can close this as completed by https://github.com/ethereum-optimism/superchain-ops/pull/319, do you agree?", "2024-09-27T17:29:59Z", "2024-09-27T17:29:59Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM6OSIyp", "I_kwDOKIsnqM6CXCTe", "This has since been fixed, we now more clearly log which accounts are being used", "2024-10-01T21:36:11Z", "2024-10-01T21:36:11Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM51a9_1", "I_kwDOKIsnqM6AmyHR", "Can you check what forge version they are on? My guess is they're on an old foundry version that has a bug in the cheat where an empty string is parsed into 0.", "2024-02-28T22:10:08Z", "2024-02-28T22:10:08Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM6OS70Q", "I_kwDOKIsnqM6AmyHR", "I believe this was due to the foundry version, so closing this issue, and we can investigate if we come across this again", "2024-10-02T00:39:41Z", "2024-10-02T00:39:41Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM6OS8bw", "I_kwDOKIsnqM5-j81R", "We now have a consistent naming and numbering convention, so closing this as completed", "2024-10-02T00:40:13Z", "2024-10-02T00:40:13Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM5x3X-4", "I_kwDOKIsnqM59MydP", "[input.json](https://github.com/ethereum-optimism/superchain-ops/files/14053447/input.json)\r\n", "2024-01-25T14:38:15Z", "2024-01-25T14:38:15Z", "zchn", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM5x4iT9", "I_kwDOKIsnqM59MydP", "This is a great catch. I will include it in a PR which accompanies #42, prior to the ceremony.", "2024-01-25T17:13:32Z", "2024-01-25T17:13:32Z", "maurelian", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM5x4zVi", "I_kwDOKIsnqM59MydP", "Nice, thank you. Assigning this to you accordingly. But feel free to unassign and not work on this if this end up not blocking the extended pause upgrade and instead putting the launch date at risk", "2024-01-25T17:57:18Z", "2024-01-25T17:57:18Z", "zchn", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM6OTGQa", "I_kwDOKIsnqM59MydP", "Being able to sign from the UI would be a nice way to easily support a lot of different wallets, so I'd like to keep this open and think about how we can do this. However, the `MultiSendCallOnly` contract has a different call encoding which would require changes to `base-org/contracts/script/universal/MultisigBuilder.sol`, which would be nice to avoid. I wonder if there are ways to sign arbitrary data in the Safe UI, or if we can PR in Multicall3 support to the Safe Transaction Builder", "2024-10-02T00:49:17Z", "2024-10-02T00:49:17Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM6OTNhB", "I_kwDOKIsnqM58fNzR", "I'm going to close this issue as completed:\r\n- We do have the superchain registry as a dependency and use it to getch addresses\r\n- We do have post check hooks used to make assertions\r\n- We do not yet have state diff checking, but as part of a separate issue we will think about how to do that", "2024-10-02T00:56:12Z", "2024-10-02T00:56:12Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM6OTY9a", "I_kwDOKIsnqM5zyRsq", "@tynes What exactly did you have in mind for this issue's scope? Should this be closed because we do implement the postCheck hooks now, or did you mean running the standard monorepo test suite against the state resulting from a task?", "2024-10-02T01:12:10Z", "2024-10-02T01:12:10Z", "mds1", "2025-08-30 21:58:47"]
["IC_kwDOKIsnqM6OTfa6", "I_kwDOKIsnqM5zyRsq", "Good to close this based on the post hooks existing ", "2024-10-02T01:45:02Z", "2024-10-02T01:45:02Z", "tynes", "2025-08-30 21:58:47"]
["IC_kwDOMMiGhs6NgjXr", "I_kwDOMMiGhs6XzScT", "I think this is a good idea we can add this package in the ecosystem repo. We'll let you know when we publish it.", "2024-09-25T13:39:44Z", "2024-09-25T13:39:44Z", "nitaliano", "2025-08-30 21:59:16"]
["IC_kwDOMMiGhs6OOH8k", "I_kwDOMMiGhs6XzScT", "Here you go let us know if you have any issues\r\n* https://www.npmjs.com/package/supersim\r\n* https://github.com/ethereum-optimism/ecosystem/tree/main/packages/supersim", "2024-10-01T14:04:24Z", "2024-10-01T14:04:57Z", "nitaliano", "2025-08-30 21:59:16"]
["IC_kwDOMMiGhs6OOUA8", "I_kwDOMMiGhs6XzScT", "Wow that was fast!\r\n![image](https://github.com/user-attachments/assets/42c2e2ce-67cc-47a1-8517-8f2ec92d6bb2)\r\n", "2024-10-01T14:23:44Z", "2024-10-01T14:23:44Z", "roninjin10", "2025-08-30 21:59:16"]
["IC_kwDODjvEJM6OZPIH", "I_kwDODjvEJM6YtRyE", "cc @ajsutton @smartcontracts for visiblity", "2024-10-02T15:31:00Z", "2024-10-02T15:31:00Z", "mds1", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6Nz80W", "I_kwDODjvEJM6YHJZ1", "tags op-contracts/v2.0.0-beta.3 work fine", "2024-09-27T12:48:34Z", "2024-09-27T12:48:34Z", "dome", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6Ox7_y", "I_kwDODjvEJM6YHJZ1", "But it was working correctly before on v1.9.1", "2024-10-06T14:11:41Z", "2024-10-06T14:11:41Z", "askucher", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OFRq1", "I_kwDODjvEJM6YCSx6", "I manually tested go1.22 programs, particularly alloc.go, on MT-Cannon and it works fine. The Go GC is utilized and alloc.go succeeds in reducing maximum memory usage throughout its MT-Cannon execution.\nAll that's left is to recompile the Go programs for go1.22. This gives us coverage on singlethreaded and multithreaded VMs for go1.22", "2024-09-30T17:13:31Z", "2024-09-30T17:13:31Z", "Inphi", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OG0UU", "I_kwDODjvEJM6YCSx6", "I think it was go1.23 that caused issues for me with cannon-mt - it used a new syscall.", "2024-09-30T21:11:09Z", "2024-09-30T21:11:09Z", "ajsutton", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OG7ak", "I_kwDODjvEJM6YCSx6", "> I think it was go1.23 that caused issues for me with cannon-mt - it used a new syscall.\r\n\r\nYup. I think I tested go1.23 as well when I ran into gc issues. We'll deal with go1.23 compat when the time is right.", "2024-09-30T21:32:28Z", "2024-09-30T21:32:28Z", "Inphi", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OvSc3", "I_kwDODjvEJM6Xekzr", "I think this is done. Closing.", "2024-10-04T22:46:35Z", "2024-10-04T22:46:35Z", "pauldowman", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6MgFsH", "I_kwDODjvEJM6W8Czy", "See https://github.com/ethereum-optimism/optimism/pull/11100 for some existing work that may make this easier to implement", "2024-09-17T23:58:15Z", "2024-09-17T23:58:15Z", "tynes", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OuICv", "I_kwDODjvEJM6W8Czy", "I already moved the typical `L1Origin` selection into an [async process](https://github.com/ethereum-optimism/optimism/pull/12134), but the next step after selecting the `L1Origin` is the payload attributes generation. The only implementation of this `AttributesBuilder` interface is the `FetchingAttributesBuilder`, which fetches the L1 block info and receipts inside of the call to `PreparePayloadAttributes`.\r\n\r\nIf we were to pass the L1 info into [this method](https://github.com/ethereum-optimism/optimism/blob/develop/op-node/rollup/derive/attributes.go#L46) instead of having it fetch the data, then we could choose to only progress the `L1Origin` once we already have all of the data required to build attributes for this new origin. In other words, the `L1OriginSelector` may need to return `L1Info` and receipts instead of just an `L1BlockRef`, but then the `AttributesBuilder` would not need to make any network calls.\r\n\r\nAnother option would be to replace the `L1OriginSelector` with an `AttributesWithOriginSelector` which bundles together these two responsibilities, returning `PayloadAttributes` and an `L1BlockRef` for any new `L2Head`. This feels like an unnecessary degree of coupling, so my inclination is to go with the first option.\r\n\r\nIs this the intended use of the event system, or should we be adding more intermediate events to coordinate the need for and availability of new L1 origin data?", "2024-10-04T19:40:21Z", "2024-10-04T19:40:30Z", "BrianBland", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6KEFlQ", "I_kwDODjvEJM6UXtTV", "I'm seeing a situation with our batcher that I think/hope this PR might fix:\r\n\r\nWe're configured to auto-switch, with max 2 parallel sends and 5 blobs per blob tx.\r\n\r\nFor over 30 minutes, I saw the batcher repeatedly alternative between trying to send a calldata tx and then a blob tx. Each tx kept failing with \"AlreadyReserved\", since it was being blocked from reaching the mempool by the previous one.\r\n\r\nUsually when you get AlreadyReserved, a cancellation transaction would be sent to clear out the mempool. But this wasn't happening.  From what I gathered, the batcher was stuck doing this flip-flopping in the for {} loop in the batcher's publishStateToL1(). This call blocks the sender loop, so the cancellation traffic couldn't go out.\r\n\r\nThe situation eventually cleared on its own, but I'm not entirely sure how. \r\n\r\nOne fix would be to check the txpool state in publishStateToL1() to get it to abort if the transaction pool gets blocked.\r\n", "2024-08-28T22:06:14Z", "2024-08-28T22:06:14Z", "roberto-bayardo", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6KEIEv", "I_kwDODjvEJM6UXtTV", "While looking into making publishStateToL1() abort if the txpool gets blocked, I noticed what might be a race condition from this:\r\n\r\nhttps://github.com/ethereum-optimism/optimism/blame/36f093a10da09496c3ef5a706cd494a4e2a9b9bd/op-batcher/batcher/driver.go#L317\r\n\r\nNow that there are two vars being updated (txpool state and txpoolblockedblob), we probably want to use a mutex to update them both atomically?", "2024-08-28T22:14:56Z", "2024-08-28T22:14:56Z", "roberto-bayardo", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6KGsYe", "I_kwDODjvEJM6UXtTV", "> Now that there are two vars being updated (txpool state and txpoolblockedblob), we probably want to use a mutex to update them both atomically?\r\n\r\nYeah I was lazy when implementing this and thought the single atomic state var management should be sufficient and deemed it extremely unlikely that we'd overwrite the blob state with the wrong routine. \r\n\r\nI think a more fundamental aspect to change about the cancellation handling is that we need to send unblock transactions _for a specific nonce_. We currently just send a cancellation tx _candidate_ and then let the txmgr choose the nonce, which doesn't make much sense for getting txs unstuck of wrong type of a particular nonce, especially during times where regular switching between calldata and blobs is occurring. The cancellation code also needs to get better and stopping to cancel if it becomes evident that another tx for that nonce already got confirmed.\r\n\r\nFor Holocene, I'd like to rework some of the batcher and txmgr behavior that might actually simplify this part, because we should now attach a specific batch range to a nonce and try to submit that tx at all costs, because of the new strict batch ordering rules.", "2024-08-29T08:27:11Z", "2024-08-29T12:06:22Z", "sebastianst", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6KJYuX", "I_kwDODjvEJM6UXtTV", "I think the [txmgr PR](https://github.com/ethereum-optimism/optimism/pull/11614) I'm wrapping up should fix the issue of cancellation transactions getting stuck, along with [this one ](https://github.com/ethereum-optimism/optimism/pull/11633) I just sent which will make the batcher send loop abort while cancellation is being handled.\r\n\r\nAgree though the entire batcher/txmgr flow could use a rethink.", "2024-08-29T13:49:00Z", "2024-08-29T13:49:00Z", "roberto-bayardo", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OVqEx", "I_kwDODjvEJM6UXtTV", "We've seen the new behaviour play out on sepolia:\r\n\r\n```\r\n t = 0            | Requeue and start using CALLDATA\r\ndt = 11s          | Publish tx\r\ndt = 3m           | Requeue and start using BLOBS\r\ndt = 4m40s        | Requeue and start using CALLDATA\r\ndt = 5s           | Publish tx\r\ndt = 8s           | Publish tx\r\ndt = 3m57s        | Requeue and start using BLOBS\r\ndt = 4m36s        | Publish tx \r\n```\r\n\r\nIt is expected that switching from calldata to blobs results in a delay to transaction publishing, since the channel has much more capacity then and has more space to fill with block data before being sent. \r\n\r\nWhen switching from blobs to calldata, we only expect a short delay due to processing time requeing blocks. It would be interesting to benchmark some worst-case numbers for this. ", "2024-10-02T09:32:20Z", "2024-10-02T09:32:20Z", "geoknee", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OVzXt", "I_kwDODjvEJM6UXtTV", "I believe the `Requeue` method is only ever going to requeue the blocks from a single channel. This is because the batcher now only requeues blocks from channels which didn't start to get sent on chain yet, and we don't make a new channel (and dequeue the blocks for it) until we have started to send the previous one.\r\n\r\nSo the worst case seems to be when we have a single very large channel to requeue. ", "2024-10-02T09:40:23Z", "2024-10-02T09:40:23Z", "geoknee", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OWMho", "I_kwDODjvEJM6UXtTV", "Sketch of a benchmark here https://github.com/ethereum-optimism/optimism/compare/gk/batcher-requeue-bench?expand=1:\r\n```\r\ngo test -benchmem -run=^$ -bench ^BenchmarkChannelManager_Requeue$ github.com/ethereum-optimism/optimism/op-batcher/batcher -v\r\n\r\ngoos: darwin\r\ngoarch: arm64\r\npkg: github.com/ethereum-optimism/optimism/op-batcher/batcher\r\nBenchmarkChannelManager_Requeue\r\nBenchmarkChannelManager_Requeue-14    \t       4\t 296146114 ns/op\t74611654 B/op\t   32528 allocs/op\r\nPASS\r\nok  \tgithub.com/ethereum-optimism/optimism/op-batcher/batcher\t3.803s\r\n\r\n```\r\nIt took on average 296ms to requeue 500 single-tx blocks and then add some back to a calldata channel and generate the first frame / tx data to send. ", "2024-10-02T09:58:11Z", "2024-10-02T09:58:44Z", "geoknee", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OaQV1", "I_kwDODjvEJM6UXtTV", "Thanks for adding the benchmarks! The worst-case is a chain with no transactions and a long channel duration. Would be great to parameterize the benchmark to also run it with\r\n- span batches\r\n- zero transactions per block\r\n- making sure we bump the L1 origin every 6 L2 blocks (not sure if this is guaranteed in the current random generator)\r\n- 30 * 60 * 5 = 9,000 blocks which is 5h worth of blocks, which is what a batcher with 5h channel duration would need to re-batch.", "2024-10-02T17:40:51Z", "2024-10-02T17:41:08Z", "sebastianst", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OOy4j", "I_kwDODjvEJM6TB0M5", "After a lot of research and design for this task, we realized blueprints were unnecessary, so the need for CREATE3 was minimal, so we decided to use CREATE2 instead.\nLong story short, we realized the final design would be very similar to the existing Create2Deployer but with an added deployment registry. We don't think it makes sense to introduce a new factory just for that, and suggest using the preinstall instead. \nIf, at any point, someone is interested in having a deployment history, they could introduce an additional factory contract that calls the Create2Deployer and thats it. No need for it to be a predeploy.\n\nAll of this was discussed with @tynes, and he agreed with us.", "2024-10-01T14:59:21Z", "2024-10-01T14:59:21Z", "gotzenx", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6MmfXY", "I_kwDODjvEJM6RpLWY", "Linking review support ticket here https://github.com/ethereum-optimism/security-pod/issues/160 , note the FMA should be ready for initial review by October 8th. ", "2024-09-18T16:19:24Z", "2024-09-18T16:19:24Z", "geoknee", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6Om3R-", "I_kwDODjvEJM6Mq8xB", "This has been implemented in #11417  (I think we wound up with duplicate tickets somewhere along the line). ", "2024-10-04T00:22:01Z", "2024-10-04T00:22:01Z", "ajsutton", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6BQGDQ", "I_kwDODjvEJM6Jegyv", "I've updated the title to `Upgrade #10 Release Tracker`, per the new smart contract versioning rules, since Upgrade #9 (Fjord) is the most recent one", "2024-06-14T17:37:55Z", "2024-06-14T17:37:55Z", "mds1", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6OSfJG", "I_kwDODjvEJM6Jegyv", "Closing this issue because it is out of date", "2024-10-01T22:41:18Z", "2024-10-01T22:41:18Z", "tynes", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM5yfQXR", "I_kwDODjvEJM59vYLE", "Noticed something similar, I'm on op-node 1.4.3. What's interesting is that I have 4 different pods that use the same L1 RPC url and not all of them fail at the exact same l1 block. It's currently a hit or miss\r\n\r\n```\r\noptimism-snapshot-0 op-node t=2024-02-01T06:03:20+0000 lvl=warn msg=\"Derivation process temporary error\"     attempts=1 err=\"temp: failed to fetch receipts of L1 block 0x8c097b788ea6560e3141494d6b1b57060d03ae47f3ab5d89f87dfae447fb9944:19131314 (parent: 0x0635b596a6ab9cb4b6185e7195f4b31a43425625483936186b2e08e5c0f60242:19131313) for L1 sysCfg update: invalid receipts: got 0 receipts but expected 152\"\r\n```\r\n\r\nFull logs\r\n```shell\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:03+0000 lvl=info msg=\"disconnected from peer\"                 peer=16Uiu2HAm6h61WPbW7tySjVqJsF1ArvQ5G2HWSGicNTh31TBdHcVB addr=/ip4/116.202.215.20/tcp/9222\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:03+0000 lvl=info msg=\"connected to peer\"                      peer=16Uiu2HAkvspxvzuCxyChrLF3c3YHZk1JjLJCvXAmryGGT1AqTapu addr=/ip4/176.9.111.210/tcp/9222\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:03+0000 lvl=info msg=\"Starting P2P sync client event loop\"    peer=16Uiu2HAkvspxvzuCxyChrLF3c3YHZk1JjLJCvXAmryGGT1AqTapu\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:03+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAmUXhiDnUaKFDUh5pf3JNskdSWhxuKz1h1ua2DA1X96teK\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:03+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAm5SosPy758VbgU1JfVoM6ytcv1J4KhfUXEibwzf4jeYXA\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:03+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xf5afca046305624f2866afeaf92daf385d4aa7054e475f84130be5b96254b127:115606023 peer=16Uiu2HAm1A4meETRCdaVdCnxtqFA7AnFDtsKvJYmD1gxyJ7VR1dY\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:03+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xf5afca046305624f2866afeaf92daf385d4aa7054e475f84130be5b96254b127:115606023\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:04+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAkzJEZcmp2T8mEW1CAWBF5BaJCFQN2vdgyGi3phu6Vsjs9\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:04+0000 lvl=info msg=\"disconnected from peer\"                 peer=16Uiu2HAm2S7iFb15gjN4KEkYk35pCnQ5HTWfmk1HzXRWwydSeoAJ addr=/ip4/173.231.40.194/tcp/31367\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:04+0000 lvl=warn msg=\"Derivation process temporary error\"     attempts=2 err=\"temp: failed to fetch receipts of L1 block 0x8c097b788ea6560e3141494d6b1b57060d03ae47f3ab5d89f87dfae447fb9944:19131314 (parent: 0x0635b596a6ab9cb4b6185e7195f4b31a43425625483936186b2e08e5c0f60242:19131313) for L1 sysCfg update: invalid receipts: got 0 receipts but expected 152\"\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:05+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xdefac110a701cd8e14c2577b948cf71a116e0f0f49cba8919eb1914c4ae2bca9:115606024 peer=16Uiu2HAm8HjPrubW1mjGgqSs1doXuTLD5nDzrSAFMkLn6TGRnH9C\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:05+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xdefac110a701cd8e14c2577b948cf71a116e0f0f49cba8919eb1914c4ae2bca9:115606024\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:06+0000 lvl=warn msg=\"failed p2p sync request\"                peer=16Uiu2HAkyfu9f78iAbNzq6bohjL4MXWQBkQ31WxDEb5M73WJs78F num=115,606,018 err=\"failed to open stream: failed to negotiate protocol: protocols not supported: [/opstack/req/payload_by_number/10/0]\"\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:07+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x62951ff3535c8cde29b08ab66b17c4c84386559e59dc595e3550eff0046615d3:115606025 peer=16Uiu2HAm8HjPrubW1mjGgqSs1doXuTLD5nDzrSAFMkLn6TGRnH9C\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:07+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x62951ff3535c8cde29b08ab66b17c4c84386559e59dc595e3550eff0046615d3:115606025\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:08+0000 lvl=warn msg=\"Derivation process temporary error\"     attempts=3 err=\"temp: failed to fetch receipts of L1 block 0x8c097b788ea6560e3141494d6b1b57060d03ae47f3ab5d89f87dfae447fb9944:19131314 (parent: 0x0635b596a6ab9cb4b6185e7195f4b31a43425625483936186b2e08e5c0f60242:19131313) for L1 sysCfg update: invalid receipts: got 0 receipts but expected 152\"\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:10+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAkuSMo7R59r39ZoFrVUrokQLrvrNsx4K2h6QnSJ69cu2fk\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:10+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAmJZhUCVoAYBYS7XygYrq53MLFXFv1pRRXGw9EMj8wqn4m\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:10+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAmPhZ5EJavkVcwwYsBj2MKLKaN1j53RvMnWo5ZS7kfs9WW\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:10+0000 lvl=warn msg=\"failed p2p sync request\"                peer=16Uiu2HAm3EFdZW6NY1bCrdgxApQTT4crm6trebU7vb6Hywp5g6hc num=115,606,018 err=\"failed to open stream: failed to negotiate protocol: protocols not supported: [/opstack/req/payload_by_number/10/0]\"\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:10+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x75dcee6c522c311c339923200a243433b967c4a58ee491386d15cf0dfc62dc0a:115606026 peer=16Uiu2HAm1A4meETRCdaVdCnxtqFA7AnFDtsKvJYmD1gxyJ7VR1dY\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:10+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x75dcee6c522c311c339923200a243433b967c4a58ee491386d15cf0dfc62dc0a:115606026\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:11+0000 lvl=info msg=\"disconnected from peer\"                 peer=16Uiu2HAkvAmZsULW3K1uij4g1nU3ypRDELaLP1sD2wjuqCsA4TgS addr=/ip4/136.243.16.41/tcp/9222\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:11+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xd5ffcbd47f133d007b4c16abf2a63fac59a13449bc820f2aa994564b8b12a2d0:115606027 peer=16Uiu2HAm1A4meETRCdaVdCnxtqFA7AnFDtsKvJYmD1gxyJ7VR1dY\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:11+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xd5ffcbd47f133d007b4c16abf2a63fac59a13449bc820f2aa994564b8b12a2d0:115606027\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:13+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAkzSeapxGvEZvxxj1EE2orxehRz8KL6A7572Jd1rzAFRik\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:13+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAmFCGhfVNtEUAVYtTgqjjkSnUXWJRio3oG8seV6bGy4pse\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:13+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x3f19d3aa4c4357f194a7a9c9b1ef34ab04de23e1637d4beeed6eb31e957882ed:115606028 peer=16Uiu2HAm8HjPrubW1mjGgqSs1doXuTLD5nDzrSAFMkLn6TGRnH9C\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:13+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x3f19d3aa4c4357f194a7a9c9b1ef34ab04de23e1637d4beeed6eb31e957882ed:115606028\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:14+0000 lvl=info msg=\"attempting connection\"                  peer=16Uiu2HAmB4NjKRgNZjyfwXEyTzNUgrzwCouaqAv7XRtE9T4YKiNN\r\noptimism-snapshot-0 op-node t=2024-02-01T18:07:14+0000 lvl=warn msg=\"failed p2p sync request\"                peer=16Uiu2HAm1hmjDZjF27FP1oCQBS8ND9DWXZX1yyRNexTyr7eC5WL3 num=115,606,018 err=\"failed to open stream: failed to negotiate protocol: protocols not supported: [/opstack/req/payload_by_number/10/0]\"\r\n```", "2024-02-01T08:48:11Z", "2024-02-01T18:07:59Z", "tobidae-cb", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM5yjbfg", "I_kwDODjvEJM59vYLE", "I can confirm that this is a bug in case that the inner receipts fetcher returns a `nil` error together with an empty receipts list. Here, the wrong assumption was made that\r\n```\r\nr, err := p.inner.FetchReceipts(ctx, block, txHashes)\r\n```\r\nwould always return a non-`nil` error if the receipts couldn't be retrieved in full. But it is happening, which is a separate issue to investigate. \r\n\r\nWe will fix this behavior soon to make sure we only cache healthy receipts.\r\n\r\nIt would be helpful to know what rpc kinds you are using.\r\n\r\n@tobidae-cb I don't see relevant errors in your full log dump? Only the one line log.", "2024-02-01T17:52:09Z", "2024-02-01T17:52:09Z", "sebastianst", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM5yjk8N", "I_kwDODjvEJM59vYLE", "> @tobidae-cb I don't see relevant errors in your full log dump? Only the one line log.\r\nI've edited the logs to contain relevant logs.\r\n\r\nFor the Eth L1 RPC, I use an eth archival deployment that has 3 pods with separate datastores. The routing method round-robin. On a side note, would an eth full deployment work for op-node? It used to be able to work but for the past couple months, I've had to use an archival deployment\r\n\r\nHere are some of the op-node details\r\n```\r\nop-geth - v1.101305.0 - c50337a60a1309a0f1dca3bf33ed1bb38c46cdd7\r\nop-node - v1.4.3 - 28c359620f2b28238163eced79b39335d3a40661\r\nop-legacy-geth - efea631cf19d57d3a64374cb81ce545f958bd38f\r\n```\r\n\r\n```\r\n# op-geth\r\n/usr/local/bin/geth \\\r\n    --ws \\\r\n    --ws.port=\"8546\" \\\r\n    --ws.addr=0.0.0.0 \\\r\n    --ws.origins=\"*\" \\\r\n    --http \\\r\n    --http.port=\"8545\" \\\r\n    --http.addr=0.0.0.0 \\\r\n    --http.vhosts=\"*\" \\\r\n    --http.corsdomain=\"*\" \\\r\n    --http.api=web3,debug,eth,txpool,net,engine \\\r\n    --authrpc.addr=0.0.0.0 \\\r\n    --authrpc.jwtsecret=\"/data/secrets/jwt.txt\" \\\r\n    --authrpc.port=\"8551\" \\\r\n    --authrpc.vhosts=\"*\" \\\r\n    --datadir=/data/op-geth \\\r\n    --verbosity=\"4\" \\\r\n    --rollup.disabletxpoolgossip=true \\\r\n    --rollup.historicalrpc=\"http://localhost:8547/\" \\\r\n    --rollup.sequencerhttp=\"https://mainnet-sequencer.optimism.io/\" \\\r\n    --rollup.historicalrpctimeout=240s \\\r\n    --nodiscover \\\r\n    --syncmode=full \\\r\n    --maxpeers=100 \\\r\n    --gcmode=archive \\\r\n    --snapshot=false \\\r\n    --cache=\"16384\" \\\r\n    --op-network=\"op-mainnet\"  \\\r\n    --rollup.halt=minor\r\n\r\n# op-legacy-geth\r\n/usr/local/bin/geth \\\r\n    --datadir=/data/op-legacy-geth \\\r\n    --config=\"/app/assets/l2geth.toml\" \\\r\n    --gcmode=archive \\\r\n    --port=29888\r\n\r\n# op-node\r\n/usr/local/bin/op-node \\\r\n    --l1=\"<redacted eth archival node>\" \\\r\n    --l2=\"ws://localhost:8551/\" \\\r\n    --l2.jwt-secret=\"/data/secrets/jwt.txt\" \\\r\n    --l1.trustrpc=\"true\" \\\r\n    --p2p.disable=\"false\" \\\r\n    --network=\"mainnet\" \\\r\n    --rpc.addr=0.0.0.0 \\\r\n    --rpc.port=\"9545\" \\\r\n    --p2p.peerstore.path memory \\\r\n    --p2p.discovery.path memory \\\r\n    --p2p.priv.path \"/data/secrets/p2p.txt\" \\\r\n    --rollup.halt=minor  \\\r\n    --rollup.load-protocol-versions=true\r\n```", "2024-02-01T18:11:50Z", "2024-02-01T18:11:50Z", "tobidae-cb", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM5ymeMu", "I_kwDODjvEJM59vYLE", "> I can confirm that this is a bug in case that the inner receipts fetcher returns a `nil` error together with an empty receipts list. Here, the wrong assumption was made that\r\n> \r\n> ```\r\n> r, err := p.inner.FetchReceipts(ctx, block, txHashes)\r\n> ```\r\n> \r\n> would always return a non-`nil` error if the receipts couldn't be retrieved in full. But it is happening, which is a separate issue to investigate.\r\n> \r\n> We will fix this behavior soon to make sure we only cache healthy receipts.\r\n> \r\n> It would be helpful to know what rpc kinds you are using.\r\n> \r\n> @tobidae-cb I don't see relevant errors in your full log dump? Only the one line log.\r\n\r\nI am a developer from Huawei Cloud. we provide HA capability which allow user to wrap several nodes into 1 endpoint. Welcome to ask me for more details :)", "2024-02-02T02:39:12Z", "2024-02-02T02:39:12Z", "LilkkJerry", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM50i0Jh", "I_kwDODjvEJM59vYLE", "Hi, what's the status on a potential fix here?", "2024-02-20T23:09:06Z", "2024-02-20T23:09:06Z", "tobidae-cb", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM50i-xu", "I_kwDODjvEJM59vYLE", "This is fixed as of `op-node/v1.5.1`. Please upgrade to latest `op-node/v1.6.1`.", "2024-02-20T23:25:26Z", "2024-02-20T23:25:26Z", "sebastianst", "2025-08-30 22:00:33"]
["IC_kwDODjvEJM6Oynwr", "I_kwDODjvEJM59vYLE", "got it in 1.9.1\r\n\r\n```\r\n184|op-node  | t=2024-10-06T23:26:55+0000 lvl=warn msg=\"Engine temporary error\" err=\"temp: failed to fetch receipts of L1 block 0x26e6f7c5c14f2a35e3513d1f9cfe43148577141f9a8d1717cfc56bb95deba733:56 (parent: 0xb19ffb7493ddc69ea00e4f98b110569b376618d59d86361c8b513a61f0c27816:55) for L1 sysCfg update: failed to fetch list of receipts: expected receipt root 0xbe2657e2ef940ad2bb8cf7745343bc209ffe16cdb7fc3c369973e49a8b576e64 but computed 0x6e4bec7d9ad661e3df512985bd021125139ea67a60cd2aef36449cc7775ecab6 from retrieved receipts\" \r\n```", "2024-10-06T23:28:41Z", "2024-10-06T23:30:15Z", "askucher", "2025-08-30 22:00:33"]
["IC_kwDOKIwiaM6PEWCr", "I_kwDOKIwiaM6Y2CLq", "@Samykoke09 When this is implemented, i'll revert here.", "2024-10-08T16:12:31Z", "2024-10-08T16:12:31Z", "krofax", "2025-08-30 22:01:09"]
["IC_kwDOKIwiaM6Ojk9j", "I_kwDOKIwiaM6Y13aG", "@Samykoke09 Can you specify the tutorials, pages you would like to see this?", "2024-10-03T15:17:49Z", "2024-10-03T15:18:03Z", "krofax", "2025-08-30 22:01:09"]
["IC_kwDOKIwiaM6PEUwS", "I_kwDOKIwiaM6Y13aG", "Closing this for now.\r\nwe can revisit later.", "2024-10-08T16:11:51Z", "2024-10-08T16:11:51Z", "krofax", "2025-08-30 22:01:09"]
["IC_kwDOLB-lzc5_7WAo", "I_kwDOLB-lzc6A2Nmv", "This could be extended to support more than L1 fee abstraction but instead abstract the full fee payment. This would be more intrusive to EVM equivalence, but would enable a lot of experimentation. It is possible that only abstracting the L1 portion of the fee is enough. We would want to think through the exact data passed into the smart contract if we were abstracting the entire fee payment. Likely we would want `gasUsed`, `success` and the raw transaction data.", "2024-06-03T22:57:18Z", "2024-06-03T22:57:18Z", "tynes", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc5_7hqT", "I_kwDOLB-lzc6A2Nmv", "This could also be used for MEV rebates by reducing fees for users that need a rebate. A mechanism for tracking who deserves rebates would need to be developed. This could be managed by the chain operator as an oracle, or on chain applications could track this information. For example Uniswap v4 hooks could be very useful for automating the process of determining who deserves MEV rebates ", "2024-06-03T23:47:39Z", "2024-06-03T23:47:39Z", "tynes", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6KTfCg", "I_kwDOLB-lzc6A2Nmv", "I believe this proposal has great potential and is also highly flexible. I have carefully reviewed the document. However, there are some critical details that I have doubts about. I hope we can clarify these through discussion. We would like to contribute to accelerating its implementation if necessary.\r\n\r\nFee ratio: The exchange rate between the native token and ether on a custom gas token chain.\r\n\r\n> A solidity interface could be defined that accepts the RLP encoded transaction and potentially some other data and it is expected to return a uint256 that represents the L1 fee or the L1 gas used\r\n\r\nFor this implementation, is a fee ratio needed? If so, where should it be stored, and who should update the fee ratio as it changes frequently?\r\nIf not, how should the exchange rate between ether and the native token be handled?\r\n\r\n> This could be extended to support more than L1 fee abstraction but instead abstract the full fee payment. \r\n\r\nSince the L1 fee is used to purchase data availability for Ethereum and involves only the data size of the transaction content, it can be computed using a smart contract. However, the L2 fee involves the EVM execution process; how can this approach support it?", "2024-08-30T07:40:53Z", "2024-08-30T07:40:53Z", "PinelliaC", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6LEMIV", "I_kwDOLB-lzc6A2Nmv", "Regarding the fee ratio, there is no enshrined concept of a fee ratio. It is up to the chain governor to choose the implementation details of the contract that computes the fee. If the chain governor wants to use a fee ratio in their contract, they are free to do so, they could also using an oracle based an an AMM or ecdsa signature.\r\n\r\n> For this implementation, is a fee ratio needed? If so, where should it be stored, and who should update the fee ratio as it changes frequently?\r\nIf not, how should the exchange rate between ether and the native token be handled?\r\n\r\nThese are implementation specific details. I am happy to provide advice on how it can be done, but ultimately all the protocol is aware of is EVM execution where bytes are passed in and the amount of gas to be charged is returned. For illiquid assets, you likely want a more centralized oracle solution that is able to push exchange rate updates directly to the L2. For liquid assets, you could use an AMM based oracle for the exchange rate.\r\n\r\n> This could be extended to support more than L1 fee abstraction but instead abstract the full fee payment.\r\n\r\nPerhaps its best to just keep it simple and only add this sort of abstraction to the L1 portion of the fee. This would be a much more simple change. There wouldn't be a way to turn off the execution fee or modify it but I suppose that is fine.", "2024-09-06T03:02:52Z", "2024-09-06T03:02:52Z", "tynes", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6PYOn2", "I_kwDOLB-lzc6A2Nmv", "How would this change impact the `Receipt` type? It currently contains multiple L1 fee-related fields (`L1GasPrice`, `L1BlobBaseFee`, `L1Fee`, ...), some of which were stored in the DB pre-bedrock, but are now decoded from the `L1BlockInfo` tx in `func (rs Receipts) DeriveFields` function. \r\n\r\nThe L1 gas price info returned from the on-chain function would then again need to be stored in DB for each `Receipt`, otherwise:\r\n- the receipt lookup (e.g. via RPC) will not return the correct L1 costs associated with the tx execution,\r\n- or, in case the gas cost function would be re-executed on lookup, one would need to run an archive node to be able to compute it.\r\n\r\nDo you see re-adding some L1 gas cost fields to `Receipt` as an acceptable outcome of fee abstraction change? Could be implemented similar to `DepositNonce`/`DepositReceiptVersion`.", "2024-10-10T15:53:37Z", "2024-10-10T15:53:37Z", "ArtificialPB", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6UBNoI", "I_kwDOLB-lzc6A2Nmv", "@ArtificialPB I haven't thought super deeply about all of the database implications but you raise good points. You don't want to require an archive node to be able to dynamically recreate the values. It does make sense to index them with the receipt.", "2024-11-18T15:16:31Z", "2024-11-18T15:16:31Z", "tynes", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6Yrh43", "I_kwDOLB-lzc6A2Nmv", "@tynes Using the example where we have a highly liquid asset and want to pull prices from an AMM, presumably it'd be helpful to have a TWAP so it couldn't be manipulated. Unless that's implemented natively by the AMM, it means that there needs to be some consistent process for caching AMM values to use in those calculations.\n\nI see two possible ways of doing this:\n\n1) Add a preblock action (like the 4788 beacon root call) to get the price from the AMM and store it. This seems pretty non-uniform and prone to error.\n\n2) Change this from a STATICCALL to a CALL, which would allow us to grab the AMM price and save it to our contract's storage before the first transaction in the block from within the call.\n\nThe second option seems pretty clearly superior, but curious if you have any specific side effects in mind that pushed you towards wanting to do this as a STATICCALL.", "2024-12-25T02:10:30Z", "2024-12-25T02:13:06Z", "zobront", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6Yrk-O", "I_kwDOLB-lzc6A2Nmv", "The staticcall can be less of a dos vector and I didn't have a good reason to allow for state modifications. Open to the idea but need benchmarking to be sure it's safe", "2024-12-25T02:39:34Z", "2024-12-25T02:39:34Z", "tynes", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6YtC7i", "I_kwDOLB-lzc6A2Nmv", "@tynes Yeah. Chain operators would definitely need to be careful to be sure whatever functionality they include doesn't impose any DOS risks. \n\nThere's some question about how much to restrict them (ie only allow a fixed amount of gas to be passed to the CALL, with some default to handle failure) vs writing some benchmark tests and trusting them to make the right choices.\n\nDo you have a sense of how OP likes to approach these things? The former probably seems safer to me.", "2024-12-25T16:30:11Z", "2024-12-25T16:30:11Z", "zobront", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6YtR8g", "I_kwDOLB-lzc6A2Nmv", "We should figure out how much gas is required to be useful for chain operators given a few standard use cases and then benchmark it to see how it impacts disk i/o. Another good benchmark would be to implement the existing fee algo in solidity and see how it impacts sync time on OP Mainnet and Base. There needs to be a cap to the amount of gas passed into the call no matter what but unclear what it should be without understanding how it will be used", "2024-12-25T21:49:57Z", "2024-12-25T21:51:14Z", "tynes", "2025-08-30 22:55:46"]
["IC_kwDOLB-lzc6YyJj4", "I_kwDOLB-lzc6A2Nmv", "Reflecting on this more, I don't think my idea will work. We'd need to set a gas cap that is capable of performing the upgrades, but this will be called every transaction (not just every block). That gas cap either needs to be too low to do anything useful, or poses a DOS risk if it's used in full every transaction. Tracking across transactions seems unnecessarily complex.\n\nThis brings me back to the other possibility discussed above, where a `CALL` to update values happens independently of the `STATICCALL` to perform the calculation. \n\nMy thought on the simplest option would be that we can piggyback this off the `L1Block.setL1BlockValues()` that is already happening regularly. This could either be accomplished by (a) having `L1Block.setL1BlockValues()` call out to a new Predeploy `L1DataCostOracle.update()` function or (b) by incorporating this functionality directly into the `L1Block` contract.\n\n***\n\nI'd lean towards the second option, as it seems simplest and cleanest. \n\nOn the vanilla OP Stack chain, It would involve:\n- Replacing the L1 Data Cost op-geth calculation with a call to `L1Block.getL1DataCost(bytes calldata cd)`.\n- Implementing this function on the L1Block contract that mirrors the current logic.\n\nOn a CGT chain that uses an AMM for prices, it would also require:\n- Adding logic to the `setL1BlockValues()` to read the AMM and store the values locally.\n- Writing a new `getL1DataCost()` function that uses those stored values to calculate the correct price.\n\n@tynes Thoughts?", "2024-12-27T03:45:43Z", "2024-12-27T03:45:43Z", "zobront", "2025-08-30 22:55:46"]
["IC_kwDODjvEJM6YGxac", "I_kwDODjvEJM6jyN4a", "You could be able to use the following flag to turn it off:\n\nhttps://github.com/ethereum-optimism/optimism/blob/d3a50acff5831aee28de941f3bd8bc9d06e6eff3/op-program/host/flags/flags.go#L103-L107\n\n```\n--l1.trustrpc true\n```", "2024-12-18T17:37:00Z", "2024-12-18T17:37:00Z", "tynes", "2025-08-30 22:55:50"]
["IC_kwDODjvEJM6YG51_", "I_kwDODjvEJM6jyN4a", "@tynes Thanks would try now", "2024-12-18T17:56:16Z", "2024-12-18T17:56:16Z", "tornadocontrib", "2025-08-30 22:55:50"]
["IC_kwDODjvEJM6YIEZV", "I_kwDODjvEJM6jyN4a", "Hey - Erigon team here. I would like to chime in and say that this should be default behaviour or at least be very well documented. `eth_getProof` is not needed by most people running a VC chain because they will end up running their own node anyway. this is just a suggestion but I always need to direct people and troubleshoot optimism node issue for some reason when someone wants to sync with Erigon. another point is that  `eth_getProof` for historical query requires multiples  TBs in all clients.", "2024-12-18T20:56:14Z", "2024-12-18T21:01:17Z", "Giulio2002", "2025-08-30 22:55:50"]
["IC_kwDODjvEJM6Yla2Y", "I_kwDODjvEJM6jyN4a", "Have synced op-geth with op-node on Erigon3 L1 using `--l1.rpckind=\"standard\" --l1.trustrpc` though all good", "2024-12-23T15:57:11Z", "2024-12-23T15:57:11Z", "tornadocontrib", "2025-08-30 22:55:50"]
["IC_kwDODjvEJM6X88xQ", "I_kwDODjvEJM6jk8uI", "Hi @siddharth0a this is a \"normal\" situation that the system has considered.\n\n1. Raft guarantees that at any point in time there will only be one leader\n2. There might be issues with closing / stopping sequencer (thus active sequencer), or even the conductor process is gone entirely and the way we guarantee it not cause disruption to the network is to have active sequencer commit unsafe payload to conductor first before gossiping it out, this way we make sure that the unsafe head is agreed upon during all the sequencers.\n\nSo as a result, I don't think it could `makes disorder to cluster`. You just need to do normal recovery for that node (figure out why it cannot be stopped, and restart it). The situation currently feels like a misconfig between conductor and sequencer that conductor somehow cann't stop sequencer from sequencing after leadership is transferred", "2024-12-17T18:54:01Z", "2024-12-17T19:36:57Z", "0x00101010", "2025-08-30 22:55:50"]
["IC_kwDODjvEJM6YgW-G", "I_kwDODjvEJM6jk8uI", "Close due to not further activities", "2024-12-22T22:05:43Z", "2024-12-22T22:05:43Z", "0x00101010", "2025-08-30 22:55:50"]
["IC_kwDODjvEJM6WUvqo", "I_kwDODjvEJM6iK1fb", "This is expected if the safe head hasn't advanced.  There is no value in proposing the same output root again - any withdrawals could just use the existing proposal.", "2024-12-06T03:01:30Z", "2024-12-06T03:01:30Z", "ajsutton", "2025-08-30 22:55:50"]
["IC_kwDODjvEJM6WUv5T", "I_kwDODjvEJM6iK1fb", "Sorry, hit enter too soon.  It's expected behaviour from the contracts, but making the proposer smarter does make sense - can you open that as a PR?", "2024-12-06T03:02:34Z", "2024-12-06T03:02:34Z", "ajsutton", "2025-08-30 22:55:50"]
["IC_kwDOI7W0xc6XRtzP", "I_kwDOI7W0xc6hvPoM", "#1157 ", "2024-12-12T07:21:55Z", "2024-12-12T07:21:55Z", "chuxinh", "2025-08-30 22:55:50"]
["IC_kwDOFpg0Ns6ZSfnG", "I_kwDOFpg0Ns6NbByM", "I would like to work on this please assign it to me.", "2025-01-05T21:51:55Z", "2025-01-05T21:51:55Z", "shraddha38", "2025-08-30 23:43:05"]
["IC_kwDOKIwiaM6ZJ4Dj", "I_kwDOKIwiaM6kbCX8", "thanks for calling this out, we fixed the issue :)", "2025-01-03T16:30:46Z", "2025-01-03T16:30:46Z", "sbvegan", "2025-08-30 23:43:06"]
["IC_kwDOKIwiaM6SMO6t", "I_kwDOKIwiaM6cwdDs", "@pauldowman initially should these go under the experimental section or directly into the OP Stack fault proof system?", "2024-11-01T22:39:52Z", "2024-11-01T22:39:52Z", "sbvegan", "2025-08-30 23:43:06"]
["IC_kwDOKIwiaM6SVS30", "I_kwDOKIwiaM6cwdDs", "Good question. I think experimental.", "2024-11-04T15:44:26Z", "2024-11-04T15:44:26Z", "pauldowman", "2025-08-30 23:43:06"]
["IC_kwDOKIwiaM6UqFUE", "I_kwDOKIwiaM6cwdDs", "@sbvegan  - I don't know who will take this work in the future, but a good starting point for Kona is Kona book: https://anton-rs.github.io/kona/\n", "2024-11-22T15:44:23Z", "2024-11-22T15:44:23Z", "BlocksOnAChain", "2025-08-30 23:43:06"]
["IC_kwDOKIwiaM6ZKMjS", "I_kwDOKIwiaM6cwdDs", "Closing in favor of https://github.com/ethereum-optimism/devrel/issues/439", "2025-01-03T17:36:22Z", "2025-01-03T17:36:22Z", "bradleycamacho", "2025-08-30 23:43:06"]
["IC_kwDOKIwiaM6Y8wdG", "I_kwDOKIwiaM6aBeT9", "https://docs.optimism.io/chain/testing/dev-node", "2024-12-31T01:46:14Z", "2024-12-31T01:46:14Z", "sbvegan", "2025-08-30 23:43:06"]
["IC_kwDOKSJyfM6YWvPE", "I_kwDOKSJyfM6kBFkP", "@pharger please confirm the requirements above and make any necessary changes to this flow! Thanks!", "2024-12-20T01:45:41Z", "2024-12-20T01:45:41Z", "fainashalts", "2025-08-31 00:13:21"]
["IC_kwDOKSJyfM6Zbavq", "I_kwDOKSJyfM6kBFkP", "Updated the PRD for this to simplify: https://docs.google.com/document/d/1HQkgJP3eETHGQy3rm18X3suu2taaJ3UjYILaqXzhLTo/edit?pli=1&tab=t.0\n\nI dont think this should be blocked by API Key - think we should be able to do the MVP work here to cleanup the wallet connect and profile.", "2025-01-06T23:25:43Z", "2025-01-06T23:25:43Z", "pharger", "2025-08-31 00:13:21"]
["IC_kwDOKSJyfM6QPFi9", "I_kwDOKSJyfM6ahH-N", "We've come to the decision to showcase cross-chain asset transfers through a prediction market contract that can establish bets on any ethereum log event in the superchain\r\n\r\n**Summary**\r\nWe want to showcase the unique value prop of the interop protocol with event interoperability that underpins the message passing protocol. Event interop allows contracts to be composable cross-chain in the same way contracts in a single chain are composable.\r\n\r\n**Prediction Market Game**\r\nWe can imagine a world where their is a prediction market chain in the superchain. We can reduce this down to a single prediction market smart contract that exists on 1 chain.\r\n\r\n1. We'll showcase asset transfers but allowing anyone to place an ETH bet from any chain. This will use cross-chain messaging to bridge the ETH to the destination chain & place a bet with the smart contract\r\n2. We'll showcase event interoperability with two types of predictions markets can can be placed\r\n    - Establish a prediction market if the block height at X for chain Y is even or odd\r\n    - Establish a prediction market on the outcome of the demo tictactoe game. This is cool because it showcases how unrelated apps can build upon each other\r\n\r\n\r\n**Design Requirements**\r\nWe don't have too long to build this so lets be minimal. Focusing on (2a) first and a fast follow with (2b) if time allows\r\n \r\n- User can connect their wallet and see active prediction markets, see stats/odds of the market. They can participate with ETH from any chain\r\n    - Since the prediction market only exists on 1 chain. If the user wants to use ETH from a separate chain, there will be a UX flow of switching networks and initiating the tx to bridge funds & execute the bet.\r\n    - Autorelay of supersim can take care of executing this on the destination chain. However the UI should have elements of \"waiting\" for the bridged tx to execute.\r\n    \r\n- User can create a new prediction market\r\n    - (2a) pick a chain in the superchain and L2 block height for which the market anchors on\r\n    - (2b) select from a list of active tic tac toe games\r\n", "2024-10-17T15:32:47Z", "2024-10-17T15:33:02Z", "hamdiallam", "2025-08-31 00:13:21"]
["IC_kwDOKSJyfM6QrAiN", "I_kwDOKSJyfM6ahH-N", "@mayanksayshi any update on the designs for this dapp? No rush but would like to hack this together this week", "2024-10-21T16:46:01Z", "2024-10-21T16:46:01Z", "hamdiallam", "2025-08-31 00:13:21"]
["IC_kwDOKSJyfM6QrEt6", "I_kwDOKSJyfM6ahH-N", "@hamdiallam, there is no update on this. I am currently working on the Multichain faucet flow, which is the higher priority. I will tackle this hopefully later this :)  \r\n\r\ncc @pharger  ", "2024-10-21T16:48:07Z", "2024-10-21T16:48:07Z", "mayanksayshi", "2025-08-31 00:13:21"]
["IC_kwDOKSJyfM6Qr1BZ", "I_kwDOKSJyfM6ahH-N", "okay sounds good! Totally fine to do this later, still need to first get the contracts working for this", "2024-10-21T18:17:11Z", "2024-10-21T18:17:11Z", "hamdiallam", "2025-08-31 00:13:21"]
["IC_kwDOKSJyfM6RuoDW", "I_kwDOKSJyfM6ahH-N", "@hamdiallam, here are the latest designs for the prediction market app. Looking forward to your feedback. :) \r\n\r\nhttps://www.figma.com/design/SnDEKbBN5bCQpabXdW1rQm/Prediction-Market-Reference-app?node-id=0-3818&t=T9HkWKbt1g70jVXe-1\r\n\r\n\r\ncc @fainashalts @pharger ", "2024-10-29T17:28:05Z", "2024-10-29T19:25:53Z", "mayanksayshi", "2025-08-31 00:13:21"]
["IC_kwDOKSJyfM6ZZ0eP", "I_kwDOKSJyfM6ahH-N", "Updating here that this piece was cut from scope:\n\n> We'll showcase asset transfers but allowing anyone to place an ETH bet from any chain. This will use cross-chain messaging to bridge the ETH to the destination chain & place a bet with the smart contract\n\n\nCan be added in as a followup if desired", "2025-01-06T18:33:06Z", "2025-01-06T18:33:40Z", "hamdiallam", "2025-08-31 00:13:21"]
["IC_kwDOKIwiaM6SMJx6", "I_kwDOKIwiaM6cwcZE", "@mbaxter @Inphi can you take a look at the checklist here and add general types of things that need documenting?\r\n\r\nOr I'm also wondering if the changes are small and specific enough that the easiest thing might just be for one of you to go through the docs repo for any cannon references and just make a PR?", "2024-11-01T22:13:07Z", "2024-11-01T22:13:07Z", "pauldowman", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6SVrsd", "I_kwDOKIwiaM6cwcZE", "The challenger cli options don't change. Otherwise, the list covers the docs changes. Specifically, the [cannon page on docs.optimism.io](https://docs.optimism.io/stack/fault-proofs/cannon) should be updated.", "2024-11-04T16:26:02Z", "2024-11-04T16:26:02Z", "Inphi", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6SVx7T", "I_kwDOKIwiaM6cwcZE", "There's also a page on [MIPS.sol](https://docs.optimism.io/stack/fault-proofs/mips) that looks like it should be updated.", "2024-11-04T16:37:04Z", "2024-11-04T16:37:04Z", "mbaxter", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6SeZ1U", "I_kwDOKIwiaM6cwcZE", "Also, there's a relevant [issue in the specs repo](https://github.com/ethereum-optimism/specs/issues/227) suggesting the [MIPS.sol doc](https://docs.optimism.io/stack/fault-proofs/mips) moves to the specs repo. ", "2024-11-05T15:15:58Z", "2024-11-05T15:15:58Z", "mbaxter", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6UrsPR", "I_kwDOKIwiaM6cwcZE", "See also:\n- https://github.com/ethereum-optimism/devrel/issues/440\n- https://github.com/ethereum-optimism/devrel/issues/441", "2024-11-22T18:19:47Z", "2024-11-22T18:20:11Z", "pauldowman", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6ZZtZD", "I_kwDOKIwiaM6cwcZE", "Closing in favor of tracking work in https://github.com/ethereum-optimism/devrel/issues/439", "2025-01-06T18:14:46Z", "2025-01-06T18:14:46Z", "bradleycamacho", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6ZZy-3", "I_kwDOKIwiaM6DLkO7", "This is live now.\nClosing this", "2025-01-06T18:29:27Z", "2025-01-06T18:29:27Z", "krofax", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6ZZx88", "I_kwDOKIwiaM6DLkNq", "Closing this, we already have a [PR](https://github.com/ethereum-optimism/docs/pull/966) ", "2025-01-06T18:26:43Z", "2025-01-06T18:26:43Z", "krofax", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6ZZysK", "I_kwDOKIwiaM6DLkMa", "We have treated this.", "2025-01-06T18:28:41Z", "2025-01-06T18:28:41Z", "krofax", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM579c6l", "I_kwDOKIwiaM6B8BOa", "might be addressed by #464 ", "2024-04-26T16:23:53Z", "2024-04-26T16:23:53Z", "cpengilly", "2025-08-31 00:13:24"]
["IC_kwDOKIwiaM6ZZhxD", "I_kwDOKIwiaM6B8BOa", "This [issue](https://github.com/ethereum-optimism/docs/issues/464) addresses this.\n", "2025-01-06T17:46:18Z", "2025-01-06T17:46:18Z", "krofax", "2025-08-31 00:13:24"]
["IC_kwDOLB-lzc6SOR_k", "I_kwDOLB-lzc6boPuG", "Nice writeup!  One quick question is what is the relationship of Interop ZK Proofs vs ZK Fault Proof? I feel they should have a lot of similarities but also subtle differences.", "2024-11-02T22:51:26Z", "2024-11-02T22:51:26Z", "qizhou", "2025-08-31 00:13:29"]
["IC_kwDOI7W0xc6XmCou", "I_kwDOI7W0xc6jUwU1", "See exploration here:\n\nhttps://github.com/ethereum-optimism/op-analytics/issues/1055#issuecomment-2537093676", "2024-12-14T20:14:21Z", "2024-12-14T20:14:21Z", "MSilb7", "2025-08-31 00:13:33"]
["IC_kwDOI7W0xc6XYL8G", "I_kwDOI7W0xc6jHEVB", "Transaction aggregation options\n\nMaybe:\n- Enriched\n- From / To / Method\n- To\n\n<img width=\"723\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c0300cce-2e38-4538-8518-48b6bdc381ee\" />\n", "2024-12-12T18:11:53Z", "2024-12-12T18:11:53Z", "MSilb7", "2025-08-31 00:13:33"]
["IC_kwDODjvEJM6Z0VR2", "I_kwDODjvEJM6lfvbD", "I believe that we have this already, alongside autoclosing", "2025-01-09T15:54:52Z", "2025-01-09T15:54:52Z", "tynes", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6Z0nZj", "I_kwDODjvEJM6lfvbD", "do you have any links to the code? @tynes ", "2025-01-09T16:18:28Z", "2025-01-09T16:18:28Z", "emhane", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6ZoiIE", "I_kwDODjvEJM6lJnGd", "closing since we will do a release in later gov cycle", "2025-01-08T12:20:23Z", "2025-01-08T12:20:23Z", "BlocksOnAChain", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YVCDI", "I_kwDODjvEJM6j-xAg", "Oh, also all of the above applies to the Legacy OptimismPortal as well. \n\nIn order to keep things simple, I think that we should not yet rename `OptimismPortal2`. If we decide to do so, it can be done in a follow up PR", "2024-12-19T19:48:00Z", "2024-12-19T19:49:28Z", "maurelian", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YWbm5", "I_kwDODjvEJM6j-xAg", "> In order to keep things simple, I think that we should not yet rename OptimismPortal2. If we decide to do so, it can be done in a follow up PR\n\nAgree on follow up PR", "2024-12-20T00:31:36Z", "2024-12-20T00:31:36Z", "tynes", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6ZWa88", "I_kwDODjvEJM6j-xAg", "Now that the L2OutputOracle is deprecated, how can I get the latest L2 root from Ethereum? Is that even possible anymore?\n\n@maurelian ", "2025-01-06T10:13:20Z", "2025-01-06T10:13:33Z", "jonas089", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6XjgMe", "I_kwDODjvEJM6jRHac", "I'll try this one. Thank you.", "2024-12-14T02:13:16Z", "2024-12-14T02:13:16Z", "FinParker", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6X2BTO", "I_kwDODjvEJM6jRHac", "Nice", "2024-12-17T05:09:46Z", "2024-12-17T05:09:46Z", "UnionSummer09", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YaYTU", "I_kwDODjvEJM6jRHac", "is the issue fixed?", "2024-12-20T13:48:06Z", "2024-12-20T13:48:06Z", "anushkasomani", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6ZYIII", "I_kwDODjvEJM6jRHac", "This will be fixed by #13563. ", "2025-01-06T14:34:36Z", "2025-01-06T14:34:36Z", "maurelian", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6XPMde", "I_kwDODjvEJM6iyV9W", "This PR takes the signal to derive from a L1 all the way from the Supervisor to the Sync Nodes:\n\nhttps://github.com/ethereum-optimism/optimism/pull/13344\n\nFrom this PR it should be possible to build the derivation path on the node.", "2024-12-11T22:36:11Z", "2024-12-11T22:36:11Z", "axelKingsley", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6XizbK", "I_kwDODjvEJM6iyV9W", "L1 Traversal Managed Mode WIP:\nhttps://github.com/ethereum-optimism/optimism/pull/13392", "2024-12-13T22:28:50Z", "2024-12-13T22:28:50Z", "axelKingsley", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YUxBO", "I_kwDODjvEJM6iyV9W", "TODOs\n\nop-supervisor:\n- Merge `interop-managed-sync`\n- Error handling when local-safe DB update\n  - Includes sending the right reset to node\n- Exhaust-L1 updates, pull from L1 accessor, reply to sync node\n  - backend needs to expose L1BlockRefByNumber so that sync-controller can use it through the backend interface.\n- Fix `UpdateFinalizedL1` deadlock: db.finalizedL1 is locked while trying to read it in NotifyL2Finalized\n\nop-node:\n- L1Traversal AdvanceL1Block to use the data from supervisor\n- op-node ManagedMode unit tests\n\nLow-hanging fruit:\n- docs on new types\n  - DerivedPair\n  - L1Accessor\n- `AddSubscriptions` overwriting subscriptions seems bad, can we fix?\n- fix import lint in `op-node/node`\n- check sufficient channel capacity for each subscription, prevent blocking on slow consumers\n- Ensure we stop legacy op-node finalizer on interop fork\n- Ensure we remove update API methods from supervisor frontend\n\nLater polish:\n- L1Accessor should not use PollBlockChanges, but use eth-subscribe, with the polling as fallback, like op-node does.\n- L1Accessor needs to lock on SetTipHeight\n- UpdateFinalizedL1 typo\n- NotifyL2Finalized log if finality subscription of chain not found\n", "2024-12-19T19:07:26Z", "2024-12-19T19:07:26Z", "protolambda", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YU8kD", "I_kwDODjvEJM6iyV9W", "Random thing I remembered: We are newly recording `oldL2:newL1` in the database when a new L1 is discovered, to enforce the \"only one block height increments\" invariant. Now that the L1 Processor doesn't exist, this isn't happening. It'll have to happen from the L1 Exhaustion signal now.", "2024-12-19T19:34:03Z", "2024-12-19T19:34:03Z", "axelKingsley", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YVSYI", "I_kwDODjvEJM6iyV9W", "Axel: \n- fix onDerivationUpdate\n- maybe use same helper func to fix onUnsafeBlock\n- deadlock\n- low hanging fruit\n\nProto:\n- fix onExhaustL1Event\n- fix op-node side of L1 traversal with events\n\n", "2024-12-19T20:29:01Z", "2024-12-19T20:29:01Z", "protolambda", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YbM3i", "I_kwDODjvEJM6iyV9W", "- reset onDerivationUpdate: https://github.com/ethereum-optimism/optimism/pull/13497\n- Not sure the function is suitable for onUnsafeBlock too\n- deadlock: https://github.com/ethereum-optimism/optimism/pull/13506", "2024-12-20T15:55:24Z", "2024-12-20T15:55:24Z", "axelKingsley", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YbrI2", "I_kwDODjvEJM6iyV9W", "First half of the low hanging fruit list: https://github.com/ethereum-optimism/optimism/pull/13507", "2024-12-20T17:01:24Z", "2024-12-20T17:01:24Z", "axelKingsley", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YcJP_", "I_kwDODjvEJM6iyV9W", "Regarding: \"Ensure we remove update API methods from supervisor frontend\"\n\nThe UpdateFrontend is already gone it would appear. \u2705 ", "2024-12-20T18:26:10Z", "2024-12-20T18:26:10Z", "axelKingsley", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YcnV2", "I_kwDODjvEJM6iyV9W", "Updated TODOs:\n Remaining low-hanging fruit\n    - `AddSubscriptions` overwriting subscriptions seems bad, can we fix?\n- Improve op-supervisor testing\n    - Controller test doesn\u2019t cover everything yet.\n    - Model the depencies of sync controller better\n- op-node ManagedMode unit tests\n    - Like other deriver tests, just with mock emitter, assert in/output events\n- Fix action-tests\n    - Make the background routines of the sync-controller optional. Disable in action tests.\n    - Add some method to drain all buffered events, basically run what the background routine would do, synchronously.\n- Fix reset loop: see todo about tracking the last known block of the ManagedNode, and then reset to one prior, until it succeeds.", "2024-12-20T19:56:35Z", "2024-12-20T19:56:35Z", "protolambda", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6YdfFG", "I_kwDODjvEJM6iyV9W", "> - AddSubscriptions overwriting subscriptions seems bad, can we fix?\n\nI believe this was fixed in https://github.com/ethereum-optimism/optimism/commit/38c0bb5efab56f3d6ebc9efe5394043606a980b7", "2024-12-20T23:10:08Z", "2024-12-20T23:10:08Z", "axelKingsley", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6Wp8lD", "I_kwDODjvEJM6fUQvH", "Hey!\nDid you meant something like this:\n\n```diff\ndiff --git a/op-node/rollup/derive/system_config.go b/op-node/rollup/derive/system_config.go\nindex 72c4e713c..a6fcd908e 100644\n--- a/op-node/rollup/derive/system_config.go\n+++ b/op-node/rollup/derive/system_config.go\n@@ -93,6 +93,7 @@ func ProcessSystemConfigUpdateLogEvent(destSysCfg *eth.SystemConfig, ev *types.L\n \t\t\treturn NewCriticalError(errors.New(\"too many bytes\"))\n \t\t}\n \t\tdestSysCfg.BatcherAddr = address\n+\t\tfmt.Printf(\"batch-sender address update:: %x\\n\", address)\n \t\treturn nil\n \tcase SystemConfigUpdateFeeScalars:\n \t\tif pointer, err := solabi.ReadUint64(reader); err != nil || pointer != 32 {\n```\n\nAny advice how can I simulate input, parameters for `ProcessSystemConfigUpdateLogEvent(destSysCfg *eth.SystemConfig, ev *types.Log, rollupCfg *rollup.Config, l1Time uint64)`  so I could test my changes?", "2024-12-09T10:51:45Z", "2024-12-09T10:51:45Z", "bordolot", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6ZKfSl", "I_kwDODjvEJM6eJOWN", "This is being addressed by the Anchor-point change in the dataflow refactor PR.\n", "2025-01-03T18:43:00Z", "2025-01-03T18:43:00Z", "protolambda", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM6Z5b1j", "I_kwDODjvEJM6eJOWN", "This was resolved in https://github.com/ethereum-optimism/optimism/pull/13481 ", "2025-01-10T07:38:06Z", "2025-01-10T07:38:06Z", "unknown", "2025-08-31 00:14:16"]
["IC_kwDODjvEJM575K1u", "I_kwDODjvEJM6AwcKY", "I'd love to start contributing on here with this issue. Why was the previous PR closed though, so I'm better informed @t4sk ", "2024-04-26T03:53:32Z", "2024-04-26T03:53:53Z", "ledragon0x", "2025-08-31 00:14:17"]
["IC_kwDOKSJyfM6Z2_gB", "I_kwDOKSJyfM6loVno", "this was my previous attempt at doing it at compile time. def would be nice to have both so ppl don't have to update packages to get new updates\nhttps://github.com/jakim929/superchain-tools/tree/main/packages/chains", "2025-01-09T22:04:45Z", "2025-01-09T22:04:45Z", "jakim929", "2025-08-31 00:15:38"]
["IC_kwDOKSJyfM6Z2_rA", "I_kwDOKSJyfM6loVno", "For the compile time chains I wrote a script months ago when the registry was first starting to autogenerate chain defs that don't exist in viem\n\n* https://github.com/ethereum-optimism/ecosystem/blob/main/packages/op-app/src/configs/chains.ts\n* https://github.com/ethereum-optimism/ecosystem/blob/main/packages/op-app/scripts/codegen.ts\n* https://github.com/ethereum-optimism/ecosystem/blob/main/packages/op-app/templates/chains.eta\n\nWe can do something similar for the viem package and when we run the generate command it can pull the latest registry file https://github.com/ethereum-optimism/ecosystem/blob/15ca959d96828bd34541f862716b11b20299f885/packages/viem/package.json#L18", "2025-01-09T22:05:14Z", "2025-01-09T22:06:38Z", "nitaliano", "2025-08-31 00:15:38"]
["IC_kwDOH2Qg5s6aRf8X", "I_kwDOH2Qg5s6mBnvL", "Closing this due, will use backporting instead", "2025-01-13T21:41:28Z", "2025-01-13T21:41:28Z", "tynes", "2025-08-31 00:15:40"]
["IC_kwDOH2Qg5s6afFDZ", "I_kwDOH2Qg5s6M7MHv", "I also encountered the same problem, I don't know how you solved it", "2025-01-15T07:37:04Z", "2025-01-15T07:37:04Z", "dreamer-zq", "2025-08-31 00:15:40"]
["IC_kwDOI7W0xc6W5qTM", "I_kwDOI7W0xc6gd86f", "MVP is just the current & not worry about historical changes", "2024-12-10T13:51:33Z", "2024-12-10T13:51:33Z", "MSilb7", "2025-08-31 00:15:42"]
["IC_kwDOLB-lzc6alrpH", "I_kwDOLB-lzc6mUwAi", "won't fix: blobs don't apply on OP", "2025-01-15T17:43:00Z", "2025-01-15T17:43:00Z", "emhane", "2025-08-31 00:15:43"]
["IC_kwDOLB-lzc6alq92", "I_kwDOLB-lzc6mUtRG", "won't fix: Blobs don't apply to OP", "2025-01-15T17:41:42Z", "2025-01-15T17:41:42Z", "emhane", "2025-08-31 00:15:43"]
["IC_kwDOLB-lzc6alB-_", "I_kwDOLB-lzc6mUFai", "EIP 7549 makes no changes to the execution layer.\nThe consensus layer changes are specific to the beacon chain and do not need to be implemented.", "2025-01-15T16:33:15Z", "2025-01-15T16:33:15Z", "refcell", "2025-08-31 00:15:43"]
["IC_kwDOLB-lzc6alk1p", "I_kwDOLB-lzc6mUFSs", "EIP-7742 was cancelled for Pectra.\n\nSee the [pectra specs here](https://notes.ethereum.org/@ethpandaops/pectra-devnet-5)", "2025-01-15T17:29:20Z", "2025-01-15T17:29:20Z", "refcell", "2025-08-31 00:15:43"]
["IC_kwDOLB-lzc6aS02B", "I_kwDOLB-lzc6mBcGp", "this means any change to the set, including removals, correct? ", "2025-01-14T02:03:51Z", "2025-01-14T02:03:51Z", "sambacha", "2025-08-31 00:15:43"]
["IC_kwDOLB-lzc6abbCR", "I_kwDOLB-lzc6mBcGp", "> this means any change to the set, including removals, correct?\n\nRemovals of a chain in a cluster are not in scope. Its just too complex to decouple the assets. The only way to leave a cluster would be to do a network upgrade that removes the other chains from your dependency set.\n\nEach modification to the dependency set is a hardfork", "2025-01-14T18:56:50Z", "2025-01-14T18:56:50Z", "tynes", "2025-08-31 00:15:43"]
["IC_kwDOLB-lzc6LJdsN", "I_kwDOLB-lzc6VOw3y", "hi @clabby, understanding the problem this creates for `checkWithdrawal`. Spitballing one initial impl idea just to get things started:\r\n\r\nwhen calling `setRespectedGameType` we could save a `previousRespectedGameType` to storage, save the last game's `respectedGameTypeUpdatedAt` as `validFrom`, and set a `validUntil` on it.\r\n\r\n```solidity\r\nstruct PreviousRespectedGameType {\r\n  GameType gameType;\r\n  uint64 validFrom;\r\n  uint64 validUntil;\r\n}\r\n```\r\n\r\nwe could then expand on the conditional branches a bit in `checkWithdrawal` to accommodate this `previousRespectedGameType`:\r\n\r\n```solidity\r\n if (disputeGameProxy.gameType().raw() != respectedGameType.raw()) {\r\n  if(disputeGameProxy.gameType().raw() != previousRespectedGameType.gameType.raw() {\r\n    revert InvalidGameType();\r\n  }\r\n  require(\r\n    createdAt >= previousRespectedGameType.validFrom && block.timestamp <= previousRespectedGameType.validUntil\r\n  );\r\n}\r\n...\r\n```\r\n\r\nif the guardian didn't want the previous respected game type to hang around, they could specify that, and the `previousRespectedGameType` could be set to 0,0,0.\r\n\r\na few questions to start:\r\n- will we ever want to support multiple previous `respectedGameType`s?\r\n- is it ok to let the guardian set their own `validUntil` timestamp on the `PreviousRespectedGameType`? (perhaps it could be bounded using `DISPUTE_GAME_FINALITY_DELAY_SECONDS` and/or `PROOF_MATURITY_DELAY_SECONDS`)\r\n- is it ok to let the guardian also \"set\" the `lastRespectedGameType` (in the case where a bad game type is mistakenly set, and another one is set immediately after that?)\r\n\r\nkeen to hear from you, would be happy to hear suggestions or considerations to keep in mind.", "2024-09-06T17:28:23Z", "2024-09-06T19:29:08Z", "wildmolasses", "2025-08-31 00:15:43"]
["IC_kwDOLB-lzc6L52gU", "I_kwDOLB-lzc6VOw3y", "Does it simplify anything if we thought of this as just respecting already proven withdrawals rather than trying to continue respecting the previous game type? e.g\r\n1. Respected game type is 0\r\n2. User A proves against a game of type 0\r\n3. Respected game type is switched to 1\r\n4. User A can still finalise their withdrawal (assuming the game resolved as DefenderWins and all the time delays are completed)\r\n5. But no new withdrawals can be proven against games of type 0\r\n\r\nSo `proveWithdrawal` logic I think would be unchanged since it just requires the game type to be the currently respected type, but we'd only update `respectedGameTypeUpdatedAt` when changing the respected game type if we were choosing to invalidate all withdrawals. And in `checkWithdrawal` we remove the check that the game type is the current respected game type - it must have been the respected game type when it was proven or `proveWithdrawal` would have reverted so as long as the game was created after the `respectedGameTypeUpdatedAt` we are ok to allow the withdrawal.\r\n\r\nSomeone more familiar with this code should definitely double check that logic, but that's the kind of thing I was hoping we'd be able to do.", "2024-09-12T20:39:05Z", "2024-09-12T20:39:05Z", "ajsutton", "2025-08-31 00:15:43"]
["IC_kwDOJ_r-bs6aRfv_", "I_kwDOJ_r-bs6mBrCk", "After chatting with @mslipper about this, we decided that backporting new releases of the `superchain-registry` on top of older release of `op-geth` and `op-node` makes more sense for now. The problem with the `absolutePrestate` might sort of be fixed by using shared proof contracts, but it will not solve the problem between clusters", "2025-01-13T21:40:56Z", "2025-01-13T21:40:56Z", "tynes", "2025-08-31 00:15:53"]
["IC_kwDOKIsnqM6aYNAQ", "I_kwDOKIsnqM6lUl1s", "Referencing this issue: https://github.com/ethereum-optimism/superchain-ops/issues/445 which proposes a solution to the above. ", "2025-01-14T14:15:40Z", "2025-01-14T14:15:40Z", "blmalone", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6aQXMY", "I_kwDOKIsnqM6lUBAC", "duplicate of https://github.com/ethereum-optimism/superchain-ops/issues/342 ?", "2025-01-13T19:12:00Z", "2025-01-13T19:12:00Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6aYQs_", "I_kwDOKIsnqM6lUBAC", "I'm going to close this issue in favor of https://github.com/ethereum-optimism/superchain-ops/issues/342. I'll like this issue in #342 - Please let me know if I'm missing something and this isn't a duplicate. ", "2025-01-14T14:21:48Z", "2025-01-14T14:21:48Z", "blmalone", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6Z6axO", "I_kwDOKIsnqM6lT08P", "We can probable merge this issue and https://github.com/ethereum-optimism/superchain-ops/issues/451 as a solution to it?", "2025-01-10T09:49:50Z", "2025-01-10T09:49:50Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6aYMLJ", "I_kwDOKIsnqM6lT08P", "@sebastianst yes I think that makes sense. ", "2025-01-14T14:14:32Z", "2025-01-14T14:14:32Z", "blmalone", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6aYNXI", "I_kwDOKIsnqM6lT08P", "Closing in favor of https://github.com/ethereum-optimism/superchain-ops/issues/451", "2025-01-14T14:16:15Z", "2025-01-14T14:16:15Z", "blmalone", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6aQW_s", "I_kwDOKIsnqM6kICSD", "duplicate of https://github.com/ethereum-optimism/superchain-ops/issues/342 ?", "2025-01-13T19:11:33Z", "2025-01-13T19:11:33Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6W7hE0", "I_kwDOKIsnqM6itMBJ", "This is likely a device id not existing in the ledger Go library used by geth", "2024-12-10T16:15:21Z", "2024-12-10T16:15:21Z", "tynes", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6XWxdJ", "I_kwDOKIsnqM6itMBJ", "Just tried signing task `sep/021` again with a Flex and `eip712sign v0.0.8` and again got error `Warning: signer creation failed: no ledgers found, please connect your ledger`.\n\nI can normally sign transactions with the Flex and the Ledger Live app.", "2024-12-12T15:45:46Z", "2024-12-12T15:46:49Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6aQQ9u", "I_kwDOKIsnqM6itMBJ", "This got fixed by https://github.com/ethereum-optimism/superchain-ops/pull/453", "2025-01-13T18:58:49Z", "2025-01-13T18:58:49Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6W7gjm", "I_kwDOKIsnqM6ip_i1", "Decision: we are going to redo the upgrade and call `initialize` as part of the upgrade\n\nThe post checks should assert specifically on `basefeeScalar()` and `blobBasefeeScalar()`", "2024-12-10T16:14:28Z", "2024-12-10T23:14:05Z", "tynes", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6YG0eK", "I_kwDOKIsnqM6ip_i1", "@tynes what should we do for chains that have a `scalar` set to something we can't decode? See here https://sepolia.etherscan.io/address/0x5D63A8Dc2737cE771aa4a6510D063b6Ba2c4f6F2#readProxyContract#F33", "2024-12-18T17:43:48Z", "2024-12-18T17:43:48Z", "geoknee", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6YHha7", "I_kwDOKIsnqM6ip_i1", "Can you give more details? I don't understand why you cant decode `684000`. Encoded as `bytes32`, it is version 0 so it should be fine", "2024-12-18T19:28:21Z", "2024-12-18T19:28:21Z", "tynes", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6YH7Wg", "I_kwDOKIsnqM6ip_i1", "Oh yeah right, that's a pre-Ecotone encoding, so this is a base fee scalar of `684000` (and implicitly blob base fee scalar of 0) because it's a version 0 pre-Ecotone encoding. See [specs](https://specs.optimism.io/protocol/system-config.html#ecotone-scalar-overhead-uint256uint256-change) for more detail @geoknee on how to parse version-0 scalars.", "2024-12-18T20:31:35Z", "2024-12-18T20:31:35Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6YITbR", "I_kwDOKIsnqM6ip_i1", "That makes sense. I had thought there was a problem because I couldn't get the `ecotone-scalar` to decode it. I think it is a bit buggy, will send a fix. ", "2024-12-18T21:37:43Z", "2024-12-18T21:37:43Z", "geoknee", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6YIjh_", "I_kwDOKIsnqM6ip_i1", "https://github.com/ethereum-optimism/optimism/pull/13472", "2024-12-18T22:20:52Z", "2024-12-18T22:20:52Z", "geoknee", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6Ybk4g", "I_kwDOKIsnqM6ip_i1", "Just to point out that [initialize will reencode with version 1 of the scalar encoder,](https://github.com/ethereum-optimism/optimism/blob/92513ea878d13312c8442f1ea1e726faa6cd2b42/packages/contracts-bedrock/src/L1/SystemConfig.sol#L379) meaning that chains with a version 0 scalar will  get a non trivial change to the raw data of `scalar` (even if semantically there is no change). ", "2024-12-20T16:44:42Z", "2024-12-20T16:44:42Z", "geoknee", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6Yb12d", "I_kwDOKIsnqM6ip_i1", "> Just to point out that [initialize will reencode with version 1 of the scalar encoder,](https://github.com/ethereum-optimism/optimism/blob/92513ea878d13312c8442f1ea1e726faa6cd2b42/packages/contracts-bedrock/src/L1/SystemConfig.sol#L379) meaning that chains with a version 0 scalar will get a non trivial change to the raw data of `scalar` (even if semantically there is no change).\n\nI discussed this with George. I believe it should be fine because both encodings lead to the same outcome (if blob base fee is 0). We're not emitting a config change event during `initialize()`, so this just migrates the contract state into a consistent state.", "2024-12-20T17:32:15Z", "2024-12-20T17:32:15Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6YcY3y", "I_kwDOKIsnqM6ip_i1", "> Just to point out that [initialize will reencode with version 1 of the scalar encoder,](https://github.com/ethereum-optimism/optimism/blob/92513ea878d13312c8442f1ea1e726faa6cd2b42/packages/contracts-bedrock/src/L1/SystemConfig.sol#L379) meaning that chains with a version 0 scalar will get a non trivial change to the raw data of `scalar` (even if semantically there is no change).\n\nWhat does non trivial mean? From the pov of bytes32, its a single bit. From the pov of uint256, its a massively different number. Is this what you mean? This is expected behavior.", "2024-12-20T19:12:38Z", "2024-12-20T19:12:38Z", "tynes", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6ZJ2pt", "I_kwDOKIsnqM6ip_i1", "I just mean that when validating the upgrade, we need to allow for this field to change for certain chains. Previously I had been requiring it to not change.", "2025-01-03T16:26:22Z", "2025-01-03T16:26:22Z", "geoknee", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6Ww_FF", "I_kwDOKIsnqM6iUvFx", "Could this be related to [ops fail on ARM machines due to eip712signer incompatibility](https://github.com/ethereum-optimism/superchain-ops/issues/375) ?", "2024-12-09T20:53:09Z", "2024-12-09T20:53:09Z", "geoknee", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6W7_mT", "I_kwDOKIsnqM6iUvFx", "> Could this be related to [ops fail on ARM machines due to eip712signer incompatibility](https://github.com/ethereum-optimism/superchain-ops/issues/375) ?\n\nI don't think so. The `eip712signer` works \"fine\" in the sense that it runs but is unable to recognize the ledger device. Whereas, I think the problem in that issue is that `eip712signer` would get killed by the OS because its binary target isn't expected.", "2024-12-10T17:07:33Z", "2024-12-10T17:07:33Z", "Inphi", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6W8IJn", "I_kwDOKIsnqM6iUvFx", "@Inphi This is with a ledger nano s?", "2024-12-10T17:23:31Z", "2024-12-10T17:23:31Z", "mds1", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6W8KQf", "I_kwDOKIsnqM6iUvFx", "@mds1 It's a Ledger Nano S Plus.", "2024-12-10T17:27:35Z", "2024-12-10T17:27:35Z", "Inphi", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6W8xed", "I_kwDOKIsnqM6iUvFx", "Okay, I am currently with @Inphi and we tried multiples hardware things as restart, change with a low level terminal (terminal native of macos) also changing usb and usbc port.\n\nI also add the same issue with @sebastianst this morning but I was thinking was related to only the **Flex**.\nhttps://github.com/ethereum-optimism/superchain-ops/issues/395\nstill investigating", "2024-12-10T18:14:09Z", "2024-12-10T18:14:09Z", "Ethnical", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6acGTY", "I_kwDOKIsnqM6iUvFx", "This is completed by https://github.com/ethereum-optimism/superchain-ops/pull/453", "2025-01-14T20:38:12Z", "2025-01-14T20:38:12Z", "mds1", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6U6c58", "I_kwDOKIsnqM6gTKml", "As part of superchain-ops improvements that are going to kick off soon, we are planning to support  \"template\" tasks. Opening a PR to execute a template task will use existing solidity code from the template that is parameterized via an input TOML file. This essentially is what you describe here, as no new solidity will be needed for these tasks :) ", "2024-11-25T15:35:39Z", "2024-11-25T15:35:39Z", "mds1", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6VBd_-", "I_kwDOKIsnqM6gTKml", "Sounds good, great to hear this is coming and already on your radar!", "2024-11-26T10:07:06Z", "2024-11-26T10:07:06Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDOKIsnqM6aQWb3", "I_kwDOKIsnqM6gTKml", "This has been added in https://github.com/ethereum-optimism/superchain-ops/pull/429", "2025-01-13T19:10:18Z", "2025-01-13T19:10:18Z", "sebastianst", "2025-08-31 00:16:02"]
["IC_kwDODjvEJM6aoiKz", "I_kwDODjvEJM6mRfmj", "please give more context if you are having an issue. make sure to deposit to the proxy and not the portal implementation", "2025-01-16T02:04:20Z", "2025-01-16T02:04:29Z", "tynes", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6aOlpJ", "I_kwDODjvEJM6lmEAS", "Smart-contract side is done.\nOpened an issue to track the op-supervisor side of this: https://github.com/ethereum-optimism/optimism/issues/13732\n", "2025-01-13T15:58:28Z", "2025-01-13T15:58:28Z", "protolambda", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6aQy5u", "I_kwDODjvEJM6je-6V", "closing since we agreed on the fixes scope and will work on getting the audit reports our next", "2025-01-13T20:09:51Z", "2025-01-13T20:09:51Z", "BlocksOnAChain", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6aaFjQ", "I_kwDODjvEJM6i60C7", "Edits made here https://www.notion.so/oplabs/RB-144-Alert-L2-Safe-Heads-Stalled-14efecdec7de44009aa141048a905770", "2025-01-14T16:49:27Z", "2025-01-14T16:49:27Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6XJUna", "I_kwDODjvEJM6i3TCA", "## `.env`\n\nPartial content:\n```\nNETWORK=mainnet\n\n# Address of deployed `PreimageOracle` contract.\nPREIMAGE_ORACLE_ADDR=0x9c065e11870B891D214Bc2Da7EF1f9DDFA1BE277\n\n# Address of deployed `AnchorStateRegistry` proxy contract.\nANCHOR_STATE_REGISTRY_PROXY_ADDR=0x18DAc71c228D1C32c99489B7323d441E1175e443\n\n# Address of the `SuperchainConfig` proxy contract.\nSUPERCHAIN_CONFIG_PROXY_ADDR=0x95703e0982140D16f8ebA6d158FccEde42f04a4C\n\n# Address of deployed `ProxyAdmin` contract.\nPROXY_ADMIN_ADDR=0x543bA4AADBAb8f9025686Bd03993043599c6fB04\n\n# Address of deployed `SystemConfig` proxy contract.\nSYSTEM_CONFIG_PROXY_ADDR=0x229047fed2591dbec1eF1118d64F7aF3dB9EB290\n\n# Address of deployed `DisputeGameFactory` proxy contract.\nDISPUTE_GAME_FACTORY_PROXY_ADDR=0xe5965Ab5962eDc7477C8520243A95517CD252fA9\n\nUSE_FAULT_PROOFS=true\nUSE_PERMISSIONLESS_FAULT_PROOFS=true\n\nOP_CONTRACTS_RELEASE=\"v1.8.0-rc.3\"\n\nSYSTEM_CONFIG_IMPL_ADDR=0x0000000000000000000000000000000000000000\nMIPS_IMPL_ADDR=0x0000000000000000000000000000000000000000\nDELAYED_WETH_IMPL_ADDR=0x71e966Ae981d1ce531a7b6d23DC0f27B38409087\n```\n\n\n## `deploy-config.json`\n\nUsing mainnet one with `\"faultGameAbsolutePrestate\": \"0x03f89406817db1ed7fd8b31e13300444652cdb0b9c509a674de43483b2f83568\"`\n\n## Deployment\n\n* Funded deployer account with [this tx](https://etherscan.io/tx/0xded72f97d361d68e22fee20efe288b45eec1e73a6ffae74d9cfb07ce6c614d26)\n* `deployments.json`\n```json\n{\n  \"SystemConfig\": \"0xAB9d6cB7A427c0765163A7f45BB91cAfe5f2D375\",\n  \"MIPS\": \"0x5fE03a12C1236F9C22Cb6479778DDAa4bce6299C\",\n  \"FaultDisputeGame\": \"0x27B81db41F586016694632193b99E45b1a27B8f8\",\n  \"PermissionedDisputeGame\": \"0x91a661891248d8C4916FB4a1508492a5e2CBcb87\"\n}\n```\n\n* Full [deploy.log](https://github.com/user-attachments/files/18095521/deploy.log)\n", "2024-12-11T12:02:25Z", "2024-12-11T12:36:02Z", "sebastianst", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WZ43J", "I_kwDODjvEJM6iHv1H", "Post-audit todo:\n- [ ] audit fixes\n- [ ] docs\n- [ ] standard config changes (to reflect whether L2 standard genesis has been shipped or not)", "2024-12-06T14:27:47Z", "2024-12-06T14:27:47Z", "benjaminion", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WaBOS", "I_kwDODjvEJM6iHv1H", "To expand a bit on the CGT ship options:\n\nThe findings from the audit referenced above can be found here (it's a private repo, so folks may not have access): https://github.com/MiloTruck/cgt-l2config-audit/issues. There are a few CGT findings that we will want to triage and potentially fix before shipping any CGT code to production. \n\nCGT code includes both L1 and L2 contract changes. We know OPCM is shipping first, which means we have a few options for shipping this feature:\n1. Ship L1 CGT code only: Fix the CGT L1 issues, L2 standard genesis ships later. This means we ship dead L1 CGT code and not the corresponding L2 CGT code.\n2. Ship the full CGT feature: This likely means delaying OPCM so that OPCM, CGT, and Standard L2 Genesis all ship at the same time. This means we ship the full CGT feature.\n\nFor either of those two cases:\n1. The items listed by @benjaminion above are all required\n2. The standard config changes would only allow ETH as the gas token\n3. We need a DRI for this feature to complete those tasks, ensure both of those options are safe, and figure out any details and logistics for shipping this\n\ncc @tynes for confirmation on everything here since you led the implementation of CGT ", "2024-12-06T14:42:38Z", "2024-12-06T14:42:38Z", "mds1", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6az8qK", "I_kwDODjvEJM6iHv1H", "Closing this", "2025-01-17T02:34:02Z", "2025-01-17T02:34:02Z", "tynes", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6V71PS", "I_kwDODjvEJM6h4DvF", "@vdamle it would also be great to run it with an unpatched op-reth and confirm that we see the chain split. ", "2024-12-03T20:29:49Z", "2024-12-03T20:29:49Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6V74Yo", "I_kwDODjvEJM6h4DvF", "> @vdamle it would also be great to run it with an unpatched op-reth and confirm that we see the chain split.\n\nMy bad, I missed noting that. Yes, I've built an image based off of `[v1.1.2](https://github.com/paradigmxyz/reth/releases/tag/v1.1.2) and will check that I can rep the original issue.", "2024-12-03T20:37:07Z", "2024-12-03T20:37:07Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6V75LP", "I_kwDODjvEJM6h4DvF", "Once we have something working e2e, it would be good to:\n- include nethermind in the tests\n- consider how to modularize the setup so that we can add more tests for future hardforks and run in CI", "2024-12-03T20:39:03Z", "2024-12-03T20:39:03Z", "tynes", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WD9O6", "I_kwDODjvEJM6h4DvF", "## Without transactions, with fix `6cea9955`\n\n* Reth logs show correct version and fork timestamp for holocene:\n```\n2024-12-04T13:20:24.678853Z  INFO Initialized tracing, debug log directory: /root/.cache/reth/logs/2151908\n2024-12-04T13:20:24.681567Z  INFO Starting reth version=\"1.1.2-dev (6cea9955)\"\n2024-12-04T13:20:24.681716Z  INFO Opening database path=\"/data/op-reth/execution-data/db\"\n2024-12-04T13:20:24.699872Z  INFO Saving prune config to toml file\n2024-12-04T13:20:24.700200Z  INFO Configuration loaded path=\"/data/op-reth/execution-data/reth.toml\"\n2024-12-04T13:20:24.700892Z  INFO Verifying storage consistency.\n2024-12-04T13:20:24.726454Z  INFO Database opened\n2024-12-04T13:20:24.726509Z  INFO Starting metrics endpoint at 0.0.0.0:9001\nPost-merge hard forks (timestamp based):\n- Regolith                         @0\n- Shanghai                         @0\n- Canyon                           @0\n- Cancun                           @0\n- Ecotone                          @0\n- Fjord                            @0\n- Granite                          @0\n- Holocene                         @1733318418\n2024-12-04T13:31:42.810281Z  INFO Status connected_peers=1 latest_block=0\n2024-12-04T13:31:43.542625Z  INFO Block added to canonical chain number=1 hash=0xbf5f9fed60870ebb22513168c357b706832f34e8bf753e0a3e16b351c75f4e60 peers=1 txs=1 gas=160.44 Kgas gas_throughput=3.57 Mgas/second full=0.3% base_fee=1.00gwei blobs=0 excess_blobs=0 elapsed=44.894175ms\n2024-12-04T13:31:43.556073Z  INFO Canonical chain committed number=1 hash=0xbf5f9fed60870ebb22513168c357b706832f34e8bf753e0a3e16b351c75f4e60 elapsed=105.664\u00b5s\n2024-12-04T13:31:43.559202Z  INFO Block added to canonical chain number=2 hash=0xc2fa2c807b02aff42303e31f4be20152a039afc92ac76a9dabcea482074434a0 peers=1 txs=1 gas=43.84 Kgas gas_throughput=21.61 Mgas/second full=0.1% base_fee=0.99gwei blobs=0 excess_blobs=0 elapsed=2.029127ms\n2024-12-04T13:31:43.559668Z  INFO Canonical chain committed number=2 hash=0xc2fa2c807b02aff42303e31f4be20152a039afc92ac76a9dabcea482074434a0 elapsed=75.04\u00b5s\n2024-12-04T13:31:43.561686Z  INFO Block added to canonical chain number=3 hash=0x9bde1550c17a776c20a824375a9fe0160a24ebcf9c33444d6a3d47dcb055ce11 peers=1 txs=1 gas=52.24 Kgas gas_throughput=37.00 Mgas/second full=0.1% base_fee=0.99gwei blobs=0 excess_blobs=0 elapsed=1.412056ms\n2024-12-04T13:31:43.562063Z  INFO Canonical chain committed number=3 hash=0x9bde1550c17a776c20a824375a9fe0160a24ebcf9c33444d6a3d47dcb055ce11 elapsed=13.333\u00b5s\n2024-12-04T13:31:43.563530Z  INFO Block added to canonical chain number=4 hash=0x18072d9f821355da8f12c64c6bd1aa35bce6f8c9c81c3de965fa92523321efd5 peers=1 txs=1 gas=43.85 Kgas gas_throughput=40.84 Mgas/second full=0.1% base_fee=0.98gwei blobs=0 excess_blobs=0 elapsed=1.073729ms\n2024-12-04T13:31:43.564060Z  INFO Canonical chain committed number=4 hash=0x18072d9f821355da8f12c64c6bd1aa35bce6f8c9c81c3de965fa92523321efd5 elapsed=16.792\u00b5s\n2024-12-04T13:31:43.565695Z  INFO Block added to canonical chain number=5 hash=0xff79963042d75f0a1c477af719a4ae1a14fa8a3f01c3f25a2e4b70c6ae332e8d peers=1 txs=1 gas=43.85 Kgas gas_throughput=41.70 Mgas/second full=0.1% base_fee=0.98gwei blobs=0 excess_blobs=0 elapsed=1.051772ms\n2024-12-04T13:31:43.566169Z  INFO Canonical chain committed number=5 hash=0xff79963042d75f0a1c477af719a4ae1a14fa8a3f01c3f25a2e4b70c6ae332e8d elapsed=14.291\u00b5s\n2024-12-04T13:31:43.567703Z  INFO Block added to canonical chain number=6 hash=0xa8b33501380d531e8b8b1cca0b082a9110a845258323f0d2aeca8441185ffdd0 peers=1 txs=1 gas=43.85 Kgas gas_throughput=44.54 Mgas/second full=0.1% base_fee=0.98gwei blobs=0 excess_blobs=0 elapsed=984.648\u00b5s\n```\n\n* block hashes are consistent pre-holocene, at holocene (block 4) and post holocene\n\n```\n\u279c  ~ ~/scripts/check-block-cast.sh 33023 33030\nChecking block hash for 0...\nGeth RPC 33023 - Block 0 Hash: 0x0f88d8da443ce62946b773d7ea66f8facca308420ccd8d3bfeca0bd040ea384a\nReth RPC 33030 - Block 0 Hash: 0x0f88d8da443ce62946b773d7ea66f8facca308420ccd8d3bfeca0bd040ea384a\n\nChecking block hash for 1...\nGeth RPC 33023 - Block 1 Hash: 0xbf5f9fed60870ebb22513168c357b706832f34e8bf753e0a3e16b351c75f4e60\nReth RPC 33030 - Block 1 Hash: 0xbf5f9fed60870ebb22513168c357b706832f34e8bf753e0a3e16b351c75f4e60\n\nChecking block hash for 3...\nGeth RPC 33023 - Block 3 Hash: 0x9bde1550c17a776c20a824375a9fe0160a24ebcf9c33444d6a3d47dcb055ce11\nReth RPC 33030 - Block 3 Hash: 0x9bde1550c17a776c20a824375a9fe0160a24ebcf9c33444d6a3d47dcb055ce11\n\nChecking block hash for 4...\nGeth RPC 33023 - Block 4 Hash: 0x18072d9f821355da8f12c64c6bd1aa35bce6f8c9c81c3de965fa92523321efd5\nReth RPC 33030 - Block 4 Hash: 0x18072d9f821355da8f12c64c6bd1aa35bce6f8c9c81c3de965fa92523321efd5\n\nChecking block hash for 5...\nGeth RPC 33023 - Block 5 Hash: 0xff79963042d75f0a1c477af719a4ae1a14fa8a3f01c3f25a2e4b70c6ae332e8d\nReth RPC 33030 - Block 5 Hash: 0xff79963042d75f0a1c477af719a4ae1a14fa8a3f01c3f25a2e4b70c6ae332e8d\n\nChecking block hash for 100...\nGeth RPC 33023 - Block 100 Hash: 0x04a0bd9df4d015c18e9b8d6be7582910846540bd430643b4432077b2e922b8e9\nReth RPC 33030 - Block 100 Hash: 0x04a0bd9df4d015c18e9b8d6be7582910846540bd430643b4432077b2e922b8e9\n```", "2024-12-04T14:35:59Z", "2024-12-04T14:35:59Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WFNv8", "I_kwDODjvEJM6h4DvF", "## Without transactions, on release `v1.1.2`\n\n**I did not observe a chain split with this test**\n\n* Reth logs show correct version and fork timestamp for holocene:\n\n```\n2024-12-04T16:02:00.600014Z  INFO Initialized tracing, debug log directory: /root/.cache/reth/logs/2151908\n2024-12-04T16:02:00.602263Z  INFO Starting reth version=\"1.1.2-dev (496bf0bf)\"\n2024-12-04T16:02:00.602371Z  INFO Opening database path=\"/data/op-reth/execution-data/db\"\n2024-12-04T16:02:00.629342Z  INFO Saving prune config to toml file\n2024-12-04T16:02:00.629536Z  INFO Configuration loaded path=\"/data/op-reth/execution-data/reth.toml\"\n2024-12-04T16:02:00.630123Z  INFO Verifying storage consistency.\n2024-12-04T16:02:00.655553Z  INFO Database opened\n....\nPost-merge hard forks (timestamp based):\n- Regolith                         @0\n- Shanghai                         @0\n- Canyon                           @0\n- Cancun                           @0\n- Ecotone                          @0\n- Fjord                            @0\n- Granite                          @0\n- Holocene                         @1733328114\n2024-12-04T16:02:00.725364Z  INFO Transaction pool initialized\n2024-12-04T16:02:00.739686Z  INFO P2P networking initialized enode=enode://addf62c286f8f3ed7a47cdaab6b2d56f4642ebd132e4faaf89f81906fb9de0c98f564abdd5bba4532ad1a734ebbbdda376398c0804b5cea8f4ae8d141940b98a@172.16.0.21:30303\n2024-12-04T16:02:00.740225Z  INFO StaticFileProducer initialized\n```\n\n* Block hashes are again consistent:\n\n```\n\u279c  ~ ~/scripts/check-block-cast.sh 33075 33082\nChecking block hash for 0...\nGeth RPC 33075 - Block 0 Hash: 0x910ad0af7965cd61b9cdaafab6014add621ee0961bb39c04cc818c7522b34975\nReth RPC 33082 - Block 0 Hash: 0x910ad0af7965cd61b9cdaafab6014add621ee0961bb39c04cc818c7522b34975\n\nChecking block hash for 1...\nGeth RPC 33075 - Block 1 Hash: 0x142956217ed6014cd45b828ffc4b0d541eb69ab525f79107a8b6c01856f7ce12\nReth RPC 33082 - Block 1 Hash: 0x142956217ed6014cd45b828ffc4b0d541eb69ab525f79107a8b6c01856f7ce12\n\nChecking block hash for 3...\nGeth RPC 33075 - Block 3 Hash: 0x8db0085449343985adad7b65f01ffb3c537106ea67210de5b5a2d84651377227\nReth RPC 33082 - Block 3 Hash: 0x8db0085449343985adad7b65f01ffb3c537106ea67210de5b5a2d84651377227\n\nChecking block hash for 4...\nGeth RPC 33075 - Block 4 Hash: 0x67c4118acedf7bb5178fa44f032baa060285c4c3175221dd500f152cd031482d\nReth RPC 33082 - Block 4 Hash: 0x67c4118acedf7bb5178fa44f032baa060285c4c3175221dd500f152cd031482d\n\nChecking block hash for 5...\nGeth RPC 33075 - Block 5 Hash: 0x04a6f3652be38d6d578b78f9c40dbee637af48225c38f403f1146950500e01a9\nReth RPC 33082 - Block 5 Hash: 0x04a6f3652be38d6d578b78f9c40dbee637af48225c38f403f1146950500e01a9\n\nChecking block hash for 100...\nGeth RPC 33075 - Block 100 Hash: 0x755ac773c06d2c5513e0010b5591871a6560cb911776239339ccb69b2adf36e0\nReth RPC 33082 - Block 100 Hash: 0x755ac773c06d2c5513e0010b5591871a6560cb911776239339ccb69b2adf36e0\n```\n\n", "2024-12-04T16:26:36Z", "2024-12-04T16:26:36Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WGNtl", "I_kwDODjvEJM6h4DvF", "@geoknee and I looked into this. As noted above, I attempted to trigger a chain split using an unpatched version of `op-reth`. However, it appears that the initial values of the EIP1559 params parsed from the extradata (i.e. those coming from the holocene route) are the same as the pre holocene chain spec values. I don't know if it is possible to change the values before the holocene activation in a way which won't break the chain (cause it to halt). Ordinarily we wouldn't set dynamically eip1559 params before holocene activates.\n\nhttps://github.com/ethereum-optimism/op-geth/blob/optimism/consensus/misc/eip1559/eip1559.go#L144\nhttps://github.com/ethereum-optimism/op-geth/blob/optimism/params/config.go#L897-L899\n\nI believe in the case of the Kurtosis setup, both `c.Optimism.EIP1559DenominatorCanyon` and `c.Optimism.EIP1559Denominator` are the same and we're not seeing any difference between the clients due to that?\n\nBased on the above determination, we believe the right next step is to concentrate on another bug https://github.com/paradigmxyz/reth/issues/13121 that Minyhuk found, which should allow us to trigger a chain split at any point in time. To do this, we just need to call `setEIP1559Params()` on the SystemConfigProxy L1 contract. This is permissioned and needs to be invoked by the SystemConfigOwner . We should use the values quoted in the above issue.", "2024-12-04T18:24:24Z", "2024-12-04T18:26:33Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WSaUz", "I_kwDODjvEJM6h4DvF", "Reproduced the bug reported in https://github.com/paradigmxyz/reth/issues/13121 using the image with the commit `6cea9955` with following procedure in Kurtosis:\n\n* In order to invoke `setEIP1559Params()` on the `SystemConfigProxy` contract, we need the address and private key of the [SystemConfigOwner](https://github.com/ethereum-optimism/optimism/blob/develop/op-chain-ops/devkeys/devkeys.go#L159-L160) role, to send the transaction.\n* The address can be found by inspecting the `state.json` file generated by the kurtosis enclave. The command to dump all the files generated by kurtosis and the path to the `state.json` file are as follows:\n```\nkurtosis enclave ls\nkurtosis enclave inspect <name>  -- this lists all running containers and the URLs/ports for RPC/P2P etc.\nkurtosis enclave dump <name>\ncd <dump_dir>\n\n\u279c  lively-rift--bf8dbc805ba84277b691c316fc8f5e55 git:(main) \u2717 cat files/op-deployer-configs/state.json| grep systemConfigProxyAddress\n      \"systemConfigProxyAddress\": \"0x42de298b17bbaa3b0f55abc1d3db7b0d0b0fd656\",\n\u279c  lively-rift--bf8dbc805ba84277b691c316fc8f5e55 git:(main) \u2717 cat files/op-deployer-configs/state.json| grep systemConfigOwner\n          \"systemConfigOwner\": \"0x8e44febeae9c2f2bc89480ae8084328badc496c2\",\n```\nAs seen above, `systemConfigOwner` is `\"0x8e44febeae9c2f2bc89480ae8084328badc496c2\"` and the `systemConfigProxy` is deployed at address `\"0x42de298b17bbaa3b0f55abc1d3db7b0d0b0fd656\"`.\n* The addresses for all roles are based off a a HD Wallet - the seed and the path can be found in `./files/op-deployer-fund-script/fund.sh`. The key for each role is at the following path `\"m/44'/60'/2'/$chain_id/$role_index`. We can generate the private key for the `systemConfigOwner` and verify that the address is the same as the one specified in `state.json`\n```\n\u279c  ~ cast wallet private-key 'test test test test test test test test test test test junk' 'm/44'\\''/60'\\''/2'\\''/2151908/10'\n0x28978385446dde6ce493da611d6c9ae6193d960c1cb73ecfb25800e9c9017f4c\n\u279c  ~ cast wallet address 0x28978385446dde6ce493da611d6c9ae6193d960c1cb73ecfb25800e9c9017f4c\n0x8e44fEbeaE9C2F2Bc89480aE8084328Badc496c2\n```\n* To be able to invoke the contract function from the `systemConfigOwner`, we need to fund it using the pre-funded wallet (grab the L1 RPC port from the L1 EL container listed in the output of `kurtosis enclave inspect`)\n\n```\n\u279c  ~ cast nonce 0x589A698b7b7dA0Bec545177D3963A2741105C7C9  --rpc-url 127.0.0.1:33197\n31\n\u279c  ~ cast send 0x8e44fEbeaE9C2F2Bc89480aE8084328Badc496c2 --value 10ether  --private-key eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23  --nonce 31 --rpc-url 127.0.0.1:33197 --async\n0x50f698e0274c2b6232260cae4f81e126548e989f95595bd4a36572aa4ad45774\n```\n* Finally, invoke the contract function `setEIP1559Params()` on the `SystemConfigProxy` contract, using the private key we derived above and send it to the `systemConfigProxyAddress`\n```\n\u279c  ~ cast send --private-key 0x28978385446dde6ce493da611d6c9ae6193d960c1cb73ecfb25800e9c9017f4c  0x42de298b17bbaa3b0f55abc1d3db7b0d0b0fd656  \"setEIP1559Params(uint32,uint32)\" \"0xfa\" \"0x3c\" --rpc-url 127.0.0.1:33197\n\nblockHash               0xba58836f1540521f0b403f4af6b764959a71417f5176d03991ddd391f1423234\nblockNumber             118\ncontractAddress\ncumulativeGasUsed       54016\neffectiveGasPrice       5000000241\nfrom                    0x8e44fEbeaE9C2F2Bc89480aE8084328Badc496c2\ngasUsed                 54016\nlogs                    [{\"address\":\"0x42de298b17bbaa3b0f55abc1d3db7b0d0b0fd656\",\"topics\":[\"0x1d2b0bda21d56b8bd12d4f94ebacffdfb35f5e226f84b461103bb8beab6353be\",\"0x0000000000000000000000000000000000000000000000000000000000000000\",\"0x0000000000000000000000000000000000000000000000000000000000000004\"],\"data\":\"0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000fa0000003c\",\"blockHash\":\"0xba58836f1540521f0b403f4af6b764959a71417f5176d03991ddd391f1423234\",\"blockNumber\":\"0x76\",\"transactionHash\":\"0x213035f8e5fc3a440543ada39c7447a98981d62fd544fca1c1dbec5298062819\",\"transactionIndex\":\"0x0\",\"logIndex\":\"0x0\",\"removed\":false}]\nlogsBloom               0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001008000000000000000000000000000000000000000000000000000000000000000000000000000000000008000000020000000000000000000800000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000008000000000000000000020000010000000000000000000000020000000000000000000000000008000000000\nroot\nstatus                  1 (success)\ntransactionHash         0x213035f8e5fc3a440543ada39c7447a98981d62fd544fca1c1dbec5298062819\ntransactionIndex        0\ntype                    2\nblobGasPrice\nblobGasUsed\nauthorizationList\nto                      0x42dE298b17bbAA3B0F55ABC1d3Db7B0d0B0FD656\n```\n* We can check that the values were set correctly\n```\n\u279c  ~ cast call 0x42de298b17bbaa3b0f55abc1d3db7b0d0b0fd656 \"eip1559Denominator()\" --rpc-url 127.0.0.1:33197\n0x00000000000000000000000000000000000000000000000000000000000000fa\n\u279c  ~ cast call 0x42de298b17bbaa3b0f55abc1d3db7b0d0b0fd656 \"eip1559Elasticity()\" --rpc-url 127.0.0.1:33197\n0x000000000000000000000000000000000000000000000000000000000000003c\n```\n\nWe can now see in the logs of the `op-node` paired with `reth` and the block is invalid:\n\n```\nt=2024-12-05T19:38:37+0000 lvl=info msg=\"Advancing bq origin\" origin=0x0a35703f0572c4f0d9b2e7791cb294e2e7fa055fa25c064bc0c1c2cd26f34520:121 originBehind=false\nt=2024-12-05T19:38:38+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 peer=16Uiu2HAm7mnrzyuGEsHr24w5XtH6L5eDSy3JkNnfMSjeEhym4zyv txs=1\nt=2024-12-05T19:38:38+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356\nt=2024-12-05T19:38:39+0000 lvl=info msg=\"failed to insert payload\" ref=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 txs=1 err=\"temp: cannot process unsafe payload: new - 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356; parent: 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f:355; err: execution payload 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 was INVALID! Latest valid hash is 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ignoring bad block: 0x4001ec3680\"\nt=2024-12-05T19:38:39+0000 lvl=warn msg=\"Dropping invalid unsafe payload\" hash=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83 number=356 timestamp=1733427519\nt=2024-12-05T19:38:39+0000 lvl=warn msg=\"Payload was invalid\" block=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 err=\"execution payload 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 was INVALID! Latest valid hash is 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ignoring bad block: 0x4001ec3680\" timestamp=1733427519\nt=2024-12-05T19:38:39+0000 lvl=warn msg=\"Engine temporary error\" err=\"temp: cannot process unsafe payload: new - 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356; parent: 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f:355; err: execution payload 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 was INVALID! Latest valid hash is 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ignoring bad block: 0x4001ec3680\"\nt=2024-12-05T19:38:40+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xa9672293397c93d9fd3f05c108b6392e6beba36e11ae42e6fa73c91fdf3c6ca1:357 peer=16Uiu2HAm7mnrzyuGEsHr24w5XtH6L5eDSy3JkNnfMSjeEhym4zyv txs=1\nt=2024-12-05T19:38:40+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xa9672293397c93d9fd3f05c108b6392e6beba36e11ae42e6fa73c91fdf3c6ca1:357\nt=2024-12-05T19:38:41+0000 lvl=info msg=\"processing L2 range request\" target=355 end=0xa9672293397c93d9fd3f05c108b6392e6beba36e11ae42e6fa73c91fdf3c6ca1:357 rangeReqId=2\nt=2024-12-05T19:38:41+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 peer=16Uiu2HAm7mnrzyuGEsHr24w5XtH6L5eDSy3JkNnfMSjeEhym4zyv txs=1\nt=2024-12-05T19:38:41+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356\nt=2024-12-05T19:38:41+0000 lvl=info msg=\"failed to insert payload\" ref=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 txs=1 err=\"temp: cannot process unsafe payload: new - 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356; parent: 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f:355; err: execution payload 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 was INVALID! Latest valid hash is 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ignoring bad block: 0x40007bfc80\"\nt=2024-12-05T19:38:41+0000 lvl=warn msg=\"Dropping invalid unsafe payload\" hash=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83 number=356 timestamp=1733427519\nt=2024-12-05T19:38:41+0000 lvl=warn msg=\"Payload was invalid\" block=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 err=\"execution payload 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 was INVALID! Latest valid hash is 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ignoring bad block: 0x40007bfc80\" timestamp=1733427519\nt=2024-12-05T19:38:41+0000 lvl=warn msg=\"Engine temporary error\" err=\"temp: cannot process unsafe payload: new - 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356; parent: 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f:355; err: execution payload 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 was INVALID! Latest valid hash is 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ignoring bad block: 0x40007bfc80\"\nt=2024-12-05T19:38:42+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x37605cd343efeef5c629d30f1feeeee6df92aebcdd91b55c09d0e721eb829805:358 peer=16Uiu2HAm7mnrzyuGEsHr24w5XtH6L5eDSy3JkNnfMSjeEhym4zyv txs=1\nt=2024-12-05T19:38:42+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x37605cd343efeef5c629d30f1feeeee6df92aebcdd91b55c09d0e721eb829805:358\nt=2024-12-05T19:38:44+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x9e11fec3aac55a8c4134681d3f9e5176ce137b09615e25612fcf7e9a8694eca4:359 peer=16Uiu2HAm7mnrzyuGEsHr24w5XtH6L5eDSy3JkNnfMSjeEhym4zyv txs=1\nt=2024-12-05T19:38:44+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x9e11fec3aac55a8c4134681d3f9e5176ce137b09615e25612fcf7e9a8694eca4:359\nt=2024-12-05T19:38:45+0000 lvl=info msg=\"processing L2 range request\" target=355 end=0xa9672293397c93d9fd3f05c108b6392e6beba36e11ae42e6fa73c91fdf3c6ca1:357 rangeReqId=3\nt=2024-12-05T19:38:45+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 peer=16Uiu2HAm7mnrzyuGEsHr24w5XtH6L5eDSy3JkNnfMSjeEhym4zyv txs=1\nt=2024-12-05T19:38:45+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356\nt=2024-12-05T19:38:45+0000 lvl=info msg=\"failed to insert payload\" ref=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 txs=1 err=\"temp: cannot process unsafe payload: new - 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356; parent: 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f:355; err: execution payload 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 was INVALID! Latest valid hash is 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ignoring bad block: 0x4001b94530\"\nt=2024-12-05T19:38:45+0000 lvl=warn msg=\"Dropping invalid unsafe payload\" hash=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83 number=356 timestamp=1733427519\nt=2024-12-05T19:38:45+0000 lvl=warn msg=\"Payload was invalid\" block=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 err=\"execution payload 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83:356 was INVALID! Latest valid hash is 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ignoring bad block: 0x4001b94530\" timestamp=1733427519\n```\n\nand in reth as well:\n\n```\n2024-12-05T19:38:37.002065Z  INFO Canonical chain committed number=355 hash=0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f elapsed=59.126\u00b5s\n2024-12-05T19:38:38.995501Z  WARN Failed to validate header 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83 against parent: block base fee mismatch: got 241700508, expected 241654910 block=SealedBlockWithSenders { block: SealedBlock { header: SealedHeader { hash: 0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83, header: Header { parent_hash: 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ommers_hash: 0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347, beneficiary: 0x4200000000000000000000000000000000000011, state_root: 0xb48b808b33e926ee3b17e8b4027fffcdcc2e7058b3b33164d406fa13e905859f, transactions_root: 0x37bb7714ba13b6d498d90f17b70d2c1c8053419d842d3b56bab36850c9b1a887, receipts_root: 0x9441f3e15878077b6969efc4830bbe393e600bba5cc8ddd9a6a006124f365986, logs_bloom: 0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, difficulty: 0, number: 356, gas_limit: 60000000, gas_used: 49394, timestamp: 1733427519, extra_data: 0x00000000fa0000003c, mix_hash: 0x6cde3099341ba98e0680256792b4534af72bf14e075bb1597b8b89fddd2edc98, nonce: 0x0000000000000000, base_fee_per_gas: Some(241700508), withdrawals_root: Some(0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421), blob_gas_used: Some(0), excess_blob_gas: Some(0), parent_beacon_block_root: Some(0x6f46d7cec19c69325a4185396847de932d3cd1e97443994f736f1bd8ea69e4f4), requests_hash: None, target_blobs_per_block: None } }, body: BlockBody { transactions: [TransactionSigned { hash: OnceLock(<uninit>), signature: PrimitiveSignature { y_parity: false, r: 0, s: 0 }, transaction: Deposit(TxDeposit { source_hash: 0xc63ba642e3f2a474aac34d81e6a7e2d9398caa53a8094eac7f56fa9a5b77420d, from: 0xdeaddeaddeaddeaddeaddeaddeaddeaddead0001, to: Call(0x4200000000000000000000000000000000000015), mint: None, value: 0, gas_limit: 1000000, is_system_transaction: false, input: 0x440a5e2000000558000c5fc500000000000000000000000067520117000000000000007700000000000000000000000000000000000000000000000000000000000000d40000000000000000000000000000000000000000000000000000000000000001344af750b8527102592d76a6b6215e1cb6ef8e738f635a9f009a30bc224e713b000000000000000000000000d3f2c5afb2d76f5579f326b0cd7da5f5a4126c35 }) }], ommers: [], withdrawals: Some(Withdrawals([])) } }, senders: [0xdeaddeaddeaddeaddeaddeaddeaddeaddead0001] }\n2024-12-05T19:38:39.013302Z  WARN Invalid block error on new payload invalid_hash=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83 invalid_number=356 validation_err=block base fee mismatch: got 241700508, expected 241654910\n2024-12-05T19:38:39.013352Z  WARN Bad block with hash hash=0x6c4a3fe708668af66b29a361ef3773d8f61a9d13a5044e37e474e31e69d0ce83 header=Header { parent_hash: 0x70b361d7f5c1adf2181bbac6aab041fdd4d9b93442c8f2eba99060451c183a9f, ommers_hash: 0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347, beneficiary: 0x4200000000000000000000000000000000000011, state_root: 0xb48b808b33e926ee3b17e8b4027fffcdcc2e7058b3b33164d406fa13e905859f, transactions_root: 0x37bb7714ba13b6d498d90f17b70d2c1c8053419d842d3b56bab36850c9b1a887, receipts_root: 0x9441f3e15878077b6969efc4830bbe393e600bba5cc8ddd9a6a006124f365986, logs_bloom: 0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, difficulty: 0, number: 356, gas_limit: 60000000, gas_used: 49394, timestamp: 1733427519, extra_data: 0x00000000fa0000003c, mix_hash: 0x6cde3099341ba98e0680256792b4534af72bf14e075bb1597b8b89fddd2edc98, nonce: 0x0000000000000000, base_fee_per_gas: Some(241700508), withdrawals_root: Some(0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421), blob_gas_used: Some(0), excess_blob_gas: Some(0), parent_beacon_block_root: Some(0x6f46d7cec19c69325a4185396847de932d3cd1e97443994f736f1bd8ea69e4f4), requests_hash: None, target_blobs_per_block: None }\n```\n", "2024-12-05T21:01:52Z", "2024-12-05T21:11:42Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WS0qG", "I_kwDODjvEJM6h4DvF", "Verified that the bug cannot be reproduced in `reth` with the latest tip of `main` that includes the [fix](https://github.com/paradigmxyz/reth/pull/13122):\n\n```\n2024-12-05T21:24:37.420462Z  INFO Initialized tracing, debug log directory: /root/.cache/reth/logs/2151908\n2024-12-05T21:24:37.422898Z  INFO Starting reth version=\"1.1.2-dev (8226fa0c)\"\n2024-12-05T21:24:37.423022Z  INFO Opening database path=\"/data/op-reth/execution-data/db\"\n2024-12-05T21:24:37.446958Z  INFO Saving prune config to toml file\n2024-12-05T21:24:37.447269Z  INFO Configuration loaded path=\"/data/op-reth/execution-data/reth.toml\"\n2024-12-05T21:24:37.468301Z  INFO Verifying storage consistency.\n2024-12-05T21:24:37.503755Z  INFO Database opened\n```\n\nWe can see that updated `extraData` is present in all blocks: `0x00000000fa0000003c` after invoking `setEIP1559Params()`\n```\nChecking block hash for latest...\nGeth RPC 33232 - Block latest Hash: \"0x784315014d85fe2f2b79ecbaa5b9b17c1fbb0886374e1c4ea70b181655c3073f 0x1f8 0x00000000fa0000003c\"\nReth RPC 33239 - Block latest Hash: \"0x784315014d85fe2f2b79ecbaa5b9b17c1fbb0886374e1c4ea70b181655c3073f 0x1f8 0x00000000fa0000003c\"\n```\n\n```\n\u279c  ~ ~/scripts/check-block-cast.sh 33232 33239\nChecking block hash for latest...\nGeth RPC 33232 - Block latest Hash: \"0x007996546fb6336f8aaa053ae7d0d0c350cd3169f2985ca7d850de7d262e88ee 0x253 0x00000000fa0000003c\"\nReth RPC 33239 - Block latest Hash: \"0x007996546fb6336f8aaa053ae7d0d0c350cd3169f2985ca7d850de7d262e88ee 0x253 0x00000000fa0000003c\"\n```", "2024-12-05T21:44:28Z", "2024-12-05T21:44:28Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6Wu5WU", "I_kwDODjvEJM6h4DvF", "@vdamle what's the best way to include Nethermind in your testing?\ncould you point to us to places where we should make PRs and follow the discussion on e2e testing?", "2024-12-09T17:44:43Z", "2024-12-09T17:44:43Z", "swapnilraj", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6Ww9xb", "I_kwDODjvEJM6h4DvF", "\n> [@vdamle](https://github.com/vdamle) what's the best way to include Nethermind in your testing? could you point to us to places where we should make PRs and follow the discussion on e2e testing?\n\n@swapnilraj If you build a Nethermind docker image, you could spin up a 2 node chain running `op-geth` and `nethermind` using [kurtosis](https://github.com/ethpandaops/optimism-package?tab=readme-ov-file#l2-customization-with-hard-fork-transitions). The instructions in that repo are fairly easy to follow along, but if you need any help/have questions with it, please do tag me on ~any of the protocol~ `#tests` discord channel and I'd be happy to take a look! Let me know how it goes for you.", "2024-12-09T20:50:01Z", "2024-12-09T20:53:18Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6W0Tbq", "I_kwDODjvEJM6h4DvF", "Earlier today, @geoknee outlined a couple more scenarios for testing with Kurtosis using `op-batcher/v1.10.0-rc.3` image for `op-batcher`. See [discord thread](https://discord.com/channels/1244729134312198194/1315746187810439168/1315793026362380401)\n\nEnvironment setup:\n\n* Built an image of the batcher in the monorepo at tag `op-batcher/v1.10.0-rc.3` using `make golang-docker` in the monorepo.\n* Tweaked the kurtosis package yaml to use the custom batcher image:\n\n```\noptimism_package:\n  chains:\n    - participants:\n      - el_type: op-geth\n        cl_type: op-node\n      - el_type: op-reth\n        el_image: \"op-reth:holocene-fix-2\"\n        cl_type: op-node\n      network_params:\n        fjord_time_offset: 0\n        granite_time_offset: 0\n        holocene_time_offset: 10\n      batcher_params:\n        image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-batcher:1.10.0\n```\n\n* Built a local `tx-fuzz` image using instructions in: https://github.com/MariusVanDerWijden/tx-fuzz\n* Created an enclave using the above parameters and started the fuzzer using L2's pre-funded account\n\n```\n./livefuzzer spam  --sk \"0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\" --rpc http://127.0.0.1:32987 --slot-time 2\n```\n\n## Batcher Restart Test after a Pause\n\n* Initially, verified that the batcher was operating normally using the command `kurtosis service logs <enclave-name> op-batcher-op-kurtosis -f`\n* Stopped the `op-batcher` service: `kurtosis service stop <enclave-name> op-batcher-op-kurtosis`\n* Started the `op-batcher` service after a few minutes using: `kurtosis service start <enclave-name> op-batcher-op-kurtosis`\n\n`op-batcher` appeared to recover correctly after a restart, the logs show that it detected a backlog of blocks and proceeded to work on the backlog successfully and moved forward.\n\n* Logs before restart:\n```\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Channel is fully submitted\" id=d3e2ec1ba14446bd18c81382f3c78916 min_inclusion_block=46 max_inclusion_block=46\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=warn msg=\"Aborting state publishing, max duration exceeded\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0x3fcc0474677c089832ae6e807ce7f8c4d0f40f37950db645b69795b37e1b4419 nonce=19 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=122 end=127\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Added L2 block to local state\" block=0x053caece73683cf83fbf7ade0b5354d3f24be6b85d5d0cf56e84b5a14ded3ab2:122 tx_count=1 time=1733803985\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0x3fcc0474677c089832ae6e807ce7f8c4d0f40f37950db645b69795b37e1b4419 nonce=19 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000 tx=0x3fcc0474677c089832ae6e807ce7f8c4d0f40f37950db645b69795b37e1b4419\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Added L2 block to local state\" block=0xa960e600ac05b668f2a60c9fc4d5ac7e101065ebc9a46f047835cd8f9791f1cc:123 tx_count=120 time=1733803987\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Added L2 block to local state\" block=0x08d7c297d0d2e80d5e62ba9d8f695b9f2abef2a3563191b7b30733fa4876bf8f:124 tx_count=1 time=1733803989\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Added L2 block to local state\" block=0x9a368b011c93ee6e0c1504ab0b0999fe7070dd3ee11fcb771bedd5e353b28231:125 tx_count=1 time=1733803991\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Added L2 block to local state\" block=0x0f9a289577c8db151987f580ebcdae07fcd945735cc3ee499b05a11fa9b0edcd:126 tx_count=101 time=1733803993\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Added L2 block to local state\" block=0xc89d851fc05639443b1176e6a284cbfe96c3994c4e9d929570a42a624aed3bbf:127 tx_count=54 time=1733803995\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Created channel\" id=dc7ccdf36a86596d43c3ab2d081ff286 l1Head=0x6abc3bbe2f5e470a6b757e411a5719f430750d02062deff2f68db46eed01aae9:47 blocks_pending=6 l1OriginLastSubmittedChannel=0x01eb53061c10beba342d10fbbe54be59dc622df3f1064bf1e46298fe3a3b9e81:38 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Channel closed\" id=dc7ccdf36a86596d43c3ab2d081ff286 blocks_pending=0 num_frames=1 input_bytes=51497 output_bytes=44547 oldest_l1_origin=0x633c890882c10459a32422076a9c03de20ccaf8ff50ac845f8b2639a008379e2:39 l1_origin=0x5a6647ea9825686bba4ab8f43ae15d766d0189bfc963ce6a99ebebcb9f175c37:40 oldest_l2=0x053caece73683cf83fbf7ade0b5354d3f24be6b85d5d0cf56e84b5a14ded3ab2:122 latest_l2=0xc89d851fc05639443b1176e6a284cbfe96c3994c4e9d929570a42a624aed3bbf:127 full_reason=\"channel full: max channel duration reached\" compr_ratio=0.8650406819814747\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:15+0000 lvl=info msg=\"Building Blob transaction candidate\" size=44547 last_size=44547 num_blobs=1\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Transaction confirmed\" service=batcher tx=0x3fcc0474677c089832ae6e807ce7f8c4d0f40f37950db645b69795b37e1b4419 block=0x0a3dd9307c66a90a3f5e3c6f7b4166b9ed4fb78898321b2d4124f01889af947b:48 effectiveGasPrice=5002708861\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Handling receipt\" id=0c40acc8f2e7bf74d2945f8d2f0266fe:0\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Transaction confirmed\" tx_id=0c40acc8f2e7bf74d2945f8d2f0266fe:0 tx=0x3fcc0474677c089832ae6e807ce7f8c4d0f40f37950db645b69795b37e1b4419 block=0x0a3dd9307c66a90a3f5e3c6f7b4166b9ed4fb78898321b2d4124f01889af947b:48\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Channel is fully submitted\" id=0c40acc8f2e7bf74d2945f8d2f0266fe min_inclusion_block=48 max_inclusion_block=48\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=warn msg=\"Aborting state publishing, max duration exceeded\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0x2b9b6d4b866b34b3ba12df1d5b872ceeb817dc826cfb05c5d106cab770e55d0c nonce=20 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=128 end=133\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Added L2 block to local state\" block=0x81974e2fc7f9f5c7e5b130ba301e2179561a65fda5996585a3e7a50e1fb848fd:128 tx_count=73 time=1733803997\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Added L2 block to local state\" block=0xc0e1614181ba48883fe512eec05ebabf6a1ccde09838f6c2021f921df8c4ee1b:129 tx_count=1 time=1733803999\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Added L2 block to local state\" block=0x0596d7bfd0b93331f3e258404c7d7783ff27ef8d20553ee88f9d8c5093974db9:130 tx_count=1 time=1733804001\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0x2b9b6d4b866b34b3ba12df1d5b872ceeb817dc826cfb05c5d106cab770e55d0c nonce=20 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000 tx=0x2b9b6d4b866b34b3ba12df1d5b872ceeb817dc826cfb05c5d106cab770e55d0c\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Added L2 block to local state\" block=0x28ebee23321a3c6be1790e968f52fcca10c0313b9bbbd2ed05774c60bca08d48:131 tx_count=101 time=1733804003\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Added L2 block to local state\" block=0x6c239c58aa9bec321d841998b64bb31621dbe60e8c400f4e59fd72acc1553dc6:132 tx_count=1 time=1733804005\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Added L2 block to local state\" block=0x1e1b3cfdecbbc0edd5cc01f167f04cb3b0a6406ae997bc03afbe0e96c0c3b96a:133 tx_count=149 time=1733804007\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Created channel\" id=69aa531e62ac39741f6dc1ff804533c5 l1Head=0xbfba7276d721512e2f2c72e98f9d578d4e8f85f41f077dd0bed09d087417adf2:49 blocks_pending=6 l1OriginLastSubmittedChannel=0x5a6647ea9825686bba4ab8f43ae15d766d0189bfc963ce6a99ebebcb9f175c37:40 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Channel closed\" id=69aa531e62ac39741f6dc1ff804533c5 blocks_pending=0 num_frames=1 input_bytes=61935 output_bytes=53225 oldest_l1_origin=0xd97d78da8e6a8fe47ec85451161d927a8390e16d00e08c7260755f5fd3323bbe:41 l1_origin=0x579274746ac1867b3183f7577bc5273a8609dc01a4dbf6dcc3a7a6f18e7f6195:42 oldest_l2=0x81974e2fc7f9f5c7e5b130ba301e2179561a65fda5996585a3e7a50e1fb848fd:128 latest_l2=0x1e1b3cfdecbbc0edd5cc01f167f04cb3b0a6406ae997bc03afbe0e96c0c3b96a:133 full_reason=\"channel full: max channel duration reached\" compr_ratio=0.8593686929845806\n[op-batcher-op-kurtosis] t=2024-12-10T04:13:27+0000 lvl=info msg=\"Building Blob transaction candidate\" size=53225 last_size=53225 num_blobs=1\n```\n\n* Logs after restart\n```\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"Starting JSON-RPC server\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"Starting batcher\" notSubmittingOnStart=false\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"Starting Batch Submitter\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"Clearing state\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"Clearing state with safe L1 origin\" origin=0x579274746ac1867b3183f7577bc5273a8609dc01a4dbf6dcc3a7a6f18e7f6195:42\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"State cleared\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"Batch Submitter started\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"Starting DA throttling loop\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:53+0000 lvl=info msg=\"Starting receipts processing loop\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"no blocks in state\" syncActions=\"SyncActions{blocksToPrune: 0, channelsToPrune: 0, clearState: <nil>, blocksToLoad: &{134 176}}\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=134 end=176\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x541694795137840056c5c69bbed07dd792f7c1b4197977494757092ba51d54b0:134 tx_count=1 time=1733804009\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x228935ede19c327090562f7f247e43dc18974f941d37076f37423682e04c6fb8:135 tx_count=1 time=1733804011\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x45e24542576fd0f7fee3496e214318b1f36a60a50b462206d89078a8bfcffe48:136 tx_count=101 time=1733804013\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x8d950652f29311bd3b8f573b9b97b0defa66c7c3e6f2f09f988813c193a80f03:137 tx_count=1 time=1733804015\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x57cfd702915abbcb099278862da1f46d5ac0ddcbc3e446f0a6783a9c1e32b5c3:138 tx_count=126 time=1733804017\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x9c878b03fd8f3bef495cffbe859e517bef89aa340efbf34481a2619d5ae28f8a:139 tx_count=1 time=1733804019\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x24bc491a7c9baa43ad0f06e944ccf9307737efb871d65f41f5f2919ee87165c8:140 tx_count=1 time=1733804021\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xe956a1d95ae4e03a1eb317836cc22fc01839ce7e9e3787779ee9fe2ae9b7bef0:141 tx_count=74 time=1733804023\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x4d9330c19755caf6d5dfaf87b8f2062e6bac2ac0b54bc76631c762dfd4766c09:142 tx_count=28 time=1733804025\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x6cb6384e29fd4e38a1104f781f7c56bb3efe2ba46768e437e82e8ae114ff4d5a:143 tx_count=1 time=1733804027\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xd134b77c891001595f96d8b08fa8dc7d3329a4e8591ba76d896197fe8f84ba45:144 tx_count=133 time=1733804029\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xb4f85165b296d4f666952b06079d68cfc69cd1d380c37800e97f79af9011016d:145 tx_count=1 time=1733804031\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x019abef09e83e187c98ae61f70da7bd16989162ccf4119bfef4ba0f2062af24e:146 tx_count=1 time=1733804033\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x484370f0afb23e664206d5114f5cc7fac996a4672b3c162113f307d6e7e9f1d7:147 tx_count=85 time=1733804035\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xa89c189c58093cbe932582484a3e70bf1185b6cffd92a44cde542a9b93788386:148 tx_count=17 time=1733804037\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x23373757c25c85cc2d525bfca41b05503b5d028c0fbfa377ac8ab997478da997:149 tx_count=1 time=1733804039\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x7915ce3a820ffa3c36d05c4a9403d943bc5374d5a7837fc7561f24712497e133:150 tx_count=137 time=1733804041\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x5b22c054a286e552399bcf8d4e2cc142dfa61332fbde7c1fb8f0e79c53afec1d:151 tx_count=45 time=1733804043\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x9a8cf0edc4ddbd1d20eb81158640a2073d05b2591960ff1a3f316f9bb15f3e78:152 tx_count=57 time=1733804045\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x19ca42b19bd11fa430913a1fa0e4290e0d8b47da27880e99f95ea4f6678d41c8:153 tx_count=1 time=1733804047\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x457a3f2d80f3c92bb651584c57130affb4056eda7ce6fc894ab21ec666e8181f:154 tx_count=117 time=1733804049\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xe485e674ed3e182a99c7e7cb9e6127866c92f0234d0c6f01e9b39e142de0c18f:155 tx_count=1 time=1733804051\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x0b13604ebcfa7042bfd9ca7c26fb5b61cd4fb7313958a580850141064134ef0c:156 tx_count=1 time=1733804053\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x2e3ef78b81a74748d51cf738a3a4b12da5af7f8be8a6b3cffcf04e21e23009b5:157 tx_count=67 time=1733804055\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x2bad9950d846d82cb4fdb136af1fccaba62912684609345f00c1f6cca4268319:158 tx_count=35 time=1733804057\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xfec05c5cf0b2240a68176f5069aa42aa667723b4d9fe576a1caf98759c0a411d:159 tx_count=1 time=1733804059\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xbf9ddb319b489a3076cd9194ca1468df28b0a8fc15c6bb4ee5b0644a9316e078:160 tx_count=133 time=1733804061\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x41ae1376a5ab8019a19d72ed8545b4e5c70ab0df091c4f576266ef0d310e7eb6:161 tx_count=1 time=1733804063\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x947560a47620b8a89a230aa48a60d210dd99b52dff66176c246ab549d5b51589:162 tx_count=1 time=1733804065\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xb754f993f72bf6b81336948e505ad09fa66e20464280bbce9a8d56ab8b89dacc:163 tx_count=84 time=1733804067\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x4dad6e79c76ca5f264b8610d95f6bbb8200a42f684931dbd777d4c14436024c1:164 tx_count=18 time=1733804069\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x72daac6a284dd78dbf80e535b675c9cf0f79ffe11fac00b53b4fbd9afba4a816:165 tx_count=1 time=1733804071\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xf9b8715601f6ca82a919141df8fde1be38c242b3310f7fb584e062537fdf94e7:166 tx_count=169 time=1733804073\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xd2c7281c5ea7ce2722326066b9fad6f693998334011f0dff1113448a27f369dc:167 tx_count=1 time=1733804075\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x2ee0cfad5318742c421ff44ae2968ced29140f904d2318e224b07e30c3c58fcb:168 tx_count=1 time=1733804077\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x006f6ca075545911711900965db532f721c2ac3be40a3d76aba87db177e345cb:169 tx_count=85 time=1733804079\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x2cdab790133ae51afd45cb1a2c06a28b1d5b24ef936f8d58960f95b82ad80dd2:170 tx_count=17 time=1733804081\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x11a027df3838ac210f0d81dc211e44d135221f9e9b8b2e981230875d07fe8f60:171 tx_count=1 time=1733804083\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x95ebc5d27af12de5afba3ca10dad55ca125827afd4b1f65d90b50663d6720c14:172 tx_count=160 time=1733804085\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x87ee79e6d3bc5099d7d5f7777468988be591f24f058f272c8cd19b37114f8e6f:173 tx_count=1 time=1733804087\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x658b4ef2784460679e544979a065121639c730ae84be32999b8bfdc01c73c688:174 tx_count=1 time=1733804089\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0xf872b15f8162d3e531c20e4cc2da1f9229262916725d6b59ddf4a7075c3f137a:175 tx_count=101 time=1733804091\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Added L2 block to local state\" block=0x457dcefb7d24ca18f1df282e1fe7254814a07750b91978fc5c242460105c3dbd:176 tx_count=1 time=1733804093\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Created channel\" id=421aa55ea54d4d41a23815bb1eae0be0 l1Head=0x2e5558686c7a5fd49d8e66498b3a1211387a59e1b2570c82b1dcb291f4a1f15b:63 blocks_pending=43 l1OriginLastSubmittedChannel=0x579274746ac1867b3183f7577bc5273a8609dc01a4dbf6dcc3a7a6f18e7f6195:42 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Channel closed\" id=421aa55ea54d4d41a23815bb1eae0be0 blocks_pending=23 num_frames=1 input_bytes=163589 output_bytes=117026 oldest_l1_origin=0x2bd648a16df07ee185ebe243c5cab69b64f975ef217a112cded8f06d71390ebc:43 l1_origin=0xbfba7276d721512e2f2c72e98f9d578d4e8f85f41f077dd0bed09d087417adf2:49 oldest_l2=0x541694795137840056c5c69bbed07dd792f7c1b4197977494757092ba51d54b0:134 latest_l2=0x19ca42b19bd11fa430913a1fa0e4290e0d8b47da27880e99f95ea4f6678d41c8:153 full_reason=\"channel full: compressor is full\" compr_ratio=0.715365947588163\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Building Blob transaction candidate\" size=117026 last_size=117026 num_blobs=1\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0x822bad9422895eb877cd2ed0884ab64f6350a3ca85daa00626961b99569d0196 nonce=22 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Created channel\" id=ae64fa41a1a30a0926697e345ef8064d l1Head=0x2e5558686c7a5fd49d8e66498b3a1211387a59e1b2570c82b1dcb291f4a1f15b:63 blocks_pending=23 l1OriginLastSubmittedChannel=0xbfba7276d721512e2f2c72e98f9d578d4e8f85f41f077dd0bed09d087417adf2:49 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Channel closed\" id=ae64fa41a1a30a0926697e345ef8064d blocks_pending=5 num_frames=1 input_bytes=167116 output_bytes=111926 oldest_l1_origin=0xbfba7276d721512e2f2c72e98f9d578d4e8f85f41f077dd0bed09d087417adf2:49 l1_origin=0xd4d85d240f230860490fdc632c837e24549b12e6df3ea6b7477633ce836ca614:55 oldest_l2=0x457a3f2d80f3c92bb651584c57130affb4056eda7ce6fc894ab21ec666e8181f:154 latest_l2=0x11a027df3838ac210f0d81dc211e44d135221f9e9b8b2e981230875d07fe8f60:171 full_reason=\"channel full: compressor is full\" compr_ratio=0.6697503530481821\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Building Blob transaction candidate\" size=111926 last_size=111926 num_blobs=1\n[op-batcher-op-kurtosis] t=2024-12-10T04:14:54+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0x822bad9422895eb877cd2ed0884ab64f6350a3ca85daa00626961b99569d0196 nonce=22 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000 tx=0x822bad9422895eb877cd2ed0884ab64f6350a3ca85daa00626961b99569d0196\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Transaction confirmed\" service=batcher tx=0x822bad9422895eb877cd2ed0884ab64f6350a3ca85daa00626961b99569d0196 block=0xbe824bdfcaa830a52989acb11fea1be034edbca0d6bb9d3e8f85ab015ae00285:65 effectiveGasPrice=5000280021\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Handling receipt\" id=421aa55ea54d4d41a23815bb1eae0be0:0\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Transaction confirmed\" tx_id=421aa55ea54d4d41a23815bb1eae0be0:0 tx=0x822bad9422895eb877cd2ed0884ab64f6350a3ca85daa00626961b99569d0196 block=0xbe824bdfcaa830a52989acb11fea1be034edbca0d6bb9d3e8f85ab015ae00285:65\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Channel is fully submitted\" id=421aa55ea54d4d41a23815bb1eae0be0 min_inclusion_block=65 max_inclusion_block=65\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=warn msg=\"Aborting state publishing, max duration exceeded\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0xdeccdb5d31fe3ef413d4d35ea98f88805282e650068baf41e7d33240bca11dbd nonce=23 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=177 end=182\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Added L2 block to local state\" block=0xa95d19e1f4592fd1ee563697f55a62aef5a690645930a2613ee426a2f81cad56:177 tx_count=138 time=1733804095\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Added L2 block to local state\" block=0x1562530eb5aa9a19e35f94a741858b3a69e35c1029756cb17085e4429552e98a:178 tx_count=1 time=1733804097\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0xdeccdb5d31fe3ef413d4d35ea98f88805282e650068baf41e7d33240bca11dbd nonce=23 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000 tx=0xdeccdb5d31fe3ef413d4d35ea98f88805282e650068baf41e7d33240bca11dbd\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Added L2 block to local state\" block=0xf6b15822aef1d2d3168cc86897663635c9982f70c5306eb4c8a4a04e21f4156b:179 tx_count=1 time=1733804099\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Added L2 block to local state\" block=0x89d93cebd72ef77fe65710b683454949dc613eaa19a2274b4321bf44351529a1:180 tx_count=66 time=1733804101\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Added L2 block to local state\" block=0x74fca39a416cf5da144ab87fb78c8e544e56603d1cb93b040d6df2a0d19b469c:181 tx_count=36 time=1733804103\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Added L2 block to local state\" block=0x3a83f62843e681aa53d468cfb0e03dc6d93d5c3426802c6f94df55a41694281c:182 tx_count=1 time=1733804105\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Created channel\" id=4e9e200e7830989e9099d9dde2f6fa8b l1Head=0xbe824bdfcaa830a52989acb11fea1be034edbca0d6bb9d3e8f85ab015ae00285:65 blocks_pending=11 l1OriginLastSubmittedChannel=0xd4d85d240f230860490fdc632c837e24549b12e6df3ea6b7477633ce836ca614:55 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Channel closed\" id=4e9e200e7830989e9099d9dde2f6fa8b blocks_pending=0 num_frames=1 input_bytes=91272 output_bytes=78106 oldest_l1_origin=0xd4d85d240f230860490fdc632c837e24549b12e6df3ea6b7477633ce836ca614:55 l1_origin=0x7a9c6859341c6c8bdc342de397e0c0090bc45e581ec13533bf0a9d2edff9001e:58 oldest_l2=0x95ebc5d27af12de5afba3ca10dad55ca125827afd4b1f65d90b50663d6720c14:172 latest_l2=0x3a83f62843e681aa53d468cfb0e03dc6d93d5c3426802c6f94df55a41694281c:182 full_reason=\"channel full: max channel duration reached\" compr_ratio=0.8557498466123236\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:06+0000 lvl=info msg=\"Building Blob transaction candidate\" size=78106 last_size=78106 num_blobs=1\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Transaction confirmed\" service=batcher tx=0xdeccdb5d31fe3ef413d4d35ea98f88805282e650068baf41e7d33240bca11dbd block=0x94f6b7019261f8027f9b66b0cb6bebcc51c6718f1453265ff3863404dbc96af5:67 effectiveGasPrice=5000214435\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Handling receipt\" id=ae64fa41a1a30a0926697e345ef8064d:0\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Transaction confirmed\" tx_id=ae64fa41a1a30a0926697e345ef8064d:0 tx=0xdeccdb5d31fe3ef413d4d35ea98f88805282e650068baf41e7d33240bca11dbd block=0x94f6b7019261f8027f9b66b0cb6bebcc51c6718f1453265ff3863404dbc96af5:67\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Channel is fully submitted\" id=ae64fa41a1a30a0926697e345ef8064d min_inclusion_block=67 max_inclusion_block=67\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=warn msg=\"Aborting state publishing, max duration exceeded\"\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0xf995f3aae185486239a868494eb0c1ec82f27ae3313dd9f6fe1f283498e93da1 nonce=24 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=183 end=188\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Added L2 block to local state\" block=0x8e1939559007212005049e7d18b8c3076da672d08fa33a1a8d0293a12bc26d25:183 tx_count=124 time=1733804107\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0xf995f3aae185486239a868494eb0c1ec82f27ae3313dd9f6fe1f283498e93da1 nonce=24 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000 tx=0xf995f3aae185486239a868494eb0c1ec82f27ae3313dd9f6fe1f283498e93da1\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Added L2 block to local state\" block=0x83d706a89345df71c97d64cc657f57675001a5ac109bde9ac555f957957d6136:184 tx_count=1 time=1733804109\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Added L2 block to local state\" block=0x43bcc57acb51de179f552f68428ec182d89f8126f8cce0740d39403c83e0d46c:185 tx_count=1 time=1733804111\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Added L2 block to local state\" block=0xf2066786d4181e24c4a430c07e4239f30d2ffd43ee37f2ead1ba13d1ddb31eb9:186 tx_count=84 time=1733804113\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Added L2 block to local state\" block=0x6b4891ac6924d64dff26d73e2b03d6909df90c70844db52d1315a092b332eca5:187 tx_count=18 time=1733804115\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Added L2 block to local state\" block=0x957a9ffaba75ad3c0fc7269f9e015148f22fdc3f49955c6126567e69b2e3f47b:188 tx_count=1 time=1733804117\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Created channel\" id=9ddc739916686f98849924ccb9fb1a79 l1Head=0x94f6b7019261f8027f9b66b0cb6bebcc51c6718f1453265ff3863404dbc96af5:67 blocks_pending=6 l1OriginLastSubmittedChannel=0x7a9c6859341c6c8bdc342de397e0c0090bc45e581ec13533bf0a9d2edff9001e:58 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T04:15:18+0000 lvl=info msg=\"Channel closed\" id=9ddc739916686f98849924ccb9fb1a79 blocks_pending=0 num_frames=1 input_bytes=39933 output_bytes=34326 oldest_l1_origin=0x28cb23a5a2d63b679d3a401539e4ed54049573789839fc185bfe4c0026a832b4:59 l1_origin=0x3dbcf3065b6db88aca8a837a4d899d101d5ed40140e6f9014286cfadfb57f16c:60 oldest_l2=0x8e1939559007212005049e7d18b8c3076da672d08fa33a1a8d0293a12bc26d25:183 latest_l2=0x957a9ffaba75ad3c0fc7269f9e015148f22fdc3f49955c6126567e69b2e3f47b:188 full_reason=\"channel full: max channel duration reached\" compr_ratio=0.8595898129366689\n```\n", "2024-12-10T04:28:13Z", "2024-12-10T04:28:13Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6W7X1M", "I_kwDODjvEJM6h4DvF", "## Batcher and Sequencer restart in quick succession\n\n`restart timestamp: 2024-12-10T15:39:37+0000`\n\n### Sequencer log snippets\n\n* Before restart, noting 2 L1 safe heads\n\n```\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:32:06+0000 lvl=info msg=\"Scheduled sequencer action\" delta=5.709564ms\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:32:07+0000 lvl=info msg=\"Started sequencing new block\" parent=0xd9ea715bb251957318fc97e883316c820a12de0072c02881bf09a42a4e621855:197 l1Origin=0xa1a4e158751f8287681fc9c46fbecd047d33e13eddb1fea0340bd665ebcf8bba:63\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:32:07+0000 lvl=info msg=\"Scheduled sequencer action\" delta=1.921258519s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:32:08+0000 lvl=info msg=\"New L1 finalized block\" l1_finalized=0xb08e6e54ea9e26da666c3e3431e9fc1364a54eac9c960fc7076cf35d20b5c4cd:48\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:32:08+0000 lvl=info msg=\"New L1 safe block\" l1_safe=0xa4d4888af92b797ca0dc6fbd71966bf3a4ec84c7d2c45c485d6c4dc819316f6f:56\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:32:08+0000 lvl=warn msg=\"L1 head signal indicates a possible L1 re-org\" old_l1_head=0xe290b765f81d7f3653ec3661ecefd19d46881a534222008d6c4903c55aaa63ff:68 new_l1_head_parent=0x2ba6328507389dfc95a655a7a5b23150880dc08234cda0cc13174dd31b2987e5 new_l1_head=0xd1b1afd7bfc621a61d210860f4c359be9b88b96ac25795aa57015ee0e1ad3125:70\n\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:38:30+0000 lvl=info msg=\"Scheduled sequencer action\" delta=32.653032ms\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:38:31+0000 lvl=info msg=\"Started sequencing new block\" parent=0x46fe991547861504a3dd7b4485327de429e3025c1802fa225bb0cf9611ab60a8:389 l1Origin=0x49a100d26be70d889202bc411d6fcadcfdc5f795b83da35a1f1e68f9ea8aa312:127\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:38:31+0000 lvl=info msg=\"Scheduled sequencer action\" delta=1.942107488s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:38:32+0000 lvl=info msg=\"New L1 safe block\" l1_safe=0xfdbfea5f647c5993a87749ff569b337465f46ca19519b517758c82e9cf8badda:120\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:38:32+0000 lvl=info msg=\"New L1 finalized block\" l1_finalized=0x7baf0c34ab82b424222494244dd25558a0ed9333026bd34702cce7d07e7b4ce0:112\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:38:32+0000 lvl=warn msg=\"L1 head signal indicates a possible L1 re-org\" old_l1_head=0xac85f67cb0104068a0adba5d38b8a2dd8203b58ed0a16e0509d68abc11251364:132 new_l1_head_parent=0x1e6b44f4e7659588515e5bd2430d23b8b5ef13d592b3085c1443bba55bc1f4a5 new_l1_head=0x0d6d8c8c0572ecc47644215b2843f59b7895d9b9fbde462fd3425ed47ba55252:134\n```\n\n* After restart\n\n```\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:25+0000 lvl=info msg=\"Scheduled sequencer action\" delta=-83.158\u00b5s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:25+0000 lvl=warn msg=\"Deriver system is resetting\" err=\"reset: cannot continue derivation until Engine has been reset\"\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:25+0000 lvl=error msg=\"Sequencer encountered reset signal, aborting work\" err=\"reset: cannot continue derivation until Engine has been reset\"\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:25+0000 lvl=info msg=\"Loaded current L2 heads\" unsafe=0x17bdbe63d705fc35bfdc1fa0b265a0ccf02d1c5e1a5922bda80fefbb899d60a5:454 safe=0xebaa445bf421001d1af6a3633461622d1f3b2e065717c2e7226a9eaab99cff55:422 finalized=0x86e558d5bc796a6594b11633cb1dc473020b77626229e8b690acf6c06462153c:313 unsafe_origin=0xe9cb66d85355023e73bb5e4d0b5744e1cdec0ffa45b62be216f043196a5b2a22:149 safe_origin=0x600e1f6610b6aa8bfd697959ab267e8966741f8c69b346506c6a06bb7ff23f66:139\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:25+0000 lvl=info msg=\"Walking back L1Block by number\" curr=0xe9cb66d85355023e73bb5e4d0b5744e1cdec0ffa45b62be216f043196a5b2a22:149 next=0xe9cb66d85355023e73bb5e4d0b5744e1cdec0ffa45b62be216f043196a5b2a22:149 l2block=0x17bdbe63d705fc35bfdc1fa0b265a0ccf02d1c5e1a5922bda80fefbb899d60a5:454\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:25+0000 lvl=info msg=\"Walking back L1Block by hash\" curr=0xe9cb66d85355023e73bb5e4d0b5744e1cdec0ffa45b62be216f043196a5b2a22:149 next=0xaa577de407eeff8c6ffaf0ec74bd51f9b6c6e97811a090699b6fa6abcc8d7f43:148 l2block=0xe672320693224215acdb583f479dbfda26d1944da7bb1bd4a0769f60027c9e1c:451\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:25+0000 lvl=info msg=\"Walking back L1Block by hash\" curr=0xaa577de407eeff8c6ffaf0ec74bd51f9b6c6e97811a090699b6fa6abcc8d7f43:148 next=0xc5824ec99c50aaba39de6bbe5d3be837bed44c5341ef668d2065610520c090bc:147 l2block=0x97b94da16c7c799f07ff0175cb5a2cbbec928dc56834e30de070475334ac8948:450\n...\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Walking back L1Block by hash\" curr=0x081da71dda149287ba3372388b03d117d1efded4b6917dc4316abf11fef5607e:104 next=0x8a50f131a33077abaf04ff75915f9563f88230c26eb9410d21f9dbe021621f6c:103 l2block=0xe6595d92caf635c4032fd91d6a5871bcd7c00889e1f1de0e9d8c7944f043d277:318\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Walking back L1Block by hash\" curr=0x8a50f131a33077abaf04ff75915f9563f88230c26eb9410d21f9dbe021621f6c:103 next=0xde45f221d4c8ff7d331e22edad20efce98671ab3440e5bfed20c289037dd76ec:102 l2block=0x86e558d5bc796a6594b11633cb1dc473020b77626229e8b690acf6c06462153c:313\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Hit finalized L2 head, returning immediately\" unsafe=0x17bdbe63d705fc35bfdc1fa0b265a0ccf02d1c5e1a5922bda80fefbb899d60a5:454 safe=0xebaa445bf421001d1af6a3633461622d1f3b2e065717c2e7226a9eaab99cff55:422 finalized=0x86e558d5bc796a6594b11633cb1dc473020b77626229e8b690acf6c06462153c:313 unsafe_origin=0xe9cb66d85355023e73bb5e4d0b5744e1cdec0ffa45b62be216f043196a5b2a22:149 safe_origin=0x600e1f6610b6aa8bfd697959ab267e8966741f8c69b346506c6a06bb7ff23f66:139\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Current hardfork version detected\" forkName=holocene\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Engine reset confirmed, sequencer may continue\" next=true\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Scheduled sequencer action\" delta=-9.457\u00b5s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Started sequencing new block\" parent=0x17bdbe63d705fc35bfdc1fa0b265a0ccf02d1c5e1a5922bda80fefbb899d60a5:454 l1Origin=0xb1e4917e9a9ea1070f7249d205ef1bcc71fedb7f5329ae83fc73ea8e56a5bf70:150\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Scheduled sequencer action\" delta=-3\u00b5s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Sequencer sealed block\" payloadID=0x03d7d6ead86770a7 block=0xb0f96f12728acccc3d08aae0f80c6874b86555921e11923069a9be13382a2ce2:455 parent=0x17bdbe63d705fc35bfdc1fa0b265a0ccf02d1c5e1a5922bda80fefbb899d60a5:454 txs=28 time=1733845243\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Publishing signed execution payload on p2p\" id=0xb0f96f12728acccc3d08aae0f80c6874b86555921e11923069a9be13382a2ce2:455\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Sequencer inserted block\" block=0xb0f96f12728acccc3d08aae0f80c6874b86555921e11923069a9be13382a2ce2:455 parent=0x17bdbe63d705fc35bfdc1fa0b265a0ccf02d1c5e1a5922bda80fefbb899d60a5:454\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Inserted new L2 unsafe block\" hash=0xb0f96f12728acccc3d08aae0f80c6874b86555921e11923069a9be13382a2ce2 number=455 state_root=0x1cde93edb41180e701a9191ac769eb885651d5fb0d37f8c68a53c836f83048bf timestamp=1733845243 parent=0x17bdbe63d705fc35bfdc1fa0b265a0ccf02d1c5e1a5922bda80fefbb899d60a5 prev_randao=0x9ed1df7e0d8ba25149303f56d78f0cba9038206aaf026293d6c6fabea3b1ee8c fee_recipient=0x4200000000000000000000000000000000000011 txs=28 build_time=14.482ms insert_time=24.367ms total_time=38.849ms mgas=0.616406 mgasps=15.866373692449402\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Scheduled sequencer action\" delta=-20.581\u00b5s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Started sequencing new block\" parent=0xb0f96f12728acccc3d08aae0f80c6874b86555921e11923069a9be13382a2ce2:455 l1Origin=0x4f76fc49e0ec77ac3b4f7dda00e302d10d0e7b480471ffd56081a688b69bde93:151\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Scheduled sequencer action\" delta=-1.375\u00b5s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Sequencer sealed block\" payloadID=0x03b64dcb8d9f7fb0 block=0x75513b2b542dc0117b4b0f741f150d3da27f61b060a77f95c1de7b7c5ea0784d:456 parent=0xb0f96f12728acccc3d08aae0f80c6874b86555921e11923069a9be13382a2ce2:455 txs=1 time=1733845245\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Publishing signed execution payload on p2p\" id=0x75513b2b542dc0117b4b0f741f150d3da27f61b060a77f95c1de7b7c5ea0784d:456\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Sequencer inserted block\" block=0x75513b2b542dc0117b4b0f741f150d3da27f61b060a77f95c1de7b7c5ea0784d:456 parent=0xb0f96f12728acccc3d08aae0f80c6874b86555921e11923069a9be13382a2ce2:455\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Inserted new L2 unsafe block\" hash=0x75513b2b542dc0117b4b0f741f150d3da27f61b060a77f95c1de7b7c5ea0784d number=456 state_root=0x6c380f1024385731f1602cfa515b0d28220ebb543dde8b6ee9b2f62098cbe825 timestamp=1733845245 parent=0xb0f96f12728acccc3d08aae0f80c6874b86555921e11923069a9be13382a2ce2 prev_randao=0x6569742e45d2a0f785def0221741bf8f5b3c440f375fec5b856e062c8b5eb223 fee_recipient=0x4200000000000000000000000000000000000011 txs=1 build_time=3.300ms insert_time=9.887ms total_time=13.188ms mgas=0.046594 mgasps=3.532899625434087\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Scheduled sequencer action\" delta=-14.29\u00b5s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Started sequencing new block\" parent=0x75513b2b542dc0117b4b0f741f150d3da27f61b060a77f95c1de7b7c5ea0784d:456 l1Origin=0xd4d6671efa7b30281b8554eb33aeeb592cac7e76d5f16762091a9efc759a996c:152\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Scheduled sequencer action\" delta=-1.5\u00b5s\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Sequencer sealed block\" payloadID=0x03acf080cd5b3a3a block=0x0dc0df7cbb24987d9e9dcaa56651c9f611236de7c062a60fba5ce1bcab0ee247:457 parent=0x75513b2b542dc0117b4b0f741f150d3da27f61b060a77f95c1de7b7c5ea0784d:456 txs=1 time=1733845247\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Publishing signed execution payload on p2p\" id=0x0dc0df7cbb24987d9e9dcaa56651c9f611236de7c062a60fba5ce1bcab0ee247:457\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Sequencer inserted block\" block=0x0dc0df7cbb24987d9e9dcaa56651c9f611236de7c062a60fba5ce1bcab0ee247:457 parent=0x75513b2b542dc0117b4b0f741f150d3da27f61b060a77f95c1de7b7c5ea0784d:456\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Inserted new L2 unsafe block\" hash=0x0dc0df7cbb24987d9e9dcaa56651c9f611236de7c062a60fba5ce1bcab0ee247 number=457 state_root=0x03443b12ba1d159f5e36e5c91309eac1bdbe88502a0aaa448615751827631771 timestamp=1733845247 parent=0x75513b2b542dc0117b4b0f741f150d3da27f61b060a77f95c1de7b7c5ea0784d prev_randao=0x3efdc18aa2c5478162f3db79e7de2a023456c23165b182ae1909bad505d7a038 fee_recipient=0x4200000000000000000000000000000000000011 txs=1 build_time=3.445ms insert_time=9.696ms total_time=13.142ms mgas=0.046606 mgasps=3.5463227085182356\n[op-cl-1-op-node-op-geth-op-kurtosis] t=2024-12-10T15:41:26+0000 lvl=info msg=\"Scheduled sequencer action\" delta=-14.082\u00b5s\n```\n\n### Batcher code snippets\n\n* After restart\n\n```\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:27+0000 lvl=info msg=\"Starting Batch Submitter\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:27+0000 lvl=info msg=\"Clearing state\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:27+0000 lvl=info msg=\"Clearing state with safe L1 origin\" origin=0xde45f221d4c8ff7d331e22edad20efce98671ab3440e5bfed20c289037dd76ec:102\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:27+0000 lvl=info msg=\"State cleared\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:27+0000 lvl=info msg=\"Batch Submitter started\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:27+0000 lvl=info msg=\"Starting DA throttling loop\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:27+0000 lvl=info msg=\"Starting receipts processing loop\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:28+0000 lvl=info msg=\"Received empty sync status, backing off\" backoff=1s\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:29+0000 lvl=info msg=\"Received empty sync status, backing off\" backoff=2s\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:31+0000 lvl=info msg=\"Received empty sync status, backing off\" backoff=4s\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:35+0000 lvl=info msg=\"Received empty sync status, backing off\" backoff=8s\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"no blocks in state\" syncActions=\"SyncActions{blocksToPrune: 0, channelsToPrune: 0, clearState: <nil>, blocksToLoad: &{441 485}}\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=441 end=485\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0x46680c7df828b5cbcabb0d60c9e0cc4284cec8b81c11481b1c71e6bd2e527286:441 tx_count=1 time=1733845215\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0x23550cae19a6ba939f188b37271f63f6b8056f8d4f9cd4295491344c100e56c2:442 tx_count=144 time=1733845217\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0xfab0abf2898f922550307b5fe4de400a90d9c6458dd32b24f692153aebb7f886:443 tx_count=1 time=1733845219\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0x57aecf6cde3ac8ead9539d1f79fdc17be59a33a0bef2179eeb9841133ce18338:444 tx_count=1 time=1733845221\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" ...[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0x097ccaa2440c11f7f2332feeda17a5bde10642340bcf9ec02c54f7f74b8532cc:481 tx_count=1 time=1733845295\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0x599224bd38d2ddc1d8948c50df07321326979b579d81188f21e5c48ae77a72bc:482 tx_count=101 time=1733845297\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0x70ccad142bff6a4e772b5ecff729feb2105de6500b7d6b692f82e3739e0f9be7:483 tx_count=1 time=1733845299\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0x0d2516391c6ee0a273787825d57d7ce75046a7302bcc24f07f29a37b6c6eb7d6:484 tx_count=147 time=1733845301\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Added L2 block to local state\" block=0x5a7cbac3d678ea3aa29f5489824a293425697a74d4656888fbd73d564f2b090f:485 tx_count=35 time=1733845303\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Created channel\" id=3a3da8b0f2e1cdc0edef09998d0ed6b4 l1Head=0xee0f2633fd080fdb70b506a509f8ff38f433e2d0f6416ccbbaab78c26756b44b:166 blocks_pending=45 l1OriginLastSubmittedChannel=0xde45f221d4c8ff7d331e22edad20efce98671ab3440e5bfed20c289037dd76ec:102 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Channel closed\" id=3a3da8b0f2e1cdc0edef09998d0ed6b4 blocks_pending=7 num_frames=1 input_bytes=166985 output_bytes=113484 oldest_l1_origin=0xa8083d631269b98a50b5cf355bf148028c4c30772c7e192ed7cfea9695f87338:145 l1_origin=0x40da472d26dd0c63ed220b4285e5c75d330e010c5e993ade1fb6d1de033222e8:163 oldest_l2=0x46680c7df828b5cbcabb0d60c9e0cc4284cec8b81c11481b1c71e6bd2e527286:441 latest_l2=0xcddc7b6cfb4497459d3e81a393ea7ae8fddf89139c2272f0cab6722610341148:478 full_reason=\"channel full: compressor is full\" compr_ratio=0.6796059526304757\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Building Blob transaction candidate\" size=113484 last_size=113484 num_blobs=1\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0x1ef11772e73a54845daf77c7175bd3c59543f7ea10ab3d98c33f3ca506da5642 nonce=73 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Created channel\" id=b5f0e8303bd79c92352199e2f9ee6017 l1Head=0xee0f2633fd080fdb70b506a509f8ff38f433e2d0f6416ccbbaab78c26756b44b:166 blocks_pending=7 l1OriginLastSubmittedChannel=0x40da472d26dd0c63ed220b4285e5c75d330e010c5e993ade1fb6d1de033222e8:163 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Channel closed\" id=b5f0e8303bd79c92352199e2f9ee6017 blocks_pending=0 num_frames=1 input_bytes=87250 output_bytes=74993 oldest_l1_origin=0x750c62a61ff29b5b00133c8a79d139a3aaba49cacfb6a8f429a4c15e7746b8e1:164 l1_origin=0x670e71abf44c07c27de2cb185950124e77e2d920eedf57f4d4fcd85fe1fb8f59:165 oldest_l2=0x2056049b758a42983b86654affd63a741927dd25ec940f4d62065b6be641d5dc:479 latest_l2=0x5a7cbac3d678ea3aa29f5489824a293425697a74d4656888fbd73d564f2b090f:485 full_reason=\"channel full: max channel duration reached\" compr_ratio=0.8595186246418338\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Building Blob transaction candidate\" size=74993 last_size=74993 num_blobs=1\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:43+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0x1ef11772e73a54845daf77c7175bd3c59543f7ea10ab3d98c33f3ca506da5642 nonce=73 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000 tx=0x1ef11772e73a54845daf77c7175bd3c59543f7ea10ab3d98c33f3ca506da5642\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Transaction confirmed\" service=batcher tx=0x1ef11772e73a54845daf77c7175bd3c59543f7ea10ab3d98c33f3ca506da5642 block=0x700d5642288a47b5295794c22441b13abc34adab54f5bc8d5447c6f8ed13a772:168 effectiveGasPrice=5000000007\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Handling receipt\" id=3a3da8b0f2e1cdc0edef09998d0ed6b4:0\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Transaction confirmed\" tx_id=3a3da8b0f2e1cdc0edef09998d0ed6b4:0 tx=0x1ef11772e73a54845daf77c7175bd3c59543f7ea10ab3d98c33f3ca506da5642 block=0x700d5642288a47b5295794c22441b13abc34adab54f5bc8d5447c6f8ed13a772:168\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Channel is fully submitted\" id=3a3da8b0f2e1cdc0edef09998d0ed6b4 min_inclusion_block=168 max_inclusion_block=168\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=warn msg=\"Aborting state publishing, max duration exceeded\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0x538dec595a9362e7a915a43624836f464dac043c63e14d05d09816b2d50e880d nonce=74 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=486 end=491\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Added L2 block to local state\" block=0xec2e5dbbc4a8ca28bf9cbf0e4c2dcb94b813a015b6dc4ebbf7d2ba18f6778aaa:486 tx_count=1 time=1733845305\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Added L2 block to local state\" block=0x352660e8fd4c5cf6631cb83ea2935c878503a4fd8787f1fd580295ac7e021717:487 tx_count=101 time=1733845307\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Added L2 block to local state\" block=0x4d67fd24f169bb1173d73d72238d4a9281e978bce34957ff6883a2a1abda154b:488 tx_count=1 time=1733845309\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0x538dec595a9362e7a915a43624836f464dac043c63e14d05d09816b2d50e880d nonce=74 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000 tx=0x538dec595a9362e7a915a43624836f464dac043c63e14d05d09816b2d50e880d\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Added L2 block to local state\" block=0x95f4ed24516747e548ce9d6e3f2bea4c7eb43d6180a7063f7d5c20de5c46accb:489 tx_count=134 time=1733845311\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Added L2 block to local state\" block=0xd7776b4d28902cfd5182165dbe5eaeb9ed23c6d880567d44d0c2db15e2b85f76:490 tx_count=1 time=1733845313\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Added L2 block to local state\" block=0x3c4e1fc5d0c9f03b6d12600f64e7a8342c6665923278aa1a6eef7ca4a49449d8:491 tx_count=1 time=1733845315\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Created channel\" id=f8053f8dcbcb2833167beec96595ec21 l1Head=0x700d5642288a47b5295794c22441b13abc34adab54f5bc8d5447c6f8ed13a772:168 blocks_pending=6 l1OriginLastSubmittedChannel=0x670e71abf44c07c27de2cb185950124e77e2d920eedf57f4d4fcd85fe1fb8f59:165 batch_type=0 compression_algo=zlib target_num_frames=1 max_frame_size=130043 use_blobs=true\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Channel closed\" id=f8053f8dcbcb2833167beec96595ec21 blocks_pending=0 num_frames=1 input_bytes=42426 output_bytes=36300 oldest_l1_origin=0x670e71abf44c07c27de2cb185950124e77e2d920eedf57f4d4fcd85fe1fb8f59:165 l1_origin=0x670e71abf44c07c27de2cb185950124e77e2d920eedf57f4d4fcd85fe1fb8f59:165 oldest_l2=0xec2e5dbbc4a8ca28bf9cbf0e4c2dcb94b813a015b6dc4ebbf7d2ba18f6778aaa:486 latest_l2=0x3c4e1fc5d0c9f03b6d12600f64e7a8342c6665923278aa1a6eef7ca4a49449d8:491 full_reason=\"channel full: max channel duration reached\" compr_ratio=0.8556074105501343\n[op-batcher-op-kurtosis] t=2024-12-10T15:41:55+0000 lvl=info msg=\"Building Blob transaction candidate\" size=36300 last_size=36300 num_blobs=1\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Transaction confirmed\" service=batcher tx=0x538dec595a9362e7a915a43624836f464dac043c63e14d05d09816b2d50e880d block=0x3d77a1ffce4ccb1f751d32daf92611918f30dd627d1c0d6f184db0d20a20e864:170 effectiveGasPrice=5000000007\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Handling receipt\" id=b5f0e8303bd79c92352199e2f9ee6017:0\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Transaction confirmed\" tx_id=b5f0e8303bd79c92352199e2f9ee6017:0 tx=0x538dec595a9362e7a915a43624836f464dac043c63e14d05d09816b2d50e880d block=0x3d77a1ffce4ccb1f751d32daf92611918f30dd627d1c0d6f184db0d20a20e864:170\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Channel is fully submitted\" id=b5f0e8303bd79c92352199e2f9ee6017 min_inclusion_block=170 max_inclusion_block=170\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=warn msg=\"Aborting state publishing, max duration exceeded\"\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0x46aa881e01465ca70dd137440fb24f286e4ccee23e67e6cd98dcada0e7db8bc3 nonce=75 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=492 end=497\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Added L2 block to local state\" block=0xfaf7b00d20958105d265e8ece220e2c03bef2e876009254c9d28bb8bcfdb15d7:492 tx_count=70 time=1733845317\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Added L2 block to local state\" block=0x581eca0d5bc1c639cfc2015ae539bf8954e400fb796f04950e015396de36ff04:493 tx_count=32 time=1733845319\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Added L2 block to local state\" block=0xe0c4f46e6bd499c7dc053a38d2cc3e3915054d8c38142cd6b6b744dd5942e5d9:494 tx_count=1 time=1733845321\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0x46aa881e01465ca70dd137440fb24f286e4ccee23e67e6cd98dcada0e7db8bc3 nonce=75 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=21000 blobs=1 blobFeeCap=1000000000 tx=0x46aa881e01465ca70dd137440fb24f286e4ccee23e67e6cd98dcada0e7db8bc3\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Added L2 block to local state\" block=0x4fdabb0567e62a37e86b6f151e53c3665251a0b97ddde0ca6c2055680f229ba0:495 tx_count=137 time=1733845323\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Added L2 block to local state\" block=0x131b8ca84e015d700690026359aaed33fe79a53fcdd3c03d65abae9a798a06ea:496 tx_count=1 time=1733845325\n[op-batcher-op-kurtosis] t=2024-12-10T15:42:07+0000 lvl=info msg=\"Added L2 block to local state\" block=0x3c3d2a5a3a6a455294cf3e7aed476d049c57f0e0be653d19c3845b9e59b49619:497 tx_count=1 time=1733845327\n```\n\n[batcher-logs-before-after-restart.log](https://github.com/user-attachments/files/18082410/batcher-logs-before-after-restart.log)\n[sequencer-logs-before-after-restart.log](https://github.com/user-attachments/files/18082409/sequencer-logs-before-after-restart.log)\n", "2024-12-10T16:00:40Z", "2024-12-10T16:10:44Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6XMJHS", "I_kwDODjvEJM6h4DvF", "## Holocene Straddle Awareness in Batcher\n\nAs noted in: https://github.com/ethereum-optimism/optimism/issues/12122\n\nLogs from recreate in Kurtosis attached.\n\n* Holocene activation at `block 50`\n* Batcher parameters: `--max-channel-duration=60, --max-l1-tx-size-bytes=10000, --target-num-frames=5, --data-availability-type=calldata, --batch-type=0` (`calldata`)\n* Batcher image: `1.10.0-rc3`\n* Kurtosis package network params\n\n```\noptimism_package:\n  chains:\n    - participants:\n      - el_type: op-geth\n        cl_type: op-node\n      - el_type: op-reth\n        el_image: \"op-reth:v1.1.3\"\n        cl_type: op-node\n      network_params:\n        fjord_time_offset: 0\n        granite_time_offset: 0\n        holocene_time_offset: 100\n      batcher_params:\n        image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-batcher:1.10.0\n        extra_params: [--max-channel-duration=60, --max-l1-tx-size-bytes=10000, --target-num-frames=5, --data-availability-type=calldata, --batch-type=0]\n```\n\n```\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Handling receipt\" id=e092b07d4dfc60b20de2f0242f4db417:2\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Transaction confirmed\" tx_id=e092b07d4dfc60b20de2f0242f4db417:2 tx=0x6ce9e260d89ce1282611903391fedd827be0f0129fc7f044ca6d93fe7392ada6 block=0x79ae6587da1a49c449cabf9b9cca215f34421ce9166e71fb7e4f4e1a7ec9990f:30\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=warn msg=\"Aborting state publishing, max duration exceeded\"\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0x4e3e4e419cc598cd7041f5eac3e067f9c7de586fd4b062faa59515bca6460322 nonce=8 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=180340\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Transaction successfully published\" service=batcher tx=0x4e3e4e419cc598cd7041f5eac3e067f9c7de586fd4b062faa59515bca6460322 nonce=8 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=180340 tx=0x4e3e4e419cc598cd7041f5eac3e067f9c7de586fd4b062faa59515bca6460322\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=warn msg=\"sequencer did not make expected progress\" syncActions=\"SyncActions{blocksToPrune: 0, channelsToPrune: 0, clearState: 0x270033a1aa86fe162003e3e6c0b1b1fde7120264b39143ee3c7fd9b2de525747:7, blocksToLoad: &{1 74}}\" existingBlock=0x19309a04d5ff7916910a7d076efcb2d5627dfa2a131bdce826a8ef559ced7344:27 safeL2=0x51fe293ae5277fcbd1c051f4f20024d3b5775a88b29ebbbbe98bd46038bd065e:0\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=1 end=74\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Added L2 block to local state\" block=0x4691ed3762731478aad643e07569bf88aaefec5b8eab0b49ecc9c751b7e021f1:1 tx_count=1 time=1733931942\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Added L2 block to local state\" block=0x74e56b76c8b659560621dc83a9a861b0874fc553fca0bc19dbc27baf9584d790:2 tx_count=1 time=1733931944\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Added L2 block to local state\" block=0x991a85b84507efd845706f7ba80fb4b4eb475de6d89f9c04485efdcc0af8c13e:3 tx_count=1 time=1733931946\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Added L2 block to local state\" block=0xf58b4d098a32ae6ec230c7d8c27b076846e9c54ce4ecfea702f65e8268c1722b:4 tx_count=1 time=1733931948\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Added L2 block to local state\" block=0x39e48b87dcd0b316394c4341ad6f7f7a9a326c9d33835b5f7cbe4bf0cc3b776f:5 tx_count=1 time=1733931950\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Added L2 block to local state\" block=0x6b8718a31a3525c0b5e50f6371d6b53379399af93f792c3caeabd374556525c1:6 tx_count=1 time=1733931952\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Added L2 block to local state\" block=0x2cde4c708e4128ee236ab57a9c2bfadea29ecf739dac480131b40e5054b055a6:7 tx_count=1 time=1733931954\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:08+0000 lvl=info msg=\"Added L2 block to local state\" block=0xc60742dccf29c2460155c3fc2bf2d91419fa6e3212ca8c4974fb11bcce2e22a0:8 tx_count=1 time=1733931956\n...\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:09+0000 lvl=info msg=\"Added L2 block to local state\" block=0x95235be971e6c0cc3e050fc0f7888f55082300f7d7645e4a45f16ed236c4f5e1:72 tx_count=1 time=1733932084\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:09+0000 lvl=info msg=\"Added L2 block to local state\" block=0xe0c8786c4f2ac19ad463ca9a4f9614f7f0b03ce7949d42e4ebc7cf2788f10458:73 tx_count=125 time=1733932086\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:09+0000 lvl=info msg=\"Added L2 block to local state\" block=0xf48023c2b238769e5bd54734e86e298d983f738abcc38342a7bbfe19d68fe862:74 tx_count=1 time=1733932088\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:09+0000 lvl=info msg=\"Created channel\" id=1366952367c88c6be2477af019f2ecca l1Head=0x91b60111fda47a1b1c71f68db67492270d2fea2485c5fda1ba8fe95c2090194b:31 blocks_pending=74 l1OriginLastSubmittedChannel=0x270033a1aa86fe162003e3e6c0b1b1fde7120264b39143ee3c7fd9b2de525747:7 batch_type=0 compression_algo=zlib target_num_frames=5 max_frame_size=9999 use_blobs=false\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:09+0000 lvl=info msg=\"Channel closed\" id=1366952367c88c6be2477af019f2ecca blocks_pending=47 num_frames=5 input_bytes=59637 output_bytes=40714 oldest_l1_origin=0x270033a1aa86fe162003e3e6c0b1b1fde7120264b39143ee3c7fd9b2de525747:7 l1_origin=0xd3474989a3d3b6745133a51063067c706dc338ab29468b90806ff53603be8fc2:11 oldest_l2=0x4691ed3762731478aad643e07569bf88aaefec5b8eab0b49ecc9c751b7e021f1:1 latest_l2=0x19309a04d5ff7916910a7d076efcb2d5627dfa2a131bdce826a8ef559ced7344:27 full_reason=\"channel full: compressor is full\" compr_ratio=0.6826969834163354\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:09+0000 lvl=info msg=\"Building Calldata transaction candidate\" size=10000\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:20+0000 lvl=info msg=\"Transaction confirmed\" service=batcher tx=0x4e3e4e419cc598cd7041f5eac3e067f9c7de586fd4b062faa59515bca6460322 block=0xd295251398270515753cc200c8835a3934cbb327df6a5d05f4c680c64c5d90c1:33 effectiveGasPrice=5020247484\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:20+0000 lvl=info msg=\"Handling receipt\" id=e092b07d4dfc60b20de2f0242f4db417:3\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:20+0000 lvl=info msg=\"Transaction confirmed\" tx_id=e092b07d4dfc60b20de2f0242f4db417:3 tx=0x4e3e4e419cc598cd7041f5eac3e067f9c7de586fd4b062faa59515bca6460322 block=0xd295251398270515753cc200c8835a3934cbb327df6a5d05f4c680c64c5d90c1:33\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:20+0000 lvl=warn msg=\"transaction from unknown channel marked as confirmed\" id=e092b07d4dfc60b20de2f0242f4db417:3\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:20+0000 lvl=warn msg=\"Aborting state publishing, max duration exceeded\"\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:20+0000 lvl=info msg=\"Publishing transaction\" service=batcher tx=0x205392a32d59a23f8560d473fd3500d3448efe746c7bcea93e80adb7e67df00c nonce=9 gasTipCap=5000000000 gasFeeCap=7000000000 gasLimit=180616\n[op-batcher-op-kurtosis] t=2024-12-11T15:48:20+0000 lvl=info msg=\"Loading range of multiple blocks into state\" start=75 end=80\n```\n[batcher-holocene-activation-awareness.log](https://github.com/user-attachments/files/18099403/batcher-holocene-activation-awareness.log)\n[safe-head-progression-holocene-activation-awareness.log](https://github.com/user-attachments/files/18099404/safe-head-progression-holocene-activation-awareness.log)\n[sequencer-holocene-activation-awareness.log](https://github.com/user-attachments/files/18099402/sequencer-holocene-activation-awareness.log)\n", "2024-12-11T16:53:21Z", "2024-12-11T16:54:23Z", "vdamle", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6Yc42Q", "I_kwDODjvEJM6h4DvF", "On behalf of @swapnilraj, we updated Nethermind recently to include a fix for Holocene that we didn't catch until we started using Kurtosis with the Optimism Package. With this fix, it is possible to create a Docker image (we provide one in our main repo, https://github.com/NethermindEth/nethermind) and include it in the `network-params.yaml` file. As of today, we don't have a public image with the fix implemented but we expect to have one before end of year.\n\nIn our testing, all three clients (`geth`, `reth` and `nethermind`) behave as expected when enabling Holocene.", "2024-12-20T21:00:03Z", "2024-12-20T21:00:33Z", "emlautarom1", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6XKCn1", "I_kwDODjvEJM6f-c9o", "ok to close this one @sebastianst ?", "2024-12-11T13:27:39Z", "2024-12-11T13:27:39Z", "BlocksOnAChain", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6XMxDa", "I_kwDODjvEJM6f-c9o", "We need to do them again, so we should leave this open. I'm on it this week ", "2024-12-11T17:55:41Z", "2024-12-11T17:55:41Z", "sebastianst", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WD3iJ", "I_kwDODjvEJM6f6QcI", "Reopening to cover doing this for Base Sepolia. ", "2024-12-04T14:26:51Z", "2024-12-04T14:26:51Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6WFMwx", "I_kwDODjvEJM6f6QcI", "https://discord.com/channels/1244729134312198194/1309468035169386508/1313876885964324935 Instructions for preparing a superchain-ops task for Base.", "2024-12-04T16:26:08Z", "2024-12-04T16:26:08Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6XfK0q", "I_kwDODjvEJM6f6QcI", "See https://github.com/ethereum-optimism/superchain-ops/issues/394 we need to re-do this upgrade.", "2024-12-13T14:01:57Z", "2024-12-13T14:01:57Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6aQOSH", "I_kwDODjvEJM6f6QcI", "@geoknee just did it https://github.com/ethereum-optimism/superchain-ops/pull/463", "2025-01-13T18:53:10Z", "2025-01-13T18:53:10Z", "sebastianst", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6Un1I0", "I_kwDODjvEJM6ft2iH", "Op Sepolia tasks were created using the holocene upgrade script with the following env (partial):\n```\n##############################################\n#               \u2193  Required  \u2193               #\n##############################################\n\n# The network to deploy the contracts to.\n# Must be one of 'mainnet', 'sepolia'\nNETWORK=sepolia\n\n# Address of deployed `PreimageOracle` contract.\nPREIMAGE_ORACLE_ADDR=0x92240135b46fc1142dA181f550aE8f595B858854\n\n# Address of deployed `AnchorStateRegistry` proxy contract.\nANCHOR_STATE_REGISTRY_PROXY_ADDR=0x218CD9489199F321E1177b56385d333c5B598629\n\n# Address of the `SuperchainConfig` proxy contract.\nSUPERCHAIN_CONFIG_PROXY_ADDR=0xC2Be75506d5724086DEB7245bd260Cc9753911Be\n\n# Address of deployed `ProxyAdmin` contract.\nPROXY_ADMIN_ADDR=0x189aBAAaa82DfC015A588A7dbaD6F13b1D3485Bc\n\n# Address of deployed `SystemConfig` proxy contract.\nSYSTEM_CONFIG_PROXY_ADDR=0x034edD2A225f7f429A63E0f1D2084B9E0A93b538\n\n# Address of deployed `DisputeGameFactory` proxy contract.\nDISPUTE_GAME_FACTORY_PROXY_ADDR=0x05F9613aDB30026FFd634f38e5C4dFd30a197Fa1\n\nUSE_FAULT_PROOFS=true\n\nUSE_PERMISSIONLESS_FAULT_PROOFS=true\n\n###################################################\n#                 \u2193  Optional  \u2193                  #\n# Do not set if you don't know what you're doing. #\n###################################################\n\nSYSTEM_CONFIG_IMPL_ADDR=0x29d06Ed7105c7552EFD9f29f3e0d250e5df412CD\n\nMIPS_IMPL_ADDR=0x0000000000000000000000000000000000000000\n\nDELAYED_WETH_IMPL_ADDR=0x0000000000000000000000000000000000000000\n```\nand the `faultGameAbsolutePrestate: 0x03925193e3e89f87835bbdf3a813f60b2aa818a36bbe71cd5d8fd7e79f5e8afe` set in the Sepolia `deploy-config.json` passed as first argument to `just run`.\n\nThe resulting deployment was\n```\n{\n  \"SystemConfig\": \"0x29d06Ed7105c7552EFD9f29f3e0d250e5df412CD\",\n  \"MIPS\": \"0x69470D6970Cd2A006b84B1d4d70179c892cFCE01\",\n  \"FaultDisputeGame\": \"0x5e0877a8F6692eD470013e651c4357d0C4941e6C\",\n  \"PermissionedDisputeGame\": \"0x4Ed046e66c96600DaE1a4ec39267bB0cE476E8cc\"\n}\n\n```\nFull deployment log: [deploy.log](https://github.com/user-attachments/files/17872459/deploy.log)\n", "2024-11-22T10:51:46Z", "2024-11-22T15:32:03Z", "sebastianst", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6VRJSE", "I_kwDODjvEJM6ft2iH", "I needed to redeploy the `SystemConfig` impl, so I ran the tool with `SYSTEM_CONFIG_IMPL_ADDR=0x0000000000000000000000000000000000000000`. This yielded: \n\n[deploy.log](https://github.com/user-attachments/files/17937315/deploy.log)\n\n```json\n{\n  \"SystemConfig\": \"0x33b83E4C305c908B2Fc181dDa36e230213058d7d\",\n  \"MIPS\": \"0x0000000000000000000000000000000000000000\",\n  \"FaultDisputeGame\": \"0x0000000000000000000000000000000000000000\",\n  \"PermissionedDisputeGame\": \"0x0000000000000000000000000000000000000000\"\n}\n```\n\nhttps://sepolia.etherscan.io/address/0x33b83E4C305c908B2Fc181dDa36e230213058d7d", "2024-11-27T16:26:16Z", "2024-11-27T16:26:16Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6VRKmq", "I_kwDODjvEJM6ft2iH", "(It seems the previous address `0x29d06...` was deployed in preparation for the devnet upgrade). ", "2024-11-27T16:28:31Z", "2024-11-27T16:28:31Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6VUD8Q", "I_kwDODjvEJM6ft2iH", "Why would the `SystemConfig` impl be different between devnet and testnet? Shouldn't it be able to reuse the same implementation between the two?", "2024-11-28T00:54:16Z", "2024-11-28T00:54:16Z", "ajsutton", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6VW14d", "I_kwDODjvEJM6ft2iH", "Yes I think that is right in principle. However, the one deployed to the devnet was actually incorrect, having `version = 2.3.0-beta.5`. I didn't want to reuse the incorrect one, particularly thinking of validations in the superchain registry down the line.", "2024-11-28T10:36:50Z", "2024-11-28T10:36:50Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6XKCdi", "I_kwDODjvEJM6ft2iH", "@sebastianst @geoknee OK to close this one?", "2024-12-11T13:27:21Z", "2024-12-11T13:27:21Z", "BlocksOnAChain", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6XfKlG", "I_kwDODjvEJM6ft2iH", "@BlocksOnAChain no we should keep it open as the SystemConfig upgrade needs to be re-done https://github.com/ethereum-optimism/superchain-ops/issues/394", "2024-12-13T14:01:30Z", "2024-12-13T14:01:30Z", "geoknee", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6Xg3ca", "I_kwDODjvEJM6ft2iH", "Just re-deployed the contracts again, using https://github.com/ethereum-optimism/optimism/pull/13385 and the following diff to `.env`:\n```\nOP_CONTRACTS_RELEASE=v1.8.0-rc.3 # new variable\n# setting the following to empty string, so that contract addresses are read from the SCR\nSYSTEM_CONFIG_IMPL_ADDR=\nMIPS_IMPL_ADDR=\nDELAYED_WETH_IMPL_ADDR=\n```\nand using an updated `\"faultGameAbsolutePrestate\": \"0x03f89406817db1ed7fd8b31e13300444652cdb0b9c509a674de43483b2f83568\"`\nresulting in deployments:\n```\n{\n  \"SystemConfig\": \"0x33b83E4C305c908B2Fc181dDa36e230213058d7d\",\n  \"MIPS\": \"0x69470D6970Cd2A006b84B1d4d70179c892cFCE01\",\n  \"FaultDisputeGame\": \"0xe591Ebbc2Ba0EAd3db6a0867cC132Fe1c123F448\",\n  \"PermissionedDisputeGame\": \"0xb51baD2d9Da9f94d6A4A5A493Ae6469005611B68\"\n}\n```\nFull log [deploy.log](https://github.com/user-attachments/files/18129743/deploy.log)\n", "2024-12-13T17:59:09Z", "2024-12-13T17:59:55Z", "sebastianst", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6Xr16o", "I_kwDODjvEJM6ft2iH", "@geoknee understood, makes sense to me. \n@sebastianst thanks for redeploying the contracts, again.", "2024-12-16T08:00:04Z", "2024-12-16T08:00:04Z", "BlocksOnAChain", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6Xtkoe", "I_kwDODjvEJM6ft2iH", "Upgrade script improved in https://github.com/ethereum-optimism/optimism/pull/13385", "2024-12-16T11:05:21Z", "2024-12-16T11:05:21Z", "sebastianst", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6Xtl0J", "I_kwDODjvEJM6ft2iH", "@BlocksOnAChain we should close once the superchain-ops task for upgrading the FPs again on Sepolia is executed and merged.", "2024-12-16T11:06:17Z", "2024-12-16T11:06:17Z", "sebastianst", "2025-08-31 00:16:34"]
["IC_kwDODjvEJM6RIDkt", "I_kwDODjvEJM6boVN9", "Would love to hear ideas from @ajsutton @clabby @Inphi ", "2024-10-24T09:48:51Z", "2024-10-24T09:48:51Z", "sebastianst", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6RLhPo", "I_kwDODjvEJM6boVN9", "I'm having difficulty imagine a scenario where a chain reset is warranted. But a fork failing to activate seems reasonable to migitate against. Though even in that scenario I'd expect the op-program to similarly fail to activate as it shares the same fork activation logic as nodes. In which case, while the chain rules aren't desirable, the op-program doesn't diverge from the network. So effectively, it's as if a hf hasn't yet occurred.\n\nThat said, one way around this is to prevent games from being created too soon after setImplementation. This could be an hour or more, with ample time to detect a faulty hardfork. User impact is minimal given the lengthy fault dispute window.", "2024-10-24T16:15:45Z", "2024-10-24T16:17:33Z", "Inphi", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6ROvcm", "I_kwDODjvEJM6boVN9", "I think op-program and op-node/op-geth work in the same way here if you rollback the fault dispute game prestate (similar to how you have to disable the hard fork on op-node/op-geth). It would require the ProxyAdminOwner to execute a transaction to do that prestate rollback though.  With or without fault proofs, if proposals were made based on the hard fork being active the guardian would have to invalidate them as part of rolling back the chain.\r\n\r\nIt's worth noting that games created prior to the hard fork activating are not an issue even if they keep using a prestate that contains the activation. Since the activation block is never reached it won't affect the execution at all.\r\n\r\nMy suggestion if the activation goes badly is to switch to the permissioned game to provide time to rollback the prestate. That will currently invalidate all existing games but with the incident response improvements coming doing so would be optional, so we could choose to just blacklist individual games if there are only a few for blocks with the hard fork activated, but still give us more time to rollback the prestate.", "2024-10-25T00:20:19Z", "2024-10-25T00:20:19Z", "ajsutton", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6Q_jhk", "I_kwDODjvEJM6bfgi5", "check this out: https://github.com/ethereum/go-ethereum/issues/30100\r\n", "2024-10-23T15:13:51Z", "2024-10-23T15:13:51Z", "amirhoseinsh", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6NXlxb", "I_kwDODjvEJM6Xms6T", "Moving my comment from https://github.com/ethereum-optimism/optimism/pull/12035#discussion_r1772286368 here:\r\n\r\nWhat exactly would this solidity contract or structs look like? I don't fully follow the longer-term plan described in this issue", "2024-09-24T16:28:40Z", "2024-09-24T16:28:40Z", "mds1", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6MbhSP", "I_kwDODjvEJM6Wsm9X", "Spec for modification can be found here: https://github.com/ethereum-optimism/specs/pull/374", "2024-09-17T14:29:15Z", "2024-09-17T14:29:15Z", "tynes", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6NJfCU", "I_kwDODjvEJM6Wsm9X", "@cody-wang-cb will own this line of work in the future and create a spec for it.\n", "2024-09-23T12:29:41Z", "2024-09-23T12:29:41Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6NNqdS", "I_kwDODjvEJM6Wsm9X", "Hey @BlocksOnAChain , just wanna clarify, for `spec` do you mean I should create something similar to [this](https://github.com/ethereum-optimism/optimism/issues/12044)?", "2024-09-23T19:21:29Z", "2024-09-23T19:21:29Z", "cody-wang-cb", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6NUzdi", "I_kwDODjvEJM6Wsm9X", "@cody-wang-cb  - you can add things we need to cover + your to-do list to this issue, since we are already tracking this as the part of the official scope. Sounds good?", "2024-09-24T11:49:40Z", "2024-09-24T11:49:40Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6SUc9-", "I_kwDODjvEJM6Wsm9X", "https://github.com/paradigmxyz/reth/pull/11887 is merged \u2705 \n", "2024-11-04T14:16:59Z", "2024-11-04T14:17:59Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6SUdXt", "I_kwDODjvEJM6Wsm9X", "@cody-wang-cb I'm thinking about closing this issues, since all the work seems to be done. Ok with you?", "2024-11-04T14:17:42Z", "2024-11-04T14:17:42Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6SgXnd", "I_kwDODjvEJM6Wsm9X", "@BlocksOnAChain sorry missed the message. I think we still need to add the activation timestamps to op-reth when that's ready, im not sure if it's ready yet though.\r\n\r\nIf we don't need to track that follow up here then it's fine to close this!", "2024-11-05T19:10:13Z", "2024-11-05T19:10:35Z", "cody-wang-cb", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6Sp1Wn", "I_kwDODjvEJM6Wsm9X", "Oh yeah, good callout. Let's keep it open until we add the activation timestamps for op-reth. \nIt will be ready in the next week or two.", "2024-11-06T18:01:20Z", "2024-11-06T18:01:20Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6XwEg7", "I_kwDODjvEJM6Wsm9X", "@cody-wang-cb @henridevieux will you work on the adding the activation timestamps to op-reth?", "2024-12-16T15:39:11Z", "2024-12-16T15:39:11Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6Xwfcl", "I_kwDODjvEJM6Wsm9X", "@BlocksOnAChain Yes will do this week!", "2024-12-16T16:23:03Z", "2024-12-16T16:23:03Z", "cody-wang-cb", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6Mu_VJ", "I_kwDODjvEJM6V52SV", "Another consideration is how we can improve the chain assertions. From @mds1 and @maurelian in https://github.com/ethereum-optimism/optimism/pull/11994#discussion_r1766910643:\r\n\r\n> > I sorted these alphabetically to make it easier to diff the list of checks here with the list of checks in `DeployImplementations.s.sol`. This made me notice that `assertValidL1ERC721Bridge` was missing here\r\n> \r\n> yikes.\r\n> \r\n> That does get at the fact that these checks are hard to review and maintain. I can't think of a great way to improve on the situation though. Even if we had working code coverage, these aren't technically 'tests' so it's unclear if that would help us. \r\n> \r\n> Would it make sense to create a ticket for future work to use `vm.StateAccesses` to ensure that all writes are eventually read during assertions?", "2024-09-19T14:19:47Z", "2024-09-19T14:19:47Z", "mds1", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6abAYB", "I_kwDODjvEJM6V52SV", "Closing as it's addressed through https://github.com/ethereum-optimism/optimism/pull/13510/files and op-deployer checks.", "2025-01-14T18:15:41Z", "2025-01-14T18:15:41Z", "blmalone", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6Xr2dR", "I_kwDODjvEJM6RosdC", "@sebastianst ok to close it now?", "2024-12-16T08:01:26Z", "2024-12-16T08:01:26Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6aO1Jr", "I_kwDODjvEJM6RosdC", "Yes!", "2025-01-13T16:22:13Z", "2025-01-13T16:22:13Z", "sebastianst", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6SUd3E", "I_kwDODjvEJM6Pu2wk", "@sebastianst I'm thinking about closing this one, Ok with you?", "2024-11-04T14:18:31Z", "2024-11-04T14:18:31Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6SVFGE", "I_kwDODjvEJM6Pu2wk", "Not yet, still need the batcher hardening done. But it's in review. ", "2024-11-04T15:22:13Z", "2024-11-04T15:22:26Z", "sebastianst", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM5-rrr6", "I_kwDODjvEJM6JuvP7", "Hey @jinmel, I would be very supportive of this change and would review a PR if you submitted it", "2024-05-22T17:26:41Z", "2024-05-22T17:26:41Z", "tynes", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM5-2w6s", "I_kwDODjvEJM6JuvP7", "Hey @tynes, I was working on extending the current ops-bedrock docker compose project and felt that trying to fit multiple L2s into a single Docker Compose setup might not be very scalable and incurs some inevitable hacks such as writing a template for docker compose and generating with different container names. It seems like I might be reinventing the wheel, given what Kubernetes and [Kustomize](https://kustomize.io/) already accomplishes. What are your thoughts on keeping the current devnet as it is and creating a new directory with Kubernetes/Kustomize configurations? This way, people could easily extend it with their own components. For instance, we could run arbistack and opstack within the same network.", "2024-05-24T00:43:18Z", "2024-05-24T00:44:20Z", "jinmel", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM5-5hy6", "I_kwDODjvEJM6JuvP7", "I would prefer to not create a completely new solution for a devnet as it would create additional maintenance overhead for the team. We used to have a minikube based local devnet but at some point we migrated to docker compose. We want to optimize the devnet for easy maintenance and easy + reliable usage for devs who develop their applications on top of it. I would prefer to implement something more minimal that may introduce some tech debt, for example scoping the implementation directly to only supporting 2 L2s. This would remove the need to implement arbitrary templating. Would this unblock your usecase? Then in the future, the project of further improving the devnet can be revisited ", "2024-05-24T09:03:03Z", "2024-05-24T09:03:03Z", "tynes", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM5_IWwe", "I_kwDODjvEJM6JuvP7", "@tynes Great point and thanks for the clear scope of this PR.", "2024-05-27T08:08:23Z", "2024-05-27T08:08:23Z", "jinmel", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM5_bRMO", "I_kwDODjvEJM6JuvP7", "@tynes I first tried to remove the allocs and make all the l1 and l2 genesis dynamic so that it is extensible to multiple L2s in future, but I just found out that allocs to genesis files is important for unitests here: https://github.com/ethereum-optimism/optimism/tree/develop/packages/contracts-bedrock#test-setup. I attempted to squeeze in two set of allocs into L1 genesis for two chains, but it seems to create more tech debt than actual gain. What would you say if we had a devnet launch script dedicated for cross chain testing only? ", "2024-05-29T16:47:44Z", "2024-05-29T17:00:22Z", "jinmel", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM5_wqkP", "I_kwDODjvEJM6JuvP7", "We spoke about a plan going forward:\r\n\r\n- Create an interop devnet deploy script that uses the L1 contracts deploy twice\r\n- Update the devnet python code in a minimal way to call the interop devnet deploy script, perhaps with an `--interop` flag\r\n- Update the devnet python code so that it can call `docker compose up` twice to bring up 2 L2 systems\r\n\r\nThe goal is to very minimally modify the python devnet code, since it has a lot of tech debt. This will immediately unblock the development of applications that utilize interop.\r\n\r\nFor the interop deploy script, it should set the [deploy script](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/scripts/Deploy.s.sol) into state similar to the pattern here:\r\n\r\nhttps://github.com/ethereum-optimism/optimism/blob/bd7ceb02ec9f6ef2e3332cbb28201a12977f3b60/packages/contracts-bedrock/scripts/Deployer.sol#L24-L26\r\n\r\nThis ensures that we don't have the compile overhead that comes with using `CREATE` directly in the code, resulting in bloated bytecode\r\n\r\nFor longer term, we can consider using [kurtosis](https://github.com/kurtosis-tech/kurtosis). This can serve the basis for a new devnet as well as a multi client test suite", "2024-06-01T14:14:12Z", "2024-06-01T14:14:12Z", "tynes", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6BJvHM", "I_kwDODjvEJM6JuvP7", "Here's a pull request that builds on top of @barnabasbusa work on the [optimism-package](https://github.com/ethpandaops/optimism-package), it allows you to spin up `N` optimism rollups locally: https://github.com/ethpandaops/optimism-package/pull/20. Hopefully this helps", "2024-06-13T21:19:03Z", "2024-06-13T21:22:36Z", "tedim52", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6a7luN", "I_kwDODjvEJM6JuvP7", "Closing. The whole devnet has been refactored into a Kurtosis setup.", "2025-01-17T22:25:46Z", "2025-01-17T22:25:46Z", "protolambda", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6Fqyhl", "I_kwDODjvEJM6MirZI", "@tynes since we mentioned that this [line of work should be included in the granite scope](https://github.com/ethereum-optimism/specs/discussions/241#discussioncomment-9798759), do you know who from the interop team will own the implementation or it's still TBD?", "2024-07-22T10:09:17Z", "2024-07-22T10:09:17Z", "BlocksOnAChain", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6HfSl5", "I_kwDODjvEJM6MirZI", "Per @tynes, we'll defer these predeploy upgrades to isthmus so he can focus on Interop ([full context](https://discord.com/channels/1244729134312198194/1266465563568509058/1270494270528159876)).", "2024-08-07T10:18:19Z", "2024-08-07T10:18:19Z", "alfonso-op", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6az8v6", "I_kwDODjvEJM6MirZI", "Closing this", "2025-01-17T02:34:28Z", "2025-01-17T02:34:28Z", "tynes", "2025-08-31 00:16:35"]
["IC_kwDODjvEJM6az8ta", "I_kwDODjvEJM6Miree", "Closing this", "2025-01-17T02:34:17Z", "2025-01-17T02:34:17Z", "tynes", "2025-08-31 00:16:35"]
["IC_kwDOEf1bQc6bfija", "I_kwDOEf1bQc6dKKDw", "Hello @IvanSvetlov100! I do not see a description for \"Get a Grant\" feedback. Can you clarify please?", "2025-01-23T02:51:31Z", "2025-01-23T02:51:31Z", "arredr2", "2025-08-31 00:17:41"]
["IC_kwDOKSJyfM6bycWB", "I_kwDOKSJyfM6nNiDi", "https://github.com/jakim929/superchain-token-factory\n\n^ example - if we have a patterns repo we can bring it in", "2025-01-25T01:03:36Z", "2025-01-25T01:03:36Z", "jakim929", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6YXjS4", "I_kwDOKSJyfM6kB6U8", "@pharger can you add detail to this issue with what exactly you'd like to see as an outcome of this research spike? Thanks!", "2024-12-20T05:22:35Z", "2024-12-20T05:22:35Z", "fainashalts", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6ZcOYO", "I_kwDOKSJyfM6kB6U8", "Starting to meet with RPC providers on this - @fainashalts can we assign an eng DRI to attend research calls with RPCs vendors kicking off this/next week", "2025-01-07T03:17:14Z", "2025-01-07T03:17:14Z", "pharger", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6bf674", "I_kwDOKSJyfM6kB6U8", "Hi Pierce! Can you clarify what a devX engineer would do in these meetings? RPC vendors are typically a partnership concern. We don't have a clear ask from the devX/ecosystem team at this time, right? Would love more context if you have it.", "2025-01-23T04:37:07Z", "2025-01-23T04:37:07Z", "fainashalts", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6ZcOsq", "I_kwDOKSJyfM6kB6Dw", "@fainashalts can we assign DRI to lead technical discussions with AA providers and wallets? Kicking off discussions next week with pimlico, rainbow, and some others", "2025-01-07T03:18:08Z", "2025-01-07T03:18:08Z", "pharger", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6bf5jm", "I_kwDOKSJyfM6kB6Dw", "@pharger I'm not sure it makes sense for devX engineers to participate in these discussions, it sounds more like a partnerships sort of effort? Happy to determine a DRI for devX tooling work though! Its possible I'm misunderstanding the ask here, so would be happy to chat about this in more detail. ", "2025-01-23T04:31:00Z", "2025-01-23T04:31:00Z", "fainashalts", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6ZcO8Y", "I_kwDOKSJyfM6kBukg", "Sounds like wonderland may end up taking this work FYI. @zainbacchus to confirm", "2025-01-07T03:19:31Z", "2025-01-07T03:19:31Z", "pharger", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6bf4h0", "I_kwDOKSJyfM6kBukg", "@zainbacchus any updates on this? Thanks! :) ", "2025-01-23T04:26:10Z", "2025-01-23T04:26:10Z", "fainashalts", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6bgHAb", "I_kwDOKSJyfM6kBrty", "Commenting to test new slack integration! Please ignore.", "2025-01-23T05:24:33Z", "2025-01-23T05:24:33Z", "fainashalts", "2025-08-31 00:17:42"]
["IC_kwDOKSJyfM6XY9gA", "I_kwDOKSJyfM6bN7Cf", "Hi @tremarkley @jakim929 just checking in on this one -- will you need some time in Q1 to work on this, and is a PRD still the best deliverable to aim for?", "2024-12-12T20:01:42Z", "2024-12-12T20:01:42Z", "fainashalts", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6XZOED", "I_kwDOKSJyfM6bN7Cf", "@fainashalts I think the deliverable depends on how this fits into our roadmap for H1. We had a productive meeting with @pharger to discuss the painpoints and added them to the doc listed in the description. Not sure we need a formal PRD or more work on this unless we want to prioritize this in Q1. Let me know what you think.", "2024-12-12T20:38:38Z", "2024-12-12T20:38:38Z", "tremarkley", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6XZ0BW", "I_kwDOKSJyfM6bN7Cf", "Cool I reached out to @pharger to see what he thinks. I think we have enough on our plate in Q1 but also want to assess whether this is more urgent, whether enough people need assistance with this that it should be prioritized.", "2024-12-12T22:20:04Z", "2024-12-12T22:20:04Z", "fainashalts", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6XZ1G0", "I_kwDOKSJyfM6bN7Cf", "Generally agree this will be an important point but given the focus on new L2 native deployments for interop compatibility this should probably fall right below it. That said if there are specific areas worth drilling in on with upgrades I'm all ears", "2024-12-12T22:23:32Z", "2024-12-12T22:23:32Z", "pharger", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6XZ1xJ", "I_kwDOKSJyfM6bN7Cf", "Although - as we discussed updates (e.g., blocklist on a token on many chains) would be something that would be pretty important", "2024-12-12T22:25:42Z", "2024-12-12T22:25:42Z", "pharger", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6Xg5E2", "I_kwDOKSJyfM6bN7Cf", "@pharger by updates do you mean \"I deployed a token that has functions that can be called which check a blocklist, and I want to set/update the blocklist on every chain at once\"? That could fall under contract management but I don't think we have any direct product planned for this flow?", "2024-12-13T18:03:03Z", "2024-12-13T18:03:03Z", "fainashalts", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6bf-j2", "I_kwDOKSJyfM6bN7Cf", "@pharger should we de-prioritize this for now or do you think it still needs our attention?", "2025-01-23T04:50:38Z", "2025-01-23T04:50:38Z", "fainashalts", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6WGKAp", "I_kwDOKSJyfM6bN5m5", "@nitaliano do you think the SuperCLI work sort of supersedes this issue? Happy to close it if so.", "2024-12-04T18:16:43Z", "2024-12-04T18:16:43Z", "fainashalts", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6ZcOLV", "I_kwDOKSJyfM6bN5m5", "Should we kill this issue? Seems redundent of the CLI or is this something else like parallel deploy?", "2025-01-07T03:16:22Z", "2025-01-07T03:16:22Z", "pharger", "2025-08-31 00:17:43"]
["IC_kwDOKSJyfM6bf8lu", "I_kwDOKSJyfM6bN5m5", "Yep! I think its too broad and being handled elsewhere. Closing, thanks!", "2025-01-23T04:42:15Z", "2025-01-23T04:42:15Z", "fainashalts", "2025-08-31 00:17:43"]
["IC_kwDOH2Qg5s6bvMeD", "I_kwDOH2Qg5s6nUHbm", "This isn't the right place to ask this question, it depends on the policy of base ", "2025-01-24T15:47:11Z", "2025-01-24T15:47:11Z", "tynes", "2025-08-31 00:17:45"]
["IC_kwDOI7W0xc6buDov", "I_kwDOI7W0xc6ndR4f", "cc @lithium323 for visibility that this may be coming soon.", "2025-01-24T13:32:58Z", "2025-01-24T13:32:58Z", "MSilb7", "2025-08-31 00:17:46"]
["IC_kwDOI7W0xc6ZDoUn", "I_kwDOI7W0xc6kSxv5", "V1 done in Hex.\n\nChain-level and aggregate-level are still materialized in BQ. The aggregate-level materialization makes things a bit rigid, especially when I want to make other aggregations (i.e. top 5 chains and \"other\"). So maybe the aggregate-level materializations also happen in Hex. It should all be fairly quick since the chain-level is materialized already.", "2025-01-02T14:32:52Z", "2025-01-02T14:32:52Z", "MSilb7", "2025-08-31 00:17:46"]
["IC_kwDOI7W0xc6ZDsjf", "I_kwDOI7W0xc6kSxv5", "![Image](https://github.com/user-attachments/assets/afe0eeb2-31ba-49a6-9bf7-d605db4d627f)\n", "2025-01-02T14:45:09Z", "2025-01-02T14:45:09Z", "MSilb7", "2025-08-31 00:17:46"]
["IC_kwDOI7W0xc6boypC", "I_kwDOI7W0xc6kSxEW", "done", "2025-01-23T22:39:44Z", "2025-01-23T22:39:44Z", "MSilb7", "2025-08-31 00:17:46"]
["IC_kwDOI7W0xc6YlugE", "I_kwDOI7W0xc6kSvyK", "In BQ for V1, eventually we migrate to the CLI", "2024-12-23T16:56:23Z", "2024-12-23T16:56:23Z", "MSilb7", "2025-08-31 00:17:46"]
["IC_kwDOKIwiaM6bp5Ek", "I_kwDOKIwiaM6mX4ZG", "Wow thank you so much for writing such a clear issue. The bedrock release page doesn't exist. Bedrock was a migration from the legacy version of the OP Stack to its current architecture. I'll put in a PR right now to remove that line item. This page probably needs a lot more revisions too, so we'll take a look as soon as we can :)", "2025-01-24T03:08:54Z", "2025-01-24T03:08:54Z", "sbvegan", "2025-08-31 00:17:47"]
["IC_kwDOKIwiaM6bXVJJ", "I_kwDOKIwiaM6jMBNY", "Closing this because issue is now resolved", "2025-01-22T08:28:31Z", "2025-01-22T08:28:31Z", "Chomtana", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a4Xws", "I_kwDOLB-lzc6mUUBO", "does this require a noop spec if the feature is not implemented for use in isthmus, or will nothing break if it is skipped @protolambda ?", "2025-01-17T14:18:14Z", "2025-01-17T14:18:14Z", "emhane", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a4nu4", "I_kwDOLB-lzc6mUUBO", "We would need to have the enabled on L2s for interop, so I would expect it would be implemented and enabled as part of supporting Pectra in OP Stack (but not needed as part of making OP Stack work when pectra is enabled on the L1).", "2025-01-17T14:50:10Z", "2025-01-17T14:50:10Z", "ajsutton", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a4snL", "I_kwDOLB-lzc6mUUBO", "ok thanks @ajsutton , so it doesn't require any action if not included in isthmus because lack of time, capisci", "2025-01-17T15:00:06Z", "2025-01-17T15:00:06Z", "emhane", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a4tvn", "I_kwDOLB-lzc6mUUBO", "Is Isthmus activating pectra on L2? We aren't picking and choosing bits of pectra to enable other than where things just don't apply. But for things that make sense on L2 we are either activating all of pectra or none of it. ", "2025-01-17T15:02:21Z", "2025-01-17T15:02:21Z", "ajsutton", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a4wU7", "I_kwDOLB-lzc6mUUBO", "I thought there would just be one hard fork called isthmus, but I'm also the completely wrong person to ask. I think some Pectra issues have to be addressed to ensure nothing breaks when l1 forks, see issues labelled noop.", "2025-01-17T15:07:28Z", "2025-01-17T15:07:28Z", "emhane", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a44E_", "I_kwDOLB-lzc6mUUBO", "Yeah there are two parts to \"supporting pectra\".  The highest priority is making sure that pectra can be enabled on the L1 and not have things break - that's the stuff around how to parse new transaction types.\n\nThe other part is actually enabling pectra on the L2 chain.  That will be done in a future hard fork - possibly Isthmus but it depends when it's actually ready.  We \"include\" L1 hard forks in an L2 hard fork which in terms of the geth config means you'd have `isthmus_timestamp` and `pectra_timestamp` set to the same time.  But which L2 hard fork that happens in isn't set because we're trying to use a model where we decide what goes into a L2 hard fork based on what's ready vs deciding ahead of time what goes into the hard fork and rushing to have it ready in time.", "2025-01-17T15:23:01Z", "2025-01-17T15:23:01Z", "ajsutton", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a5NkT", "I_kwDOLB-lzc6mUUBO", "With the beacon-root history accumulator contract in the previous L1 upgrade, very similar to the new block-hash accumulator contract, we used a trick with deposits to deploy the contract automatically as part of the upgrade, in case it's not already permissionlessly deployed. I would strongly recommend we do the same type of thing this upgrade.\n\nAnd then as @ajsutton said, if the pectra functionality is properly enabled as part of the L2 upgrade, then the block-hashes will be applied to the block-hash contract as expected.\n\n", "2025-01-17T16:07:12Z", "2025-01-17T16:07:12Z", "protolambda", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6al6dO", "I_kwDOLB-lzc6mURS-", "opening again, to let @protolambda have a look. if included, needs just a link to L1 spec (implying no diff with L1)", "2025-01-15T18:10:33Z", "2025-01-17T14:12:41Z", "emhane", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6aocAT", "I_kwDOLB-lzc6mURS-", "Down to close this when we merge in the nodiff spec", "2025-01-16T01:44:07Z", "2025-01-17T14:12:48Z", "tynes", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a5V6K", "I_kwDOLB-lzc6mURS-", "The L2 specs should be updated, to reflect that this EIP will be active in L2 at the L2-pectra fork, even though no further changes in the stack are required beyond the regular hardfork activation code that is already in the EL.\n\nThe op-batcher will need a small diff, to handle the cost-equilibrium change, to keep the automatic DA-type switching accurate.\n\nThis op-batcher change should be part of the L1-Pectra support work though, not the L2-Pectra support work.\ncc @sebastianst @geoknee \n", "2025-01-17T16:24:12Z", "2025-01-17T16:24:12Z", "protolambda", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a6mhg", "I_kwDOLB-lzc6mURS-", "Already tracking at https://github.com/ethereum-optimism/optimism/issues/13791 @protolambda ", "2025-01-17T19:29:23Z", "2025-01-17T19:29:23Z", "sebastianst", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6aoeWh", "I_kwDOLB-lzc6mUOwb", "I don't think we will need any EL requests for anything and should assert that they are empty but still check that they are not `None` or `nil`", "2025-01-16T01:52:50Z", "2025-01-16T01:52:50Z", "tynes", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a4gSe", "I_kwDOLB-lzc6mUOwb", "keen on implementing this spec @meyer9 ?\n\ncheck out the layout to use here https://specs.optimism.io/protocol/exec-engine.html\n\ncan also check out the reth impl of the l1 spec to get a better understanding of the smol difference for L2 which should check that arg `executionRequests` is always empt list\nhttps://github.com/paradigmxyz/reth/blob/5db01290f877ea4b8762c25a343bbcb6ea053ebb/crates/rpc/rpc-engine-api/src/engine_api.rs#L253-L285", "2025-01-17T14:34:58Z", "2025-01-17T14:34:58Z", "emhane", "2025-08-31 00:17:47"]
["IC_kwDOLB-lzc6a5R3q", "I_kwDOLB-lzc6mUOwb", "+1, the gist of Pectra op-stack support I wrote was written at a time when there was more uncertainty whether or not we needed to utilize the Pectra Requests feature for any OP-Stack purposes. Currently, we do not have to.\n\nSo I recommend to not change the batch-format, and to make the requests-root part of the block-header (for Pectra compat) but make it implied (root of an always-empty list of requests). Then, when receiving a block from P2P you can assume an empty list of requests. And when building a block, you can assume an empty list of requests. And then there is nothing new to batch-submit. And EL-sync rules can exchange blocks without special changes, and just exchange the block with the empty requests-root. Note that unlike withdrawals, requests are not retained in the execution engine after block execution, and thus also not synced via EL p2p.\n\nThe other EIPs, that introduce requests for various L1 beaconchain related things, can all be adapted such that even if the contract exists and emits an event, no request is actually put in the block. This is a very tiny simple diff.\n", "2025-01-17T16:16:29Z", "2025-01-17T16:16:29Z", "protolambda", "2025-08-31 00:17:47"]
["IC_kwDOKIsnqM6XR54y", "I_kwDOKIsnqM6YttpP", "queued", "2024-12-12T07:52:23Z", "2024-12-12T07:52:23Z", "ElliotFriedman", "2025-08-31 00:17:52"]
["IC_kwDOKIsnqM6aaNGN", "I_kwDOKIsnqM6YttpP", "Closing this issue as I think this has been completed via the pr: https://github.com/ethereum-optimism/superchain-ops/pull/406", "2025-01-14T16:55:09Z", "2025-01-14T16:55:09Z", "blmalone", "2025-08-31 00:17:52"]
["IC_kwDOKIsnqM6aaOnO", "I_kwDOKIsnqM6YttpP", "Reopening this issue based on Matt's comment\n\n> This issue should only be done after https://github.com/ethereum-optimism/superchain-ops/issues/335", "2025-01-14T16:57:38Z", "2025-01-14T16:57:38Z", "blmalone", "2025-08-31 00:17:52"]
["IC_kwDOKIsnqM6bpBzx", "I_kwDOKIsnqM6YttpP", "Superchain-registry gets loaded by default. All new addresses (addresses that scr doesn't have) go into the toml file. ", "2025-01-23T23:31:14Z", "2025-01-23T23:31:14Z", "blmalone", "2025-08-31 00:17:52"]
["IC_kwDODjvEJM6bn1LM", "I_kwDODjvEJM6nVZiK", "We're tracking this as #13911 ", "2025-01-23T20:03:51Z", "2025-01-23T20:03:51Z", "ajsutton", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6bk5f7", "I_kwDODjvEJM6nUmef", "closing as a duplicate.", "2025-01-23T15:37:34Z", "2025-01-23T15:37:34Z", "BlocksOnAChain", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6bSkJR", "I_kwDODjvEJM6ms58A", "This is due to how the block builder operates as well as the existing infra. There is nothing in protocol that enforces this. As the sequencer infra improves, it should be able to include transactions at a lower latency", "2025-01-21T17:25:13Z", "2025-01-21T17:25:13Z", "tynes", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6bY2vF", "I_kwDODjvEJM6ms58A", "cool thanks @tynes \nwe've done some deep diving over the last few days and it seems like the only way to improve block1 landing consistently (still not 100% but an improvement) is tx submission speed relative to last block confirmation (assuming gas params are sufficient) \nwe've found some RPC providers are very late to notify of new blocks, putting you at a disadvantage of up to 1s\n\nthanks for the reply!", "2025-01-22T11:25:52Z", "2025-01-22T11:25:52Z", "m-quigley", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6SX2rU", "I_kwDODjvEJM6c-Xl2", "Deploying MIPS.sol should be permissionless so not need superchain-ops - probably more op-deployer and related to the work @smartcontracts is doing to deploy new FaultDisputeGame contracts.  Then you should be able to use https://github.com/ethereum-optimism/superchain-ops/pull/363 to create the tasks to call setImplementation with the new FaultDisputeGame and PermissionedDisputeGame contracts.", "2024-11-04T21:21:30Z", "2024-11-04T21:21:48Z", "ajsutton", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6blgYC", "I_kwDODjvEJM6c-Xl2", "These tasks have been written and merged. ", "2025-01-23T16:24:06Z", "2025-01-23T16:24:06Z", "mbaxter", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6bU2ur", "I_kwDODjvEJM6bzBwK", "This is done. Holocene landed, and `concluding == true` for every block derived from the new holocene batch-stage. There's just one event left to remove, that currently doesn't trigger anything. Will include that removal as part of the local-safe invalidation code change.", "2025-01-21T23:17:08Z", "2025-01-21T23:17:08Z", "protolambda", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6H1A5R", "I_kwDODjvEJM6Sik0t", "If you build from source on an old release, its still broken?\r\n\r\nWhat release would you like this backported to?", "2024-08-09T22:18:06Z", "2024-08-09T22:18:06Z", "tynes", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6H1WCd", "I_kwDODjvEJM6Sik0t", "If you have an existing `path` local devnet db (like the log implies), then you need `--state.scheme=path`. We made it explicitly `--state.scheme=hash` because the `hash` scheme is required when using `archive` mode (for fault-proof proof generation and op-proposer output-root generation).", "2024-08-10T00:31:47Z", "2024-08-10T00:31:47Z", "protolambda", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6H1Wfu", "I_kwDODjvEJM6Sik0t", "If you start up any version from scratch. V18 or v19 I tested and do make devnet-up you will see this issue.\n\nThe changes @protolambda made fixed it.\nIf we can just cut develop into a new patch release that would be enough in this case. ", "2024-08-10T00:36:31Z", "2024-08-10T00:36:31Z", "imnotanoob", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6H1WlX", "I_kwDODjvEJM6Sik0t", "> If you have an existing `path` local devnet db (like the log implies), then you need `--state.scheme=path`. We made it explicitly `--state.scheme=hash` because the `hash` scheme is required when using `archive` mode (for fault-proof proof generation and op-proposer output-root generation).\n\nI ran this in a brand new server as well as inside a docker container using DIND, so there was no existing path local devnet db", "2024-08-10T00:37:27Z", "2024-08-10T00:37:27Z", "imnotanoob", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6H1dvZ", "I_kwDODjvEJM6Sik0t", "on develop branch, I'm still running into issues where the op-node crashes:\r\n```\r\ndocker ps -a\r\nCONTAINER ID   IMAGE                                                                  COMMAND                  CREATED          STATUS                     PORTS                                                                            NAMES\r\n5ed5d921ea16   us-docker.pkg.dev/oplabs-tools-artifacts/images/op-challenger:devnet   \"op-challenger\"          5 seconds ago    Up 5 seconds                                                                                                ops-bedrock-op-challenger-1\r\nbe4f5caf445d   us-docker.pkg.dev/oplabs-tools-artifacts/images/op-proposer:devnet     \"op-proposer\"            6 seconds ago    Up 5 seconds               0.0.0.0:6062->6060/tcp, 0.0.0.0:7302->7300/tcp, 0.0.0.0:6546->8545/tcp           ops-bedrock-op-proposer-1\r\n8039d322e390   us-docker.pkg.dev/oplabs-tools-artifacts/images/op-batcher:devnet      \"op-batcher\"             6 seconds ago    Up 5 seconds               0.0.0.0:6061->6060/tcp, 0.0.0.0:7301->7300/tcp, 0.0.0.0:6545->8545/tcp           ops-bedrock-op-batcher-1\r\ndf34ba5fe086   nginx:1.25-alpine                                                      \"/docker-entrypoint.\u2026\"   6 seconds ago    Up 5 seconds               0.0.0.0:8080->80/tcp                                                             ops-bedrock-artifact-server-1\r\nfd7d268ab859   us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:devnet         \"op-node --l1=ws://l\u2026\"   6 seconds ago    Exited (1) 5 seconds ago                                                                                    ops-bedrock-op-node-1\r\n0d166d7a8cda   ops-bedrock-l2                                                         \"/bin/sh /entrypoint\u2026\"   11 seconds ago   Up 10 seconds              8546/tcp, 30303/tcp, 30303/udp, 0.0.0.0:8060->6060/tcp, 0.0.0.0:9545->8545/tcp   ops-bedrock-l2-1\r\n9ba3b88f72ea   ops-bedrock-l1-vc                                                      \"/bin/sh /entrypoint\u2026\"   12 seconds ago   Up 12 seconds                                                                                               ops-bedrock-l1-vc-1\r\ne3267dcf7929   ops-bedrock-l1-bn                                                      \"/bin/sh /entrypoint\u2026\"   12 seconds ago   Up 12 seconds              0.0.0.0:5052->5052/tcp, 0.0.0.0:9000->9000/tcp                                   ops-bedrock-l1-bn-1\r\nf918bc29a982   ops-bedrock-l1                                                         \"/bin/bash /entrypoi\u2026\"   12 seconds ago   Up 12 seconds              0.0.0.0:8545-8546->8545-8546/tcp, 30303/tcp, 30303/udp, 0.0.0.0:7060->6060/tcp   ops-bedrock-l1-1\r\n```\r\nwith op-node logs being:\r\n```\r\ndocker logs fd7d268ab859\r\nt=2024-08-10T02:05:18+0000 lvl=warn msg=\"Found a deprecated flag which will be removed in a future version\" flag_name=p2p.scoring.peers\r\nt=2024-08-10T02:05:18+0000 lvl=info msg=\"Not opted in to ProtocolVersions signal loading, disabling ProtocolVersions contract now.\"\r\nt=2024-08-10T02:05:19+0000 lvl=info msg=\"No persisted sequencer state loaded\"\r\nt=2024-08-10T02:05:19+0000 lvl=info msg=\"Rollup Config\" l2_chain_id=901 l2_network=\"unknown L2\" l1_chain_id=900 l1_network=\"unknown L1\" l2_start_time=1723177525 l2_block_hash=0x2d6440c06e0837bc8c9d3a1b899839918611e7dace7bc5ec26c33e7eaf3fbf5e l2_block_number=0 l1_block_hash=0xd1c1fa8f05dba47ceffc8ad00eceb17ba102516214301b3c83ada6438ca69791 l1_block_number=1 regolith_time=\"@ genesis\" canyon_time=\"@ genesis\" delta_time=\"@ genesis\" ecotone_time=\"@ genesis\" fjord_time=\"@ genesis\" granite_time=\"(not configured)\" holocene_time=\"(not configured)\" interop_time=\"(not configured)\" plasma_mode=false\r\nt=2024-08-10T02:05:19+0000 lvl=info msg=\"Initializing rollup node\" version=v0.0.0-773e476b-1723243615\r\nt=2024-08-10T02:05:19+0000 lvl=error msg=\"Error initializing the rollup node\" err=\"failed to init L1: failed to validate the L1 config: failed to get L1 genesis blockhash: failed to fetch header by num 1: not found\"\r\nt=2024-08-10T02:05:19+0000 lvl=crit msg=\"Application failed\" message=\"failed to setup: unable to create the rollup node: failed to init L1: failed to validate the L1 config: failed to get L1 genesis blockhash: failed to fetch header by num 1: not found\"\r\n```\r\n\r\nAny idea what's going on?\r\n\r\nEDIT: nvm, seems fixed after `make devnet-clean`. There was probably some old config that wasn't properly getting overwritten?", "2024-08-10T02:07:39Z", "2024-08-10T19:38:48Z", "samlaf", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6IJdCJ", "I_kwDODjvEJM6Sik0t", "@tynes , @protolambda - any chance we can cut this into a new release branch?", "2024-08-12T14:44:10Z", "2024-08-12T14:44:10Z", "imnotanoob", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6IKHFZ", "I_kwDODjvEJM6Sik0t", "@imnotanoob We will be doing a release sometime soon. For now you should be able to use a specific commit on `develop` that includes the fix", "2024-08-12T15:57:11Z", "2024-08-12T15:57:11Z", "tynes", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6IKpOs", "I_kwDODjvEJM6Sik0t", "> @imnotanoob We will be doing a release sometime soon. For now you should be able to use a specific commit on `develop` that includes the fix\r\n\r\nThanks, yep doing that for now.", "2024-08-12T16:51:11Z", "2024-08-12T16:51:11Z", "imnotanoob", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6IZdyu", "I_kwDODjvEJM6Sik0t", "I get this error when I tried with develop and v1.9.0. In develop there is no package.json either.\r\n\r\ngit submodule update --init --recursive\r\nmake[1]: Entering directory '/home/aks/Documents/optimism/optimism'\r\n./ops/scripts/geth-version-checker.sh && \\\r\n \t(echo \"Geth versions match, not installing geth...\"; true) || \\\r\n \t\t(echo \"Versions do not match, installing geth!\"; \\\r\n \t\t\tgo install -v github.com/ethereum/go-ethereum/cmd/geth@v1.13.14; \\\r\n \t\t\techo \"Installed geth!\"; true)\r\n./ops/scripts/geth-version-checker.sh: line 7: geth: command not found\r\nGeth version does not match!\r\nLocal geth version: v\r\nExpected geth version: v1.13.14-stable\r\nVersions do not match, installing geth!\r\n// github.com/ethereum/go-ethereum/cmd/geth\r\nlink: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld\r\nInstalled geth!\r\nmake[1]: Leaving directory '/home/aks/Documents/optimism/optimism'\r\nmake[1]: Entering directory '/home/aks/Documents/optimism/optimism'\r\ngo install -v github.com/protolambda/eth2-testnet-genesis@v0.10.0\r\nmake[1]: Leaving directory '/home/aks/Documents/optimism/optimism'\r\nmake[1]: Entering directory '/home/aks/Documents/optimism/optimism'\r\nmake -C ./op-program op-program\r\nmake[2]: Entering directory '/home/aks/Documents/optimism/optimism/op-program'\r\nenv GO111MODULE=on GOOS= GOARCH= CGO_ENABLED=0 go build -v -ldflags \"-X main.GitCommit=ec45f6634ab2855a4ae5d30c4e240d79f081d689 -X main.GitDate=1723023640 -X github.com/ethereum-optimism/optimism/op-program/version.Version=v0.0.0 -X github.com/ethereum-optimism/optimism/op-program/version.Meta=\" -o ./bin/op-program ./host/cmd/main.go\r\n//command-line-arguments\r\nlink: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld\r\nmake[2]: *** [Makefile:24: op-program-host] Error 1\r\nmake[2]: Leaving directory '/home/aks/Documents/optimism/optimism/op-program'\r\nmake[1]: *** [Makefile:129: op-program] Error 2\r\nmake[1]: Leaving directory '/home/aks/Documents/optimism/optimism'\r\nmake: *** [Makefile:166: pre-devnet] Error 2", "2024-08-14T10:17:29Z", "2024-08-14T10:17:29Z", "avishkarabhishek786", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6IaDSh", "I_kwDODjvEJM6Sik0t", "> I get this error when I tried with develop and v1.9.0. In develop there is no package.json either.\r\n> \r\n> git submodule update --init --recursive make[1]: Entering directory '/home/aks/Documents/optimism/optimism' ./ops/scripts/geth-version-checker.sh && (echo \"Geth versions match, not installing geth...\"; true) || (echo \"Versions do not match, installing geth!\"; go install -v github.com/ethereum/go-ethereum/cmd/geth@v1.13.14; echo \"Installed geth!\"; true) ./ops/scripts/geth-version-checker.sh: line 7: geth: command not found Geth version does not match! Local geth version: v Expected geth version: v1.13.14-stable Versions do not match, installing geth! // github.com/ethereum/go-ethereum/cmd/geth link: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld Installed geth! make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make[1]: Entering directory '/home/aks/Documents/optimism/optimism' go install -v github.com/protolambda/eth2-testnet-genesis@v0.10.0 make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make[1]: Entering directory '/home/aks/Documents/optimism/optimism' make -C ./op-program op-program make[2]: Entering directory '/home/aks/Documents/optimism/optimism/op-program' env GO111MODULE=on GOOS= GOARCH= CGO_ENABLED=0 go build -v -ldflags \"-X main.GitCommit=ec45f6634ab2855a4ae5d30c4e240d79f081d689 -X main.GitDate=1723023640 -X github.com/ethereum-optimism/optimism/op-program/version.Version=v0.0.0 -X github.com/ethereum-optimism/optimism/op-program/version.Meta=\" -o ./bin/op-program ./host/cmd/main.go //command-line-arguments link: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld make[2]: *** [Makefile:24: op-program-host] Error 1 make[2]: Leaving directory '/home/aks/Documents/optimism/optimism/op-program' make[1]: *** [Makefile:129: op-program] Error 2 make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make: *** [Makefile:166: pre-devnet] Error 2\r\n\r\n**UPDATE**\r\n\r\nSo my Go version was 1.23.0. When I switched to 1.21.13 things started working for me. I guess the reason is mentioned in [memsize github](https://github.com/fjl/memsize) - \"NOTE: As of Go 1.23, memsize no longer works because of a restriction added by the\r\nGo toolchain.\"\r\n\r\nHowever, I am now getting a new error - \r\n\r\n`eth2-testnet-genesis path: \r\nl1-generate-beacon-genesis.sh: 7: eth2-testnet-genesis: not found\r\nTraceback (most recent call last):\r\n  File \"/home/aks/Documents/optimism/optimism/./bedrock-devnet/main.py\", line 9, in <module>\r\n    main()\r\n  File \"/home/aks/Documents/optimism/optimism/./bedrock-devnet/main.py\", line 5, in main\r\n    devnet.main()\r\n  File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 118, in main\r\n    devnet_deploy(paths)\r\n  File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 201, in devnet_deploy\r\n    run_command([\r\n  File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 365, in run_command\r\n    return subprocess.run(\r\n  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['sh', 'l1-generate-beacon-genesis.sh']' returned non-zero exit status 127.\r\nmake: *** [Makefile:177: devnet-up] Error 1\r\n`\r\n\r\n[Full log](https://gist.github.com/avishkarabhishek786/c69f6b7aa95c8d5d38b514891d686976)", "2024-08-14T11:47:15Z", "2024-08-14T11:47:15Z", "avishkarabhishek786", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6IfU8x", "I_kwDODjvEJM6Sik0t", "> > I get this error when I tried with develop and v1.9.0. In develop there is no package.json either.\r\n> > git submodule update --init --recursive make[1]: Entering directory '/home/aks/Documents/optimism/optimism' ./ops/scripts/geth-version-checker.sh && (echo \"Geth versions match, not installing geth...\"; true) || (echo \"Versions do not match, installing geth!\"; go install -v github.com/ethereum/go-ethereum/cmd/geth@v1.13.14; echo \"Installed geth!\"; true) ./ops/scripts/geth-version-checker.sh: line 7: geth: command not found Geth version does not match! Local geth version: v Expected geth version: v1.13.14-stable Versions do not match, installing geth! // github.com/ethereum/go-ethereum/cmd/geth link: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld Installed geth! make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make[1]: Entering directory '/home/aks/Documents/optimism/optimism' go install -v github.com/protolambda/eth2-testnet-genesis@v0.10.0 make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make[1]: Entering directory '/home/aks/Documents/optimism/optimism' make -C ./op-program op-program make[2]: Entering directory '/home/aks/Documents/optimism/optimism/op-program' env GO111MODULE=on GOOS= GOARCH= CGO_ENABLED=0 go build -v -ldflags \"-X main.GitCommit=ec45f6634ab2855a4ae5d30c4e240d79f081d689 -X main.GitDate=[1723023640](tel:1723023640) -X github.com/ethereum-optimism/optimism/op-program/version.Version=v0.0.0 -X github.com/ethereum-optimism/optimism/op-program/version.Meta=\" -o ./bin/op-program ./host/cmd/main.go //command-line-arguments link: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld make[2]: *** [Makefile:24: op-program-host] Error 1 make[2]: Leaving directory '/home/aks/Documents/optimism/optimism/op-program' make[1]: *** [Makefile:129: op-program] Error 2 make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make: *** [Makefile:166: pre-devnet] Error 2\r\n> \r\n> **UPDATE**\r\n> \r\n> So my Go version was 1.23.0. When I switched to 1.21.13 things started working for me. I guess the reason is mentioned in [memsize github](https://github.com/fjl/memsize) - \"NOTE: As of Go 1.23, memsize no longer works because of a restriction added by the Go toolchain.\"\r\n> \r\n> However, I am now getting a new error -\r\n> \r\n> `eth2-testnet-genesis path: l1-generate-beacon-genesis.sh: 7: eth2-testnet-genesis: not found Traceback (most recent call last): File \"/home/aks/Documents/optimism/optimism/./bedrock-devnet/main.py\", line 9, in <module> main() File \"/home/aks/Documents/optimism/optimism/./bedrock-devnet/main.py\", line 5, in main devnet.main() File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 118, in main devnet_deploy(paths) File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 201, in devnet_deploy run_command([ File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 365, in run_command return subprocess.run( File \"/usr/lib/python3.10/subprocess.py\", line 526, in run raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['sh', 'l1-generate-beacon-genesis.sh']' returned non-zero exit status 127. make: *** [Makefile:177: devnet-up] Error 1 `\r\n> \r\n> [Full log](https://gist.github.com/avishkarabhishek786/c69f6b7aa95c8d5d38b514891d686976)\r\n\r\nThink you just need to install eth2-testnet-genesis tool. look it up it has its own GitHub repo you can install it via go toolchain.", "2024-08-14T21:12:22Z", "2024-08-14T21:12:22Z", "samlaf", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6I5ceW", "I_kwDODjvEJM6Sik0t", "> @imnotanoob We will be doing a release sometime soon. For now you should be able to use a specific commit on `develop` that includes the fix\r\n\r\nHey tynes. Just wanted to follow up :)", "2024-08-19T14:46:20Z", "2024-08-19T14:46:20Z", "imnotanoob", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6KJNbM", "I_kwDODjvEJM6Sik0t", "> > > I get this error when I tried with develop and v1.9.0. In develop there is no package.json either.\r\n> > > git submodule update --init --recursive make[1]: Entering directory '/home/aks/Documents/optimism/optimism' ./ops/scripts/geth-version-checker.sh && (echo \"Geth versions match, not installing geth...\"; true) || (echo \"Versions do not match, installing geth!\"; go install -v github.com/ethereum/go-ethereum/cmd/geth@v1.13.14; echo \"Installed geth!\"; true) ./ops/scripts/geth-version-checker.sh: line 7: geth: command not found Geth version does not match! Local geth version: v Expected geth version: v1.13.14-stable Versions do not match, installing geth! // github.com/ethereum/go-ethereum/cmd/geth link: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld Installed geth! make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make[1]: Entering directory '/home/aks/Documents/optimism/optimism' go install -v github.com/protolambda/eth2-testnet-genesis@v0.10.0 make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make[1]: Entering directory '/home/aks/Documents/optimism/optimism' make -C ./op-program op-program make[2]: Entering directory '/home/aks/Documents/optimism/optimism/op-program' env GO111MODULE=on GOOS= GOARCH= CGO_ENABLED=0 go build -v -ldflags \"-X main.GitCommit=ec45f6634ab2855a4ae5d30c4e240d79f081d689 -X main.GitDate=1723023640 -X github.com/ethereum-optimism/optimism/op-program/version.Version=v0.0.0 -X github.com/ethereum-optimism/optimism/op-program/version.Meta=\" -o ./bin/op-program ./host/cmd/main.go //command-line-arguments link: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld make[2]: *** [Makefile:24: op-program-host] Error 1 make[2]: Leaving directory '/home/aks/Documents/optimism/optimism/op-program' make[1]: *** [Makefile:129: op-program] Error 2 make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make: *** [Makefile:166: pre-devnet] Error 2\r\n> > \r\n> > \r\n> > **UPDATE**\r\n> > So my Go version was 1.23.0. When I switched to 1.21.13 things started working for me. I guess the reason is mentioned in [memsize github](https://github.com/fjl/memsize) - \"NOTE: As of Go 1.23, memsize no longer works because of a restriction added by the Go toolchain.\"\r\n> > However, I am now getting a new error -\r\n> > `eth2-testnet-genesis path: l1-generate-beacon-genesis.sh: 7: eth2-testnet-genesis: not found Traceback (most recent call last): File \"/home/aks/Documents/optimism/optimism/./bedrock-devnet/main.py\", line 9, in <module> main() File \"/home/aks/Documents/optimism/optimism/./bedrock-devnet/main.py\", line 5, in main devnet.main() File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 118, in main devnet_deploy(paths) File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 201, in devnet_deploy run_command([ File \"/home/aks/Documents/optimism/optimism/bedrock-devnet/devnet/__init__.py\", line 365, in run_command return subprocess.run( File \"/usr/lib/python3.10/subprocess.py\", line 526, in run raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['sh', 'l1-generate-beacon-genesis.sh']' returned non-zero exit status 127. make: *** [Makefile:177: devnet-up] Error 1 `\r\n> > [Full log](https://gist.github.com/avishkarabhishek786/c69f6b7aa95c8d5d38b514891d686976)\r\n> \r\n> Think you just need to install eth2-testnet-genesis tool. look it up it has its own GitHub repo you can install it via go toolchain.\r\n\r\nThank you for that. I was struggling for a long time to find the problem, as `make devnet-up` would actually try starting the devnet without `eth2-testnet-genesis` (I did not set `PATH=\"$PATH:$HOME/go/bin\"`).\r\n\r\nFirst run would error with `eth2-testnet-genesis: command not found`. The second run would start the devnet, but error with `\"fetching start block by number: operation failed permanently after 24 attempts: not found\"`.\r\n\r\nThis was happening because the L1 was still at block 0, as no contracts were deployed. You can confirm that by running \r\n```\r\ncurl -X POST -H \"Content-Type: application/json\" --data  \\\r\n  '{\"jsonrpc\":\"2.0\",\"method\":\"eth_blockNumber\",\"params\":[],\"id\":1}' \\\r\n localhost:8545\r\n ```\r\n\r\nThe problem was lack of `eth2-genesis-testnet` command, but I'm not sure why it does run on the second try.\r\n\r\nThe fix for me was adding `export PATH=\"$PATH:$HOME/go/bin\"` to `~/.zshrc`. Hope it helps someone.\r\n\r\n\r\n\r\n", "2024-08-29T13:31:51Z", "2024-08-29T13:31:51Z", "michalsidzej", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6O4suL", "I_kwDODjvEJM6Sik0t", "Hi, I was having a similar issue where the \"First run would error with eth2-testnet-genesis: command not found. The second run would start the devnet, but error with \"fetching start block by number: operation failed permanently after 24 attempts: not found\".\"\r\n\r\nI used your solution of adding `export PATH=\"$PATH:$HOME/go/bin\"` to the .bashrc but now I am getting a new error. Upon the first make devnet-up run. \r\n\r\nFor note I am running on ubuntu ubuntu 24.04.1 with go version go1.23.2 linux/amd64. \r\n\r\n<img width=\"1059\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c59a6286-63ec-4d75-9c52-9cc15677f198\">\r\n\r\nHave you seen this error:  \r\nl2_output_oracle = addresses['L2OutputOracleProxy']\r\n                       ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\r\nKeyError: 'L2OutputOracleProxy'\r\n\r\n@samlaf @tynes @imnotanoob @michalsidzej \r\n\r\nThanks for the help.\r\n\r\n", "2024-10-07T15:19:26Z", "2024-10-07T15:19:26Z", "nic225", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6O789b", "I_kwDODjvEJM6Sik0t", "@nic225 that's a new regression: see https://github.com/ethereum-optimism/optimism/issues/12354", "2024-10-07T23:00:57Z", "2024-10-07T23:00:57Z", "samlaf", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6Rbsoh", "I_kwDODjvEJM6Sik0t", "> I get this error when I tried with develop and v1.9.0. In develop there is no package.json either.\r\n> \r\n> git submodule update --init --recursive make[1]: Entering directory '/home/aks/Documents/optimism/optimism' ./ops/scripts/geth-version-checker.sh && (echo \"Geth versions match, not installing geth...\"; true) || (echo \"Versions do not match, installing geth!\"; go install -v github.com/ethereum/go-ethereum/cmd/geth@v1.13.14; echo \"Installed geth!\"; true) ./ops/scripts/geth-version-checker.sh: line 7: geth: command not found Geth version does not match! Local geth version: v Expected geth version: v1.13.14-stable Versions do not match, installing geth! // github.com/ethereum/go-ethereum/cmd/geth link: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld Installed geth! make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make[1]: Entering directory '/home/aks/Documents/optimism/optimism' go install -v github.com/protolambda/eth2-testnet-genesis@v0.10.0 make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make[1]: Entering directory '/home/aks/Documents/optimism/optimism' make -C ./op-program op-program make[2]: Entering directory '/home/aks/Documents/optimism/optimism/op-program' env GO111MODULE=on GOOS= GOARCH= CGO_ENABLED=0 go build -v -ldflags \"-X main.GitCommit=ec45f6634ab2855a4ae5d30c4e240d79f081d689 -X main.GitDate=1723023640 -X github.com/ethereum-optimism/optimism/op-program/version.Version=v0.0.0 -X github.com/ethereum-optimism/optimism/op-program/version.Meta=\" -o ./bin/op-program ./host/cmd/main.go //command-line-arguments link: github.com/fjl/memsize: invalid reference to runtime.stopTheWorld make[2]: *** [Makefile:24: op-program-host] Error 1 make[2]: Leaving directory '/home/aks/Documents/optimism/optimism/op-program' make[1]: *** [Makefile:129: op-program] Error 2 make[1]: Leaving directory '/home/aks/Documents/optimism/optimism' make: *** [Makefile:166: pre-devnet] Error 2\r\n\r\nNever versi\u00f3n have a package json. How You can resolver this ???", "2024-10-27T10:30:10Z", "2024-10-27T10:30:10Z", "Tron2024-cpu", "2025-08-31 00:18:11"]
["IC_kwDODjvEJM6bRNLe", "I_kwDODjvEJM6Sik0t", "The devnet is no longer with us.", "2025-01-21T15:03:14Z", "2025-01-21T15:03:14Z", "sebastianst", "2025-08-31 00:18:11"]
["IC_kwDOMMiGhs6XZw_p", "I_kwDOMMiGhs6hxP0l", "This probably makes sense to add to upcoming Supersim improvements.", "2024-12-12T22:11:50Z", "2024-12-12T22:11:50Z", "fainashalts", "2025-08-31 00:18:16"]
["IC_kwDOL-xLQ86cKxTz", "I_kwDOL-xLQ86n49qT", "Resolved by https://github.com/ethereum-optimism/infra/pull/130", "2025-01-28T21:14:11Z", "2025-01-28T21:14:11Z", "scharissis", "2025-08-31 00:19:28"]
["IC_kwDOL-xLQ86cKvVN", "I_kwDOL-xLQ86n45dh", "Closed by https://github.com/ethereum-optimism/infra/pull/131", "2025-01-28T21:09:17Z", "2025-01-28T21:09:17Z", "scharissis", "2025-08-31 00:19:28"]
["IC_kwDOKIwiaM6b_UJj", "I_kwDOKIwiaM6nvYsH", "Hey @Wazabie, thanks for raising this! Just opened a PR fixing this: https://github.com/ethereum-optimism/docs/pull/1296\n\nThanks for filling out the issue so completely! In the future for a small issue like this, feel free to just ignore the template to save some of your time. You can also just hop on the #docs channel in slack or discord too! ", "2025-01-27T22:52:46Z", "2025-01-27T22:52:46Z", "bradleycamacho", "2025-08-31 00:19:31"]
["IC_kwDOKIwiaM6YlY-T", "I_kwDOKIwiaM6kBCE9", "Good call out! Here's a PR that will address this issue: https://github.com/ethereum-optimism/docs/pull/1125", "2024-12-23T15:51:20Z", "2024-12-23T15:51:20Z", "sbvegan", "2025-08-31 00:19:31"]
["IC_kwDOKIwiaM6ZJ7OT", "I_kwDOKIwiaM6kBCE9", "@sbvegan This [PR](https://github.com/ethereum-optimism/docs/pull/1125) doesn't seem to align with the standard configuration @ZakAyesh mentioned. Would you like me to create a new PR?\n\nIf it meets expectation then we can close this issue?", "2025-01-03T16:39:16Z", "2025-01-03T16:40:27Z", "krofax", "2025-08-31 00:19:31"]
["IC_kwDOKIwiaM6ZKQda", "I_kwDOKIwiaM6kBCE9", "## Description \n\nTo address this issue, lets add a sub section to the blockspace charter/standard charter section of our documentation to address devsup's recurring question about standardization.\n\n## User story\n\n- A chain governor should be able to understand what defines a standard chain (on a technical and governance level)\n\n## Resources\n\n- [Standard Rollup Configuration Page](https://docs.optimism.io/superchain/blockspace-charter)\n- Zak and other developer support engineers who have gotten this question\n- Tess is the product manager on the superchain registry and knows a lot about standarization\n\n## Acceptance Criteria\n\n- A new section that describes what a standard chain is and links out to the specs to explicity defines the technical stuff\n- Information about the op-deployer and how it helps with standardization\n- Why standardization is important\n- Context on what is and what is not standard\n\n## Action items\n\n1. Reach out to Zak and devsup if you need more context\n2. Open a PR to add context that addresses the acceptance criteria\n3. Get review from zak and tess\n4. peer review merge\n5. If needed, open a new issue for next sprint about opening a PR to the specs site to organize the standard config info.\n", "2025-01-03T17:49:55Z", "2025-01-06T18:26:32Z", "sbvegan", "2025-08-31 00:19:31"]
["IC_kwDOKIwiaM6a5NlV", "I_kwDOKIwiaM6kBCE9", "@ZakAyesh @tessr @sbvegan Ready for review.", "2025-01-17T16:07:14Z", "2025-01-17T16:07:14Z", "krofax", "2025-08-31 00:19:31"]
["IC_kwDOKIwiaM6cfLzR", "I_kwDOKIwiaM6ePrb1", "Completed by #1303 ", "2025-01-30T19:43:50Z", "2025-01-30T19:43:50Z", "krofax", "2025-08-31 00:19:31"]
["IC_kwDOLB-lzc6b9oth", "I_kwDOLB-lzc6mfmsF", "closing, will be included after all since is needed for interop full nodes", "2025-01-27T18:48:39Z", "2025-01-27T18:48:39Z", "emhane", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6aodqk", "I_kwDOLB-lzc6mUUpD", "We need to update the spec for span batches to support 7702 txs. The span batch serialization is domain specific and has to know about each transaction type it is compressing. Some details can be found in https://github.com/ethereum-optimism/optimism/issues/12435\n\nRight now the derivation pipeline deserializes all L1 transactions. This means that it has to know about the L1 tx types. We generally want to decouple having to update the derivation pipeline from L1 hardforks to remove stressful releases based on L1 timelines. We are planning on retrofitting the spec to only deserialize the transaction types that exist today so it ignores 7702 transactions. If we do not do this before Pectra goes live on L1, then both the proof and the L2 nodes will break. See https://github.com/ethereum-optimism/optimism/issues/13379#issuecomment-2547430177 and https://github.com/ethereum-optimism/optimism/issues/13627 for context", "2025-01-16T01:50:36Z", "2025-01-16T01:50:36Z", "tynes", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6cfI1g", "I_kwDOLB-lzc6mUUpD", "This was merged in!", "2025-01-30T19:37:20Z", "2025-01-30T19:37:20Z", "meyer9", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6amBsO", "I_kwDOLB-lzc6mUI1Y", "IMO we should not use an upgrade-tx. We have done this for contracts that are expected to work with op-stack, like the beacon-block-roots accumulator. But the beacon deposit contract, as well as other pectra-requests-generating contracts, are not expected to work. We should ignore them. It's much more safe to just change the request generation to not include requests of type X (see EIPs) than to try and change the bytecode to something that happens to not insert it. Especially since some of these pectra contracts can be deployed permissionlessly ahead of the op-stack upgrade, and we don't want to modify bytecode.\n", "2025-01-15T18:26:12Z", "2025-01-15T18:26:12Z", "protolambda", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6a3-Et", "I_kwDOLB-lzc6mUI1Y", "are changes required to `op-node` @protolambda ?", "2025-01-17T13:24:41Z", "2025-01-17T13:24:41Z", "emhane", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6a5UCe", "I_kwDOLB-lzc6mUI1Y", "@emhane no op-node changes required for this one. Although, for the more generic requests-handling, it would be good to make the op-node abort block-building if the engine-API ever returns a block with a non-empty requests list. To ensure we don't accidentally build the block if the EL / block-builder API has some bug.\n\nAnd this then also means to not deploy an upgrade-tx for this contract. See above comment.\nJust a tiny EL diff, by not putting this type of request in the block if it's an op-stack chain, regardless of what happens in the EVM.\n", "2025-01-17T16:20:49Z", "2025-01-17T16:20:49Z", "protolambda", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6alT5-", "I_kwDOLB-lzc6mUF8n", "@protolambda is there anything to do here?", "2025-01-15T17:02:01Z", "2025-01-15T17:02:01Z", "emhane", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6amCtH", "I_kwDOLB-lzc6mUF8n", "See comment on #505, very similar reasoning applies here. Let's change the block-sealing code to not add requests of type `CONSOLIDATION_REQUEST_TYPE` (and for other EIPs, see their respective request type), if the optimism config is active. (Doesn't necessarily need a specific fork check, since the request shouldn't exist pre-pectra).\n", "2025-01-15T18:28:34Z", "2025-01-15T18:28:34Z", "protolambda", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6JRLdq", "I_kwDOLB-lzc6SI50M", "Span batch support for the [set code tx type](https://github.com/ethereum/EIPs/blob/f8adac6a3be5de45dc2b6077f8d959c0765dabf5/EIPS/eip-7702.md) was implemented [here](https://github.com/Rjected/optimism/commit/dcc08095e125b42415ad8c9a21d4b11ec1fb2333)", "2024-08-21T20:44:51Z", "2024-08-21T20:44:51Z", "tynes", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6JRLwP", "I_kwDOLB-lzc6SI50M", "`op-geth` with 7702 support was implemented [here](https://github.com/ethereum-optimism/op-geth/compare/optimism...Rjected:op-geth:optimism)", "2024-08-21T20:45:35Z", "2024-08-21T20:45:35Z", "tynes", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6P2chf", "I_kwDOLB-lzc6SI50M", "A new transaction type is added.", "2024-10-15T09:47:41Z", "2024-10-15T09:47:41Z", "threewebcode", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6atZXd", "I_kwDOLB-lzc6SI50M", "this involves a maili change after it's spec'd right @refcell ?", "2025-01-16T13:06:43Z", "2025-01-16T13:06:43Z", "emhane", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6ataGa", "I_kwDOLB-lzc6SI50M", "> The span batch spec mentions a [v2](https://github.com/ethereum-optimism/specs/blob/afadeea642d05eda6e111d9098c9a842c2ecf322/specs/protocol/delta/span-batches.md#future-batch-format-extension), we should consider adding EIP-7702 support to span batches v2.\n\n~~wouldn't that be v3 @tynes ?~~ ah no it would be like a release of v2, which was prev only beta released, from linked spec\n> This is an experimental extension of the span-batch format, and not activated with the Delta upgrade yet.", "2025-01-16T13:07:39Z", "2025-01-16T14:47:23Z", "emhane", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6auuKN", "I_kwDOLB-lzc6SI50M", "as far as specs are concerned, the action items are:\n- move the previously experimental v2 span batch format up under main heading https://github.com/ethereum-optimism/specs/blob/afadeea642d05eda6e111d9098c9a842c2ecf322/specs/protocol/delta/span-batches.md#span-batch-format\n- move v1 span batch format down below, and give new heading \"Legacy span batch format\"\n\nright @tynes ?\n\nare there any blockers/open security concerns with stable releasing v2 format? considering it has been experimental until now, what has stabilised it? @protolambda @mslipper ", "2025-01-16T14:52:57Z", "2025-01-16T14:52:57Z", "emhane", "2025-08-31 01:44:22"]
["IC_kwDOLB-lzc6a6Neu", "I_kwDOLB-lzc6SI50M", "@emhane the `fee_recipients` experimental extension of span-batches isn't supported fully, and was added based on speculation of needs by external L2 chains which have since been unresponsive.\n\nI think it's best if we keep the `fee_recipients` extension in a \"legacy\" sub-section. And then use type-3 span-batch for the EIP-7702 support, to avoid any possible conflict with other chains. And not include the `fee_recipients` feature there.\n\nWe should ask someone from product to investigate if `fee_recipients` is even used anywhere in the wild today.\n", "2025-01-17T18:25:14Z", "2025-01-17T18:25:14Z", "protolambda", "2025-08-31 01:44:22"]
["IC_kwDODjvEJM6cWF0S", "I_kwDODjvEJM6n-h6C", "https://github.com/ethpandaops/optimism-package/pull/150 will fix this", "2025-01-29T22:39:08Z", "2025-01-29T22:39:08Z", "sigma", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cd_pZ", "I_kwDODjvEJM6n-h6C", "also want to mention: we removed mini-devnet, recommend using simple-devnet", "2025-01-30T17:20:02Z", "2025-01-30T17:20:11Z", "zhwrd", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6ceSYM", "I_kwDODjvEJM6n-h6C", "Thanks @zhwrd, unfortunately the simple-devnet is still failing with the issue described here - https://github.com/ethereum-optimism/optimism/issues/14037", "2025-01-30T17:51:25Z", "2025-01-30T17:51:25Z", "piersy", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6ccsIk", "I_kwDODjvEJM6n99eN", "cc @maurelian @mds1 ", "2025-01-30T15:11:28Z", "2025-01-30T15:11:28Z", "tynes", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6ce5RK", "I_kwDODjvEJM6n99eN", "can you  confirm your foundry binaries are aligned with the mise.toml requirements? Lately our code size has been very much tied to a specific toolchain", "2025-01-30T19:06:01Z", "2025-01-30T19:06:01Z", "sigma", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6clQyX", "I_kwDODjvEJM6n99eN", "Hi @sigma, thanks, mise was the issue, I was totally unaware of the requirement to use mise.\n\nIn case anyone else has this problem, you need to [install and activate mise](https://mise.jdx.dev/getting-started.html) and then run `mise install` before trying to run the devnet.", "2025-01-31T11:45:46Z", "2025-01-31T11:53:01Z", "piersy", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cMA2u", "I_kwDODjvEJM6n6IWJ", "@ajsutton for viz - probably worth looking through the diff for any other usages of fmt", "2025-01-29T01:29:13Z", "2025-01-29T01:29:13Z", "meyer9", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cMA8M", "I_kwDODjvEJM6n6IWJ", "But `eth.ChainID` has a `String()` that returns the decimal value. So:\n\n```\nfunc TestFormatChainID(t *testing.T) {\n\tchainID := ChainIDFromUInt64(429824929)\n\trequire.Equal(t, \"429824929\", fmt.Sprintf(\"%v\", chainID))\n}\n```\n\n", "2025-01-29T01:29:30Z", "2025-01-29T01:29:30Z", "ajsutton", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cMBTA", "I_kwDODjvEJM6n6IWJ", "And the tests in `chaincfg_test.go` are still correctly loading the custom config files.  What am I missing?", "2025-01-29T01:30:59Z", "2025-01-29T01:30:59Z", "ajsutton", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cMB5u", "I_kwDODjvEJM6n6IWJ", "Sorry! I was looking at a %d formatting, not %v: https://github.com/ethereum-optimism/optimism/blob/41ff0d8ead89460b37460c9b74756fe175f43a13/op-program/chainconfig/chaincfg.go#L61\n\nThat's probably fine then", "2025-01-29T01:33:18Z", "2025-01-29T01:36:05Z", "meyer9", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6b8uoK", "I_kwDODjvEJM6ns3Nc", "I'm dispatching the workflow on op-reth branch `emhane/exec-verif-storage-root`\nhttps://github.com/ethereum-optimism/op-reth/actions/runs/12993424665/job/36235604096#step:1:32\n\nand configuring op-node fork `https://github.com/emhane/optimism.git` and branch `isthmus-withdrawals`\nhttps://github.com/ethereum-optimism/op-reth/actions/runs/12993424665/job/36235603683#step:4:1\nhttps://github.com/ethereum-optimism/op-reth/actions/runs/12993424665/job/36235603683#step:5:1\n\nthe op-node fork branch `isthmus-withdrawals` has merged https://github.com/ethereum-optimism/optimism/pull/13962, https://github.com/ethereum-optimism/optimism/pull/13973 and https://github.com/ethereum-optimism/optimism/pull/13933", "2025-01-27T17:06:34Z", "2025-01-27T17:06:55Z", "emhane", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6b81WF", "I_kwDODjvEJM6ns3Nc", "<img width=\"1343\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b942154f-3588-40a0-b3b5-bc7142760ea0\" />\n\nlike this ^", "2025-01-27T17:17:35Z", "2025-01-27T17:17:35Z", "emhane", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6b-R6K", "I_kwDODjvEJM6ns3Nc", "Is reth following the spec for the genesis block hash post isthmus? https://github.com/ethereum-optimism/specs/blob/main/specs/protocol/isthmus/exec-engine.md#genesis-block", "2025-01-27T20:13:31Z", "2025-01-27T20:13:31Z", "tynes", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cHh7d", "I_kwDODjvEJM6ns3Nc", "this is tricky in op-reth since, writing the alloc state to db requires that the genesis block already be inserted. cc @Rjected", "2025-01-28T14:48:39Z", "2025-01-28T14:48:39Z", "emhane", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cQG5a", "I_kwDODjvEJM6ns3Nc", "op-reth was missing hard fork path from genesis, fixed in https://github.com/ethereum-optimism/op-reth/pull/48", "2025-01-29T12:09:12Z", "2025-01-29T12:09:12Z", "emhane", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6b74T-", "I_kwDODjvEJM6nn9Mq", "Good luck with this work!", "2025-01-27T16:04:17Z", "2025-01-27T16:04:17Z", "tynes", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6b7_w0", "I_kwDODjvEJM6nn9Mq", "Hi @tynes  where should i start if Im going to that direction by introducing a flag ?", "2025-01-27T16:15:57Z", "2025-01-27T16:15:57Z", "bechain-foundation", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6b74_x", "I_kwDODjvEJM6njRqH", "This is a known issue, we are working on a fix. Thank you for opening this issue", "2025-01-27T16:05:17Z", "2025-01-27T16:05:17Z", "tynes", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cfU35", "I_kwDODjvEJM6mqx1I", "Closing this. We will need to implement this in a companion \"CheckAssertions\" contract which is future work. For now the checks in [DeployImplementations](https://github.com/ethereum-optimism/optimism/blob/a0341cc1e1c98884249f9d268b1d1245c663d8a7/packages/contracts-bedrock/scripts/deploy/DeployImplementations.s.sol#L246) will have to suffice.", "2025-01-30T20:02:20Z", "2025-01-30T20:02:20Z", "maurelian", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cA0-G", "I_kwDODjvEJM6lv42K", "Asterisc has been deprioritized in favour of focusing on Interop. It has been removed from the roadmap for now, and we'll revisit it in the future.", "2025-01-28T01:38:25Z", "2025-01-28T01:38:25Z", "pauldowman", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cVCXc", "I_kwDODjvEJM6loN41", "no longer planned", "2025-01-29T20:21:35Z", "2025-01-29T20:21:35Z", "maurelian", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6WyG8Q", "I_kwDODjvEJM6iM1KJ", "Depends on https://github.com/ethereum-optimism/optimism/issues/12614.", "2024-12-09T23:00:33Z", "2024-12-09T23:00:33Z", "mslipper", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cIqT8", "I_kwDODjvEJM6iM1KJ", "Initial work for this issue pushed to [this branch](https://github.com/ethereum-optimism/optimism/tree/ss/deployer-upgrade). @mslipper took the baton from there and will presumable push to a diff branch", "2025-01-28T16:38:11Z", "2025-01-28T16:38:11Z", "bitwiseguy", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cgRpL", "I_kwDODjvEJM6iM1KJ", "Closed by https://github.com/ethereum-optimism/optimism/pull/14028", "2025-01-30T22:12:44Z", "2025-01-30T22:12:44Z", "bitwiseguy", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6UJX_u", "I_kwDODjvEJM6dUqa4", "Also experiencing same issue with Holesky.\n\n`INFO [11-19|17:08:52.416] artifacts download progress              current=4096 total=54,132,933\nINFO [11-19|17:08:53.436] artifacts download progress              current=199,444 total=54,132,933\nINFO [11-19|17:08:54.436] artifacts download progress              current=2,957,312 total=54,132,933\nINFO [11-19|17:08:55.440] artifacts download progress              current=6,752,022 total=54,132,933\nINFO [11-19|17:08:56.441] artifacts download progress              current=10,553,110 total=54,132,933\nINFO [11-19|17:08:57.442] artifacts download progress              current=14,354,198 total=54,132,933\nINFO [11-19|17:08:58.445] artifacts download progress              current=17,991,446 total=54,132,933\nINFO [11-19|17:08:59.445] artifacts download progress              current=21,939,990 total=54,132,933\nINFO [11-19|17:09:00.446] artifacts download progress              current=25,691,926 total=54,132,933\nINFO [11-19|17:09:01.449] artifacts download progress              current=29,493,014 total=54,132,933\nINFO [11-19|17:09:02.456] artifacts download progress              current=33,326,870 total=54,132,933\nINFO [11-19|17:09:03.461] artifacts download progress              current=36,833,046 total=54,132,933\nINFO [11-19|17:09:04.464] artifacts download progress              current=40,863,510 total=54,132,933\nINFO [11-19|17:09:05.465] artifacts download progress              current=44,664,598 total=54,132,933\nINFO [11-19|17:09:06.495] artifacts download progress              current=47,023,894 total=54,132,933\nINFO [11-19|17:09:07.495] artifacts download progress              current=50,337,558 total=54,132,933\nINFO [11-19|17:09:08.502] artifacts download progress              current=53,380,886 total=54,132,933\nApplication failed: error getting superchain config: unsupported chain ID: 17000`", "2024-11-19T11:43:21Z", "2024-11-19T11:43:21Z", "priyanshuthapliyal", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6VPPnF", "I_kwDODjvEJM6dUqa4", "Hey, I am also having this issue with a local chain deployed with [builder-playground](https://github.com/flashbots/builder-playground)\n\n```\n$ cd builder-playground \n$ go run main.go\n```\n\nNow, use op-deployer:\n\n```\n$ go run main.go init --l1-chain-id 1337 --l2-chain-ids 2\n$ go run main.go apply --l1-rpc-url http://localhost:8545 --private-key <valid priv key>\n...\nApplication failed: error in pipeline stage apply: error getting superchain config: unsupported chain ID: 1337\n```\n", "2024-11-27T12:51:30Z", "2024-11-27T12:51:30Z", "ferranbt", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6VTv_x", "I_kwDODjvEJM6dUqa4", "This should be fixed now. Note that if you decide to deploy to a new chain like Holesky, you will get a big warning reminding you that you won't necessarily be running governance approved versions of the contracts. This is due to a limitation of our smart contract deployment process.", "2024-11-27T23:39:43Z", "2024-11-27T23:39:43Z", "mslipper", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6aPgdw", "I_kwDODjvEJM6dUqa4", "I see you've reverted the warning in https://github.com/ethereum-optimism/optimism/pull/13231/files. Is this a WONTFIX now?\n\nCan we have this enabled again, perhaps behind a very-very-scary configuration flag? Manually patching the deployer to allow for Holesky testing is annoying.", "2025-01-13T17:35:48Z", "2025-01-13T17:35:48Z", "TimTinkers", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cEH3m", "I_kwDODjvEJM6dUqa4", "I am still getting the same error that error `getting superchain config: unsupported chain ID: 17000`. I don't think this has been fixed?", "2025-01-28T08:52:25Z", "2025-01-28T08:52:25Z", "priyanshuthapliyal55", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cqjVE", "I_kwDODjvEJM6cqh2N", "Completed in https://github.com/ethereum-optimism/optimism/pull/13126", "2025-01-31T21:08:05Z", "2025-01-31T21:08:05Z", "unknown", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM5u9fqt", "I_kwDODjvEJM558oKV", "Make sure that you are using the version of foundry defined in `.foundryrc`. This issue does not exist in our CI", "2023-12-18T20:53:49Z", "2023-12-18T20:53:49Z", "tynes", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cVrZX", "I_kwDODjvEJM558oKV", "if anyone else runs into this problem (i did), the issue is resolved in: https://github.com/ethereum-optimism/optimism/pull/9018\n\nmake sure you have [jq](https://jqlang.github.io/jq/) installed", "2025-01-29T21:45:51Z", "2025-01-29T21:46:10Z", "mw2000", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM5uy2HV", "I_kwDODjvEJM553x9F", "The cause has been identified. Out of memory.\r\nsolc requires up to 12 G of memory.", "2023-12-16T13:46:28Z", "2023-12-16T13:46:56Z", "sameplacewei", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM5u0a7Y", "I_kwDODjvEJM553x9F", "Same error, but I allocated 20GB for the project\r\n\r\n```\r\nroot@ubuntu:/mnt/volume_sfo3_01/optimism# pnpm build\r\n\r\n> optimism@1.0.0 build /mnt/volume_sfo3_01/optimism\r\n> npx nx run-many --target=build\r\n\r\n\r\n    \u2714  nx run @eth-optimism/core-utils:build  [existing outputs match the cache, left as is]\r\n    \u2714  nx run @eth-optimism/common-ts:build  [existing outputs match the cache, left as is]\r\n\r\n    \u2716  nx run @eth-optimism/contracts-bedrock:build\r\n       > @eth-optimism/contracts-bedrock@0.16.2 prebuild /mnt/volume_sfo3_01/optimism/packages/contracts-bedrock\r\n       > ./scripts/verify-foundry-install.sh\r\n\r\n       Using foundry version: forge 0.2.0 (88ae503 2023-12-17T00:18:48.520002484Z)\r\n\r\n       > @eth-optimism/contracts-bedrock@0.16.2 build /mnt/volume_sfo3_01/optimism/packages/contracts-bedrock\r\n       > forge build\r\n\r\n       Compiling 1 files with 0.5.17\r\n       Compiling 323 files with 0.8.15\r\n       Solc 0.5.17 finished in 63.82ms\r\n       Compiling 127 files with 0.8.19\r\n       Solc 0.8.19 finished in 298.54s\r\n       Error:\r\n       solc exited with signal: 9 (SIGKILL)\r\n       <empty output>\r\n       \u2009ELIFECYCLE\u2009 Command failed with exit code 1.\r\n\r\n\r\n\r\n \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\r\n >  NX   Ran target build for 8 projects (5m)\r\n\r\n    \u2714    2/3 succeeded [2 read from cache]\r\n\r\n    \u2716    1/3 targets failed, including the following:\r\n         - nx run @eth-optimism/contracts-bedrock:build\r\n\r\n   View structured, searchable error logs at https://nx.app/runs/HwtY3Ac3Nt\r\n\r\n\u2009ELIFECYCLE\u2009 Command failed with exit code 1.\r\n```", "2023-12-17T17:51:46Z", "2023-12-17T17:52:26Z", "69popovvlad", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM5u3e4T", "I_kwDODjvEJM553x9F", "Environment\r\n```\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 23.10\r\nRelease:        23.10\r\nCodename:       mantic\r\n\r\nFilesystem      Size  Used Avail Use% Mounted on\r\n/dev/vda1        87G  6.8G   80G   8% /\r\n\r\nDependency | Minimum | Actual\r\ngit          2         2.40.1\r\ngo           1.21      1.21.5\r\nnode         20        20.10.0\r\npnpm         8         8.12.1\r\nfoundry      0.2.0     0.2.0\r\nmake         3         4.3\r\njq           1.6       1.6\r\ndirenv       2         2.32.1\r\n```\r\n\r\nCommit:\r\n```\r\ncommit eb0d89ddaa57e6335d92dfa379f6036847c5cc18 (HEAD, tag: op-proposer/v1.4.0-rc.1, tag: op-node/v1.4.0, tag: op-batcher/v1.4.0)\r\nAuthor: Joshua Gutow <jgutow@oplabs.co>\r\nDate:   Mon Dec 11 14:34:53 2023 -0800\r\n\r\n    op-node: Activate Delta on testnets\r\n```\r\n\r\nCommands used before error:\r\n```\r\ngit clone https://github.com/ethereum-optimism/optimism.git\r\ncd optimism\r\ngit checkout op-node/v1.4.0\r\npnpm install\r\nmake op-node op-batcher op-proposer\r\npnpm build\r\n```\r\n\r\nError:\r\n```\r\n\r\n> optimism@1.0.0 build /root/optimism\r\n> npx nx run-many --target=build\r\n\r\n\r\n    \u2714  nx run @eth-optimism/core-utils:build  [existing outputs match the cache, left as is]\r\n    \u2714  nx run @eth-optimism/common-ts:build  [existing outputs match the cache, left as is]\r\n\r\n    \u2716  nx run @eth-optimism/contracts-bedrock:build\r\n       > @eth-optimism/contracts-bedrock@0.16.2 prebuild /root/optimism/packages/contracts-bedrock\r\n       > ./scripts/verify-foundry-install.sh\r\n\r\n       Using foundry version: forge 0.2.0 (477b345 2023-12-18T00:22:09.434601489Z)\r\n\r\n       > @eth-optimism/contracts-bedrock@0.16.2 build /root/optimism/packages/contracts-bedrock\r\n       > forge build\r\n\r\n       Compiling 1 files with 0.5.17\r\n       Compiling 323 files with 0.8.15\r\n       Solc 0.5.17 finished in 47.93ms\r\n       Compiling 127 files with 0.8.19\r\n       Solc 0.8.19 finished in 276.81s\r\n       Error:\r\n       solc exited with signal: 9 (SIGKILL)\r\n       <empty output>\r\n       \u2009ELIFECYCLE\u2009 Command failed with exit code 1.\r\n\r\n\r\n\r\n \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\r\n >  NX   Ran target build for 8 projects (5m)\r\n\r\n    \u2714    2/3 succeeded [2 read from cache]\r\n\r\n    \u2716    1/3 targets failed, including the following:\r\n         - nx run @eth-optimism/contracts-bedrock:build\r\n\r\n   View structured, searchable error logs at https://nx.app/runs/OabAAeY9az\r\n\r\n\u2009ELIFECYCLE\u2009 Command failed with exit code 1.\r\n```\r\n\r\n", "2023-12-18T10:23:15Z", "2023-12-18T10:23:15Z", "69popovvlad", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM5u_kAh", "I_kwDODjvEJM553x9F", "Thanks for the report. Going to look into this.", "2023-12-19T05:41:29Z", "2023-12-19T05:41:29Z", "smartcontracts", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM5vBGZK", "I_kwDODjvEJM553x9F", "@smartcontracts \r\nThis is my mistake, I thought 12GB of disk space, but it was said about RAM.\r\nThis is of course inconvenient for a build on a dedicated server, but I got it done. Thank you", "2023-12-19T11:07:22Z", "2023-12-19T11:07:22Z", "69popovvlad", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM5vD144", "I_kwDODjvEJM553x9F", "I was able to build correctly with a docker container with 8gb ram\r\n\r\n```\r\nroot@c3ef57f59f63:/optimism# free\r\n               total        used        free      shared  buff/cache   available\r\nMem:         8032524      808036     5546772        2620     1677716     6873572\r\nSwap:        1048572       80472      968100\r\n```\r\n\r\nGoing to try a dedicated server with 8gb ram to see what happens\r\n", "2023-12-19T18:26:01Z", "2023-12-19T18:26:01Z", "smartcontracts", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM5vD2_t", "I_kwDODjvEJM553x9F", "@mslipper says this is a \"known issue\" with the solidity compiler when compiling many files at the same time. Going to see if foundry can potentially address this by limiting the number of files per compiler run.", "2023-12-19T18:29:43Z", "2023-12-19T18:29:43Z", "smartcontracts", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM58_qiy", "I_kwDODjvEJM553x9F", "> I was able to build correctly with a docker container with 8gb ram\r\n> \r\n> ```\r\n> root@c3ef57f59f63:/optimism# free\r\n>                total        used        free      shared  buff/cache   available\r\n> Mem:         8032524      808036     5546772        2620     1677716     6873572\r\n> Swap:        1048572       80472      968100\r\n> ```\r\n> \r\n> Going to try a dedicated server with 8gb ram to see what happens\r\n\r\nI am getting this issue as well an I only have 8GB of memory, how did you get it to build?", "2024-05-06T22:57:18Z", "2024-05-06T22:57:18Z", "SquilliamX", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6GLqLl", "I_kwDODjvEJM553x9F", "@smartcontracts @mslipper I am getting the same issue when trying to build the contracts locally (8GB ram):\r\n\r\n```\r\ncarter@laptop:~/optimism/packages/contracts-bedrock$ forge build\r\nMissing dependencies found. Installing now...\r\n\r\n...\r\n\r\n[\u2806] Compiling...\r\n[\u2830] Compiling 16 files with Solc 0.8.19\r\n[\u2814] Compiling 394 files with Solc 0.8.15\r\n[\u2830] Compiling 138 files with Solc 0.8.25\r\n[\u2822] Solc 0.8.19 finished in 2.79s\r\n[\u280a] Solc 0.8.25 finished in 35.60s\r\nError: \r\nsolc exited with signal: 9 (SIGKILL)\r\n```\r\n\r\nIs there a way around this?", "2024-07-25T18:59:13Z", "2024-07-25T18:59:13Z", "pegahcarter", "2025-08-31 01:44:36"]
["IC_kwDODjvEJM6cVCsc", "I_kwDODjvEJM553x9F", "![Image](https://github.com/user-attachments/assets/b9f0ba49-11d9-45aa-b70a-27efc8ab1d26)\nanyone who got this error and knows how to fix it i tried  online i  couldn't find  how to fix it just explanation of what it implies ", "2025-01-29T20:22:15Z", "2025-01-29T20:22:15Z", "prey801", "2025-08-31 01:44:36"]
["IC_kwDOMMiGhs6V02NV", "I_kwDOMMiGhs6hv0b6", "@tremarkley I want to take on this. Appreciate if assigned to me. ", "2024-12-03T06:20:08Z", "2024-12-03T06:20:08Z", "s29papi", "2025-08-31 01:44:59"]
["IC_kwDOMMiGhs6V901X", "I_kwDOMMiGhs6hv0b6", "@s29papi assigned to you", "2024-12-04T02:12:30Z", "2024-12-04T02:12:30Z", "tremarkley", "2025-08-31 01:44:59"]
["IC_kwDOMMiGhs6W9-qI", "I_kwDOMMiGhs6hv0b6", "Hey @tremarkley,\n\nI've been trying to trace back from the error point to identify a good starting point in the code to address this issue, but the nature of the bug has made it quite challenging. However, I am still eager to tackle it.\n\nRegarding your comment about the potential race condition involving different timestamps between the two L2s, I wanted to ask if you think this could be related to the processEventLog function in the interop indexer. I noticed that the source L2 timestamp is passed there, and I'm curious if you believe this could be a contributing factor.\n\nCould you provide more details on where you think this issue might be originating from? I would like to review the relevant code to see if I can spot the problem directly. Any insights you can share would be incredibly helpful in guiding my investigation.\n\nThank you!", "2024-12-10T20:34:47Z", "2024-12-10T20:34:47Z", "s29papi", "2025-08-31 01:44:59"]
["IC_kwDOMMiGhs6b3JO9", "I_kwDOMMiGhs6hv0b6", "Hi, I am encountering the same issue and was wondering if anyone is actively working on it. Are there any known solutions or temporary fixes? Thanks in advance!", "2025-01-27T06:14:40Z", "2025-01-27T06:14:40Z", "0xBhumi", "2025-08-31 01:44:59"]
["IC_kwDOMMiGhs6cL59K", "I_kwDOMMiGhs6hv0b6", "@s29papi sorry for the delay here and appreciate you volunteering. There has been a change at the protocol level and the message validation logic for interop has been updated to take place offchain with https://github.com/ethereum-optimism/optimism/issues/13658. Closing this for now since this race condition was due to an issue with the timestamp check at the protocol level that used to take place inside `CrossL2Inbox`. Now that the timestamp check is being done offchain, supersim will need to be updated with the latest version of `CrossL2Inbox` and the timestamp check will need to be updated to take place in `opsimulator.go`. I opened https://github.com/ethereum-optimism/supersim/issues/328 and our team will be triaging this", "2025-01-29T01:01:11Z", "2025-01-29T01:01:11Z", "tremarkley", "2025-08-31 01:44:59"]
["IC_kwDOKIsnqM6chC89", "I_kwDOKIsnqM6oNOaV", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/485 ", "2025-01-30T23:23:46Z", "2025-01-30T23:23:46Z", "blmalone", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6crJ_G", "I_kwDOKIsnqM6oMAiR", "A large part of this is slow compilation due to via-ir usage. Without via-ir you get the following error message:\n\n```\nCompiler run failed:\nError: Unimplemented feature (/solidity/libsolidity/codegen/ArrayUtils.cpp:228):Copying of type struct AddressRegistry.Superchain memory[] memory to storage not yet supported.\nUnimplementedFeatureError: Copying of type struct AddressRegistry.Superchain memory[] memory to storage not yet supported.\n```\n\nBut we should fix that to not rely on that feature", "2025-01-31T23:23:36Z", "2025-01-31T23:23:36Z", "mds1", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6crXKy", "I_kwDOKIsnqM6oMAiR", "Build times resolved in https://github.com/ethereum-optimism/superchain-ops/pull/528\n\nNext culprit should be caching dependency installs so we don't have to reinstall them between CI runs", "2025-02-01T00:24:35Z", "2025-02-01T00:24:35Z", "mds1", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6cKyly", "I_kwDOKIsnqM6nYFfn", "We should also have a variant that does not pin to a block but always has deterministic calldata, such as an ownership transfer. ", "2025-01-28T21:17:25Z", "2025-01-28T21:17:25Z", "blmalone", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6cL5Hs", "I_kwDOKIsnqM6nYFfn", "the call it will make is deterministic, but the calldata depends on the nonce, so unless we have a way to set the nonce, the entire transaction calldata is determined based on the safe nonce.\n", "2025-01-29T00:57:46Z", "2025-01-29T00:57:46Z", "ElliotFriedman", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6cDV8Z", "I_kwDOKIsnqM6nYFGf", "I think it makes more sense to focus on integration testing as unit testing would require making our own superchain-registry entry and mocks for everything.\n\nIntegration testing will allow us to test everything we need without making mock code and testing the many different paths in the multisig task file.", "2025-01-28T07:36:17Z", "2025-01-28T07:36:17Z", "ElliotFriedman", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6cJIW-", "I_kwDOKIsnqM6nYFGf", "I think we should still have a `MultiSigTask.t.sol` file before mainnet. ", "2025-01-28T17:29:02Z", "2025-01-28T17:29:02Z", "blmalone", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6cNBib", "I_kwDOKIsnqM6nYFGf", "@blmalone PR is up here https://github.com/ethereum-optimism/superchain-ops/pull/497", "2025-01-29T03:41:05Z", "2025-01-29T03:41:05Z", "ElliotFriedman", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6bocDB", "I_kwDOKIsnqM6mogJX", "We should be able to close this now that PT: 1 merged https://github.com/ethereum-optimism/superchain-ops/pull/467", "2025-01-23T21:37:44Z", "2025-01-23T21:37:44Z", "ElliotFriedman", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6bocaU", "I_kwDOKIsnqM6mZNyY", "Done in part 2 https://github.com/ethereum-optimism/superchain-ops/pull/479/files#diff-22f669fd06475c30bd313a5b033feb6f3502f2ba1f98e74ce6683d08dccb6274R185-R203", "2025-01-23T21:38:43Z", "2025-01-23T21:38:43Z", "ElliotFriedman", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6YKQO8", "I_kwDOKIsnqM6jzrg8", "I'm tackling a lot of the safety scaffolding around the multisig state diff pre and post proposal. Once that is across the line I'm going to investigate this.", "2024-12-19T05:33:31Z", "2024-12-19T05:33:31Z", "ElliotFriedman", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6cg7uT", "I_kwDOKIsnqM6jzrg8", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/479 ", "2025-01-30T23:18:36Z", "2025-01-30T23:18:36Z", "blmalone", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6W_V04", "I_kwDOKIsnqM6ixZkD", "This is great, I was going to ask why we use the gnosis safe compatible JSON at this point if we never use the gnosis safe UI for signing. I think that we should optimize the file to be human readable, so a TOML file that includes a list of calls would be great. There should be an option to pass in raw calldata or to define an ABI encoding and the arguments to be encoded. Ideally this is written up as a design doc and shared with the protocol team before any implementation, as getting feedback from users is important", "2024-12-10T23:12:25Z", "2024-12-10T23:12:25Z", "tynes", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6XR7GK", "I_kwDOKIsnqM6ixZkD", "the way we are thinking about this from a high level is having JSON file structures for different kinds of templatized tasks. We weren't planning on making things gnosis safe UI compatible for the JSON.\n\nideally the templates would not take in raw calldata to make things as human readable as possible and allow for a standardized set of checks that are specific to each task type, e.g. bounds checking.", "2024-12-12T07:55:05Z", "2024-12-12T07:55:05Z", "ElliotFriedman", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6XR7Iz", "I_kwDOKIsnqM6ixZkD", "queued", "2024-12-12T07:55:12Z", "2024-12-12T07:55:12Z", "ElliotFriedman", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6XW1n3", "I_kwDOKIsnqM6ixZkD", "@tynes the UX that we discussed (and ran by @sebastianst and @ajsutton, though mainly verbally, so we can write a design doc to ensure alignment) is that repeatable tasks have a template. Setting ProtocolVersions is a good example: The template is a single solidity file that defines the calls, and the validation checks. Those calls and checks are the same every time. Then, there are inputs (e.g. what should the new recommended protocol version be) that are read in from a TOML file. So creating a new task from a template would involve scaffolding the directory (like we already do), adding a TOML file with your input parameters, and running the solidity file from the template directory.\n\nIt sounds like what you are describing is for bespoke tasks where you need custom calls and calldata. For those the UX will essentially be writing a regular forge script that holds the calls to make and the validation checks", "2024-12-12T15:51:50Z", "2024-12-12T15:51:50Z", "mds1", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6cg6sf", "I_kwDOKIsnqM6ixZkD", "Closed with: https://github.com/ethereum-optimism/superchain-ops/pull/479 ", "2025-01-30T23:17:50Z", "2025-01-30T23:17:50Z", "blmalone", "2025-08-31 01:45:02"]
["IC_kwDOKIsnqM6XR5yY", "I_kwDOKIsnqM6ixUNG", "in flight", "2024-12-12T07:52:11Z", "2024-12-12T07:52:11Z", "ElliotFriedman", "2025-08-31 01:45:03"]
["IC_kwDOKIsnqM6cg6JD", "I_kwDOKIsnqM6ixUNG", "This was closed with https://github.com/ethereum-optimism/superchain-ops/pull/406. ", "2025-01-30T23:17:21Z", "2025-01-30T23:17:21Z", "blmalone", "2025-08-31 01:45:03"]
["IC_kwDOEf1bQc6dPUJu", "I_kwDOEf1bQc6o6wDE", "cc @opmxwell ty!", "2025-02-05T21:15:29Z", "2025-02-05T21:15:29Z", "opjulian", "2025-08-31 01:46:10"]
["IC_kwDOEf1bQc6dZo2f", "I_kwDOEf1bQc6o6wDE", "resolved here @alexsotodigital !!\n\nhttps://www.notion.so/Token-House-Governance-Hub-decae75a0fa248e38f969abe8edef565", "2025-02-06T18:58:57Z", "2025-02-06T18:58:57Z", "opmxwell", "2025-08-31 01:46:10"]
["IC_kwDOKIsnqM6dQdpe", "I_kwDOKIsnqM6o7XdW", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/558 ", "2025-02-06T00:30:22Z", "2025-02-06T00:30:22Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6dYfNS", "I_kwDOKIsnqM6oulHP", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/555 ", "2025-02-06T16:49:37Z", "2025-02-06T16:49:37Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6dQEU5", "I_kwDOKIsnqM6oWOU6", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/538 ", "2025-02-05T23:07:56Z", "2025-02-05T23:07:56Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6c3Q3u", "I_kwDOKIsnqM6oWLWW", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/531 ", "2025-02-03T18:17:40Z", "2025-02-03T18:17:40Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6c3Z25", "I_kwDOKIsnqM6oTov-", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/532 ", "2025-02-03T18:36:06Z", "2025-02-03T18:36:06Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6cDbHC", "I_kwDOKIsnqM6mgIrS", "Is this a card to wire the scripts up to the justfiles, or is this a docs update?", "2025-01-28T07:49:53Z", "2025-01-28T07:49:53Z", "ElliotFriedman", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6cHm3Z", "I_kwDOKIsnqM6mgIrS", "This will require some work wiring the new commands into the existing just files.  e.g. https://github.com/ethereum-optimism/superchain-ops/blob/main/single.just and https://github.com/ethereum-optimism/superchain-ops/blob/main/nested.just", "2025-01-28T14:55:54Z", "2025-01-28T15:32:15Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6cKvet", "I_kwDOKIsnqM6mgIrS", "This is likely going to mean we create new just files inside the `src` directory, with the intention of replacing the current ones. ", "2025-01-28T21:09:39Z", "2025-01-28T21:09:39Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6c3b48", "I_kwDOKIsnqM6mgIrS", "I am working on this issue.", "2025-02-03T18:40:17Z", "2025-02-03T18:40:17Z", "prat-gpt", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6dMem5", "I_kwDOKIsnqM6mgIrS", "Partially resolved by: https://github.com/ethereum-optimism/superchain-ops/pull/539 ", "2025-02-05T15:45:17Z", "2025-02-05T15:45:17Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6aubcK", "I_kwDOKIsnqM6mZQNy", "This is being implemented by @ElliotFriedman ", "2025-01-16T14:22:40Z", "2025-01-16T14:22:40Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6aycB7", "I_kwDOKIsnqM6mZQNy", "1. create upstream PR in foundry with the L2 chainids and RPC URL's we need\n2. remove the chainids from the Constants file", "2025-01-16T21:22:27Z", "2025-01-16T21:22:27Z", "ElliotFriedman", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6bUq5Q", "I_kwDOKIsnqM6mZQNy", "Created upstream PR https://github.com/foundry-rs/forge-std/pull/646", "2025-01-21T22:40:42Z", "2025-01-21T22:40:42Z", "ElliotFriedman", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6c4wcH", "I_kwDOKIsnqM6mZQNy", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/527/files ", "2025-02-03T21:33:07Z", "2025-02-03T21:33:07Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6Pg9NU", "I_kwDOKIsnqM6ZdZOU", "This sounds like a perfect use case for FPS! I don't have context on how you guys structure your repositories or tests. Would you be able to summarize things here so I can make a recommendation on architecture?", "2024-10-11T16:46:50Z", "2024-10-11T16:46:50Z", "ElliotFriedman", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6PiXo2", "I_kwDOKIsnqM6ZdZOU", "Yes definitely! So I recall last time we chatted about this a tricky part was that the upgrade transactions are executed in this repo, but our test suite is in the monorepo (https://github.com/ethereum-optimism/optimism/tree/develop/packages/contracts-bedrock). However this repo does already have the monorepo as a [dependency](https://github.com/ethereum-optimism/superchain-ops/blob/808004cd933e0bfa36e6d9124701e296639a6d41/.gitmodules#L7-L10), so that might help\r\n\r\nFirst, some background on the monorepo setup:\r\n- The core test suite is a standard forge test suite. We also have a few kontrol proofs, and a few differential ffi tests to ensure go and solidity implementations match.\r\n- We run all tests (including the go ffi tests) with `just test`, and the proofs are run with `just test-kontrol`.\r\n- Only running `just test` would be be a great start and kontrol can be added later.\r\n\r\nNow on this repo, superchain-ops\r\n- \"Tasks\"are transactions to broadcast, like contract upgrade transactions or transactions that just change a config value. This is essentially the same as what Maker calls \"spells\"\r\n- All tasks live in a directory of `tasks/{L1-network}/{taskName}`, and contain `input.json` bundles that define the transaction to be sent, along with a single solidity script that simulates the task based on that JSON bundle. \r\n- Some tasks are \"single\" meaning it's a regular safe that needs to send the transaction, like the 5/7 Foundation Safe. Other tasks are \"nested\" meaning it's a 2/2 Safe that needs to send the transaction (where each signer on that 2/2 is a Safe, specifically the Foundation and the Security Council)\r\n- The core logic to parse the bundle and simulate the transaction happens using the https://github.com/base-org/contracts/ dependency. We run the simulation in foundry within that dep, and have a `postCheck` hook that runs a bunch of assertions on the post-simulation state\r\n\r\nI would say it's safe to consider most options on the table. Some examples I thought about:\r\n- Does changing the shape of the `input.json` bundles help?\r\n- Does replacing `base-org/contracts` with FPS help?\r\n- Maker executes tests against their proposals like [this](https://github.com/makerdao/spells-mainnet/blob/master/src/DssSpell.t.sol), by basically defining a static test suite that always runs. This could also be an option for us, i.e. parameterize the key tests in the monorepo, that way we can import them here and run them", "2024-10-11T21:37:01Z", "2024-10-11T21:37:01Z", "mds1", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6X8ipY", "I_kwDOKIsnqM6ZdZOU", "Our current thoughts on how to run monorepo forge tests in the superchain ops repo post task execution:\n\n1. setup anvil mainnet fork\n2. run the tasks that are in non terminal state, applying the changes to the anvil fork\n3. change directories into the monorepo submodule\n4. run the monorepo just test commands against this changed state on the anvil fork. [`test-upgrade-against-anvil`](https://github.com/ethereum-optimism/optimism/pull/13323/files#diff-af1134a1914156ce009809f863f08d6d66f1d5c0921759283793e236c85c7134R85)", "2024-12-17T18:09:56Z", "2024-12-17T18:09:56Z", "ElliotFriedman", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6beZak", "I_kwDOKIsnqM6ZdZOU", "After some consideration, we've landed on one possible solution to deliver this feature. \n\nAt a high-level, we want to be able to grab the resultant state from the task that ran and then use that state inside the monorepos tests.\n\nTo do this, we can use a combination of foundry cheatcodes. Firstly, we can use [`dumpState(string calldata pathToStateJson)`](https://github.com/foundry-rs/forge-std/blob/master/src/Vm.sol#L1858). This produces a JSON file that contains allocs created during the duration of the script. e.g. \n\n```json\n{\n    \"0x5aadfb43ef8daf45dd80f4676345b7676f1d70e3\": {\n        \"nonce\": \"0x1\",\n        \"balance\": \"0x0\",\n        \"code\": \"0x68.......\",\n        \"storage\": {\n            \"0x0000000000000000000000000000000000000000000000000000000000000000\": \"0x0000000000000000000000000000000000000000000000000000000000000001\",\n            \"0x0000000000000000000000000000000000000000000000000000000000000001\": \"0x68656c6c6f00000000000000000000000000000000000000000000000000000a\"\n        }\n    }\n}\n```\n\nWith these allocs, we can _then_ use another foundry cheat code, namely [`loadAllocs(string calldata pathToAllocsJson)`](https://github.com/foundry-rs/forge-std/blob/master/src/Vm.sol#L1875). This loads in everything in the json file (_addresses, code, nonces, balances, state etc_). e.g. \n\n```solidity\nvm.loadAllocs(\"../state3.json\");\nconsole.log(foo.val()); // would've reverted if loadAllocs had not have been invoked.\n```\nA note worth mentioning is that before we load allocations using the JSON file, we may want to do some processing to the file. As we know, deployments won't occur in superchain-ops tasks, so we may be able to remove any allocs that contain a `code` property, for example e.g. `MultisigTask` or `AddressRegistry`. \n\n\n---\nEach test in the monorepo has the following structure: \n![Image](https://github.com/user-attachments/assets/1143d4b2-3f22-40c8-bf07-898f949a05ee)\n\nWe will want to integrate our new logic into the above architecture. We don't want our new logic to live within [`Deploy.s.sol`](https://github.com/ethereum-optimism/optimism/blob/de3865c0a2d32821faa19165ff1a3232589b90c1/packages/contracts-bedrock/scripts/deploy/Deploy.s.sol#L4) because we _don't_ want to deploy a new chain (proxies, implementation contracts etc). The reason being that all superchain-ops tasks work on existing chain states and in _most_ (if not all) cases, they don't deploy contracts as part of the task (think setting a new gas limit on an already existing SystemConfig contract). \n\nOur new logic will want to leverage [`ForkLive.s.sol`](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/test/setup/ForkLive.s.sol). Currently, `ForkLive.s.sol` reads addresses for a given network (mainnet/sepolia) from the superchain-registry and stores then so that they can be accessed later. This only grabs addresses for one chain at a time (e.g. base, zora or op), we may want to update this logic or loop at a higher level for all the chains in the superchain-ops task. \n\nWe will need to update many files, not limited to: \n- https://github.com/ethereum-optimism/superchain-ops/blob/main/single.just\n- https://github.com/ethereum-optimism/superchain-ops/blob/main/nested.just\n- https://github.com/ethereum-optimism/superchain-ops/blob/main/.circleci/config.yml\n- https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/justfile#L72\n- https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/test/setup/ForkLive.s.sol\n- https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/test/setup/Setup.sol\n", "2025-01-22T22:57:47Z", "2025-01-22T22:57:47Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6bnqwK", "I_kwDOKIsnqM6ZdZOU", "Once we dump the state we will need to do some processing on it. We know this because as part of the `MultisigTask` scripts, we deploy various contracts. They include the `AddressRegistry.sol` and the script contract itself. In the _actual_ execution of the task, we know that these contracts are **not** deployed. Therefore, removing them is important to keep the allocs from the simulation as close to the production allocs as possible. \n\nIn order to correctly identify the allocs we need to remove from the dumped state, we want to make sure that the deployed contracts have a deterministic address. \nWe have a few options to realize this. The leading approach is to use [create3](https://github.com/0xsequence/create3) ([example](https://github.com/ethereum-optimism/optimism/blob/47c00f9975d2455d52263c0e54d52eb4b483b288/packages/contracts-bedrock/src/L2/OptimismSuperchainERC20Factory.sol#L60) of this being used in the monorepo). This is give us a deterministic address of the contracts that we deploy during a task. For the script contract itself, it's likely that this address is always the same and derived from a default forge address and nonce. \n", "2025-01-23T19:41:43Z", "2025-01-23T19:41:43Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6b9K-Q", "I_kwDOKIsnqM6ZdZOU", "https://github.com/ethereum-optimism/superchain-ops/issues/343#issuecomment-2610867210 \nWe're going to punt on state processing. For now, we don't know if we need this so we will delay it for now. ", "2025-01-27T17:50:41Z", "2025-01-27T17:50:41Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6cKEdc", "I_kwDOKIsnqM6ZdZOU", "For performance reasons I would suggest only running the monorepo tests in CI on some schedule (e.g. every 4 hours) to avoid having to wait multiple minutes to simulate and execute tasks (e.g. if we ran tests as part of the simulate command)", "2025-01-28T19:29:37Z", "2025-01-28T19:29:37Z", "mds1", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6cKO0O", "I_kwDOKIsnqM6ZdZOU", "> For performance reasons I would suggest only running the monorepo tests in CI on some schedule (e.g. every 4 hours) to avoid having to wait multiple minutes to simulate and execute tasks (e.g. if we ran tests as part of the simulate command)\n\nWhat does this mean for CI integration testing?\n\nAre you saying we shouldn't run monorepo tests everytime a new PR is opened on superchain ops? If so, what would you like the algorithm to be for running these tests in the superchain ops repo?", "2025-01-28T19:52:12Z", "2025-01-28T19:52:12Z", "ElliotFriedman", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6dCciv", "I_kwDOKIsnqM6ZdZOU", "> For performance reasons I would suggest only running the monorepo tests in CI on some schedule (e.g. every 4 hours) to avoid having to wait multiple minutes to simulate and execute tasks (e.g. if we ran tests as part of the simulate command)\n\nDigging into the circle ci docs, what you are asking for is possible, it will take a bit of configurations that I won't be able to add as an external contributor.\n\nThe architecture you are describing is possible with the following steps:\n\n1. when a new PR is opened a cron job runs and calls the circleci API which creates a new scheduled pipeline against that PR to run integration tests\n2. when a PR is closed, that same cron job runs and calls the circleci API and closes that cron job\n3. while the pr is open, the circleci scheduled pipeline runs on some predefined schedule\n\nWe can achieve the goal using circleci scheduled pipelines.\n\nMy recommendation would be to add this job to run against every commit on PR's. We could filter it down further by changes to toml or task config files, so that we don't waste rpc API credits on commits that don't make changes to the tasks.\n\nIf you mean we just need to run the integration tests against the main branch, making a single task that just runs against main every x hours is pretty straightforward.\n\nWhen I asked the circleci docs bot about how we can achieve this objective it gave the following answer:\n`--------------------------------------------------------------------------------------------------------------`\n\nBased on the knowledge sources provided, it is not directly possible to create a single scheduled job that runs against all PR branches on a schedule in CircleCI. However, there are some approaches you can consider to achieve similar functionality:\n\n1. Scheduled Pipelines: You can create a scheduled pipeline that runs on a specific branch, but it won't automatically run against all PR branches. As mentioned in the [Scheduled pipelines documentation](https://circleci.com/docs/scheduled-pipelines/), \"A scheduled pipeline can only be configured for one branch. If you need to schedule for two branches, you would need to set up two schedules.\"\n\n2. Dynamic Branch Selection: You could create a scheduled pipeline that dynamically selects branches to run against. This approach is described in the [How can I dynamically select branches for a scheduled pipeline?](https://support.circleci.com/hc/en-us/articles/15334848949403-How-can-I-dynamically-select-branches-for-a-scheduled-pipeline) article. You would need to create a custom script to get a list of PR branches and update the scheduled pipeline to run against those branches.\n\n3. Workflow Filtering: You can use workflow filtering to run jobs on specific branches or tags. However, this is typically used for running jobs when commits are pushed, not on a schedule for all PR branches.\n\n4. API-based Approach: You could create a separate scheduled job that uses the CircleCI API to trigger builds for all open PR branches. This would require custom scripting and is not a built-in feature of CircleCI.\n\nIt's important to note that CircleCI's default behavior is to run builds on every commit pushed to a branch, unless you've specifically configured it otherwise. As stated in a [forum post](https://discuss.circleci.com/t/9874): \"All commits on all branches build normally, unless you have blacklisted or whitelisted them.\"\n\nTo achieve your specific goal, you might need to combine scheduled pipelines with custom scripting to dynamically update the branches being tested. This would require additional setup and is not a straightforward configuration in CircleCI.\n", "2025-02-04T17:48:08Z", "2025-02-04T17:48:08Z", "ElliotFriedman", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6dQEzg", "I_kwDOKIsnqM6ZdZOU", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/555 ", "2025-02-05T23:09:26Z", "2025-02-05T23:09:26Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6WKfgk", "I_kwDOKIsnqM6ZdTbe", "Hi @mds1 I had like to take this issue.", "2024-12-05T06:40:26Z", "2024-12-05T06:40:26Z", "Dhir0808", "2025-08-31 01:46:32"]
["IC_kwDOKIsnqM6dQEaV", "I_kwDOKIsnqM6ZdTbe", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/538 ", "2025-02-05T23:08:14Z", "2025-02-05T23:08:14Z", "blmalone", "2025-08-31 01:46:32"]
["IC_kwDOLB-lzc6dORRE", "I_kwDOLB-lzc6olmEW", "I've already added this [here](https://github.com/ethereum-optimism/specs/blob/main/specs/protocol/isthmus/overview.md?plain=1#L22-L25).\n\nDo you think we should move this elsewhere to make it more visible or clear?", "2025-02-05T18:54:47Z", "2025-02-05T18:54:47Z", "refcell", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6amM4-", "I_kwDOLB-lzc6mURUV", "I took a swing at this one here: https://github.com/bluealloy/revm/pull/2000", "2025-01-15T18:51:50Z", "2025-01-15T18:51:50Z", "meyer9", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6a5csO", "I_kwDOLB-lzc6mURUV", "This needs:\n- a spec PR to explicitly state this EIP is part of the upgrade\n- monorepo test that the precompile is usable when the op-stack fork is active (covers at least op-geth, but op-reth testing would also be nice)\n- op-program change to override the new BLS precompiles, and hook them up to the preimage oracle, like we have done with other precompiles.\n\n\nAlso warning: if I understood correctly from ACD calls, there are some G2mul precompiles of the BLS precompile set that were being removed, and still seem present in devnet 5, but not in final L1 Pectra scope, due to them being considered unnecessary/duplicate. We should not include these last-minute removed precompiles in L2.", "2025-01-17T16:38:07Z", "2025-01-17T16:38:07Z", "protolambda", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6a6U3e", "I_kwDOLB-lzc6mURUV", "The removal of MUL precompiles, and pricing changes suggested in the ACD calls is implemented here in geth: https://github.com/ethereum/go-ethereum/pull/30978\n\nWe'll need to pull this into op-geth once merged.", "2025-01-17T18:39:52Z", "2025-01-17T18:39:52Z", "meyer9", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6a6y1s", "I_kwDOLB-lzc6mURUV", "We also need a performance analysis of any introduced precompiles to ensure that derivation is fault provable by cannon/op-program (and probably asterisc/kona too). Similar to the Cannon benchmarking performed by Base for the gas limit increase.\n\nIt should answer questions like: Do BLS12-381 operations require too many steps to execute under cannon? How much memory is used by the VM in the worst case execution?", "2025-01-17T20:02:54Z", "2025-01-17T20:02:54Z", "Inphi", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6bUUIY", "I_kwDOLB-lzc6mURUV", "@Inphi I believe we can utilize the L1 precompiles for this. The reason we needed to benchmark the `P256VERIFY` precompile is that it isn't implemented on L1, so the fault proof program has to execute it. For this, we're implementing Isthmus as a fast follow-up to Pectra, so the BLS operations can just be passed through. Let me know if I'm missing something though!", "2025-01-21T21:39:42Z", "2025-01-21T21:39:42Z", "meyer9", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6bfchm", "I_kwDOLB-lzc6mURUV", "Update on this work:\n\n- Added support to op-geth here: https://github.com/ethereum-optimism/op-geth/pull/472\n- Added test to monorepo here: https://github.com/ethereum-optimism/optimism/pull/13934\n\nThese are blocked by some fixes to get Isthmus/geth e2e tests working:\n- https://github.com/ethereum-optimism/op-geth/pull/471\n- https://github.com/ethereum-optimism/optimism/pull/13933\n\nPlanning on working on fault proofs accelerated precompile tests soon, and testing Reth precompiles as well later this week.", "2025-01-23T02:24:27Z", "2025-01-23T02:24:27Z", "meyer9", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6bmKhZ", "I_kwDOLB-lzc6mURUV", "> Update on this work:\n> \n> * Added support to op-geth here: [feat: add Isthmus BLS precompiles\u00a0op-geth#472](https://github.com/ethereum-optimism/op-geth/pull/472)\n> * Added test to monorepo here: [feat: add acceptance test for BLS precompiles\u00a0optimism#13934](https://github.com/ethereum-optimism/optimism/pull/13934)\n> \n> These are blocked by some fixes to get Isthmus/geth e2e tests working:\n> \n> * [feat: add Isthmus fork rules\u00a0op-geth#471](https://github.com/ethereum-optimism/op-geth/pull/471)\n> * [fix: isthmus e2e test utils and withdrawal root fix\u00a0optimism#13933](https://github.com/ethereum-optimism/optimism/pull/13933)\n> \n> Planning on working on fault proofs accelerated precompile tests soon, and testing Reth precompiles as well later this week.\n\npls add links in pm repo better, this is specs repo, and when spec is merged this issue will close, but doesn't mean implementation is done\n\nthe issue doesn't exist, so please make a new one https://github.com/ethereum-optimism/pm/issues/23", "2025-01-23T17:19:11Z", "2025-01-23T17:19:56Z", "emhane", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6bnfwn", "I_kwDOLB-lzc6mURUV", "ah makes sense! I'll make an issue there", "2025-01-23T19:18:20Z", "2025-01-23T19:18:20Z", "meyer9", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6b7rmb", "I_kwDOLB-lzc6mURUV", "I've opened a PR in the specs repo to add this to the Isthmus hardfork specs: https://github.com/ethereum-optimism/specs/pull/553", "2025-01-27T15:44:50Z", "2025-01-27T15:44:50Z", "refcell", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6cKCpW", "I_kwDOLB-lzc6mUH_6", "We need to add a note for this because it introduces a syscall that kona needs to perform in block execution", "2025-01-28T19:25:50Z", "2025-01-28T19:25:50Z", "refcell", "2025-08-31 01:46:34"]
["IC_kwDOLB-lzc6alnbJ", "I_kwDOLB-lzc6mUD6W", "@mslipper is creating a task list with following syntax disabled for this repo?\n\\`\\`\\`[tasklist]\n\\- \\[ \\] \\<issue-link\\>\n\\`\\`\\`\n\nwe need to be able to see the issue labels of the sub issues, for example like https://github.com/paradigmxyz/reth/issues/11241 . this reth task list is made with the syntax above.", "2025-01-15T17:34:29Z", "2025-01-15T17:34:29Z", "emhane", "2025-08-31 01:46:34"]
["IC_kwDOJ_r-bs6a6_yZ", "I_kwDOJ_r-bs6j2PL6", "Dupe of #13082", "2025-01-17T20:43:03Z", "2025-01-17T20:43:03Z", "maurelian", "2025-08-31 01:46:54"]
["IC_kwDODjvEJM6cu9iC", "I_kwDODjvEJM6ocX36", "Notably, the node also didn't receive an updated L1 head since the pipeline reset.  However the sync status tracker only updates the L1 finalized info in response to a `FinalizeL1Event` - it (correctly) doesn't write to it in the `ResetEvent` or `EngineResetConfirmedEvent` handling.  And the `FinalizeL1Event` always logs a message indicating the new L1 finalized being set which the logs show is never 0.\n\nSo where is the 0 finalized coming from???", "2025-02-02T21:09:46Z", "2025-02-02T21:09:46Z", "ajsutton", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c1_He", "I_kwDODjvEJM6ocX36", "Needs more investigation, but I think the source might be: https://github.com/ethereum-optimism/optimism/blob/be455ca4fc0862d15f29f9964d53d54de0b9e4fa/op-node/rollup/finality/finalizer.go#L211", "2025-02-03T15:53:59Z", "2025-02-03T15:53:59Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c4T8a", "I_kwDODjvEJM6ocX36", "> Needs more investigation, but I think the source might be:\n\nNote - it's the L1 finalized that's getting finalized out.  The L2 finalized is still non-zero.", "2025-02-03T20:29:36Z", "2025-02-03T20:29:36Z", "ajsutton", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c6T6g", "I_kwDODjvEJM6ocX36", "I think that might happen when the L1-finalized polling doesn't run early enough, and then the SyncStatus attribute is left blank.\nIt polls every L1-epoch-interval by default: https://github.com/ethereum-optimism/optimism/blob/216b0851b92ed34faeb58aa5a7e1ce08b545d342/op-node/node/node.go#L225\n\nMaybe the L1-epoch-interval used to be configured shorter, and it's longer now, and not polling before access?\n\nOr maybe it's failing to poll? Do we have any `failed to poll L1 block` messages in the logs?", "2025-02-04T01:08:55Z", "2025-02-04T01:08:55Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c2sjH", "I_kwDODjvEJM6oOsed", "The `--l2` flag in your op-node configuration is wrong. The op-node needs an L2 engine-API, also known as \"authrpc\" in geth. So that would be `--l2=ws://localhost:8552`.\nThe port it was connected to was the regular user-RPC, which is not authenticated, and does not provide an engine-API, hence the error about the `engine_newPayloadV3` method being unavailable.\n", "2025-02-03T17:08:14Z", "2025-02-03T17:08:14Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6cs8HX", "I_kwDODjvEJM6oCkZX", "Meaning, the audit firm will audit calls to the OPCM?", "2025-02-01T16:55:41Z", "2025-02-01T16:55:41Z", "nazreen", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dAydk", "I_kwDODjvEJM6oCkZX", "> Meaning, the audit firm will audit calls to the OPCM?\n\nClose, but I'd say more like auditing the result of calls to the OPCM.", "2025-02-04T14:56:48Z", "2025-02-04T14:56:48Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dEP0g", "I_kwDODjvEJM6nrDL6", "done in #14133 ", "2025-02-04T21:40:24Z", "2025-02-04T21:40:24Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c6fKa", "I_kwDODjvEJM6nVDS9", "```\ngo run ./op-deployer/cmd/op-deployer/main.go --help\n```\nfrom the main directory of the monorepo works fine for me. Can you add more data, or close this issue if it was resolved already?\n\n", "2025-02-04T01:51:25Z", "2025-02-04T01:51:25Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bCxZY", "I_kwDODjvEJM6mxmwB", "I\u2019ve created a #13853 PR that implements the proposed changes. It adds a configurable timeout flag for the L2 engine to improve synchronization flexibility. Please have a look and let me know your thoughts!", "2025-01-20T02:27:17Z", "2025-01-20T02:27:53Z", "pengin7384", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bdkCi", "I_kwDODjvEJM6mqy2U", "Work Update:\n\n@tyler-smith and I did some pair programming, setting up the bones of the work, identifying the integration points and all. I am recording the decisions so we can remember and vet them.\n\n### Integration Points\n- We're adding a new Event Type which is handled by a Chain Processor which indicates a database inconsistency was detected, and invalidation is required\n- The Event Type specifies what the originating cause of the invalid data is\n- These events are generated:\n  - When `rangeUpdate` fails, which happens if logs can't extend the database (ie an Unsafe Reorg is likely)\n  - When an L1 block is detected which doesn't extend the known L1 block in the L1 Accessor (ie an L1 Reorg is likely)\n\nWhen these events are generated, they are caught and handled by an invalidation function which must always:\n- Determine what databases are affected (L2 only, or L1&L2). If the consistency check failed during Log Indexing, it should only be the Event DB, but if it's a larger Reorg, it should rewind the Event *and* Derivation DBs.\n- Determine the last healthy point in the database. Event Emitters can't tell us this because whatever invalidated the consistency check may also invalidate arbitrary historical entries too.\n- Rewind the database to the healthy point\n\nWe are making some assumptions on how things will play out:\n- The observation of *where* a database inconsistency is found can limit the scope of the database truncations we do. For example, if the inconsistency is found during indexing of unsafe data, we can truncate just the Event DB, even if the Derivation DBs managed to become inconsistent at the same time (due to some extreme race). We make this assumption because the Derivation DB will also discover inconsistency and will result in a secondary truncation that cleans everything up again\n- If we truncate the databases back, we do not need to explicitly manage the reset of the node. Instead, when the node takes further actions, it will generate `ErrOutOfOrder` which will work as a signal to reset the node.\n- Database Corruption or other \"out of sync\" issues will manifest as consistency failures in other pathways, so we don't need to do special work to detect these. If no consistency checks ever fail, the database must not be corrupt.\n\nThe summary of our assumptions is that we believe that we can act on consistency failures by truncating the databases, and we don't need to be exhaustive about returning the system to a consistent state. Instead, other activities will emergently re-sync or further rewind the components.\n\nGiven these assumptions, the work to be done seems very simple:\n- Emit and Handle the new Inconsistency Event\n- Wire up and use all the correct `Rewind` functions\n- [Optional] refactor the block invalidation to use the same system as everything else\n\nWe also might want to locate this invalidation handler somewhere *above* the ChainProcessor, since a Chain Processor is designated to a single chain, while an L1-reorg will affect *all chains*. Maybe if the ChainID in the event is \"All\", it applies to all processors? @tyler-smith wdyt?\n\nLooking for @protolambda 's thoughts on the matter, especially the assumptions we've made.", "2025-01-22T20:39:34Z", "2025-01-22T20:39:34Z", "axelKingsley", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dKvPR", "I_kwDODjvEJM6mqy2U", "To wrap the conversation up from above: we discussed last Thursday, and work landed in #13953 \n\nClosing this now, I believe we completed the initial scope of reorg work. If there are any bugs we can follow up with new tickets. And testing is tracked in the test-checklist, and the new issues for milestones towards mainnet.\n\n", "2025-02-05T13:19:50Z", "2025-02-05T13:19:50Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6a6B-j", "I_kwDODjvEJM6mc98D", "Can you share the CLI flags / env vars you use to configure the op-node / op-geth / op-batcher?\n\nThe batcher seems to be getting stuck on invalid blocks, which might mean the op-node or op-geth node are not set up right.\n\nAnd it appears like the sequencing-window has expired, forcing the op-node verifier to generate batches, to ensure liveness. This is worst-case scenario, it happens if the batcher is not doing its job. Also double check the batcher signs batches with the same key as that the chain was configured with to trust (the batcher address).", "2025-01-17T18:00:02Z", "2025-01-17T18:00:02Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bEtR4", "I_kwDODjvEJM6mc98D", "Sure\n geth \n```\n./build/bin/geth \\\n  --datadir ./datadir \\\n  --http \\\n  --http.corsdomain=\"*\" \\\n  --http.vhosts=\"*\" \\\n  --http.addr=0.0.0.0 \\\n  --http.api=web3,debug,eth,txpool,net,engine \\\n  --ws \\\n  --ws.addr=0.0.0.0 \\\n  --ws.port=8546 \\\n  --ws.origins=\"*\" \\\n  --ws.api=debug,eth,txpool,net,engine \\\n  --syncmode=full \\\n  --gcmode=archive \\\n  --nodiscover \\\n  --maxpeers=0 \\\n  --networkid=42090 \\\n  --authrpc.vhosts=\"*\" \\\n  --authrpc.addr=0.0.0.0 \\\n  --authrpc.port=8551 \\\n  --authrpc.jwtsecret=./jwt.txt \\\n  --rollup.disabletxpoolgossip=true --http.port=8546 --port=33034 > /mnt/geth.log 2>&1\n```\n\nop-node\n```\n  ./bin/op-node \\\n  --l2=http://localhost:8551 \\\n  --l2.jwt-secret=./jwt.txt \\\n  --sequencer.enabled \\\n  --sequencer.l1-confs=5 \\\n  --verifier.l1-confs=4 \\\n  --rollup.config=./rollup.json \\\n  --rpc.addr=0.0.0.0 \\\n  --p2p.disable \\\n  --rpc.enable-admin \\\n  --p2p.sequencer.key=$GS_SEQUENCER_PRIVATE_KEY \\\n  --l1=$L1_RPC_URL \\\n  --l1.rpckind=$L1_RPC_KIND --log.level=debug > /mnt/op-node.log 2>&1\n```\nop-batcher\n```\n  ./bin/op-batcher \\\n  --l2-eth-rpc=http://localhost:8545 \\\n  --rollup-rpc=http://localhost:9545 \\\n  --poll-interval=1s \\\n  --sub-safety-margin=6 \\\n  --num-confirmations=1 \\\n  --safe-abort-nonce-too-low-count=3 \\\n  --resubmission-timeout=30s \\\n  --rpc.addr=0.0.0.0 \\\n  --rpc.port=8548 \\\n  --rpc.enable-admin \\\n  --max-channel-duration=25 \\\n  --l1-eth-rpc=$L1_RPC_URL \\\n  --private-key=$GS_BATCHER_PRIVATE_KEY --log.level=debug > /mnt/op-batcher.log 2>&1\n```\n\nand last op-proposer\n\n```\n  ./bin/op-proposer \\\n  --poll-interval=12s \\\n  --rpc.port=8560 \\\n  --rollup-rpc=http://localhost:9545 \\\n  --l2oo-address=$(cat ../packages/contracts-bedrock/deployments/getting-started/.deploy | jq -r .L2OutputOracleProxy) \\\n  --private-key=$GS_PROPOSER_PRIVATE_KEY \\\n  --l1-eth-rpc=$L1_RPC_URL --log.level=debug > /mnt/op-proposer.log 2>&1\n```", "2025-01-20T08:12:58Z", "2025-01-20T08:12:58Z", "neodiz", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c6nET", "I_kwDODjvEJM6mc98D", "There doesn't appear to be anything wrong with the configs. I think it most likely failed to submit batches in a timely manner. The batcher guide here might help better than we can help here: https://docs.optimism.io/builders/chain-operators/configuration/batcher\n", "2025-02-04T02:23:10Z", "2025-02-04T02:23:10Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ass_g", "I_kwDODjvEJM6mcr17", "log then op-batcher start \n```\nt=2025-01-13T07:29:04+0000 lvl=info msg=\"Initializing Batch Submitter\"\nt=2025-01-13T07:29:05+0000 lvl=info msg=\"metrics disabled\"\nt=2025-01-13T07:29:05+0000 lvl=info msg=\"Admin RPC enabled\"\nt=2025-01-13T07:29:05+0000 lvl=info msg=\"Starting JSON-RPC server\"\nt=2025-01-13T07:29:05+0000 lvl=info msg=\"Starting batcher\"             notSubmittingOnStart=false\nt=2025-01-13T07:29:05+0000 lvl=info msg=\"Starting Batch Submitter\"\nt=2025-01-13T07:29:05+0000 lvl=info msg=\"Batch Submitter started\"\nt=2025-01-13T07:29:06+0000 lvl=info msg=\"Starting batch-submitter work at safe-head\" safe=0xb3b1d98d30ae3b2cc3ded539866221dba859d25aba225191d6895392abb66ae3:0\nt=2025-01-13T07:29:06+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:06+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0x8ce7babeed05a25c0a8642d73f45b25ce19a9c8e30d9c6818fea33b01ecbc691:7481003 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:07+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:07+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0x8ce7babeed05a25c0a8642d73f45b25ce19a9c8e30d9c6818fea33b01ecbc691:7481003 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:08+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:08+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0x8ce7babeed05a25c0a8642d73f45b25ce19a9c8e30d9c6818fea33b01ecbc691:7481003 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:09+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:09+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0x8ce7babeed05a25c0a8642d73f45b25ce19a9c8e30d9c6818fea33b01ecbc691:7481003 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:10+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:10+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0x8ce7babeed05a25c0a8642d73f45b25ce19a9c8e30d9c6818fea33b01ecbc691:7481003 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:11+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:11+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0x8ce7babeed05a25c0a8642d73f45b25ce19a9c8e30d9c6818fea33b01ecbc691:7481003 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:12+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:12+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0x8ce7babeed05a25c0a8642d73f45b25ce19a9c8e30d9c6818fea33b01ecbc691:7481003 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:13+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:13+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:14+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:14+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:15+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:15+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:16+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:16+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:17+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:17+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:18+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:18+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:19+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:19+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:20+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:20+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:21+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:21+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:22+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:22+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:23+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:23+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:24+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:24+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xe9e50b2aec76e02223c9ea25810f5ce60b1533533e5c57640f99417b2a944245:7481004 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:25+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:25+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xfd638040bcef679c443d37e044e695c80a394e79fcdf569f20323ce1b14e818c:7481005 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:26+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:26+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xfd638040bcef679c443d37e044e695c80a394e79fcdf569f20323ce1b14e818c:7481005 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:27+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:27+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xfd638040bcef679c443d37e044e695c80a394e79fcdf569f20323ce1b14e818c:7481005 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:28+0000 lvl=warn msg=\"Error calculating L2 block range\" err=\"L2 safe head ahead of L2 unsafe head\"\nt=2025-01-13T07:29:28+0000 lvl=dbug msg=\"Requested tx data\"                l1Head=0xfd638040bcef679c443d37e044e695c80a394e79fcdf569f20323ce1b14e818c:7481005 data_pending=false blocks_pending=0\nt=2025-01-13T07:29:29+0000 lvl=info msg=\"added L2 block to local state\"    block=0x973463007c9639104c23192c42d5d33b5b981b04d427053255c6fc860e0342d3:1 tx_count=0 time=1,733,905,085\nt=2025-01-13T07:29:29+0000 lvl=warn msg=\"Invalid L2 block loaded into state\" err=\"l2 block is missing L1 info deposit tx, block hash: 0x973463007c9639104c23192c42d5d33b5b981b04d427053255c6fc860e0342d3\"\n```", "2025-01-16T12:17:10Z", "2025-01-16T12:17:10Z", "neodiz", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6a6CoR", "I_kwDODjvEJM6mcr17", "> `t=2025-01-13T07:29:29+0000 lvl=warn msg=\"Invalid L2 block loaded into state\" err=\"l2 block is missing L1 info deposit tx, block hash: 0x973463007c9639104c23192c42d5d33b5b981b04d427053255c6fc860e0342d3\"`\n\nIs the op-batcher connected to the L1 RPC instead of L2 RPC? The L2 blocks should always contain a special system deposit.\nThis chain is misconfigured.\n", "2025-01-17T18:01:40Z", "2025-01-17T18:01:40Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bEutV", "I_kwDODjvEJM6mcr17", "I start \n\n```\n  ./bin/op-batcher \\\n  --l2-eth-rpc=http://localhost:8545 \\\n  --rollup-rpc=http://localhost:9545 \\\n  --poll-interval=1s \\\n  --sub-safety-margin=6 \\\n  --num-confirmations=1 \\\n  --safe-abort-nonce-too-low-count=3 \\\n  --resubmission-timeout=30s \\\n  --rpc.addr=0.0.0.0 \\\n  --rpc.port=8548 \\\n  --rpc.enable-admin \\\n  --max-channel-duration=25 \\\n  --l1-eth-rpc=$L1_RPC_URL \\\n  --private-key=$GS_BATCHER_PRIVATE_KEY --log.level=debug > /mnt/op-batcher.log 2>&1\n```\nSure. op-batcher have current connection to L1 network.  ", "2025-01-20T08:15:58Z", "2025-01-20T08:15:58Z", "neodiz", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bKBwx", "I_kwDODjvEJM6mcr17", "i had similar issue, add `--l1.rpckind=any --l1.trustrpc` flags in op-node and restart, also make sure L1 is synced ", "2025-01-20T19:11:02Z", "2025-01-20T19:16:38Z", "hamidmuslih", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bQDMw", "I_kwDODjvEJM6mcr17", "Changed env L1_RPC_KIND to any \nStart node using param. \n```  ./bin/op-node \\\n  --l2=http://localhost:8551 \\\n  --l2.jwt-secret=./jwt.txt \\\n  --sequencer.enabled \\\n  --sequencer.l1-confs=5 \\\n  --verifier.l1-confs=4 \\\n  --rollup.config=./rollup.json \\\n  --rpc.addr=0.0.0.0 \\\n  --p2p.disable \\\n  --rpc.enable-admin \\\n  --p2p.sequencer.key=$GS_SEQUENCER_PRIVATE_KEY \\\n  --l1=$L1_RPC_URL \\\n  --l1.rpckind=$L1_RPC_KIND --l1.trustrpc --log.level=debug > /mnt/op-node.log 2>&1 \n```\nand from logs see save error\n\n```\nt=2025-01-21T12:59:51+0000 lvl=warn msg=\"Invalid L2 block loaded into state\" err=\"l2 block is missing L1 info deposit tx, block hash: 0x08b0407e64c43f293230a5b13cdb9b31ebcc7b475f36069e8708fb17d0048871\"\nt=2025-01-21T12:59:51+0000 lvl=dbug msg=\"Requested tx data\"                  l1Head=0x3badf9061c1d2dd005e6e7658c6e3b75d90de2beedea227971d6b57aa0d4ba27:7540004 data_pending=false blocks_pending=210\nt=2025-01-21T12:59:51+0000 lvl=eror msg=\"unable to get tx data\"              err=\"adding block[0] to channel builder: converting block to batch: block 0x6f724ca7c10b00ad565fc1dcceb26bc4cb2d3900d291b01245b9924491dc53be has no transactions\"\n```\n", "2025-01-21T13:01:35Z", "2025-01-21T13:01:35Z", "neodiz", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bQaIi", "I_kwDODjvEJM6mcr17", "hmm i have run with success with the below commands. \n```\nInit the geth\nbuild/bin/geth init --datadir=datadir --state.scheme hash genesis.json\n \nRun geth\n./build/bin/geth  --datadir ./datadir --http --http.corsdomain=\"*\" --http.vhosts=\"*\" --http.addr=0.0.0.0 --http.api=web3,debug,eth,txpool,net,engine --ws --ws.addr=0.0.0.0  --ws.port=8546  --ws.origins=\"*\"  --ws.api=debug,eth,txpool,net,engine  --syncmode=full --gcmode=archive --nodiscover --maxpeers=0 --networkid=42069 --authrpc.vhosts=\"*\" --authrpc.addr=0.0.0.0  --authrpc.port=8551 --authrpc.jwtsecret=./jwt.txt --rollup.disabletxpoolgossip=true\n \nOp-node\n./bin/op-node --l2=http://localhost:8551 --l2.jwt-secret=./jwt.txt --sequencer.enabled --sequencer.l1-confs=5 --verifier.l1-confs=4 --rollup.config=./rollup.json --rpc.addr=0.0.0.0 --p2p.disable --rpc.enable-admin --p2p.sequencer.key=$GS_SEQUENCER_PRIVATE_KEY --l1=$L1_RPC_URL --l1.rpckind=$L1_RPC_KIND --l1.trustrpc\n \nOp-batcher\n./bin/op-batcher --l2-eth-rpc=http://localhost:8545 --rollup-rpc=http://localhost:9545 --poll-interval=1s --sub-safety-margin=6 --num-confirmations=1 --safe-abort-nonce-too-low-count=3 --resubmission-timeout=30s --rpc.addr=0.0.0.0 --rpc.port=8548 --rpc.enable-admin --max-channel-duration=25 --l1-eth-rpc=$L1_RPC_URL --private-key=$GS_BATCHER_PRIVATE_KEY\n \nOp-proposer\n./bin/op-proposer --poll-interval=12s --rpc.port=8560 --rollup-rpc=http://localhost:9545 --l2oo-address=$(cat ../packages/contracts-bedrock/deployments/getting-started/.deploy | jq -r .L2OutputOracleProxy) --private-key=$GS_PROPOSER_PRIVATE_KEY --l1-eth-rpc=$L1_RPC_URL\n```\n", "2025-01-21T13:40:59Z", "2025-01-21T13:40:59Z", "hamidmuslih", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bWm9h", "I_kwDODjvEJM6mcr17", "Repo optimism i use branch tutorials/chain (from documentation)\nRepo op-geth  i use optimism. \nMaybe that's the problem. I try deploy smart from branch v1.10.0  ", "2025-01-22T06:23:23Z", "2025-01-22T06:23:23Z", "neodiz", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dAYnq", "I_kwDODjvEJM6mcr17", "`converting block to batch: block 0x6f724ca7c10b00ad565fc1dcceb26bc4cb2d3900d291b01245b9924491dc53be has no transactions`\n\nThis log states that the L2 block does not have transactions. A valid op-stack block always at least has 1. Maybe the chain database needs to be cleared and re-synced, to get valid blocks. Or maybe it is a configuration issue, and connecting to the wrong chain.\n\nAs others have shared, there are configurations for this that do work as expected, so I am closing the issue here since it has been inactive for 2 weeks.\n\n", "2025-02-04T14:25:42Z", "2025-02-04T14:25:42Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dNdgY", "I_kwDODjvEJM6lv3N_", "closed by https://github.com/ethereum-optimism/optimism/pull/13653", "2025-02-05T17:18:06Z", "2025-02-05T17:18:06Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6b7xzZ", "I_kwDODjvEJM6lpuXy", "Fixed in #13814", "2025-01-27T15:54:21Z", "2025-01-27T15:54:21Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6Z2yyR", "I_kwDODjvEJM6ln8HS", "> Where state vars are removed a spacer should be added.\n\n@AmadiMichael In case you aren't familiar, info on our spacing naming convention here: https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/meta/STYLE_GUIDE.md#spacers", "2025-01-09T21:26:46Z", "2025-01-09T21:26:46Z", "mds1", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6Z94R0", "I_kwDODjvEJM6ln8HS", "When will CGT be added back again?", "2025-01-10T16:17:41Z", "2025-01-10T16:17:41Z", "zhiqiangxu", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6awDc2", "I_kwDODjvEJM6lfQTe", "> In deploy implementatoins: use create 2 to deploy, and if there is an address collision, keep the same implementation address as before.\n\nThis was completed by https://github.com/ethereum-optimism/optimism/pull/13717. \n\nWoohoo!\n\n> In OPCM upgrade, detect when this is the case and ensure that such contracts are not upgraded.\n\nAs for this, the current direction plans to depend less on identifying the various scenarios automatically, and more on the dev to explicitly declare their intention.", "2025-01-16T17:18:30Z", "2025-01-16T17:18:30Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dhxqr", "I_kwDODjvEJM6lemqn", "Closing this as we did not change implement the parent issue.", "2025-02-07T13:15:21Z", "2025-02-07T13:15:21Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6cfMMi", "I_kwDODjvEJM6lMJYc", "This was completed in #14000 ", "2025-01-30T19:44:39Z", "2025-01-30T19:44:39Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dXVhy", "I_kwDODjvEJM6lJoQF", "The draft post is here https://www.notion.so/oplabs/MT-Cannon-189f153ee16280e0b00dccf938625750", "2025-02-06T15:26:41Z", "2025-02-06T15:26:41Z", "pauldowman", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZX-nb", "I_kwDODjvEJM6lJfhC", "> My intuition is that the affected contracts should be limited to the `SystemConfig` and `OptimismPortal`.\n\nI believe this is correct. But this is the PR where the CGT functionality was merged: https://github.com/ethereum-optimism/optimism/pull/10143. @AmadiMichael, you can take a look at the L1 contracts diff there to make sure we don't miss any L1 changes.", "2025-01-06T14:14:57Z", "2025-01-06T14:14:57Z", "mds1", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZYBf8", "I_kwDODjvEJM6lJfhC", "On it \ud83e\udee1", "2025-01-06T14:20:52Z", "2025-01-06T14:20:52Z", "AmadiMichael", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZZodr", "I_kwDODjvEJM6lJfhC", "I was wrong. The diff will also include the L1StandardBridge and L1CrossDomainMessenger.", "2025-01-06T18:02:51Z", "2025-01-06T18:02:51Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c6gjR", "I_kwDODjvEJM6kd1jY", "Closing, insufficient information to resolve and no activity.\n", "2025-02-04T01:57:12Z", "2025-02-04T01:57:12Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6b7hc1", "I_kwDODjvEJM6j19Gj", "can we please close this issue since @CreeptoGengar fixed it?", "2025-01-27T15:29:01Z", "2025-01-27T15:29:01Z", "nditanaka", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6XxTBe", "I_kwDODjvEJM6jT9ny", "It was [raised](https://github.com/ethereum-optimism/optimism/pull/13323#discussion_r1887082317) that not all chains will need to be skipped when `skipIfForkTest()` is called depending on the chain and what version of contracts it has.\n\nThus when this testing is added we should think deeper about which tests can be fully executed on which chains.", "2024-12-16T17:55:30Z", "2024-12-16T17:55:30Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6YQWKS", "I_kwDODjvEJM6iye40", "Included in #13323 ", "2024-12-19T14:50:59Z", "2024-12-19T14:50:59Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6W92OW", "I_kwDODjvEJM6imZrf", "FYI, I opened an issue to track the op-node side of this: #13336 \nThis GH issue can then focus on the op-supervisor part of the managed-mode syncing.\n", "2024-12-10T20:24:22Z", "2024-12-10T20:24:22Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6W-068", "I_kwDODjvEJM6imZrf", "https://github.com/ethereum-optimism/optimism/pull/13344\n\nHere's the start of the Node Controller code", "2024-12-10T22:13:43Z", "2024-12-10T22:13:43Z", "axelKingsley", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c_toq", "I_kwDODjvEJM6imZrf", "This is done", "2025-02-04T13:31:48Z", "2025-02-04T13:31:48Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6XZLdS", "I_kwDODjvEJM6iVXHf", "The logs look like you are mixing up the L1 RPC and L2 RPC URLs", "2024-12-12T20:31:52Z", "2024-12-12T20:31:52Z", "tynes", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dAVtq", "I_kwDODjvEJM6iVXHf", "From your logs: `incorrect L1 RPC chain id 11155420, expected 11155111`\nSo you connected the L1-input (chain `11155111` = sepolia) of op-node to the L2 RPC (chain `11155420` is op-sepolia).\n", "2025-02-04T14:21:14Z", "2025-02-04T14:21:14Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c6gyv", "I_kwDODjvEJM6iNUvX", "duplicate of https://github.com/ethereum-optimism/optimism/issues/13272", "2025-02-04T01:57:51Z", "2025-02-04T01:57:51Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6WGYu-", "I_kwDODjvEJM6iCW97", "I would like to see a checklist specific for isthmus. I am less concerned about a universal checklist. When we are shipping the next hardfork after isthmus, we can make a new checklist. Then we can compare the number of items and have a way to quantify the improvement in the process", "2024-12-04T18:42:45Z", "2024-12-04T18:42:45Z", "tynes", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6VGeaJ", "I_kwDODjvEJM6gqavH", "Wondering if one option here would be to do the differential testing in Solidity instead? We'd get the coverage for free that way.", "2024-11-26T17:15:23Z", "2024-11-26T17:15:23Z", "smartcontracts", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6VGhSq", "I_kwDODjvEJM6gqavH", "> Wondering if one option here would be to do the differential testing in Solidity instead? We'd get the coverage for free that way.\n\nWe attempted this for the OG MIPS.sol tests but it is too slow to run differential tests in solidity. The ffi overhead is no joke. Particularly once you have fuzz tests. I actually looked into speeding up ffi in forge some time back using a client-server model, but didn't get anywhere.", "2024-11-26T17:18:38Z", "2024-11-26T17:18:38Z", "Inphi", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6VHF4S", "I_kwDODjvEJM6gqavH", "A couple considerations. The source maps found in forge artifacts aren't perfect. Code inlining, common function elimination, and a couple other compiler tricks make it non-trivial to corroborate program counters in the EVM with the source code. It would be useful to take a look at this [source map tracer](https://github.com/ethereum-optimism/optimism/blob/develop/op-chain-ops/srcmap/solutil.go) to figure out how it handles some broken cases in source maps. \nBut even if the coverage data isn't perfect, it's still better than zero coverage. And if there's a way we can rewrite contracts or use certain solc compiler flags to make it easy to generate coverage, it'll be worth doing.", "2024-11-26T18:34:44Z", "2024-11-26T18:35:57Z", "Inphi", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6VHMxi", "I_kwDODjvEJM6gqavH", "I'll circle around this issue and come back with some ideas.", "2024-11-26T18:50:28Z", "2024-11-26T18:50:28Z", "JosepBove", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6VHckS", "I_kwDODjvEJM6gqavH", "We should do this in coordination with @AmadiMichael who is making sure that we are able to compile the contracts without the optimizer enabled, which will make coverage easier.", "2024-11-26T19:28:11Z", "2024-11-26T19:28:11Z", "smartcontracts", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6VOK39", "I_kwDODjvEJM6gqavH", "This is the rough idea of what I thought, any comments about it? There's a lot of fine tuning to do like detect not executed lines, funcions, branches etc...", "2024-11-27T10:35:46Z", "2024-11-27T10:41:50Z", "JosepBove", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6VcQFe", "I_kwDODjvEJM6gqavH", "The MVP is done, I need a review now and feedback. Let's not center on the implementation for now as it's WIP, let's discuss about the design, for example if adding a custom tracer and then a .generateLCOV is enough, if the output should be a text and not a file etc..\n\nI still need to experiment a lot to see if branching can be possible but looks extremely complicated for now, let me know if you have any idea.\n\nAs right now this is the output:\n![Image](https://github.com/user-attachments/assets/435a68fb-4029-42e3-add0-49240c8c3cca)\n\ndb is in red as on the test I don't test the getter", "2024-11-29T07:10:00Z", "2024-11-29T08:01:12Z", "JosepBove", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6Vv_G2", "I_kwDODjvEJM6gqavH", "The interface looks good overall. Very simple to use. For `generateLCOV`, I suggest have it write to an `io.Writer` so the user can adapt it as desired.\n\nIs the tracer able to produce coverage of imported solidity contracts and libraries? I think it'll be a great UX if it can auto-include other .sol files in the lcov report, rather than have the user manually specify them.", "2024-12-02T18:33:51Z", "2024-12-02T18:33:51Z", "Inphi", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6VwcXL", "I_kwDODjvEJM6gqavH", "Let me do some testing around imported & libraries and come back", "2024-12-02T19:08:59Z", "2024-12-02T19:08:59Z", "JosepBove", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6U92Gg", "I_kwDODjvEJM6gfWUX", "Need to check if this will actually help given that resolving the game requires resolving claims first so we may wind up loading the same data anyway.", "2024-11-25T23:26:48Z", "2024-11-25T23:26:48Z", "ajsutton", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZYjGj", "I_kwDODjvEJM6gfWUX", "We will need to load all the claims  to check if we can resolve any, but after we check what can be resolved we currently load the game state again separately to see if we should move.  This makes sense if we did resolve any claims as we should act based on the latest info but it does mean we're doubling the requests for claims in the common case where the game can't be resolved.  We also check if the l2 block number has been challenged which is an extra request that could potentially be skipped.\n\nWe could also optimise to skip loading claims to see if we can resolve them if the game was created within the last 3.5 days as well, though that would require changing the abstractions a bit for how we load game data.", "2025-01-06T15:29:46Z", "2025-01-06T15:29:46Z", "ajsutton", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6Wufi9", "I_kwDODjvEJM6gdlht", "Capturing a key decision about this feature: \n\n- Test will default to running against one of either OP Mainnet or OP Sepolia. \n- It will be easy to change which of those two options is the default\n- It should also be easy to change the test target to another OP Chain\n- Testing against releases which are not yet available on either Sepolia or Mainnet / not contained in the superchain-registry is outside of the current scope of work.", "2024-12-09T17:06:57Z", "2024-12-09T17:06:57Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6YQVmo", "I_kwDODjvEJM6gdlht", "Fixed by #13323 ", "2024-12-19T14:50:33Z", "2024-12-19T14:50:33Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6b9iji", "I_kwDODjvEJM6gdjL5", "closed by #13814 ", "2025-01-27T18:36:12Z", "2025-01-27T18:36:12Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZrSXp", "I_kwDODjvEJM6gdfzV", "A more thorough spec for this can now be [found here](https://github.com/ethereum-optimism/optimism/issues/13071#issuecomment-2576221272). ", "2025-01-08T17:44:53Z", "2025-01-08T17:44:53Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ajrmu", "I_kwDODjvEJM6gdfzV", "We've elected not to do this, as it would have modified the bytecode of all contracts unnecessarily.", "2025-01-15T14:42:12Z", "2025-01-15T14:42:12Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6YHcdo", "I_kwDODjvEJM6gdWms", "I've moved this to the post-mainnet milestone, as the current system cannot make use of it.", "2024-12-18T19:16:36Z", "2024-12-18T19:16:36Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6aHB7v", "I_kwDODjvEJM6gdWms", "No longer using this approach", "2025-01-12T01:26:33Z", "2025-01-12T01:26:33Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6YH76t", "I_kwDODjvEJM6gdWlK", "I think the AddressManager is actually not accessible onchain, though I recall @smartcontracts might have had an idea for how to access it? https://github.com/ethereum-optimism/client-pod/issues/699\n\nFor DelayedWETH, I agree with using the `gameImpl(DisputeGameFactory.gameImpls(n)).weth()` approach. We might want to have a new struct that returns addresses that are not part of the existing `Addresses` struct?", "2024-12-18T20:33:05Z", "2024-12-18T20:33:05Z", "mds1", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6YJ2Wf", "I_kwDODjvEJM6gdWlK", "> I think the AddressManager is actually not accessible onchain\n\nThis is true, we'll need to provide that address as an input to this upgrade.\n\n\n> For DelayedWETH, I agree with using the gameImpl(DisputeGameFactory.gameImpls(n)).weth() approach. We might want to have a new \nstruct that returns addresses that are not part of the existing Addresses struct?\n\nTo help me understand: what do you think that struct looks like, what else is in it? what would it be called?", "2024-12-19T03:52:42Z", "2024-12-19T03:53:06Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZjPkr", "I_kwDODjvEJM6gdWlK", "> I think the AddressManager is actually not accessible onchain\n\nTurns out it's very easy to get from `ProxyAdmin.addressManager()`.\n\n", "2025-01-07T20:21:31Z", "2025-01-07T20:21:31Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZjRHZ", "I_kwDODjvEJM6gdWlK", "I reviewed the full set of contracts deployed by `OPCM.deploy()`, here is what I documented: \n\n| OPCM Deploys                         | System Config Has?             | Notes / Discoverable from                                    | Action? |\n| ------------------------------------ | ------------------------------ | ------------------------------------------------------------ | ------- |\n| `opChainProxyAdmin`                  | No                             | `UpgradeInput` (as currently planned), but could be avoided. | None    |\n| `addressManager`                     | No                             | Discover from: `ProxyAdmin`                                  | None    |\n| `l1ERC721BridgeProxy`                | `l1ERC721Bridge`               |                                                              | None    |\n| `systemConfigProxy`                  | No                             | `UpgradeInput`                                               | None    |\n| `optimismMintableERC20FactoryProxy`  | `optimismMintableERC20Factory` |                                                              | None    |\n| `l1StandardBridgeProxy`              | `l1StandardBridge`             |                                                              | None    |\n| `l1CrossDomainMessengerProxy`        | `l1CrossDomainMessenger`       |                                                              | None    |\n| `optimismPortalProxy`                | `optimismPortal`               |                                                              | None    |\n| `disputeGameFactoryProxy`            | `disputeGameFactory`           |                                                              | None    |\n| `faultDisputeGame`                   | No                             | Discoverable from: `disputeGameFactory.gameImpls(0)`.        | None    |\n| `permissionedDisputeGame`            | No                             | Discoverable from: `disputeGameFactory.gameImpls(1)`.        | None    |\n| `anchorStateRegistryProxy`           | No                             | Discoverable from: `faultDisputeGame.anchorStateRegistry()`  | None    |\n| `anchorStateRegistryImpl`            | No                             | Will be deployed by `OPCM.upgrade()`                         | None    |\n| `delayedWETHPermissionlessGameProxy` | No                             | Discoverable from: `permissionlessDisputeGame.weth()`        | None    |\n| `delayedWETHPermissionedGameProxy`   | No                             | Discoverable from: `permissionedDisputeGame.weth()`          | None    |\n\n\nIn this table `UpgradeInput` is a placeholder for anything which is currently planned to be an input to `OPCM.upgrade(ISystemConfig[], IProxyAdmin[])`. \n\nSo, we could simplify future upgrades by adding `ProxyAdmin` address to the `SystemConfig.Addresses` struct, so that we only need to provide a single argument to the upgrade function per chain, however this is a nice to have, so I think that we can remove this from scope.", "2025-01-07T20:25:53Z", "2025-01-07T20:27:07Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ajtxv", "I_kwDODjvEJM6gdWjD", "We have elected not to do this, as we are no longer moving to Oz Init V5, which would have moved the location of the `_initialized` value. \n\nSince we are no longer doing that, we don't need to ensure that the `_initialized` value is non-zero (as it already is). \n\nTherefore we can simply update the implementation slot in the proxy, using `upgradeTo()` instead of `upgradeToAndCall()`.", "2025-01-15T14:45:17Z", "2025-01-15T14:45:17Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6YG8VO", "I_kwDODjvEJM6gdWXL", "Big fan of this idea. We should reduce dependency on oz as much as possible. My one concern is if there is inheritance hell with the file, the solution would be to vendor in all deps in a single file\n\nGiven it's so important to ensure correctness, I would like to have tests for this code in the monorepo. The downside risk is literally all money in bridge stolen \n\nQuestion: does it use any features that don't work on 0.8.15? If no, it should be ^0.8.0 for simplicity ", "2024-12-18T18:01:58Z", "2024-12-18T18:01:58Z", "tynes", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6YHB73", "I_kwDODjvEJM6gdWXL", "> My one concern is if there is inheritance hell with the file, the solution would be to vendor in all deps in a single file\n\nFortunately it looks like the is only [one import](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/v5.0.2/contracts/proxy/utils/Initializable.sol#L6) with [no ancestors](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/v5.0.2/contracts/utils/Address.sol) of its own.\n\n> Given it's so important to ensure correctness, I would like to have tests for this code in the monorepo. The downside risk is literally all money in bridge stolen\n\nWorth noting that [InitializableOZv5.t.sol](https://github.com/ethereum-optimism/optimism/blob/upgrade-against-registry/packages/contracts-bedrock/test/vendor/InitializableOZv5.t.sol#L1) exists, to test some interop code. We should be able to mostly duplicate that file. \n\n> Question: does it use any features that don't work on 0.8.15? If no, it should be ^0.8.0 for simplicity\n\nnot afaict...", "2024-12-18T18:14:52Z", "2024-12-18T18:14:52Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZjgRY", "I_kwDODjvEJM6gdWXL", "## Context\n\nThere are basically two upgrade paths for an proxy being upgraded: \n\n1. Logic only upgrades: handled via `ProxyAdmin.upgrade()`\n2. Logic and state upgrades: handled via `ProxyAdmin.upgradeAndCall()` where the data is a call to the release specific `upgrade()` function.\n\nCase 1 is simple. Case 2 requires us to ensure that the `upgrade()` function cannot be called a second time. We can achieve that by:\n\n1. using an industry standard off the shelf component.\n2. reusing the release version so as to not add a new constant that needs to be tracked.\n\n## Reinitializer Implementation Details\n\n1. We'll use OpenZeppelin v5's `Initializable` to ensure that `initialize()` and `upgrade()` can only be called once.\n2. The v5 version of `Initializable` use a uint64 to store the initialization state, and allows reinitialization if the new version is greater than the current version.\n\nAll contracts which require an `upgrade()` function should be reinitialized with the new version. This version will be stored in a new library `Release.sol`:\n\n```solidity\n/// @title Release\n/// @notice Library for handling release versioning with padded spacing\nlibrary Release {\n\n    /// @notice Version identifier, used for upgrades.\n    uint32 constant MAJOR = 2;\n    uint16 constant MINOR = 3;\n    uint16 constant PATCH = 4;\n\n    /// @notice Spacing constants for version components.\n    /// @dev These values are used to convert the MAJOR, MINOR, and PATCH components to a uint64.\n    uint64 constant SPACING = 1_000_000;\n\n    /// @notice Converts release components to a uint64 with padding\n    /// @return version Padded version number as uint64\n    function toUint64() internal pure returns (uint64 version) {\n        // Calculate padded version number\n        version = uint64((MAJOR * SPACING ** 2) + (MINOR * SPACING) + PATCH);\n    }\n}\n```\n\nThus `toUint64(2,1,1)` will return `2000003000004` (we can fiddle with the spacing here). It might be better to use bit shifting instead of multiplication and division, but this was an easy demo.\n\n## Emergency Patch Scenario Example\n\nIt's important to think through how this will affect different upgrade apths.\n\nConsider the following scenario:\n\n- Chains A, B: Running `v2.0.0`\n- Chains C, D: Already upgraded to `v2.1.0`\n- Critical issue discovered in `v2.0.0` that needs immediate fixing\n\n## Procedure\n\n- Create a new branch from the `v2.0.0` tag\n- Implement fix on the `v2.0.0` branch\n- Create new release `v2.0.1`\n- Deploy a new OPCM for `v2.0.1`\n- Upgrade the affected chains from `v2.0.0` to `v2.0.1`\n\nThen there are two possible scenarios for upgrading those chains from `v2.0.1` to `v2.1.0`:\n\n1. OPCM 2.0.0 does not work for that upgrade path. In this case we will need to deploy a special purpose OPCM `v2.1.0-patch-from-2.0.0`\n1. OPCM 2.1.0 already works for that upgrade path (this would need to be tested and verified).\n\nMy intuition is that the second case is much more likely in most scenarios, but we do at least have a path forward in the event of the first case.\n\n## Risks\n\nThe primary risk is that upgrading to a new `Initializable` version, will migrates the location of the `initialized()`. This is pretty straightforward to test and mitigate though.\n\nMitigations: \n\n1.  exposed the `_initialized` value so that the OPCM can assert that all affected contracts are indeed initialized.\n2. Add tests (including on the Forked upgrade test path) to show that it's impossible to call either `initialize` or `upgrade` after the upgrade.\n3. Open to others.\n", "2025-01-07T21:07:02Z", "2025-01-07T21:11:49Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ajvuR", "I_kwDODjvEJM6gdWXL", "We've elected not to do this for a couple reasons: \n\n1. it opened up a huge bikeshed about how to ensure that the `_initialized` value is set.\n2. it increased churn in the contracts, making #13072 necessary in order to set `_initialized` in the new location.\n3. the refactor turned out to be more involved than expected, as there are several contracts which inherit from `OwnableUpgradeable` which inherits from `Initializable` and thus we would need to vendor it in as well.", "2025-01-15T14:48:20Z", "2025-01-15T14:48:20Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6W720e", "I_kwDODjvEJM6gdUX6", "Hey @maurelian\n\nWe're automating this process in Kailua here:\nhttps://github.com/risc0/kailua/blob/v2/bin/cli/src/deploy.rs\n\nWe don't rely on the DelayedWethProxy, but instead use another contract for storing bonds.\n\nWould this break our flow somehow?", "2024-12-10T16:51:39Z", "2024-12-10T16:51:39Z", "hashcashier", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZZSk8", "I_kwDODjvEJM6gdUX6", "Sorry @hashcashier I missed this when you first posted it.\n\n> We don't rely on the DelayedWethProxy, but instead use another contract for storing bonds.\n\nI gather from this that your chain is non-standard. From my naive perspective, I don't think that this will actually break your flow. It shouldn't affect the how the proofs system is deployed and initialized, it's just moving the orchestration of it on-chain into the `OPContractsManager`. ", "2025-01-06T17:11:17Z", "2025-01-06T17:11:17Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6ZZfy_", "I_kwDODjvEJM6gdUX6", "No worries, thanks! Looking forward to testing this out and using it!", "2025-01-06T17:41:34Z", "2025-01-06T17:41:34Z", "hashcashier", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6TnZEi", "I_kwDODjvEJM6eRssB", "You need to specify `IMPL_SALT` env var as a random bytes32", "2024-11-14T14:54:46Z", "2024-11-14T14:54:46Z", "tynes", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6TnfO2", "I_kwDODjvEJM6eRssB", "> You need to specify `IMPL_SALT` env var as a random bytes32\n\nIt's already set in `.envrc`:\n\n```\nexport IMPL_SALT=$(openssl rand -hex 32)\n```\n\nIf I echo, it shows:\n\n```\n# echo $IMPL_SALT\n789b62c5670c9712f7d2c763c5feed54a4128565a46cbb54602fd75b47968352\n```", "2024-11-14T15:00:33Z", "2024-11-14T15:00:33Z", "zhiqiangxu", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dAHSb", "I_kwDODjvEJM6eRssB", "Are you still using the original deployment setup, or are you using op-deployer? If op-deployer, then maybe we close this issue and continue in issue #13327 ", "2025-02-04T13:59:24Z", "2025-02-04T13:59:24Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dAQJm", "I_kwDODjvEJM6eRssB", "This issue can be closed since we're now using op-deployer.", "2025-02-04T14:12:32Z", "2025-02-04T14:12:32Z", "zhiqiangxu", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6W-PDi", "I_kwDODjvEJM6dV-vG", "Re-opening as #12746 was closed.", "2024-12-10T21:09:25Z", "2024-12-10T21:09:25Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6RXGCP", "I_kwDODjvEJM6b22FZ", "This is blocked by #12356 ", "2024-10-25T20:36:58Z", "2024-10-25T20:36:58Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6RZM7w", "I_kwDODjvEJM6b22FZ", "Strongly in favor of this, the unstructured storage is great, I wish that we did that from the start to remove a lot of need for spacers", "2024-10-26T04:21:27Z", "2024-10-26T04:21:27Z", "tynes", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6U9rpt", "I_kwDODjvEJM6bydhl", "Flagging context from Kelvin that \"this technically does change the contract source code because the import paths will change, so it does need to be included in an upgrade\"", "2024-11-25T22:52:40Z", "2024-11-25T22:52:40Z", "K-Ho", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6bQyNu", "I_kwDODjvEJM6bvowR", "@smartcontracts @AmadiMichael I think we can call this done right?", "2025-01-21T14:20:38Z", "2025-01-21T14:20:38Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6RHMh7", "I_kwDODjvEJM6bmMQo", "We used to have some Go types that made it easy to build Safe compatible JSON files. They were removed in https://github.com/ethereum-optimism/optimism/pull/11021, you might be able to find them useful for this", "2024-10-24T08:07:03Z", "2024-10-24T08:07:03Z", "tynes", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6amdH8", "I_kwDODjvEJM6bmMQo", "A few questions before diving into implemenation:\n\n1. What do you think of this user flow?\n    * `op-deployer apply --deployment-target=calldata` - stores calldata for multisig tx in state file\n    * `op-deployer safe sign --safe-address=<address> --private-key=<key>` - creates a safe tx (i.e. generates the hash to be signed) from the calldata stored by previous command. This needs to be run multiple times, each with a different private key until the sig threshold is met. Is it safe to assume there will only be a single multisig tx at a time or should the user specify exactly which tx (by tx hash, nonce, or other identifier) which one they intend to sign?\n    * `op-deployer safe execute --l1-rpc-url=<rpc>` - publishes tx to L1 chain, errors before publishing if not enough sigs have been collected. Same question as above: should we expect the user to identify exactly which tx  to publish (by nonce or other id) with a cli flag here (e.g. --nonce=<nonce>)?\n\n2. Should we only support adding gnosis safe sigs to the local state file for now? Or do we need to support a shared, remote state file (e.g. within s3 bucket) such that signers on different machines could add their signatures without passing the state file around? Trying to better understand the current signatures collection process so that we can provide a nice user-experience here", "2025-01-15T19:30:40Z", "2025-01-15T19:30:40Z", "bitwiseguy", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6dNd5I", "I_kwDODjvEJM6bmMQo", "Do we still need this?", "2025-02-05T17:18:49Z", "2025-02-05T17:18:49Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6SoN9t", "I_kwDODjvEJM6bWhjq", "@AmadiMichael, I think you're going to take this on as part of https://github.com/ethereum-optimism/design-docs/pull/144. \r\n\r\nDo you want to assign yourself this issue or do you have another one already?", "2024-11-06T15:09:16Z", "2024-11-06T15:09:16Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6SoQiL", "I_kwDODjvEJM6bWhjq", "Yeah, I'll assign this to myself though there's a more general issue here https://github.com/ethereum-optimism/optimism/issues/12634", "2024-11-06T15:13:07Z", "2024-11-06T15:13:07Z", "AmadiMichael", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6RVtxA", "I_kwDODjvEJM6bWgFL", "[Also](https://github.com/ethereum-optimism/design-docs/pull/97#issuecomment-2430181814): \r\n \r\n> I think the second is doable so long as op-node can load it from the latest L1 block reliably. Otherwise it doesn't know which block the system config first existed in. In most cases (probably not all) op-node walks along L1 from the start block forwards loading data from the block its up to so it has a consistent view. This would need to be a case where it always goes to the latest value. The start block has to be immutable so that seems reasonable to me.\r\n\r\n", "2024-10-25T17:18:48Z", "2024-10-25T17:18:57Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6UrsS_", "I_kwDODjvEJM6bWezO", "I implemented a test for this in the PR which was merged then reverted and will be merged again.", "2024-11-22T18:19:55Z", "2024-11-22T18:19:55Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6W-Pcy", "I_kwDODjvEJM6bWezO", "Re-opening as #12746 was closed.", "2024-12-10T21:10:26Z", "2024-12-10T21:10:26Z", "maurelian", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c6iIF", "I_kwDODjvEJM6bDU78", "See https://docs.optimism.io/builders/chain-operators/tutorials/create-l2-rollup", "2025-02-04T02:01:58Z", "2025-02-04T02:01:58Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c_tIc", "I_kwDODjvEJM6aVb-f", "There's some overlap in block-building of verifier and sequencer, resulting in this. More of a feature than a bug at this point. So let's leave it as-is. We may simplify it more in future refactors that improve the engine package.\n", "2025-02-04T13:31:24Z", "2025-02-04T13:31:24Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6Pgp5z", "I_kwDODjvEJM6Z4ABb", "We are in the process of deprecating the docker composed based devnet in favor of using kurtosis. See https://github.com/ethereum-optimism/optimism/issues/11562 for details", "2024-10-11T15:58:05Z", "2024-10-11T15:58:05Z", "tynes", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6XyBKl", "I_kwDODjvEJM6Z3tOX", "I think this task needs to be reviewed, as it potentially isn't accurate to supervisor behavior anymore. The supervisor tracks the dependency set, and warns when the there is no Managed Node to do L1 derivation against.", "2024-12-16T19:28:50Z", "2024-12-16T19:28:50Z", "axelKingsley", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c6YN4", "I_kwDODjvEJM6Z3tOX", "Stale, closing.", "2025-02-04T01:23:54Z", "2025-02-04T01:23:54Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6WPirr", "I_kwDODjvEJM6ZwbiL", "Working on it on #13318 ", "2024-12-05T15:38:55Z", "2024-12-09T13:33:10Z", "JosepBove", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6PZDAI", "I_kwDODjvEJM6Zvk6d", "> Should be done in the Circle CI workflow instead of in GitHub actions.\r\n\r\nNote that we deliberately used github actions instead of CircleCI to leverage https://github.com/crytic/slither-action, which makes it easier for findings to be posted as comments and PRs + show up in the repo Security tab", "2024-10-10T17:46:59Z", "2024-10-10T17:46:59Z", "mds1", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6OSffS", "I_kwDODjvEJM6YlnbE", "A two step ownable has been explored previously and we generally want to move away from dependency on openzeppelin were possible. This would look like rolling our own role based auth system", "2024-10-01T22:42:37Z", "2024-10-01T22:42:37Z", "tynes", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6Nk1RD", "I_kwDODjvEJM6X5oRb", "A second report:\r\n\r\nAs it happens, we actually experienced this (or some version of it) twice over the last few days. Based on our latest investigation, it appears that part of the problem is due to the following behaviour:\r\n\r\n1. The Sequencers have the `OP_NODE_P2P_STATIC` env var set to the relay peer IDs.\r\n2. At Sequencer startup time, this flag is respected and the sequencers appear to connect to the listed static peers as expected.\r\n3. However, if one of the static peers restarts while the Sequnecer is running, the Sequencer attempts to re-connect to the peer via a different IP and not the one listed in `OP_NODE_P2P_STATIC`. The different IP is not accessible from the Sequencer so the reconnection attempt fails.\r\nA Sequencer restart generally fixes the issue - presumably because it connects via the IP specified in `OP_NODE_P2P_STATIC` rather than the other one. However, this makes Relay operations much riskier and operationally heavy since they always require a sequencer restart as well.", "2024-09-25T20:56:17Z", "2024-09-25T20:56:17Z", "sbvegan", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6Q__ls", "I_kwDODjvEJM6X5oRb", "This seems related to: https://github.com/ethereum-optimism/optimism/pull/11781", "2024-10-23T15:48:47Z", "2024-10-23T15:48:47Z", "sbvegan", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c2p4s", "I_kwDODjvEJM6X5oRb", "This problem will be addressed by https://github.com/ethereum-optimism/optimism/issues/14108\n", "2025-02-03T17:03:51Z", "2025-02-03T17:03:51Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6NFQl9", "I_kwDODjvEJM6XbYJM", "I would strongly prefer to avoid using feature branches and instead make inclusion in Holocene optional by using a feature toggle specific to this functionality.  ie instead of `IsHoloceneEnabled` use `IsL2WithdrawalsRootEnabled`. While we are on track to include this in Holocene `IsL2WithdrawalsRootEnabled` can just delegate to `IsHoloceneEnabled` and if we decide not to include it we simply change it to return false.\r\n\r\nUsing a feature branch breaks continuous integration so we wind up having to deal with merge conflicts and aren't testing the the combined code until close to release which significantly increases the risk of bugs going undetected.", "2024-09-22T21:09:56Z", "2024-09-22T21:09:56Z", "ajsutton", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6NNIBa", "I_kwDODjvEJM6XbYJM", "Agree with @ajsutton on using feature flags instead of feature branches. We've used a light version of this as a showcase at https://github.com/ethereum-optimism/optimism/commit/1958945c9a1e7d4e30129f8d4fb97c463189476c.", "2024-09-23T18:15:21Z", "2024-09-23T18:15:21Z", "sebastianst", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6NNhlF", "I_kwDODjvEJM6XbYJM", "> Specs update to describe CL ExecutionPayload SSZ encoding with new withdrawals-root included.\r\n>    -> We remove the withdrawals list from the encoding, diverging from the L1 beacon-chain ExecutionPayload encoding.\r\n\r\nI disagree that we should be changing up the `ExecutionPayload`. Creating a new type here and/or having disparate encoding rules is pretty painful, especially when they're not just additive.\r\n\r\nWhat do you think about just keeping the withdrawals list non-nil + empty, like we do post-Shanghai? LMK if I'm missing something, but it doesn't seem like we need the bridge storage root in the payload, nor change up the payload's understanding of beacon-withdrawals being empty.", "2024-09-23T19:04:54Z", "2024-09-23T19:04:54Z", "clabby", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6NPYBT", "I_kwDODjvEJM6XbYJM", "> What do you think about just keeping the withdrawals list non-nil + empty, like we do post-Shanghai? LMK if I'm missing something, but it doesn't seem like we need the bridge storage root in the payload, nor change up the payload's understanding of beacon-withdrawals being empty.\r\n\r\nWe do need it in `ExecutionPayload`, otherwise we're unable to relay the withdrawals-root on p2p gossip, and then the op-node won't be able to insert it as unsafe block into the EL node.\r\n\r\nWe can keep the list around as always-empty list for compatibility, but the new attribute will break the gossip payload regardless.\r\nAnd we'll need to break the engine API with a new function argument if we don't add it as optional RPC field to the `ExecutableData` (the RPC equivalent of `ExecutionPayload`). I was hoping to add it to the execution-payload-envelope, but then more has to change in the RPC than would have if we modify the payload itself. Open to suggestions for a better diff though.\r\n", "2024-09-23T22:13:47Z", "2024-09-23T22:13:47Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6NPZLh", "I_kwDODjvEJM6XbYJM", "Ok, feature-flags, not feature-branch, it is. But please do document the changes / PRs, so the shared diff can be reviewed easily, especially if this work is picked up by a newer contributor. It's important that the changes are all consistent with each other and the specs.\r\n", "2024-09-23T22:16:53Z", "2024-09-23T22:16:53Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6NUNbX", "I_kwDODjvEJM6XbYJM", "@vdamle as agreed with @tynes and @protolambda on discord, I assigned this issue to you, so we can start looking at it, as part of the holocene hardfork work.", "2024-09-24T10:30:03Z", "2024-09-24T10:30:03Z", "BlocksOnAChain", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6NW9Qc", "I_kwDODjvEJM6XbYJM", "@protolambda Makes sense. I prefer the idea of extending the type rather than removing the old field on top of that.", "2024-09-24T15:16:28Z", "2024-09-24T15:16:28Z", "clabby", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6NjK3a", "I_kwDODjvEJM6XbYJM", "@vdamle I created a [new milestone](https://github.com/ethereum-optimism/optimism/milestone/10) for this feature, please add all related issues and PRs to it going forward.", "2024-09-25T18:01:30Z", "2024-09-25T18:01:30Z", "sebastianst", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6RXLyF", "I_kwDODjvEJM6XbYJM", "As I conclude the work in op-geth, I observe that there's a change required in [superchain-registry](https://github.com/ethereum-optimism/superchain-registry/blob/main/superchain/superchain.go#L480-L502) to:\n\n* add info about Isthmus fork.\n* ~read the `withdrawalsRoot` from the genesis data, if Isthmus is active.~\n\n~Subsequently, `op-geth`'s setup of genesis when reading from superchain-registry needs to be updated.~\n\nPR in superchain-registry to add `IsthmusTime`: https://github.com/ethereum-optimism/superchain-registry/pull/692", "2024-10-25T20:48:57Z", "2024-11-18T17:18:12Z", "vdamle", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6Za67Q", "I_kwDODjvEJM6XbYJM", "Sharing product context on the importance of this feature-\n\nProblem:\nEvery chain operator currently needs to run a proposer, which requires running an archive node to propose output roots. This becomes increasingly difficult as archive nodes become more difficult to run. \nAlso as chains like Base move all archive nodes to Reth this becomes more of a problem since Reth archive nodes struggle to run `eth_getProof` for proposing\n\nThe OP Labs dispute monitoring service (`op-dispute-mon`) requires running archive nodes for every chain in the Superchain, which is high lift for OP Labs to run stably, making it harder to guarantee the security of fault proofs for the chains in the Superchain as the number of chains scales\n\nSeparately, as we start treating Reth as a first class citizen within the OP stack, we have an overarching goal to enable the use of either Geth or Reth archive nodes to execute proofs.\n\nSolution:\n\nThis allows proposers to just run a full node, lowering lift for chain operators\n\nThis also gives us more confidence in our ability to safely monitor the security of withdrawals in the Superchain, by allowing us to only have to run full nodes for chains to operate `op-dispute-mon`.\n", "2025-01-06T21:37:16Z", "2025-01-06T21:37:16Z", "K-Ho", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6c6cVm", "I_kwDODjvEJM6XbYJM", "Closing, this is part of Isthmus and has landed as part of https://github.com/ethereum-optimism/optimism/pull/13962\n", "2025-02-04T01:39:31Z", "2025-02-04T01:39:31Z", "protolambda", "2025-08-31 01:47:13"]
["IC_kwDODjvEJM6MmZZd", "I_kwDODjvEJM6XBf3e", "When fault proofs are enabled, the `L2OutputOracle` is no longer in the hot path of the system. I recommend using the fault proof based system with the permissioned dispute game if you plan on launching a new chain that doesn't have a live proof system. This would be very similar to how the `L2OutputOracle` works in practice.\r\n\r\nThe real fix for this problem would be to make the proposer send transactions in parallel. It falls behind due to it waiting for the transaction to be included before sending the next one.", "2024-09-18T16:07:43Z", "2024-09-18T16:07:43Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6NHJHq", "I_kwDODjvEJM6XBf3e", "OK, makes sense. Should we close this as \"won't fix\" then, or is anyone interested in improving the situation for the L2OO?", "2024-09-23T07:48:28Z", "2024-09-23T07:48:28Z", "karlb", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAJg3", "I_kwDODjvEJM6XBf3e", "Closing as won't fix due to inactivity.", "2025-02-04T14:01:53Z", "2025-02-04T14:01:53Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6Z9s", "I_kwDODjvEJM6Wvo5H", "This is mostly done, but there still are some weird remnants of op-deployer in op-chain-ops that would be nice to clean up.\nSee legacy build entrypoint: https://github.com/ethereum-optimism/optimism/blob/216b0851b92ed34faeb58aa5a7e1ce08b545d342/op-chain-ops/justfile#L21\n\nI see the `op-stack-docker` file nicely passes the git info via env because we don\u2019t load the git into the docker build context. But then the `Makefile` calls the `justfile`, which again tries to get all version vars from git, which aren't there in the docker build context.\nAnd since it uses a `just` recipe for the build instead of a plain `go build` command. So I am not sure if the go mod/build cache that is configured in the `Dockerfile` is being applied.\n\nThere\u2019s also a related open PR from a contributor who removes the version var override from op-deployer / op-program builds, but those version vars still exist but were moved. See https://github.com/ethereum-optimism/optimism/pull/13553\n\n", "2025-02-04T01:30:14Z", "2025-02-04T01:30:14Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6ioF", "I_kwDODjvEJM6WiXMC", "Can this be closed?\n", "2025-02-04T02:04:01Z", "2025-02-04T02:04:01Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAv02", "I_kwDODjvEJM6WiXMC", "@protolambda Yes! Thanks for the heads up. Closing it. This was a straggler issue we had opened when the check for matching interfaces and contracts was added.\n\nThis is now resolved.", "2025-02-04T14:53:17Z", "2025-02-04T14:53:17Z", "0xng", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6VRQ", "I_kwDODjvEJM6WYqS7", "Closing this. Doesn't really need to be that configurable, and we're going to migrate to op-deployer, to unify the chain-genesis code more.", "2025-02-04T01:14:10Z", "2025-02-04T01:14:10Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6LyiP1", "I_kwDODjvEJM6VkhzU", "This issue can be fixed by using the `p2p.sync.onlyreqtostatic` flag introduced [here](https://github.com/ethereum-optimism/optimism/pull/11011).", "2024-09-12T05:04:25Z", "2024-09-12T05:04:25Z", "zhiqiangxu", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6Lyrf9", "I_kwDODjvEJM6VkhzU", "I wondered where that static code came from. You'll be pleased to learn the PR mitigates the need for the flag.", "2024-09-12T05:41:27Z", "2024-09-12T05:41:27Z", "anacrolix", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6LytHR", "I_kwDODjvEJM6VkhzU", "Yeah the ultimate goal is the same: to find trusted nodes to sync, either manually by `p2p.sync.onlyreqtostatic`, or automatically with your change :)", "2024-09-12T05:47:31Z", "2024-09-12T05:56:01Z", "zhiqiangxu", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c2qxA", "I_kwDODjvEJM6VkhzU", "We are no longer aiming to support req-resp p2p sync, and instead we aim for removal of it, in favor of better integration with execution-layer sync.\n\nSee issue https://github.com/ethereum-optimism/optimism/issues/14108\n", "2025-02-03T17:05:17Z", "2025-02-03T17:05:17Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6LDzPh", "I_kwDODjvEJM6ViXDC", "New chains should set the start block to 0 in the config. This is a footgun right now", "2024-09-06T01:46:13Z", "2024-09-06T01:46:13Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dA17H", "I_kwDODjvEJM6ViXDC", "Please give op-deployer a try if you have not already: it automates the chain deployment, and removes footguns like these.\n", "2025-02-04T15:02:00Z", "2025-02-04T15:02:00Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6Rs1n1", "I_kwDODjvEJM6VhGQ1", "We have the `L1BlockNumber` contract for backwards compatibility. The only reason for the deployer allowlist at this point is to prevent that address from being used again. We could drop it from genesis generation", "2024-10-29T14:33:57Z", "2024-10-29T14:33:57Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6Rs3BK", "I_kwDODjvEJM6VhGQ1", "> The only reason for the deployer allowlist at this point is to prevent that address from being used again\r\n\r\nWe should have some way of ensuring this. \r\n\r\nPerhaps a `contract Deprecated` with a reverting fallback?", "2024-10-29T14:36:01Z", "2024-10-29T14:36:01Z", "maurelian", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6Rs4an", "I_kwDODjvEJM6VhGQ1", "I like `contract Deprecated`", "2024-10-29T14:38:09Z", "2024-10-29T14:38:09Z", "smartcontracts", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6KtB-u", "I_kwDODjvEJM6UUdVi", "Hi Ivan,\r\n\r\nThe `contracts-bedrock` image is used for our internal deployment tools and is built outside of the smart contract release process, which leads to issues like the one you described above. For that reason we're looking into building a canonical contracts image, which we'll backport to all recommended versions. We'll also improve the tooling inside the image so it's easier to deploy contracts using it.\r\n\r\nI don't have an ETA for you on this yet - will let you know once I have it.", "2024-09-03T17:25:32Z", "2024-09-03T17:26:18Z", "mslipper", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6jSr", "I_kwDODjvEJM6Tyhum", "It's gone \ud83c\udf89 ", "2025-02-04T02:07:03Z", "2025-02-04T02:07:03Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6I1Gvb", "I_kwDODjvEJM6TXByV", "You should be able to pass in the arguments that would otherwise be autofilled by loading the config. These arguments should act as overrides compared to values sourced from the config. Good find!", "2024-08-19T04:01:03Z", "2024-08-19T04:01:03Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBXx_", "I_kwDODjvEJM6S7ryS", "closed with: https://github.com/ethereum-optimism/optimism/pull/11475", "2025-02-04T15:52:20Z", "2025-02-04T15:52:20Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBXcc", "I_kwDODjvEJM6SoxXb", "closing as stale issue", "2025-02-04T15:51:48Z", "2025-02-04T15:51:48Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6OxTbh", "I_kwDODjvEJM6R5uOv", "I've encountered the same problem where `alchemy_getTransactionReceipts` is being called when using the Holesky network as L1, leading to errors since this RPC method is not supported. I've come up with a temporary workaround, that I hope this helps others who are facing the same problem until a more permanent solution is available.\r\n\r\n![image](https://github.com/user-attachments/assets/b9c4e601-da00-4740-881e-c3fc57ee91ca)", "2024-10-06T04:29:06Z", "2024-10-06T04:39:30Z", "log2cn", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBXIh", "I_kwDODjvEJM6R5uOv", "@siddharth0a ok to close this issue now, since it's stale for a while now?", "2025-02-04T15:51:21Z", "2025-02-04T15:51:21Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dA1DS", "I_kwDODjvEJM6RzzNa", "Sorry to bump this old issue, the ops work and responsibilities in the monorepo have changed a lot in the last half year or so.\n\nIs this still an issue? I thought this would have been fixed since https://github.com/ethereum-optimism/optimism/pull/10827\n", "2025-02-04T15:00:46Z", "2025-02-04T15:00:46Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dE0Hl", "I_kwDODjvEJM6RzzNa", "Closing this one, v1.8.0 likely still doesn't work on arm, but I've been using recent builds on arm without problems.", "2025-02-04T23:23:06Z", "2025-02-04T23:23:06Z", "ajsutton", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBURK", "I_kwDODjvEJM6RzFBE", "closing as stale issue", "2025-02-04T15:46:57Z", "2025-02-04T15:46:57Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBV7s", "I_kwDODjvEJM6RlQQH", "closed with: https://github.com/ethereum-optimism/optimism/pull/11192#issuecomment-2243047612", "2025-02-04T15:49:30Z", "2025-02-04T15:49:30Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6U6Asg", "I_kwDODjvEJM6RV0sI", "Some of this was implemented in the op-e2e system and action interop tests, but we still need a more complete test that covers everything together.", "2024-11-25T14:51:26Z", "2024-11-25T14:51:26Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c12H8", "I_kwDODjvEJM6RV0sI", "Stale issue. Remaining testing is covered in #13855 ", "2025-02-03T15:39:59Z", "2025-02-03T15:39:59Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6FZnA1", "I_kwDODjvEJM6PmW4T", "That\u2019s a good idea. It seems like it will make a lot of things possible.\r\n\r\nOne thing that just came to mind is, adding additional features as plugins is great, but how about also making the components in the monorepo into plugins? such as op-batcher or op-conductor.\r\n\r\nWe\u2019re currently facing complexity in configuration and infrastructure because we need to connect multiple components over the network. If we switch to event-based communication, wouldn\u2019t it simplify the deployment and operation of applications that require many features like a sequencer?", "2024-07-19T04:17:10Z", "2024-07-19T04:17:10Z", "ImTei", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6GXck2", "I_kwDODjvEJM6PmW4T", "I definitely could see myself having a use case for this in the future!", "2024-07-28T00:25:35Z", "2024-07-28T00:25:35Z", "roninjin10", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6FANwN", "I_kwDODjvEJM6PfJAA", "This could be done by reading the 1967 implementation slot and if its set, checking that the implementation has bytecode", "2024-07-16T17:12:10Z", "2024-07-16T17:12:10Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6FBujg", "I_kwDODjvEJM6PfJAA", "@tynes I mean this kind of inconsistency: as `alloc.json` and `genesis.json` are generated separately in solidity and golang, it can happen that when `alloc.json` is generated, `useXXX` is false but when `genesis.json` is generated, `useXXX` is true.\r\n\r\nIn this case, the 1967 implementation slot check won't work as it's not set . This is an actual bug I stepped into once so I hope it can be patched.\r\n\r\nThe correct check should be: `if useXXX && 1967 implementation slot is 0 { return error }`\r\n\r\nBut since optional deploys are quite few maybe just hardcoding all the cases will be enough?", "2024-07-16T21:02:05Z", "2024-07-16T21:53:18Z", "zhiqiangxu", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6FIskU", "I_kwDODjvEJM6PfJAA", "We make the assumption that the same deploy config is used for both scripts at the moment. There isn't really a great way to handle all edge cases in an easy to maintain way. We would be open to you opening a PR that adds these checks, otherwise its not going to be something that we prioritize in the short term", "2024-07-17T15:52:16Z", "2024-07-17T15:52:16Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAPEj", "I_kwDODjvEJM6PfJAA", "Is this issue still relevant, now that op-deployer has replaced the old L2 genesis / chain deployment flow?\n", "2025-02-04T14:10:44Z", "2025-02-04T14:10:44Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6dyv", "I_kwDODjvEJM6PMeh2", "The op-conductor solves this already, closing.\n", "2025-02-04T01:45:37Z", "2025-02-04T01:45:37Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAQdL", "I_kwDODjvEJM6PH6Pn", "The solidity stack has a style guide now: https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/meta/STYLE_GUIDE.md\nClosing, please open a PR with changes + style guide updates if you strongly believe further changes are necessary.", "2025-02-04T14:13:02Z", "2025-02-04T14:13:02Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6EQtKL", "I_kwDODjvEJM6PA8e6", "we have a config check to ensure config persistence is disabled when conductor is enabled\r\nhttps://github.com/ethereum-optimism/optimism/blob/develop/op-node/node/config.go#L166-L173", "2024-07-09T23:28:46Z", "2024-07-09T23:28:46Z", "zhwrd", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6NFTIt", "I_kwDODjvEJM6PA8e6", "Great observation! This situation could happen, when starting sequencer, leadership transfer could happen and conductor could become a non-leader.\r\n\r\nHowever, we don't need to do the check here because\r\n1. there isn't really a way to guarantee that leadership does not transfer between the time we call startSequencer and it is actually started\r\n2. and if the leadership transfer happened during that time, it is fine as sequencer won't be able to gossip its block out, and will shortly be turned off by a functioning conductor", "2024-09-22T21:40:02Z", "2024-09-22T21:40:02Z", "0x00101010", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6ddy", "I_kwDODjvEJM6PA8e6", "Closing, let me know if we missed anything still.\n", "2025-02-04T01:44:12Z", "2025-02-04T01:44:12Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6Gj8k8", "I_kwDODjvEJM6O4gXB", "Exactly the same problem", "2024-07-30T06:30:52Z", "2024-07-30T06:30:52Z", "redareda9", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dALdd", "I_kwDODjvEJM6O4gXB", "The p2p sync request system wasn't always stable, since nodes would make requests to other nodes that also don't have the data.\nWe are removing it in #14108 and changing op-node to rely more on execution-layer sync, which is much more stable and able to provide a good sync UX.\n", "2025-02-04T14:04:58Z", "2025-02-04T14:04:58Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6FnyeL", "I_kwDODjvEJM6Ow2vk", "Forge `forge coverage` has a `--minimal-ir` option that allows some optimizations at the cost of potentially breaking source maps in some places.", "2024-07-21T22:49:57Z", "2024-07-21T22:49:57Z", "iainnash", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6H0flm", "I_kwDODjvEJM6Ow2vk", "We also get stack too deep when compiling with via-ir, so the `--ir-minimum` will not work for us. (More info in https://github.com/ethereum/solidity/issues/14358#issuecomment-2086251107, I believe even when marking blocks assembly-safe we still get the issue)\r\n\r\nSince coverage maps are still not ideal with via-ir, the best solution here is to just fix all stack too deep sources so we can compile with no optimizer. Below is a snippet that will list all contracts that fail to compile due to stack too deep:\r\n```sh\r\nwhile IFS= read -r -d '' file; do\r\n  solc $(forge re) \"$file\" --bin 2>&1 | grep -q \"Stack too deep\"\r\n  if [ $? -eq 0 ]; then\r\n    echo \"Stack too deep error found in $file\"\r\n  fi\r\ndone < <(find . -type f -name \"*.sol\" -print0)\r\n```\r\n\r\nMost contracts that get listed are because a contract they inherit from (like `Deploy` and `LibKeccak` IIRC) has stack too deep itself, so in reality its probably <5 contracts to fix. What I mean is, given:\r\n\r\n```solidity\r\ncontract A { /*compiles ok */ }\r\ncontract B is A { /* stack too deep */ }\r\ncontract C is B { /* stack too deep */ }\r\n```\r\n\r\nthe above snippet will output\r\n```\r\nStack too deep error found in B.sol\r\nStack too deep error found in C.sol\r\n```\r\n\r\nBut in reality, we most likely only need to fix B.sol, as it's the cause of the stack too deep in C as well", "2024-08-09T20:20:35Z", "2024-08-09T20:20:35Z", "mds1", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6Sdk-j", "I_kwDODjvEJM6Ow2vk", "I'll take on this cc @smartcontracts can this get assigned to me?", "2024-11-05T13:47:04Z", "2024-11-05T13:47:38Z", "AmadiMichael", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6Xk9xI", "I_kwDODjvEJM6Ow2vk", "Can this get closed @tynes @smartcontracts ?", "2024-12-14T10:41:16Z", "2024-12-14T10:41:16Z", "AmadiMichael", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6amyiR", "I_kwDODjvEJM6Npmb5", "Giving this task a look over since it is quite old. I think the premise of the task is no longer correct, but there is still some work to be done.\n\nIn the past, it was possible for a Supervisor to:\n- Sync from node inputs\n- Go down for some period\n- Nodes experience a reorg in the meantime\n- Supervisor comes back up and needs to acclimate to the post-reorg reality. The supervisor should prune and resync now (this task)\n\n**Thinking about L1 Reorgs Specifically** -- \nBecause L1 data *must* flow through the Supervisor, any L1 reorg will be experienced by the Supervisor first, and nodes couldn't possibly experience one without the Supervisor actively driving it.\n\nA Supervisor would:\n- Sync from node inputs (based on L1 references *it* provides)\n- Go down for some period\n- Nodes *will not* process any new L1 inputs because the Supervisor can't supply them\n- Supervisor comes back up, and starts driving the nodes again\n\nBecause the Supervisor holds the truth, it is actually the Node who will experience a reset, which will happen as soon as the Supervisor notices the mismatch.\n\n**Thinking about L2 Reorgs Specifically** -- \nIt is possible that some indexed log data may become invalid during the time the Supervisor is down. The Node would provide Logs some logs to the Supervisor, and while the Supervisor is down an unsafe reorg occurs, making these logs invalid. Then, when the Supervisor comes back up, it knows to request the next available data, and what it discovers is that the new log data (or derived L2 data) *does not* build on the data already had in the Supervisor.\n\nThis unsafe reorg case would indeed need handling -- the Supervisor should correctly walk-back the node with incremental resets, and they would eventually arrive at the same head. However, the Supervisor would *also* need to prune data in this case, since resetting the node is only part of the solution, without pruning the op-node would go right back to suggesting log data which can't sit on top of the database.", "2025-01-15T20:23:03Z", "2025-01-15T20:23:03Z", "axelKingsley", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6am7N_", "I_kwDODjvEJM6Npmb5", "I've edited this task to be more correct to the current architecture. I still think there are some valuable activities we're missing during reset.", "2025-01-15T20:42:57Z", "2025-01-15T20:42:57Z", "axelKingsley", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6dKx", "I_kwDODjvEJM6Npmb5", "Stale github issue, reorg support has been implemented", "2025-02-04T01:43:00Z", "2025-02-04T01:43:00Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6jlt", "I_kwDODjvEJM6NoZBk", "This seems stale, have not heard of this issue in the past half year, and we soon remove the old sync-mode in #14108 ", "2025-02-04T02:08:23Z", "2025-02-04T02:08:23Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6cpx", "I_kwDODjvEJM6Nn9uk", "This is stable enough for now, and no longer an issue with interop, when we index every safety-increment in the op-supervisor.\n", "2025-02-04T01:40:55Z", "2025-02-04T01:40:55Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dALWD", "I_kwDODjvEJM6M-icU", "@sebastianst ok to close, since it's looking stale for over 6 monts?", "2025-02-04T14:04:48Z", "2025-02-04T14:04:48Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAOGA", "I_kwDODjvEJM6MSi5w", "Closing due to inactivity.", "2025-02-04T14:09:07Z", "2025-02-04T14:09:07Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6U6MPS", "I_kwDODjvEJM6MirPP", "Everything currently in the tracker is either done or stale. Reorg-handling is tracked in new smaller tickets. And we use milestones instead of trackers now. Closing this.", "2024-11-25T15:09:06Z", "2024-11-25T15:09:06Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6j1q", "I_kwDODjvEJM6IjpAs", "Closing, not much we can do about the current startup routine.", "2025-02-04T02:09:26Z", "2025-02-04T02:09:26Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAMnL", "I_kwDODjvEJM6Mq-Lg", "@sebastianst ok to close, since it's looking stale?", "2025-02-04T14:06:41Z", "2025-02-04T14:06:41Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6CFDR9", "I_kwDODjvEJM6Fk5hD", "@axelKingsley @sebastianst do you folks think we can close this one?", "2024-06-21T09:19:23Z", "2024-06-21T09:19:23Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6H1goQ", "I_kwDODjvEJM6Fk5hD", "Actually getting a bug now. I believe the cause is that they changed their image's name?\r\nHad to update use image `ghcr.io/latticexyz/sentinel:tc-build-workflow` in the ops-bedrock/docker-compose.yaml file", "2024-08-10T02:51:14Z", "2024-08-10T02:51:14Z", "samlaf", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM55leWV", "I_kwDODjvEJM6DvCB8", "I have the same observation and the node has connectivity issues therefore. also I don't see udp supported in the output list of  opp2p_self  like in the output of the thread creator. \r\n\r\nI actually explicitly allowed UDP on the firefall and the container and it looks to me that it shoudl be enabled by the default config.", "2024-04-05T13:54:22Z", "2024-04-05T13:54:22Z", "goldsquid", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5_rYM6", "I_kwDODjvEJM6DvCB8", "This issue on the surface looks like its either the networking setup or the p2p node discovery. Some things to check: \r\n\r\n- Docker network mode. In particular, in bridge mode, ensure port mapping is properly configured\r\n- Container firewall configuration on the host\r\n- Host / Docker container IP conflicts\r\n- Probe the ports to see if there are connectivity issues across the instances\r\n\r\nIf all of that looks good, then we'd suggest you look at the `op-node` logs and see if its reporting about p2p connection attempts and why they fail, if peers get banned, etc. If you find any of those, it'll be helpful for debugging.\r\n", "2024-05-31T12:57:09Z", "2024-05-31T12:57:09Z", "sbvegan", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6kgM", "I_kwDODjvEJM6DvCB8", "Closing, this seems outdated.\n\nBesides, the ENR in the response, when viewed with https://enr-viewer.com/ has the correct IP in it. I think the IP reported separate from that is just from libp2p. The ENR is all that matters for discovery.\n", "2025-02-04T02:12:09Z", "2025-02-04T02:12:09Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAKiL", "I_kwDODjvEJM6Ch2IZ", "@Ratimon ok to close this now?", "2025-02-04T14:03:27Z", "2025-02-04T14:03:27Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAL8g", "I_kwDODjvEJM6Ch2IZ", "\n\n\n\n> [@Ratimon](https://github.com/Ratimon) ok to close this now?\n\nSure!! Thanks!! ", "2025-02-04T14:05:34Z", "2025-02-04T14:05:34Z", "Ratimon", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dA8w_", "I_kwDODjvEJM6Ch2IZ", "Great, closing.", "2025-02-04T15:12:38Z", "2025-02-04T15:12:38Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBL_s", "I_kwDODjvEJM6Mq_1p", "closing as stale issue", "2025-02-04T15:35:01Z", "2025-02-04T15:35:01Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBLbY", "I_kwDODjvEJM6MrAOV", "@sebastianst ok to close as a stale issue?", "2025-02-04T15:34:13Z", "2025-02-04T15:34:13Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBXIr", "I_kwDODjvEJM6MrAOV", "We won't be working on this any time soon, so ok to close. We can reopen if this becomes relevant again.", "2025-02-04T15:51:21Z", "2025-02-04T15:51:21Z", "sebastianst", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6BuBDJ", "I_kwDODjvEJM6MrAFg", "`errors.Is` should walk the entire chain of errors to find any that match.  So long as the parent error is always added with `%w` (not `%v`) it should work fine.", "2024-02-18T21:22:22Z", "2024-02-18T21:22:22Z", "ajsutton", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6CFEwH", "I_kwDODjvEJM6MrAFg", "@axelKingsley do we plan to pick this one up in the near future or we can close it as a s stale issue for now?", "2024-06-21T09:22:19Z", "2024-06-21T09:22:19Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c_i52", "I_kwDODjvEJM6MrAFg", "This was refactored a bunch during the events changes in May/June last year. This issue seems stale now. Closing.", "2025-02-04T13:22:54Z", "2025-02-04T13:22:54Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5z22jt", "I_kwDODjvEJM5-ndun", "See also: https://github.com/ethereum-optimism/client-pod/issues/397", "2024-02-14T13:22:56Z", "2024-02-14T13:22:56Z", "mds1", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM52tNLn", "I_kwDODjvEJM5-ndun", "nix can do this for you ser", "2024-03-12T12:40:47Z", "2024-03-12T12:40:47Z", "sambacha", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5841lA", "I_kwDODjvEJM5-ndun", "Hi @smartcontracts, I implemented this at #10406 ", "2024-05-06T06:25:18Z", "2024-05-06T06:25:18Z", "richardgreg", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6BuCNY", "I_kwDODjvEJM6MrBH6", "How easy is it to identify the first-pending-tx?\r\n\r\nI also don't know how easy it is to know how many times fees have been bumped on a Tx. There's a retry loop, but I don't know if it exposes its `i`", "2024-01-26T15:28:09Z", "2024-01-26T15:28:09Z", "axelKingsley", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBRf2", "I_kwDODjvEJM57K2nT", "@sebastianst ok to close it now?", "2025-02-04T15:42:39Z", "2025-02-04T15:42:39Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBI_R", "I_kwDODjvEJM57KYzG", "closing as stale.", "2025-02-04T15:30:46Z", "2025-02-04T15:30:46Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6CFLuP", "I_kwDODjvEJM569NLn", "@roberto-bayardo what are the future steps for this issue and is it still relevant for you to work on?\r\nCC: @sebastianst @tynes for visibility and ideas.", "2024-06-21T09:39:41Z", "2024-06-21T09:39:41Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dANyv", "I_kwDODjvEJM569NLn", "closing as stale, since this was related to our work on Fjord.", "2025-02-04T14:08:37Z", "2025-02-04T14:08:37Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c_1Oe", "I_kwDODjvEJM6MrBRA", "closing since we don't plan to work on this.", "2025-02-04T13:37:52Z", "2025-02-04T13:37:52Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5v3S32", "I_kwDODjvEJM53fyYH", "Hi, @sebastianst. Is this issue still open? If so, I'd like to work on this. I saw this issue mentioned in #8130, and it's not assigned to anyone yet.", "2024-01-04T09:23:35Z", "2024-01-04T09:23:35Z", "AryanGodara", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5wAYj0", "I_kwDODjvEJM53fyYH", "@AryanGodara it's still up for grabs! Note that there's a somewhat related PR https://github.com/ethereum-optimism/optimism/pull/8187 but not sure if @Inphi still wants to complete it.", "2024-01-05T19:17:34Z", "2024-01-05T19:17:34Z", "sebastianst", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5wCFiS", "I_kwDODjvEJM53fyYH", "> @AryanGodara it's still up for grabs! Note that there's a somewhat related PR #8187 but not sure if @Inphi still wants to complete it.\r\n\r\nI see, I'm going through #8187 , and if there's no update from @Inphi , I'll start working on this asap. Will keep you updated :)", "2024-01-06T08:36:58Z", "2024-01-06T08:36:58Z", "AryanGodara", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5wCjA0", "I_kwDODjvEJM53fyYH", "@AryanGodara Confirmed with @Inphi that his work is not blocking or interfering much with this issue, so feel free to get started \ud83d\ude80 ", "2024-01-06T14:39:15Z", "2024-01-06T14:39:15Z", "sebastianst", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5wD7R9", "I_kwDODjvEJM53fyYH", "> @AryanGodara Confirmed with @Inphi that his work is not blocking or interfering much with this issue, so feel free to get started \ud83d\ude80\r\nThat's great, I'll start working on this now!\r\nCan you point me in the right direction on where I should begin? (if there are any pointers/hints)", "2024-01-07T14:38:19Z", "2024-01-07T14:38:19Z", "AryanGodara", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5y1FbJ", "I_kwDODjvEJM53fyYH", "Hi, @sebastianst, I have submitted a PR for this issue https://github.com/ethereum-optimism/optimism/pull/9349. I used \r\n an another solution.", "2024-02-05T09:08:27Z", "2024-02-05T09:08:27Z", "Nickqiaoo", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBKRz", "I_kwDODjvEJM53fyYH", "resolved with this PR: https://github.com/ethereum-optimism/optimism/pull/9904, I will close this issue now.", "2025-02-04T15:32:32Z", "2025-02-04T15:32:32Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBYcs", "I_kwDODjvEJM53fyYH", "I think we even removed the RethDB receipts fetcher.", "2025-02-04T15:53:20Z", "2025-02-04T15:53:20Z", "sebastianst", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6lR2", "I_kwDODjvEJM52Ck78", "Closing this, since it hasn't been worked on for over a year, and there are the unit-tests of https://github.com/ethereum-optimism/optimism/pull/8066/ at least.", "2025-02-04T02:15:36Z", "2025-02-04T02:15:36Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5q13ry", "I_kwDODjvEJM51yCwP", "You are not alone, see my comments here https://github.com/ethereum-optimism/optimism/pull/6243#issuecomment-1784393636", "2023-11-03T14:10:06Z", "2023-11-03T14:10:06Z", "unknown", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5q2OPw", "I_kwDODjvEJM51yCwP", "Same issue here. @kaliubuntu0206 did you find a way to get a faster sync going from the suggestions on that thread?", "2023-11-03T15:00:02Z", "2023-11-03T15:00:02Z", "jchung00", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5q2b9W", "I_kwDODjvEJM51yCwP", "@jchung00 I've used magi but I think op-node would be able to retrieve faster if you setup correct kind of rpc. Or maybe try using magi instead https://github.com/a16z/magi", "2023-11-03T15:31:09Z", "2023-11-03T15:31:09Z", "unknown", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5q5mxY", "I_kwDODjvEJM51yCwP", "https://github.com/ethereum-optimism/developers/discussions/44        \r\nWelcome to join the discussion here", "2023-11-04T16:12:20Z", "2023-11-04T16:12:20Z", "opfocus", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5rWuRV", "I_kwDODjvEJM51yCwP", "There are major updates in the thread above, feel free to join the discussion", "2023-11-08T05:29:44Z", "2023-11-08T05:29:44Z", "opfocus", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6MKpEp", "I_kwDODjvEJM51yCwP", "Newest Optimism blockchain snapshots can be downloaded here: https://www.publicnode.com/snapshots#optimism", "2024-09-15T13:41:52Z", "2024-09-15T13:41:52Z", "3eph1r0th", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5qy099", "I_kwDODjvEJM51ux4N", "hmm, we probably should have set `OP_E2E_USE_CANYON=false` for e2e-http and test-external.  They've probably gone back to running the canon tests again which isn't particularly helpful.\r\n\r\nIs the idea here to only have one e2e job in CI rather than separate ws and http jobs? If we get there, we could probably remove `OP_E2E_CANNON_ENABLED` as well.\r\n\r\nThe external client tests probably should stay since the idea there is to plug in something other than op-geth which makes nearly all the tests useful for compatibility testing (though long term it would be good to have a more dedicated set of reference tests).", "2023-11-02T23:41:33Z", "2023-11-02T23:41:33Z", "ajsutton", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5q8CYz", "I_kwDODjvEJM51ux4N", "Opened PR for span batch - https://github.com/ethereum-optimism/optimism/pull/8047", "2023-11-06T05:42:17Z", "2023-11-06T05:42:17Z", "ImTei", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6c6bHp", "I_kwDODjvEJM51ux4N", "This doesn't seem viable in op-e2e as-is. We have another shot at this with kurtosis, to do fork migration tests, with the acceptance-testing framework. Closing this issue as stale.", "2025-02-04T01:34:37Z", "2025-02-04T01:34:37Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5o6Yqf", "I_kwDODjvEJM5znalq", "Devnet addresses are dynamic and subject to change, you can get them at `GET localhost:8080/addresses.json` if you use the local devnet setup", "2023-10-12T18:16:29Z", "2023-10-12T18:16:29Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5o-Bu0", "I_kwDODjvEJM5znalq", "That's good to know. Wouldn't it be a good idea if devnet addresses *were* deterministic, though?", "2023-10-13T08:09:40Z", "2023-10-13T08:09:40Z", "Arachnid", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5o-D2C", "I_kwDODjvEJM5znalq", "Running `make devnet-up` doesn't result in anything serving on port 8080. I've tried the various ports exposed by the containers it starts up, but none of them return anything on /addresses.json.", "2023-10-13T08:16:01Z", "2023-10-13T08:16:01Z", "Arachnid", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5pAaoM", "I_kwDODjvEJM5znalq", "> That's good to know. Wouldn't it be a good idea if devnet addresses _were_ deterministic, though?\r\n\r\nIt would be nice for integrators but then it would add constraints to the way the system is deployed, if we did deployments with a create3 sort of design then it could work but we don't have time to refactor our deploy scripts to work like that right now.\r\n\r\nYou can see in the `docker-compose` file that an artifacts server is created, I will double check locally to ensure that there have been no regressions\r\n\r\nhttps://github.com/ethereum-optimism/optimism/blob/126454e907f3dae4c83914c9379112ae7e7825cc/ops-bedrock/docker-compose.yml#L135-L144", "2023-10-13T15:35:34Z", "2023-10-13T15:35:34Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5pAo8K", "I_kwDODjvEJM5znalq", "Why would you need a create3 design? Addresses should be deterministic as long as the deployment account and the sequence of operations is deterministic.", "2023-10-13T16:17:59Z", "2023-10-13T16:17:59Z", "Arachnid", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5pAs7_", "I_kwDODjvEJM5znalq", "> Why would you need a create3 design? Addresses should be deterministic as long as the deployment account and the sequence of operations is deterministic.\r\n\r\nI would prefer to not couple the deployment account and sequence of operations to the correctness of tests. There are some cases in which we would need to change the sequence of operations as we make changes to the smart contracts\r\n\r\nThis PR fixes the issue where the artifacts are not being served: https://github.com/ethereum-optimism/optimism/pull/7672", "2023-10-13T16:31:07Z", "2023-10-13T16:31:07Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5pKcRI", "I_kwDODjvEJM5znalq", "Thanks!", "2023-10-16T12:13:59Z", "2023-10-16T12:13:59Z", "Arachnid", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5pNJkq", "I_kwDODjvEJM5znalq", "> Thanks!\r\n\r\nPerhaps a function in the sdk that automates creating a client for the devnet would be the best abstraction for you. Open to any feedback on what makes it easy to use! Longer term we will be deprecating the sdk because it is a bit heavy to be used on frontends in favor of smaller utilities like https://github.com/base-org/op-viem ", "2023-10-16T18:25:29Z", "2023-10-16T18:25:29Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5pNW1L", "I_kwDODjvEJM5znalq", "Longer term the idea is that you should need a minimal amount of addresses to interact with the system, the `SystemConfig` has getters for the other L1 contracts but that release has not been rolled out to mainnet yet", "2023-10-16T19:04:02Z", "2023-10-16T19:04:02Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5pQQO0", "I_kwDODjvEJM5znalq", "I want to use the devnet for end-to-end testing, so anything that facilitates that would be ideal. Perhaps a devnet provider in the SDK, that knows how to look up the addresses via the artifact server?", "2023-10-17T07:54:11Z", "2023-10-17T07:54:11Z", "Arachnid", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAJwE", "I_kwDODjvEJM5znalq", "closing as a stale issue, since it's not active for over a year.", "2025-02-04T14:02:14Z", "2025-02-04T14:02:14Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5nT_v7", "I_kwDODjvEJM5xjvF4", "I definitely hope to support all clients with standard RPC in the future, the debug method is there because it simplified and derisked the MPT a lot, to unblock development of other parts of the proof stack.\r\n\r\nPrevious versions of Cannon have tried `eth_getProof` instead, but this added significant complexity at the time, since `eth_getProof` is meant to provide MPT data only as part of a branch to a leaf node. Edge-cases related to retrieving the right pre-images, even throughout MPT changes (node deletions etc. included), are challenging and resulted in several bugs.\r\n\r\n `debug_dbGet` is thus used instead to retrieve the MPT nodes, providing a much simpler hash->preimage lookup for the MPT nodes (at least when using the old geth tree based DB scheme).\r\n\r\nHowever, with new improvements to the FP design, there may be a way to make `eth_getProof` work: by utilizing the preimage-hinting system. I.e. the op-program client can write a new type of hint, that essentially says to the server: \"I'll need all preimages of state-root X and path y\", and the op-program host can prepare the preimages. The in-memory op-program oracle-backed state DB can then fetch the preimages lazily as necessary, and retain a copy of them to then build any new branches on top of, and cache for future use, without interrupting future preimage lookups, since hints are always written with full path/state-root information, unlike previous versions in the original minigeth that had to deal with MPT edits/deletions to fetch the right preimages.\r\n\r\nSo yes, I believe we can move it back to `eth_getProof` thanks to newer features that were introduced after the original MPT-proving issues with legacy Cannon/minigeth setup, it will still add a little bit of complexity, but remove the `debug_dbGet` dependency.", "2023-09-25T09:29:06Z", "2023-09-25T09:29:06Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAKMp", "I_kwDODjvEJM5xjvF4", "@protolambda @smartcontracts ok to close this now, since it's stale for over a year?", "2025-02-04T14:02:55Z", "2025-02-04T14:02:55Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dC6x6", "I_kwDODjvEJM5xjvF4", "I believe the geth execution-witness generation work solves this, but I don't believe that is being tracked everywhere. So until it is, let's keep this issue open.\n", "2025-02-04T18:46:34Z", "2025-02-04T18:46:34Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dDAtw", "I_kwDODjvEJM5xjvF4", "Actually, looks like we're tracking that in https://github.com/ethereum-optimism/optimism/issues/11551\nClosing this now.\n", "2025-02-04T18:58:19Z", "2025-02-04T18:58:19Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dAItK", "I_kwDODjvEJM5unkey", "@trianglesphere ok to close this one, since it's no longer active?", "2025-02-04T14:00:55Z", "2025-02-04T14:00:55Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dA_Fd", "I_kwDODjvEJM5unkey", "closing as stale issue.", "2025-02-04T15:16:04Z", "2025-02-04T15:16:04Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5kFngv", "I_kwDODjvEJM5uPEQT", "Def open to a PR implementing this", "2023-08-15T15:59:55Z", "2023-08-15T15:59:55Z", "tynes", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6BmASl", "I_kwDODjvEJM5uPEQT", "Stale issue, closing this. The op-node by now does retry, and with backoff, and a maximum number of attempts.", "2024-06-17T19:04:56Z", "2024-06-17T19:04:56Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6BmAqF", "I_kwDODjvEJM5uPEQT", "Correction: it does retry, but only for L1 connection, not L2.", "2024-06-17T19:06:03Z", "2024-06-17T19:06:03Z", "protolambda", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dA-uO", "I_kwDODjvEJM5uPEQT", "@vladopajic ok to close this one now?", "2025-02-04T15:15:31Z", "2025-02-04T15:15:31Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dBbRS", "I_kwDODjvEJM5uPEQT", "@BlocksOnAChain sure", "2025-02-04T15:57:44Z", "2025-02-04T15:57:44Z", "vladopajic", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM5Me10R", "I_kwDODjvEJM5S5DiU", "Taking a look at this! \r\n\r\nI see two main options for building linux/arm64 compatible containers:\r\n\r\n1. We add a new job that is similar to `docker-publish` to the circleci config (call it like `docker-arm-publish`) https://github.com/ethereum-optimism/optimism/blob/98035730167fd5698d0436f1a170586c4625823b/.circleci/config.yml#L87-L110 The difference is that we use the arm execution environment https://circleci.com/docs/using-arm/. If we set resource class to `resource_class: arm.xlarge` then we should be provisioning a GCE instance that is compatible with ARM (i think that is what circle is doing under the hood?). I tried this with a test `t2a-standard-4` instance and was able to create a test `hardhat-node` image which is listed as `linux/arm64/v8` arch: https://hub.docker.com/layers/mikeneuder/hardhat-node/1.0/images/sha256-08d3fb84f0c4717155abf1fc88eceb094e35a051cc8fa6952225236649a67c9d?context=repo. \r\n2. We use `dockerx` in the CI workflow to build cross architecture images: https://www.docker.com/blog/getting-started-with-docker-for-arm-on-linux/. IIUC this builds a single container that can run on either architecture. This doesn't really feel like the problem we are trying to solve, but I am not sure. \r\n\r\nIMO option 1 sounds more like what we want to do, but I wanted to touch base here. ", "2022-10-18T23:56:43Z", "2022-10-18T23:56:43Z", "michaelneuder", "2025-08-31 01:47:14"]
["IC_kwDODjvEJM6dA9xw", "I_kwDODjvEJM5S5DiU", "closing since it's no longer an active issue.", "2025-02-04T15:14:09Z", "2025-02-04T15:14:09Z", "BlocksOnAChain", "2025-08-31 01:47:14"]
["IC_kwDOH2Qg5s6blEok", "I_kwDOH2Qg5s6nP0XO", "Which chain are you running op-geth on? Was the node synced before starting it?", "2025-01-23T15:52:59Z", "2025-01-23T15:52:59Z", "protolambda", "2025-08-31 01:48:24"]
["IC_kwDOH2Qg5s6br0lx", "I_kwDOH2Qg5s6nP0XO", "snax chain... it was synced, we upgraded geth and that happened. we got past this by deleting everything and syncing again.", "2025-01-24T08:33:31Z", "2025-01-24T08:33:46Z", "TurtleSnail", "2025-08-31 01:48:24"]
["IC_kwDOH2Qg5s6bvM_W", "I_kwDOH2Qg5s6nP0XO", "Perhaps it had custom extradata configured?", "2025-01-24T15:48:15Z", "2025-01-24T15:48:15Z", "tynes", "2025-08-31 01:48:24"]
["IC_kwDOH2Qg5s6bvOAY", "I_kwDOH2Qg5s6nP0XO", "It could be a good idea to hex format the extra data in the panic so that we can more easily debug this in the future", "2025-01-24T15:50:15Z", "2025-01-24T15:50:15Z", "tynes", "2025-08-31 01:48:24"]
["IC_kwDOH2Qg5s6c1M52", "I_kwDOH2Qg5s6nP0XO", "This is a known issue of an outdated op-geth version @TurtleSnail when fixing the Holocene timestamp for a chain. Please upgrade to the latest release, it should handle this case.", "2025-02-03T14:40:32Z", "2025-02-03T14:40:32Z", "sebastianst", "2025-08-31 01:48:24"]
["IC_kwDOH2Qg5s6eGOhT", "I_kwDOH2Qg5s6nP0XO", "> This is a known issue of an outdated op-geth version [@TurtleSnail](https://github.com/TurtleSnail) when fixing the Holocene timestamp for a chain. Please upgrade to the latest release, it should handle this case.\n\n@sebastianst hi, I met the same issue in worldchain. I tried to upgrade the op-geth  to [v1.101411.7](https://github.com/ethereum-optimism/op-geth/releases/tag/v1.101411.7), the issue persists. Which release is the correct one?", "2025-02-12T01:28:38Z", "2025-02-13T00:58:03Z", "jiangjinyuan", "2025-08-31 01:48:24"]
["IC_kwDOH2Qg5s6eWtWl", "I_kwDOH2Qg5s6nP0XO", "> > This is a known issue of an outdated op-geth version [@TurtleSnail](https://github.com/TurtleSnail) when fixing the Holocene timestamp for a chain. Please upgrade to the latest release, it should handle this case.\n> \n> [@sebastianst](https://github.com/sebastianst) hi, I met the same issue in worldchain. I tried to upgrade the op-geth to [v1.101411.7](https://github.com/ethereum-optimism/op-geth/releases/tag/v1.101411.7), the issue persists. Which release is the correct one?\n\n@jiangjinyuan For worldchain, please upgrade to the latest release and use the `op-network` flag to select `worldchain-mainnet`.", "2025-02-13T14:23:52Z", "2025-02-13T14:23:52Z", "sebastianst", "2025-08-31 01:48:24"]
["IC_kwDOH2Qg5s6d3kwk", "I_kwDOH2Qg5s6XJT4a", "This generally happens when the node detects that the network that it is connected to is on a newer protocol version than what the node supports.\nWhich chain are you connected to? And have you tried updating op-node and op-geth to the latest version?\n", "2025-02-10T16:28:17Z", "2025-02-10T16:28:17Z", "protolambda", "2025-08-31 01:48:25"]
["IC_kwDOI7W0xc6eLSxp", "I_kwDOI7W0xc6ndNi5", "^ We can also use this as a backup for DEX volume in cases where Dune may have a chain or DEX that Defillama doesn't (i.e. short-term gaps in Uni v4)", "2025-02-12T13:46:37Z", "2025-02-12T13:46:37Z", "MSilb7", "2025-08-31 01:48:25"]
["IC_kwDOLB-lzc6eNim1", "I_kwDOLB-lzc6pzRR6", "I believe the Fault-Proof super-root snapshot is not ideal for reasoning about this invariant, since the snapshot composition depends on what was last cross-safe and what optimistic block directly follows.\n\nIf one part of the snapshot is 10 minutes behind, e.g. because a L2 hasn't batch-submitted for a while, that shouldn't mean that the next optimistic block is allowed to include data from the more recent entries in the snapshot (assuming \"horizon\" is defined as the maximum timestamp), nor should it mean the opposite of chains that frequently batch-submit cannot include things because other chains are not batch-submitting.\n\nThe timestamp invariant should be independent of the snapshot construction.\n\nI do agree with the problem of not having round-trips between unaligned block-timestamps or different block time increments though, that we should fix.\n\nSay for example, we have two chains:\n- Chain A with 1 second blocks\n- Chain B with 2 second blocks\n- block a0 with timestamp X\n- block b0 with timestamp X\n- block a1 with timestamp X+1\n- block b1 with timestamp X+2\n- block a2 with timestamp X+2\n- block b2 with timestamp X+4\n- block a3 with timestamp X+3\n\nThen we want a1 and b1 to be able to round-trip messages, because a1 cannot roundtrip with b0 (because b0 is already sealed and published), and we don't want to hold back on cross-rollup comms.\n\nAlso with flashblocks, smaller increments of chains should be able to communicate. E.g. `a0_flash3/8` and `b0_flash6/16` should both be equal at the same millisecond level, so should be able to interop. In the protocol we don't have to enshrine the flashblock boundaries, since we can check the relative relationships between messages are directional, but do need to check some invariants to limit the scope of messages to verify.\n\nDoes this match your understanding of the problem?\n\nI think we can change the timestamp invariant to be more relaxed, based on the next expected block time, to fix this.\n", "2025-02-12T17:08:02Z", "2025-02-12T17:08:02Z", "protolambda", "2025-08-31 01:48:28"]
["IC_kwDOLB-lzc6eN7ec", "I_kwDOLB-lzc6pzRR6", "> The timestamp invariant should be independent of the snapshot construction.\n\nI buy this. In the proof, we have context of the snapshot construction, but I can see how leaking that into the check creates issues for the simplicity of an implementation in the context of the supervisor.\n\n> In the protocol we don't have to enshrine the flashblock boundaries, since we can check the relative relationships between messages are directional, but do need to check some invariants to limit the scope of messages to verify.\n> \n> Does this match your understanding of the problem?\n\nYep, exactly.\n\n> I think we can change the timestamp invariant to be more relaxed, based on the next expected block time, to fix this.\n\nThe next expected block timestamp of which chain, though (initiating, executing, or a mixture)? In your example, `a2` and `b2` wouldn't be able to round-trip with some interpretations of this rule. This also gets a bit more complex to think about as we expand the dependency set, with more than just 2 differing blocktimes among the set of chains.\n\nw/ existing rule (`initiating_timestamp <= executing_timestamp`):\n* `a2` (initiating) -> `b2` (executing) \u2705 (`X+2 <= X+4`)\n* `b2` (initiating) -> `a2` (executing) \u274c (`X+4 > X+2`)\n\nw/ \"next expected time\" rule (`initiating_timestamp <= next_expected_time(executing_timestamp, executing_chain_id)`):\n* `a2` (initiating) -> `b2` (executing) \u2705 (`X+2 <= X+4+2`)\n* `b2` (initiating) -> `a2` (executing) \u274c (`X+4 > X+2+1`)\n\nw/ \"next expected time\" rule (`next_expected_time(initiating_timestamp, initiating_chain_id) <= executing_timestamp`):\n* `a2` (initiating) -> `b2` (executing) \u2705 (`X+2+1 <= X+4`)\n* `b2` (initiating) -> `a2` (executing) \u274c (`X+4+2 > X+2`)\n\nw/ \"next expected time\" rule (`initiating_timestamp <= next_expected_time(executing_timestamp, initiating_chain_id)`):\n> Note: Adds the _initiating chain's_ blocktime to the _executing message's_ timestamp.\n* `a2` (initiating) -> `b2` (executing) \u2705 (`X+2 <= X+4+1`)\n* `b2` (initiating) -> `a2` (executing) \u2705 (`X+4 <= X+2+2`)\n\nw/ \"next expected time\" rule (`next_expected_time(initiating_timestamp, executing_chain_id) <= executing_timestamp`):\n> Note: Adds the _executing chain's_ blocktime to the _initiating message's_ timestamp.\n* `a2` (initiating) -> `b2` (executing) \u2705 (`X+2+2 <= X+4`)\n* `b2` (initiating) -> `a2` (executing) \u274c (`X+4+1 > X+2`)", "2025-02-12T17:51:36Z", "2025-02-12T17:57:05Z", "clabby", "2025-08-31 01:48:28"]
["IC_kwDOLB-lzc6eWTlK", "I_kwDOLB-lzc6pzRR6", "I think the right invariant to use may be:\n`initiating_msg.emitted_in.parent.timestamp < executing_msg.included_in.timestamp`\n\nThis basically changes the `a <= b` to a `a -1 < b`, but in terms of blocks instead of timestamps, to accommodate the different block times.\n\nE.g. if chain A has block time 4, and chain B 1, then 4 blocks of B can all interact with 1 block of A. After passing the point where both chains are sealed at the same time.\n\n`a2` and `b2` shouldn't be able to round-trip. The blocks `a2` and `b1` are sealed at the same wallclock time, those should round-trip instead. The `a2` and `b2` block-numbers being the same is just unfortunate coincidence.\n\n`b2` should still be able to round-trip with chain A. It should do so with the next block after this equal-timestamp roundtrip of `b1` and `a2`. So that would be `a3`.\n\nWith the chain data from my previous comment:\nvalid:\n- `a0` (init) -> `b0` (exec) `X-1 < X`\n- `b0` (init) -> `a0` (exec) `X-2 < X`\n- `a0` (init) -> `b1` (exec) `X-1 < X+2`\n- `a1` (init) -> `b1` (exec) `X <  X+2`.\n- `b1` (init) -> `a1` (exec) `X < X+1`\n- `a2` (init) -> `b1` (exec) `X+1 < X+2`\n- `b1` (init) -> `a2` (exec) `X < X+2`\n- `a3` (init) -> `b2` (exec) `X+2 < X+4`\n- `b2` (init) -> `a3` (exec) `X+2 < X+3` \ninvalid as desired:\n- `a1` (init) -> `a0` (exec) `X !< X`\n- `b1` (init) -> `a0` (exec) `X !< X`\n- `b2` (init) -> `a2` (exec) `X+2 !< X+2` (i.e. need `a2` is sealed at same time as `b1`, `a3` can be used for roundtrip with `b2`)\n\n", "2025-02-13T13:45:32Z", "2025-02-13T13:45:32Z", "protolambda", "2025-08-31 01:48:28"]
["IC_kwDOLB-lzc6ecMYz", "I_kwDOLB-lzc6pzRR6", "> If one part of the snapshot is 10 minutes behind, e.g. because a L2 hasn't batch-submitted for a while\n\nThis can't happen.  A super root _must_ include the highest possible block at or before the super root's timestamp. So if you have a combination of 1 and 2 second block times, there can be at most 1s difference between any of the output roots in the super root and every second super root would have all output roots from the same timestamp.\n\nImportantly, the blocks in the super root are the chain head for fault proofs.  So at timestamp X+1, we will have a super root containing `(a1, b0)`. If `a1` references anything in `b1` it will have to be found to be invalid by the proof system because `b1` doesn't exist yet and it may not even have batch data available on L1 yet.\n\nI struggle to see how it could ever be safe to have executing messages that reference a block with a later timestamp because the world view of proofs is limited by a timestamp and nothing after that proposal timestamp exists.", "2025-02-14T03:36:27Z", "2025-02-14T03:36:27Z", "ajsutton", "2025-08-31 01:48:28"]
["IC_kwDOLB-lzc6c4zWg", "I_kwDOLB-lzc6omLXf", "Also, `ExecutableData` looks to be a geth-specific type. Can we use the `ExecutionPayload` nomenclature here instead @protolambda?", "2025-02-03T21:40:05Z", "2025-02-03T21:40:05Z", "clabby", "2025-08-31 01:48:28"]
["IC_kwDOLB-lzc6c5JCM", "I_kwDOLB-lzc6omLXf", "l1 doesn't have a clear term for executable data other than \"response\". often used however is sidecar. we can use executable data, but then we have to define the term first.", "2025-02-03T22:07:46Z", "2025-02-03T22:07:46Z", "emhane", "2025-08-31 01:48:28"]
["IC_kwDOLB-lzc6c5JZ2", "I_kwDOLB-lzc6omLXf", "ref https://github.com/ethereum/execution-apis/blob/main/src/engine/prague.md", "2025-02-03T22:08:09Z", "2025-02-03T22:08:09Z", "emhane", "2025-08-31 01:48:28"]
["IC_kwDOMMiGhs6ekRND", "I_kwDOMMiGhs6qIJmv", "Closing as a duplicate of #315 ", "2025-02-14T21:30:52Z", "2025-02-14T21:30:52Z", "hamdiallam", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6bYbqw", "I_kwDOMMiGhs6d5Abk", "@jakim929 @fainashalts I'll upload a video of getting started with the supersim in dev mod, when I'll get paid & how much????", "2025-01-22T10:36:30Z", "2025-01-22T10:36:30Z", "BenraouaneSoufiane", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6ekDHH", "I_kwDOMMiGhs6d5Abk", "Hi @BenraouaneSoufiane we are no longer participating in Scout Game so no payments are going out. However, Supersim is Open Source and we're happy to consider any contributions you'd like to make!", "2025-02-14T20:57:09Z", "2025-02-14T20:57:09Z", "fainashalts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6S_NYo", "I_kwDOMMiGhs6dl5tw", "@jakim929 whenever u are free would like to chat about this more", "2024-11-09T04:36:05Z", "2024-11-09T04:36:05Z", "supercontracts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6S_lUT", "I_kwDOMMiGhs6dl5tw", "mostly the pr would be to this https://github.com/ethereum-optimism/optimism/tree/develop/packages/contracts-bedrock/src/L2 but just starting the discussion here", "2024-11-09T09:18:03Z", "2024-11-09T09:18:03Z", "supercontracts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6X_cKh", "I_kwDOMMiGhs6dl5tw", "Hi @tabshaikh! Neat suggestion! I see you have a draft PR open - are you still interested in pursuing this and do you need any help from the DevX team? ", "2024-12-18T00:32:24Z", "2024-12-18T00:32:24Z", "fainashalts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6YGxTP", "I_kwDOMMiGhs6dl5tw", "Hey @fainashalts, I had conversation in OP R&D channel (@tagging u in the thread there) and Mark suggested having it in a seperate repository called `interop-std` in optimism org. Temporarily i moved it all to this repo https://github.com/RiftLend/interop-std and have many more contracts like `SuperPausable`, `SuperProxyAdmin`. Would love greater discussion from op devx and contracts teams so that we can create this standard in a way that is generalized for every use case and so that it gets more adoption :)", "2024-12-18T17:36:43Z", "2024-12-18T17:36:43Z", "supercontracts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6ekF7a", "I_kwDOMMiGhs6dl5tw", "Hi @supercontracts sorry for the delay in getting back to you on this! Very cool set of contracts you've put together! We've also been talking on the devX team about providing folks with more examples of multichain-enabling contracts/execution patterns. A little unsure right know what exactly that would look like but would love to include what you have when we do it. I was envisioning a set of libraries folks can import and build on top of, so its more about how to package it up for devs to make it maximally useful.", "2025-02-14T21:01:09Z", "2025-02-14T21:01:09Z", "fainashalts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6emTBa", "I_kwDOMMiGhs6dl5tw", "@fainashalts i am already using them in riftlend's multichain lending protocol https://github.com/RiftLend/contracts-v1\nalso here is the repo - https://github.com/RiftLend/interop-std", "2025-02-15T09:32:11Z", "2025-02-15T09:32:11Z", "supercontracts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6SZkZU", "I_kwDOMMiGhs6cyaRG", "thank you for the report! - will fix this", "2024-11-05T03:53:00Z", "2024-11-05T03:53:00Z", "jakim929", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6bOO3R", "I_kwDOMMiGhs6cyaRG", "It seems not resolved, now I'm working on it, when I'll get paid? is it when the pull merged? who will pay me???", "2025-01-21T09:49:15Z", "2025-01-21T09:49:15Z", "BenraouaneSoufiane", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6SWnC3", "I_kwDOMMiGhs6cxmrO", "thank you for the report - this is good feedback will incorporate into docs.\r\n\r\nLonger term def agree that handling with better error returned from op-simulator is better, but right now I think we want to keep op-simulator as minimal as possible", "2024-11-04T18:17:39Z", "2024-11-04T18:18:58Z", "jakim929", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6bXOpJ", "I_kwDOMMiGhs6cxmrO", "@jakim929, @fainashalts  can I work on this??", "2025-01-22T08:14:37Z", "2025-01-22T10:38:12Z", "BenraouaneSoufiane", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6bamQG", "I_kwDOMMiGhs6cxmrO", "@ismailmoazami I didn't found error with 'notactivated' or 'not activated', it seems depends on the forge, also initially I didn't found issues with the existing contract with foundry (forge), I'll let you know about anything update", "2025-01-22T14:45:35Z", "2025-01-22T14:45:35Z", "BenraouaneSoufiane", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6eks_d", "I_kwDOMMiGhs6cxmrO", "This error happens if you don't specify the proper evm version in `foundry.toml`. I'm going to close this issue but we'll definitely consider making errors as descriptive as possible in supersim!", "2025-02-14T22:48:16Z", "2025-02-14T22:48:16Z", "fainashalts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6X_A1c", "I_kwDOMMiGhs6asYsb", "``", "2024-12-17T23:24:13Z", "2024-12-17T23:24:13Z", "GIgako19929", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6X_BN-", "I_kwDOMMiGhs6asYsb", "/[](url)", "2024-12-17T23:24:27Z", "2024-12-17T23:24:27Z", "GIgako19929", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6P7ibu", "I_kwDOMMiGhs6YnN9s", "Might be worthwhile to consider:\r\n* direct to a .supersim\r\n* auto delete\r\n\r\nNeeds a spike to consider the best approach.", "2024-10-15T18:41:20Z", "2024-10-15T18:41:20Z", "fainashalts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6MxqEC", "I_kwDOMMiGhs6XN3rg", "good point! this will be added soon", "2024-09-19T17:50:56Z", "2024-09-19T17:50:56Z", "jakim929", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6R9CD0", "I_kwDOMMiGhs6TBxmo", "Hey! Can I work on it? @tremarkley @jakim929 ", "2024-10-30T23:57:12Z", "2024-10-31T15:12:30Z", "Gerson2102", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6SEh5K", "I_kwDOMMiGhs6TBxmo", "@tremarkley  @jakim929  Could I work on this feature?\r\n", "2024-10-31T19:26:52Z", "2024-10-31T19:33:34Z", "Ethanol48", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6SLYf4", "I_kwDOMMiGhs6TBxmo", "@Gerson2102 feel free to take a stab!\r\n\r\nSome starters -\r\n- I think the starting point is a supersim.toml file that can be passed as a flag to supersim\r\n- Can maybe use https://github.com/BurntSushi/toml\r\n- It should try to have similar structure to the existing config object, by adding struct tags `Name toml:name`\r\n\r\n@Ethanol48 - since this one is assigned, would appreciate your contribution on any other issues! ", "2024-11-01T19:21:38Z", "2024-11-01T19:21:38Z", "jakim929", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6SLhNj", "I_kwDOMMiGhs6TBxmo", "> @Gerson2102 feel free to take a stab!\n> \n> \n> \n> Some starters -\n> \n> - I think the starting point is a supersim.toml file that can be passed as a flag to supersim\n> \n> - Can maybe use https://github.com/BurntSushi/toml\n> \n> - It should try to have similar structure to the existing config object, by adding struct tags `Name toml:name`\n> \n> \n> \n> @Ethanol48 - since this one is assigned, would appreciate your contribution on any other issues! \n\nCould I work on this issue?\nhttps://github.com/ethereum-optimism/supersim/issues/150", "2024-11-01T19:48:57Z", "2024-11-01T19:48:57Z", "Ethanol48", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6SLpxi", "I_kwDOMMiGhs6TBxmo", "Hi @jakim929. Do you guys have a Telegram group or something like that to ask questions while I'm working with this issue?", "2024-11-01T20:16:12Z", "2024-11-01T20:16:12Z", "Gerson2102", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6SMGtG", "I_kwDOMMiGhs6TBxmo", "@Gerson2102 feel free to join this one https://t.me/+IfzdrLjf2_pkNmUx ", "2024-11-01T21:59:17Z", "2024-11-01T21:59:17Z", "jakim929", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6THkPX", "I_kwDOMMiGhs6TBxmo", "I'm having problems on setting up the project :(\n\nI'm using WSL2", "2024-11-11T13:53:27Z", "2024-11-11T13:53:27Z", "Gerson2102", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6TstVd", "I_kwDOMMiGhs6TBxmo", "@Gerson2102 made any progress on this ? Can i try at it ?\n", "2024-11-15T05:06:52Z", "2024-11-15T05:06:52Z", "s29papi", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6U3_Wy", "I_kwDOMMiGhs6TBxmo", "@jakim929 can I try at this ? ", "2024-11-25T11:08:08Z", "2024-11-25T11:08:08Z", "s29papi", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6V02tV", "I_kwDOMMiGhs6TBxmo", "@tremarkley I want to take on this. Can it be re-assigned ", "2024-12-03T06:21:40Z", "2024-12-03T06:21:40Z", "s29papi", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6V90l7", "I_kwDOMMiGhs6TBxmo", "@s29papi re-assigned to you", "2024-12-04T02:11:50Z", "2024-12-04T02:11:50Z", "tremarkley", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6X_hJq", "I_kwDOMMiGhs6TBxmo", "@s29papi are you still planning to work on this? or should we reassign? thanks!", "2024-12-18T00:44:38Z", "2024-12-18T00:44:38Z", "fainashalts", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6YAgds", "I_kwDOMMiGhs6TBxmo", "@fainashalts i have completed this and my pr mentions this issue ", "2024-12-18T03:56:48Z", "2024-12-18T03:56:48Z", "s29papi", "2025-08-31 01:48:44"]
["IC_kwDOMMiGhs6ekyDg", "I_kwDOMMiGhs6TBxmo", "PR that should close this issue: https://github.com/ethereum-optimism/supersim/pull/295", "2025-02-14T23:09:48Z", "2025-02-14T23:09:48Z", "fainashalts", "2025-08-31 01:48:44"]
["IC_kwDOKIsnqM6ePj8Y", "I_kwDOKIsnqM6p1e7I", "> `OWNER_SAFE`: This can be inferred by retrieving the proxy admin owner (upgrade-controller) of an L2 chain from the array. All upgrade-controllers should be the same. We already have checks later in the process to detect any inconsistencies across chains and revert if necessary.\n\nI don't think it's safe to assume this is always the proxy admin owner, sometimes it will be the system config owner, guardian, etc. So this role cannot be inferred and should be specified in the config file", "2025-02-12T21:26:42Z", "2025-02-12T21:26:42Z", "mds1", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6ePrpC", "I_kwDOKIsnqM6p1e7I", "> We should be able to automatically determine both `OWNER_SAFE` and `SCRIPT_NAME` from the `config.toml` file.\n\nI think it requires both the config.toml and solidity template files to determine which address is sending the transaction. If the safe address is not SystemConfigOwner, then this assumption will break.", "2025-02-12T21:45:16Z", "2025-02-12T21:45:16Z", "ElliotFriedman", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6ePutL", "I_kwDOKIsnqM6p1e7I", "I can remove the OWNER_SAFE in the simulate command from this PR https://github.com/solidity-labs-io/superchain-ops/pull/30/files that way the user doesn't have to specify this value. I'll just remove the `--sender` flag", "2025-02-12T21:52:43Z", "2025-02-12T21:52:43Z", "ElliotFriedman", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6ePwiB", "I_kwDOKIsnqM6p1e7I", "I'm not sure we want to remove the `--sender` flag though. Doesn't it add value by running the simulation as the sender address?  ", "2025-02-12T21:57:35Z", "2025-02-12T21:57:35Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6ePxx4", "I_kwDOKIsnqM6p1e7I", "> I'm not sure we want to remove the `--sender` flag though. Doesn't it add value by running the simulation as the sender address?\n\nI don't think it adds any value. The `simulate` task calls `simulateRun` in MultisigTask which calls\n\n```solidity\n        _taskSetup(taskConfigFilePath);\n\n        /// now execute task actions\n        build();\n        simulate();\n        validate();\n        print();\n```\n\nNone of those functions care who the foundry sender is since nothing gets broadcasted.\n\nWill test it out and report back here.", "2025-02-12T22:00:42Z", "2025-02-12T22:00:42Z", "ElliotFriedman", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6eP03k", "I_kwDOKIsnqM6p1e7I", "Ok, just confirmed we can simulate without needing to set the owner safe https://github.com/solidity-labs-io/superchain-ops/pull/30/files PR 30 also includes an example command if you'd like to test it on your machine.", "2025-02-12T22:08:14Z", "2025-02-12T22:08:14Z", "ElliotFriedman", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6d70Mf", "I_kwDOKIsnqM6pgMU5", "This PR https://github.com/solidity-labs-io/superchain-ops/pull/30 adds a .env file to the example task files", "2025-02-11T03:00:26Z", "2025-02-11T03:00:26Z", "ElliotFriedman", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6eCXOT", "I_kwDOKIsnqM6pgMU5", "closing: https://discord.com/channels/1244729134312198194/1327372079380824267/1338914075723304961", "2025-02-11T16:47:39Z", "2025-02-11T16:47:39Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6d3Ns5", "I_kwDOKIsnqM6pcuia", "FWIW the contract itself should be here on Sepolia and Mainnet: 0x95b259eae68ba96edB128eF853fFbDffe47D2Db0", "2025-02-10T16:00:56Z", "2025-02-10T16:00:56Z", "maurelian", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6d3xxc", "I_kwDOKIsnqM6pcuia", "When creating a template it can either be a series of calls or delegate calls. \n\nOPCM is basically on onchain script so we want to delegate call directly to that from the safe. \n\nWe would want a user level option to choose to delegate call or call. \n\n**UX Ideas:**\n- Recall that the upgrade function is `function upgrade(OpChainConfig[] memory _opChainConfigs)` this means that we will no longer be looping over L2 chains. \n\nLarger question, who do we organize the UX here. \n\n\n\nWe need to include the absolute prestate for each chain if we're using OPCM: https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/src/L1/OPContractsManager.sol#L121 - this would be in the `config.toml` file. \n\nEvery OPCM upgrade looks a little different. It will always have the system config, proxy admin address but the struct might be larger. \n\n\nTarget address is an input. If it's specified, then that's who you delegate call to. If it's not specified then use the normal multicall3 address. ", "2025-02-10T16:47:48Z", "2025-02-10T16:53:48Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6d36vh", "I_kwDOKIsnqM6pcuia", "There are other functions on OPCM that we will want to delegate call to in the future. \n\nTwo types of calls for superchain-ops 2.0:\n- Loop over each chain and make calls\n- Delegate call to our onchain OPCM (opcm loops - technically you can look by having an array of 1)\n\nMaybe get rid of looping in OPCM. This might be more consistent with what we already have. ", "2025-02-10T17:00:50Z", "2025-02-10T17:00:50Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6d71PG", "I_kwDOKIsnqM6pcuia", "Seems like this contract is going to make things more complex for upgrading using OPCM:\n\nhttps://sepolia.etherscan.io/address/0x95b259eae68ba96edB128eF853fFbDffe47D2Db0#code\n\nit only allows delegatecalls, so now we wrap a safe delegatecall to the multicall3delegatecall, which then delegatecalls the new OPCM contract.\n\n`safe -> delegatecalls multicall3delegatecall -> delegatecalls OPCM`\n\nseems like it would be simpler and easier to reason about if we just delegatecalled straight from the safe into the OPCM contract.\n\nMaybe I'm missing something here though and we have to make a delegatecall to the multicall3delegatecall contract.", "2025-02-11T03:04:53Z", "2025-02-11T06:53:53Z", "ElliotFriedman", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6eCHjk", "I_kwDOKIsnqM6pcuia", "I thought about it a bit more. If we can make a single delegatecall from the multisig to the OPCM contract with all of the parameters bundled up in a single call, then we can just delegatecall OPCM directly.\n\nIf we need to make multiple delegate calls to OPCM, then we need to use the multicall3delegatecall contract.", "2025-02-11T16:34:12Z", "2025-02-11T16:34:12Z", "ElliotFriedman", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6ekZUx", "I_kwDOKIsnqM6pcuia", "We don't _have_ to use `multicall3delegatecall` but we needed a way to make a `delegatecall` from the Safe while still leveraging all of the existing infrastructure in the superchain-ops repo infrastructure for signing and reading the input.json tx bundles. \n\nHowever we handle that within the new ops repo paradigm is fine with me.", "2025-02-14T21:52:59Z", "2025-02-14T21:53:41Z", "maurelian", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6ekfuy", "I_kwDOKIsnqM6pcuia", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/606 ", "2025-02-14T22:14:11Z", "2025-02-14T22:14:11Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6ejGdb", "I_kwDOKIsnqM6pPUQK", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/598", "2025-02-14T18:25:49Z", "2025-02-14T18:25:58Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6d5UMZ", "I_kwDOKIsnqM6o4vqk", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/565 \n\nnow discovering all addresses from the L1StandardBridge contract (exception exist e.g. L1ERC721Bridge contract)", "2025-02-10T19:33:32Z", "2025-02-10T19:33:32Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6dOHIC", "I_kwDOKIsnqM6n5BRc", "1. Deploy a new safe on sepolia with a threshold of 1. Configure the owners to be addresses of @ElliotFriedman, @prateek105, @blmalone etc.\n2. Using `op-deployer/v0.0.11` deploy a holocene chain. Make sure to use the new safe in the `intent.toml` for op-deployer.\n3. Once deployed, open a branch to superchain-ops that adds this new test chain.\n4. Using the commit of the branch above, reference this as the superchain-registry submodule in superchain-ops. This will ensure that the `AddressRegistry.sol` file has access to the newly deployed chain configuration. \n\n", "2025-02-05T18:35:30Z", "2025-02-05T18:35:30Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6d5Rvp", "I_kwDOKIsnqM6n5BRc", "Closed, see this message for more information: https://discord.com/channels/1244729134312198194/1327372079380824267/1337176792376807516", "2025-02-10T19:29:31Z", "2025-02-10T19:29:31Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6c4ri0", "I_kwDOKIsnqM6n4-N3", "For option 2: Compare our address.json with the address.json on the latest commit.\n\nDownside of option 1 is that you'll be updating your submodule even though an address didn't change. \n\nDiscord conversation: https://discord.com/channels/1244729134312198194/1335009077117456394/1336084466833293443", "2025-02-03T21:21:51Z", "2025-02-03T21:26:49Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6dLYeB", "I_kwDOKIsnqM6n4-N3", "I am working on this using option 2.", "2025-02-05T14:29:08Z", "2025-02-05T14:29:08Z", "prat-gpt", "2025-08-31 01:48:59"]
["IC_kwDOKIsnqM6eB573", "I_kwDOKIsnqM6n4-N3", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/574 ", "2025-02-11T16:14:07Z", "2025-02-11T16:14:07Z", "blmalone", "2025-08-31 01:48:59"]
["IC_kwDODjvEJM6ekTkS", "I_kwDODjvEJM6qJ9IS", "oops, this is a duplicate of: https://github.com/ethereum-optimism/optimism/issues/14116", "2025-02-14T21:36:07Z", "2025-02-14T21:36:07Z", "mbaxter", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d6V-j", "I_kwDODjvEJM6pbDa3", "op-challenger already automatically skips games that are already resolved but it can't know about all other in-flight transactions so it's possible for multiple challengers to see the game as unresolved, all send a tx and then only the first tx included will actually resolve the game and the others will revert. Without off-chain coordination this is impossible to resolve.\n\nYou can however use `--selective-claim-resolution` to tell op-challenger to only resolve games that it would receive bonds from (ie stop acting altruistically and resolving games it doesn't care about).", "2025-02-10T21:48:12Z", "2025-02-10T21:48:12Z", "ajsutton", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6dj4-a", "I_kwDODjvEJM6pMSwV", "The tests above should cover everything except operator fee coverage.\n\nWithdrawals root test coverage is already implemented: https://github.com/ethereum-optimism/optimism/blob/develop/op-e2e/actions/upgrades/isthmus_fork_test.go#L56-L261", "2025-02-07T16:37:31Z", "2025-02-07T16:37:31Z", "meyer9", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6dj7Y-", "I_kwDODjvEJM6pMSwV", "Operator fee might be covered here: https://github.com/ethereum-optimism/optimism/pull/12166\n\nAs long as there are no other valuable tests we want to implement.", "2025-02-07T16:40:38Z", "2025-02-07T16:40:38Z", "meyer9", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6dmacc", "I_kwDODjvEJM6pMSwV", "Can any sane tests be created for EIP-6110? I would think the request processing tests with the consolidation and withdrawal queue are sufficient and 6110 might be hard to test (would have to deploy a different deposit contract that can emit deposit logs and ensure they're ignored).", "2025-02-07T20:45:21Z", "2025-02-07T20:45:21Z", "meyer9", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6dbBgn", "I_kwDODjvEJM6pDKqG", "We could write this test in solidity ", "2025-02-06T21:36:58Z", "2025-02-06T21:36:58Z", "tynes", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6cIMhs", "I_kwDODjvEJM6nwQC1", "The op-node does not yet build a correct deposit transaction (see [here](https://github.com/ethereum-optimism/optimism/blob/b8b6606975380ddf7fc477050dc9d122ae1a619c/op-node/rollup/interop/managed/attributes.go#L31))), so this work is blocked until that's fixed in the op-node. Then we can reuse the same op-node routine in the program.", "2025-01-28T15:52:36Z", "2025-01-28T15:52:36Z", "Inphi", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6bvlYq", "I_kwDODjvEJM6nesGV", "I've added this issue to the MT cannon project backlog, but strictly not a blocker for that project. If there's a better place to put it, pls feel free to move the issue @mbaxter @Inphi \ud83d\ude04 ", "2025-01-24T16:36:08Z", "2025-01-24T16:36:08Z", "clabby", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6eI9GK", "I_kwDODjvEJM6nesGV", "@clabby May I take this?", "2025-02-12T09:42:45Z", "2025-02-12T09:42:45Z", "pcw109550", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6bwryh", "I_kwDODjvEJM6nNhpV", "We are working on a PR to avoid initializing the Interop version of L2StandardBridge (L2StandardBridgeInterop) and also skipping the corresponding tests of it.", "2025-01-24T19:24:06Z", "2025-01-24T19:24:06Z", "gotzenx", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d77J5", "I_kwDODjvEJM6nFLys", "This winds up needing so much to be stubbed out that it doesn't actually provide any value.", "2025-02-11T03:34:07Z", "2025-02-11T03:34:07Z", "ajsutton", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6bpzgj", "I_kwDODjvEJM6nFLtI", "This doesn't pan out very well.  The vast majority of the logic for mapping from the dispute game to the op-program arguments are related to the position in the game which we're stubbing in the action tests. So we don't actually gain any extra confidence from this over the unit tests we already have.", "2025-01-24T02:45:30Z", "2025-01-24T02:45:30Z", "ajsutton", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6bWJgz", "I_kwDODjvEJM6nFLS5", "Here: https://github.com/ethereum-optimism/optimism/blob/5f4757cbadc8e4b620d8e87197b5c7a7ecaeaa6d/op-challenger/game/fault/trace/super/sync.go#L16", "2025-01-22T04:34:36Z", "2025-01-22T04:34:36Z", "ajsutton", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6dNdVz", "I_kwDODjvEJM6lVasL", "@mslipper I think we can call this done right?", "2025-02-05T17:17:49Z", "2025-02-05T17:17:49Z", "maurelian", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d3_TO", "I_kwDODjvEJM6lVasL", "yes this is done", "2025-02-10T17:07:47Z", "2025-02-10T17:07:47Z", "mslipper", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d342q", "I_kwDODjvEJM6lT3ME", "If automatic NAT discovery is enabled, then LibP2P might use UPNP to discover the external IP by communicating with other network devices. And from what I can tell from googling around UPNP is based on SOAP.\n\nIt's supposed to be disabled by default, and then enabled with `--p2p.nat` in op-node.\nI'll fix the NAT options in op-node.\n\n", "2025-02-10T16:58:03Z", "2025-02-10T16:58:03Z", "protolambda", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d3_My", "I_kwDODjvEJM6lT3ME", "I think https://github.com/ethereum-optimism/optimism/pull/14280 will fix it, but I am unable to reproduce the issue.\n", "2025-02-10T17:07:39Z", "2025-02-10T17:07:39Z", "protolambda", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d5tp7", "I_kwDODjvEJM6lT3ME", "Unfortunately we cannot consistently reproduce the issue. We'll revisit this iisue once a new version of `op-node` with the fix added is released.", "2025-02-10T20:23:18Z", "2025-02-10T20:23:18Z", "emlautarom1", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6eAXkR", "I_kwDODjvEJM6jJtR_", "The same thing now exists in the ops repo, so we can [just steal that](https://github.com/ethereum-optimism/superchain-ops/blob/main/.circleci/config.yml#L146-L157).", "2025-02-11T13:54:54Z", "2025-02-11T13:54:54Z", "maurelian", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6W67RU", "I_kwDODjvEJM6irSbB", "Were you able to generate L2 allocs and L2 genesis past deploying contracts on L1?", "2024-12-10T15:24:38Z", "2024-12-10T15:24:38Z", "monk07-01", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6XP9N4", "I_kwDODjvEJM6irSbB", "> Were you able to generate L2 allocs and L2 genesis past deploying contracts on L1?\n\nYes. What does that have to do with the issue here though?", "2024-12-12T01:25:01Z", "2024-12-12T01:25:33Z", "blockchaindevsh", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6XP93m", "I_kwDODjvEJM6irSbB", "if yes then are you able to guide me for  my issue Kindly\n\nhttps://github.com/ethereum-optimism/optimism/issues/13309", "2024-12-12T01:28:09Z", "2024-12-12T01:28:29Z", "monk07-01", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6XlObL", "I_kwDODjvEJM6irSbB", "op-deployer only deploys the contracts that have been approved by Optimism governance. Custom gas tokens are [still in beta](https://docs.optimism.io/stack/beta-features/custom-gas-token), which means that they haven't yet been approved by OP gov for inclusion in the standard contracts.\n\nOur current plan is to propose an update to the standard contracts in the first half of 2025 which would add custom gas tokens to the standard contracts.* At that point, op-deployer can easily be used to deploy custom gas token chains. \n\nIn the meantime, please use the legacy deployment scripts to deploy CGT chains.\n\n[*] One note for anyone launching production OP Chains: Inclusion in the standard contracts is not the same thing as inclusion in the [standard config](https://docs.optimism.io/builders/chain-operators/configuration/rollup). If you don't know what this means, you probably don't need to worry about it :)", "2024-12-14T13:52:00Z", "2024-12-14T13:52:00Z", "tessr", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6XmT81", "I_kwDODjvEJM6irSbB", "> In the meantime, please use the legacy deployment scripts to deploy CGT chains.\n\nThe legacy deployment script is [Deploy.s.sol](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/scripts/deploy/Deploy.s.sol), but it can't be used to deploy CGT chains any more and should only be used in test now(as can be seen from [here](https://github.com/ethereum-optimism/optimism/blob/6823d6768266a4a8ddf435cc2d91bd17ca13e345/packages/contracts-bedrock/scripts/deploy/Deploy.s.sol#L314) and [here](https://github.com/ethereum-optimism/optimism/blob/6823d6768266a4a8ddf435cc2d91bd17ca13e345/packages/contracts-bedrock/scripts/deploy/Deploy.s.sol#L8)). ", "2024-12-15T01:26:40Z", "2024-12-15T01:27:04Z", "blockchaindevsh", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6YdcPy", "I_kwDODjvEJM6irSbB", "@tessr The legacy deployment script `Deploy.s.sol` is no longer maintained and it doesn't work if `--rpc-url` is specified : https://github.com/ethereum-optimism/optimism/issues/12915 .", "2024-12-20T22:53:19Z", "2024-12-20T22:53:19Z", "zhiqiangxu", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d3_DQ", "I_kwDODjvEJM6irSbB", "we're not supporting CGTs due to our focus on interop, and the code for them has been removed. closing this as won't fix for the time being.", "2025-02-10T17:07:25Z", "2025-02-10T17:07:25Z", "mslipper", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d3_xd", "I_kwDODjvEJM6iNV3e", "we no longer use the salt mixer since the deployments are deterministic.", "2025-02-10T17:08:29Z", "2025-02-10T17:08:29Z", "mslipper", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d4CB9", "I_kwDODjvEJM6ccr7r", "this has been fixed", "2025-02-10T17:12:08Z", "2025-02-10T17:12:08Z", "mslipper", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6NlL3K", "I_kwDODjvEJM6X75l8", "contracts not expecting a return value but receiving a return value after the upgrade should not be affected:\r\n\r\n```solidity\r\n// SPDX-License-Identifier: MIT\r\n\r\npragma solidity 0.8.15;\r\n\r\ninterface IFoo {\r\n    function test() external;\r\n}\r\n\r\ncontract Foo {\r\n    function test() external returns (uint256) {\r\n        return 1;\r\n    }\r\n}\r\n\r\ncontract Bar {\r\n\r\n    IFoo foo;\r\n\r\n    constructor(IFoo foo_) {\r\n        foo = foo_;\r\n    }\r\n\r\n    function returnTest() external {\r\n        foo.test(); // does not revert\r\n    }\r\n\r\n}\r\n```", "2024-09-25T21:57:17Z", "2024-09-25T21:57:17Z", "gretzke", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6LqDeO", "I_kwDODjvEJM6WJIw1", "The above applies if you ran the below command at the root of the optimism folder and didn't find the **bin** folder in the respective directories.\r\n```\r\nmake op-node op-batcher op-proposer\r\npnpm build\r\n```", "2024-09-11T08:59:07Z", "2024-09-11T08:59:07Z", "jvcByte", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d3vsy", "I_kwDODjvEJM6WJIw1", "These commands are meant to run in the respective directories, e.g. `cd op-node && make op-node && ./bin/op-node --help`\n", "2025-02-10T16:44:40Z", "2025-02-10T16:44:40Z", "protolambda", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6LS-kN", "I_kwDODjvEJM6U5Kif", "> Radix tries can more efficiently store sparse data, minimizing time spent looking up values and reducing merkleization costs. \r\n\r\nIs the benefit mainly a result of the reduction of depth of the tree, or in other words, from binary tree to n-ary tree? ", "2024-09-09T02:50:58Z", "2024-09-09T02:50:58Z", "zhiqiangxu", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6LYDV3", "I_kwDODjvEJM6U5Kif", "Yeah, I think the higher branching factor helps.  It means fewer nodes to visit during lookups and fewer nodes to rehash when a value is updated.  With MPTs you also have prefix collapsing where you can have branch or extension nodes that store a common path prefix for their children, which also reduces tree depth.\r\n\r\n@clabby - anything to add here?", "2024-09-09T14:50:03Z", "2024-09-09T14:50:03Z", "mbaxter", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6dZsh0", "I_kwDODjvEJM6U5Kif", "Howdy! So I've been working on this a bit and just wanted to give a little progress report.\n\nSo far, I've:\n- abstracted the Memory struct into a now Memory interface.\n- implemented the Memory interface where the page tables are backed by\n   - the current binary tree approach\n   - copy pasta of asterisc's MPT approach\n\nI will clean up some stuff and push a draft PR, but just wanna give an update since I'm excited lol", "2025-02-06T19:06:52Z", "2025-02-06T19:06:52Z", "ec2", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6eZxE4", "I_kwDODjvEJM6U5Kif", "Is there a a reason why we should be representing pages as [PageSize]byte{}? It seems like we should really be storing them as [PageSize/(WordSize/8)]Word instead.\n\nRight now, every memory access involves endian conversions. But if we store them as Words, then we only need to do endian conversions when hashing (which happens way less often than memory accesses). \n\nThis change is easy assuming we continue only disallowing unaligned memory accesses. ", "2025-02-13T19:41:50Z", "2025-02-13T19:41:50Z", "ec2", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6JRtq9", "I_kwDODjvEJM6TxZDw", "This doesn't necessarily need to be tied to the interop release itself and is related to ongoing holocene hardfork work in https://github.com/ethereum-optimism/specs/issues/340. A lot of projects all need this generic L1 -> L2 config setter logic", "2024-08-21T22:21:05Z", "2024-08-21T22:21:05Z", "tynes", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6N9FlC", "I_kwDODjvEJM6TxZDw", "FYI: I'm Changing the priority on this issue to P2(Optional for the Holocene release), as instructed by @sebastianst on our last weekly sync for Holocene.", "2024-09-29T21:00:03Z", "2024-09-29T21:00:03Z", "BlocksOnAChain", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6ON6uB", "I_kwDODjvEJM6TxZDw", "I will remove this item from the scope for Holocene, based on what @tynes shared on it:\n\"I would like to cut Standard L2 Genesis from scope. This would  delay the release of holocene. \nThere will be another fork after Holocene that includes the Standard L2 Genesis\"", "2024-10-01T13:52:05Z", "2024-10-01T13:52:05Z", "BlocksOnAChain", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d5J-7", "I_kwDODjvEJM6TxZDw", "Cleaning up the milestones github view, and I think it's better if we remove this from the \"L2 Genesis Simplification\" milestone, since this is not an active project, so we can remove the milestone from the milestones view.\n", "2025-02-10T19:17:17Z", "2025-02-10T19:17:17Z", "protolambda", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6HtzjN", "I_kwDODjvEJM6SbuOT", "There isn't a 1:1 mapping between a withdrawal transaction and a dispute game as the dispute game is about an output commitment which commits to all previous withdrawals. Are you trying to build a UI? What feature are you trying to build?", "2024-08-09T00:18:13Z", "2024-08-09T00:18:13Z", "tynes", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6Ht_js", "I_kwDODjvEJM6SbuOT", "For example, if a user performs a withdrawal transaction on L2 at block X, we need to find the corresponding `FaultDisputeGame` contract on-chain where the L2 block number in this contract is greater than block X, and the previous `FaultDisputeGame` contract has a l2 block number smaller than block X. This requires looping through all `FaultDisputeGame` contracts to find the correct one. By emitting this L2 block number in the event, it becomes easier for us to identify the resolved game associated with the correct L2 block number.", "2024-08-09T01:23:58Z", "2024-08-09T01:23:58Z", "boyuan-chen", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6dBWrJ", "I_kwDODjvEJM6SbuOT", "@boyuan-chen ok to close this issue now?\n", "2025-02-04T15:50:41Z", "2025-02-04T15:50:41Z", "BlocksOnAChain", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6GpPpg", "I_kwDODjvEJM6RNhhc", "I deployed a very simple [PoC](https://etherscan.io/address/0x6cbf8cd866a0fae64b9c2b007d3d47c4e1b809ff#code) on Mainnet that is a variant of the implementation above.\r\n\r\n```Solidity\r\nfunction findDelayedGameIndex(IOptimismPortal portal, uint256 delaySec) external view returns (uint256)\r\n````\r\nNotes:\r\n* `findDelayedGameIndex(` [**OP Mainnet: OptimismPortal**](https://etherscan.io/address/0xbEb5Fc579115071764c7423A4f12eDde41f106Ed#code) `, 0)` = **1125**\r\n* [**DisputeGameFactory**](https://etherscan.io/address/0xe5965Ab5962eDc7477C8520243A95517CD252fA9#readProxyContract) `.gameCount()` = **1208**\r\n* `1208 - 1125 = 83 => 83 / 24 = ~3.5days`", "2024-07-30T18:29:28Z", "2024-07-30T18:29:28Z", "adraffy", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6UHQuA", "I_kwDODjvEJM6RNhhc", "We will be implementing something similar to this in the AnchorStateRegistry", "2024-11-19T07:45:30Z", "2024-11-19T07:45:30Z", "smartcontracts", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6UHSSI", "I_kwDODjvEJM6RNhhc", "Our current proposal is to have something like `AnchorStateRegistry.getLatestFinalizedGame()`", "2024-11-19T07:46:54Z", "2024-11-19T07:46:54Z", "smartcontracts", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6UHVc-", "I_kwDODjvEJM6RNhhc", "Here is the latest version I'm using in Unruggable Gateways, a trustless multichain CCIP-Read gateway:\nhttps://github.com/unruggable-labs/unruggable-gateways/blob/main/contracts/op/OPFaultGameFinder.sol\n\n* `gameTypeBitMask` allows searching for multiple game types (`0` for respected w/o an additional RPC)\n* use `minAgeSec` = 0 for finalized (defended) or  `minAgeSec > 0` for an unchallenged game at least that old\n* `gameAtIndex()` returns block number w/o an additional RPC\n \nhttps://etherscan.io/address/0x475a86934805ef2c52ef61a8fed644d4c9ac91d8#code\nhttps://sepolia.etherscan.io/address/0x4Bf352061FEB81a486A2fd325839d715bDc4038c#code", "2024-11-19T07:52:14Z", "2024-11-19T07:57:12Z", "adraffy", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6CInoJ", "I_kwDODjvEJM6NFRQ2", "I want to explore the possibility of using https://github.com/ethereum-optimism/specs/issues/221 rather than directly supporting multiple batcher addresses. Adding multiple batcher addresses to the protocol will add tech debt to the protocol where as using a smart contract to define logic is generic and can be extended permissionlessly to support many features. We would need to engineer around the fact that batches can be posted in both blobs and calldata, but with the right smart contract architecture this is certainly possible to handle. The general philosophy that the OP Stack should follow is creating powerful abstractions that enable permissionless innovation rather than hardcoding particular solutions to problems", "2024-06-21T19:00:07Z", "2024-06-21T19:00:07Z", "tynes", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6D3ub0", "I_kwDODjvEJM6NFRQ2", "Titan builder created a special RPC called `eth_sendBlobs` where you send an array of transactions with the same nonce where each transaction has a different number of blobs. It would be assumed that we build and sign a tx with 1 blob, 2 blobs, 3 blobs all the way up to 6 blobs. Each transaction would include the same blobs, the only difference would be including the next blob. This allows their builder to more easily build a block where there are competing blob transactions that cannot all fit into the same L1 block. See https://docs.titanbuilder.xyz/api/eth_sendblobs", "2024-07-07T11:56:30Z", "2024-07-07T11:56:30Z", "tynes", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d3tlN", "I_kwDODjvEJM6NFRQ2", "All the actionable checklist items were either completed or closed deliberately, so I am closing this issue now.\n", "2025-02-10T16:41:26Z", "2025-02-10T16:41:26Z", "protolambda", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d3eI9", "I_kwDODjvEJM6MirdL", "All checklist items are done, and the issue hasn't been updated for many months, so I am closing this.", "2025-02-10T16:19:11Z", "2025-02-10T16:19:11Z", "protolambda", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6BuB-A", "I_kwDODjvEJM6MrA5L", "@trianglesphere during backlog grooming today we moved this issue out of the current cycle to the next one.\r\n\r\nThe rationale is that Base will be merging upstream geth changes this cycle (https://github.com/ethereum-optimism/protocol-quest/issues/186) and we want to see if that helps address the snapshot corruption issue first before debugging further.", "2024-03-18T14:47:13Z", "2024-03-18T14:47:13Z", "alfonso-op", "2025-08-31 01:49:16"]
["IC_kwDODjvEJM6d3sDW", "I_kwDODjvEJM6MrA5L", "This issue is stale, I have not seen this occur recently, other than the pathdb difflayers journal issue, which is tracked separately.\n", "2025-02-10T16:39:08Z", "2025-02-10T16:39:08Z", "protolambda", "2025-08-31 01:49:16"]
["IC_kwDOEf1bQc6bfh-f", "I_kwDOEf1bQc6hbroS", "Hello @ebuse123! I do not see a description of what you're referring to that won't open. Can you clarify please?", "2025-01-23T02:48:53Z", "2025-01-23T02:48:53Z", "arredr2", "2025-08-31 01:50:34"]
["IC_kwDOEf1bQc6bfiNb", "I_kwDOEf1bQc6gMoaM", "Hello @Comandante318! I do not see a description of what you're referring to wrt Balance. Can you clarify please?", "2025-01-23T02:49:56Z", "2025-01-23T02:49:56Z", "arredr2", "2025-08-31 01:50:34"]
["IC_kwDOEf1bQc6bfjXM", "I_kwDOEf1bQc6Yk3cs", "Hello @joaniefromtheblock I believe it should be here: https://docs.optimism.io/stack/getting-started#the-op-stack-today.  This section discusses the Bedrock release. \n\nYou can also check out this old blog post: https://blog.oplabs.co/introducing-optimism-bedrock/", "2025-01-23T02:55:04Z", "2025-01-23T02:55:04Z", "arredr2", "2025-08-31 01:50:34"]
["IC_kwDOEf1bQc6bfj-J", "I_kwDOEf1bQc6VYmIE", "Hello @Kya123iu! Can you please clarify what the bug or issue is?", "2025-01-23T02:57:41Z", "2025-01-23T02:57:41Z", "arredr2", "2025-08-31 01:50:34"]
["IC_kwDOEf1bQc6bfkGq", "I_kwDOEf1bQc6UJW_g", "Hello @khthai42 ! Can you please clarify what the bug or issue is?", "2025-01-23T02:58:22Z", "2025-01-23T02:58:22Z", "arredr2", "2025-08-31 01:50:34"]
["IC_kwDOL-xLQ86erE-v", "I_kwDOL-xLQ86o1Dnw", "No longer required.", "2025-02-17T05:27:14Z", "2025-02-17T05:27:14Z", "scharissis", "2025-08-31 01:50:35"]
["IC_kwDOL-xLQ86erE6i", "I_kwDOL-xLQ86n7A8w", "No longer required.", "2025-02-17T05:27:00Z", "2025-02-17T05:27:00Z", "scharissis", "2025-08-31 01:50:35"]
["IC_kwDOL-xLQ86erEzN", "I_kwDOL-xLQ86n7AtF", "No longer required.", "2025-02-17T05:26:34Z", "2025-02-17T05:26:34Z", "scharissis", "2025-08-31 01:50:35"]
["IC_kwDOL-xLQ86fHmmF", "I_kwDOL-xLQ86n5TQA", "No longer needed (the tests must use devnet-sdk)", "2025-02-19T19:29:38Z", "2025-02-19T19:29:38Z", "scharissis", "2025-08-31 01:50:35"]
["IC_kwDOL-xLQ86cLRaR", "I_kwDOL-xLQ86n46Nf", "Probably a service/service monitor to scrape the metrics. Or maybe a pod monitor in this case\n\n* Also will need to start publishing op-nat images via infra CI", "2025-01-28T22:39:33Z", "2025-01-28T22:39:33Z", "jelias2", "2025-08-31 01:50:35"]
["IC_kwDOL-xLQ86erFU5", "I_kwDOL-xLQ86n46Nf", "No longer required.", "2025-02-17T05:28:29Z", "2025-02-17T05:28:29Z", "scharissis", "2025-08-31 01:50:35"]
["IC_kwDOL-xLQ86erFq6", "I_kwDOL-xLQ86n44XU", "No longer required.", "2025-02-17T05:29:40Z", "2025-02-17T05:29:40Z", "scharissis", "2025-08-31 01:50:35"]
["IC_kwDOH2Qg5s6e3XdB", "I_kwDOH2Qg5s6qePwm", "I addition this is the parameters I\u202fuse for starting my optimism node :\n\n```\n   /usr/bin/geth \\\n      --http \\\n      --http.port=8545 \\\n      --http.addr=0.0.0.0 \\\n      --authrpc.addr=0.0.0.0 \\\n      --authrpc.jwtsecret=/secrets/jwt.hex \\\n      --authrpc.vhosts=\"*\" \\\n      --http.vhosts=\"*\" \\\n      --rollup.sequencerhttp=https://mainnet-sequencer.optimism.io/ \\\n      --op-network=op-mainnet \\\n      --datadir=/var/lib/geth \\\n      --state.scheme hash \\\n      --config /var/lib/geth/config.toml \\\n      --vmodule=p2p=5\n```\n\n", "2025-02-18T11:05:30Z", "2025-02-18T11:05:30Z", "bgiegel", "2025-08-31 01:50:37"]
["IC_kwDOKIwiaM6cIO5u", "I_kwDOKIwiaM6mrAqo", "@sbvegan This is ready for [review](https://github.com/ethereum-optimism/docs/pull/1292)", "2025-01-28T15:56:19Z", "2025-01-28T15:56:19Z", "krofax", "2025-08-31 01:50:41"]
["IC_kwDOI7W0xc6S7NNe", "I_kwDOI7W0xc6cTHsP", "cc @chuxinh on the prereq tables\r\n\r\nAlso TBD if we use the goldsky pipelines or build our own", "2024-11-08T15:08:24Z", "2024-11-08T15:08:29Z", "MSilb7", "2025-08-31 01:50:45"]
["IC_kwDOI7W0xc6XWGUy", "I_kwDOI7W0xc6cTHsP", "Dependency on first having #1134 .\n\nWe can also pull tokens from Defillama APIs that indicate underlying pool tokens.\n\nSteps are still the same:\n1. Identify a token contract\n2. Read the functions we want (symbol, name, decimals, mint/burn if superchainerc20)\n3. Have some way to identify if it's a proxy being updated, so we can re-pull / update info.", "2024-12-12T14:44:21Z", "2024-12-12T14:44:21Z", "MSilb7", "2025-08-31 01:50:45"]
["IC_kwDOI7W0xc6XWIo6", "I_kwDOI7W0xc6cTHsP", "Can share utilies with #1158 ", "2024-12-12T14:48:05Z", "2024-12-12T14:48:05Z", "MSilb7", "2025-08-31 01:50:45"]
["IC_kwDOI7W0xc6byHs6", "I_kwDOI7W0xc6cTHsP", "Another good token list for more chains https://github.com/vfat-io/vfat-tools/blob/master/src/static/js/prices.js", "2025-01-24T23:47:19Z", "2025-01-24T23:47:19Z", "MSilb7", "2025-08-31 01:50:45"]
["IC_kwDOI7W0xc6byH-l", "I_kwDOI7W0xc6cTHsP", "Note: We already ingest the Superchain Token List: https://github.com/ethereum-optimism/ethereum-optimism.github.io", "2025-01-24T23:49:01Z", "2025-01-24T23:49:01Z", "MSilb7", "2025-08-31 01:50:45"]
["IC_kwDOI7W0xc6e9n46", "I_kwDOI7W0xc6cTHsP", "Closed with this PR merged: https://github.com/ethereum-optimism/op-analytics/pull/1361", "2025-02-18T21:20:48Z", "2025-02-18T21:20:48Z", "chuxinh", "2025-08-31 01:50:45"]
["IC_kwDOKIsnqM6eY7lD", "I_kwDOKIsnqM6p-X0b", "Agree we definitely need to enforce this property. FWIW we have always been delegatecalling to Multicall3, so this is not entirely new, but it does add more risk since the OPCM logic is more complex and will change overtime. \n\nI think this could be fairly easily done in [`checkStateDiff()`](https://github.com/ethereum-optimism/superchain-ops/blob/95d16a2d05019dad3449d2ec2f5ee3180e7b9e44/src/JsonTxBuilderBase.sol#L59), since the only diff should be the nonce.", "2025-02-13T17:49:58Z", "2025-02-13T17:49:58Z", "maurelian", "2025-08-31 01:51:01"]
["IC_kwDOKIsnqM6ebXex", "I_kwDOKIsnqM6p-X0b", "New PR is up to add the `checkStateDiff` function https://github.com/ethereum-optimism/superchain-ops/pull/602\n\nThe OPCM template PR can implement this functionality.", "2025-02-14T00:15:44Z", "2025-02-14T00:15:51Z", "ElliotFriedman", "2025-08-31 01:51:01"]
["IC_kwDOKIsnqM6e1jmJ", "I_kwDOKIsnqM6pe4N-", "This card has the nested and single testing flipped\n\nwe have only currently tested the single just file and not the nested just file", "2025-02-18T07:43:12Z", "2025-02-18T07:43:12Z", "ElliotFriedman", "2025-08-31 01:51:01"]
["IC_kwDOKIsnqM6e7_Hj", "I_kwDOKIsnqM6pe4N-", "Now we have tested the nested flow on sepolia too:\n\n- execution transaction: https://sepolia.etherscan.io/tx/0x97085246a3b4b736519eb4e3383c4792dad87e2d2bc0237adbbb1fe82e26d40c#eventlog\n- approve hash transactions on child safes: https://sepolia.etherscan.io/tx/0xf8c69bf9786acf9ac2acbad99908dbd233e582fce327347b3c13a9916079a0fe https://sepolia.etherscan.io/tx/0x4b30bf16df43462403ef7ba1aa347ed02cd27e60ceb8a46b02ee002c43d9891b", "2025-02-18T18:18:15Z", "2025-02-18T18:18:15Z", "ElliotFriedman", "2025-08-31 01:51:01"]
["IC_kwDODjvEJM6fY6j_", "I_kwDODjvEJM6q9WNF", "This is documented in INTERFACES.md, linked from the contracts [README](https://github.com/ethereum-optimism/optimism/tree/develop/packages/contracts-bedrock#contract-interfaces)", "2025-02-21T10:02:20Z", "2025-02-21T10:02:20Z", "alcueca", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6fFZ_i", "I_kwDODjvEJM6qYPuC", "fixed in #14414 ", "2025-02-19T15:30:35Z", "2025-02-19T15:30:35Z", "sigma", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6eZO4P", "I_kwDODjvEJM6p01mL", "Hi @pcw109550 I wanted to flag the issue I mentioned under [`kt-devnet: switch to geth-teku for l1`](https://github.com/ethereum-optimism/optimism/pull/14305#issuecomment-2652484811), as I believe there might be some similarities (though I\u2019m not entirely sure). Whenever you have a chance, I\u2019d really appreciate it if you could take a look \u2014 it would be a big help to us. Thanks so much!", "2025-02-13T18:27:54Z", "2025-02-13T18:33:10Z", "dailinsubjam", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6egbzW", "I_kwDODjvEJM6p01mL", "I've faced same error with current [commit](https://github.com/ethereum-optimism/optimism/commit/28f2f360e62a0400d8fcc381c0ac08057c8cfaf2) like:\n\n```\nAdding service with name 'op-el-1-op-geth-op-node-op-kurtosis' and image 'us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest'\n2025/02/14 12:42:16 Error: error deploying environment: error deploying kurtosis package: execution error: An error occurred executing instruction (number 50) at github.com/ethpandaops/optimism-package/src/el/op-geth/op_geth_launcher.star[130:31]:\nadd_service(name=\"op-el-1-op-geth-op-node-op-kurtosis\", config=ServiceConfig(image=\"us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest\", ports={\"engine-rpc\": PortSpec(number=8551, transport_protocol=\"TCP\", application_protocol=\"\"), \"metrics\": PortSpec(number=9001, transport_protocol=\"TCP\", application_protocol=\"\"), \"rpc\": PortSpec(number=8545, transport_protocol=\"TCP\", application_protocol=\"http\"), \"tcp-discovery\": PortSpec(number=30303, transport_protocol=\"TCP\", application_protocol=\"\"), \"udp-discovery\": PortSpec(number=30303, transport_protocol=\"UDP\", application_protocol=\"\"), \"ws\": PortSpec(number=8546, transport_protocol=\"TCP\", application_protocol=\"\")}, files={\"/jwt\": \"op_jwt_file\", \"/network-configs\": \"op-deployer-configs\"}, entrypoint=[\"sh\", \"-c\"], cmd=[\"geth init --datadir=/data/geth/execution-data --state.scheme=hash /network-configs/genesis-2151908.json && geth --networkid=2151908 --datadir=/data/geth/execution-data --gcmode=archive --state.scheme=hash --http --http.addr=0.0.0.0 --http.vhosts=* --http.corsdomain=* --http.api=admin,engine,net,eth,web3,debug,miner --ws --ws.addr=0.0.0.0 --ws.port=8546 --ws.api=admin,engine,net,eth,web3,debug,miner --ws.origins=* --allow-insecure-unlock --authrpc.port=8551 --authrpc.addr=0.0.0.0 --authrpc.vhosts=* --authrpc.jwtsecret=/jwt/jwtsecret --syncmode=full --nat=extip:KURTOSIS_IP_ADDR_PLACEHOLDER --rpc.allow-unprotected-txs --discovery.port=30303 --port=30303 --metrics --metrics.addr=0.0.0.0 --metrics.port=9001\"], env_vars={}, private_ip_address_placeholder=\"KURTOSIS_IP_ADDR_PLACEHOLDER\", labels={\"ethereum-package.client\": \"op-geth\", \"ethereum-package.client-image\": \"us-docker-pkg-dev-oplabs-tools-artifacts-images-op-geth_latest\", \"ethereum-package.client-type\": \"execution\", \"ethereum-package.connected-client\": \"geth\", \"ethereum-package.sha256\": \"\"}, tolerations=[], node_selectors={}))\n --- at /home/circleci/project/core/server/api_container/server/startosis_engine/startosis_executor.go:157 (sendErrorAndFail) ---\nCaused by: Unexpected error occurred starting service 'op-el-1-op-geth-op-node-op-kurtosis'\n --- at /home/circleci/project/core/server/api_container/server/startosis_engine/kurtosis_instruction/add_service/add_service.go:212 (AddServiceCapabilities.Execute) ---\nCaused by: An error occurred waiting for all TCP and UDP ports to be open for service 'op-el-1-op-geth-op-node-op-kurtosis' with private IP '172.16.0.21'; this is usually due to a misconfiguration in the service itself, so here are the logs:\n== SERVICE 'op-el-1-op-geth-op-node-op-kurtosis' LOGS ===================================\nFatal: Failed to read genesis file: open /network-configs/genesis-2151908.json: no such file or directory\nFatal: Failed to read genesis file: open /network-configs/genesis-2151908.json: no such file or directory\n\n== FINISHED SERVICE 'op-el-1-op-geth-op-node-op-kurtosis' LOGS ===================================\n --- at /home/circleci/project/core/server/api_container/server/service_network/default_service_network.go:1016 (DefaultServiceNetwork.startRegisteredService) ---\nCaused by: An error occurred while waiting for all TCP and UDP ports to be open\n --- at /home/circleci/project/core/server/api_container/server/service_network/default_service_network.go:1397 (waitUntilAllTCPAndUDPPortsAreOpen) ---\nCaused by: Unsuccessful ports check for IP '172.16.0.21' and port spec '{privatePortSpec:0xc000db0480}', even after '240' retries with '500' milliseconds in between retries. Timeout '2m0s' has been reached\n --- at /home/circleci/project/core/server/api_container/server/service_network/default_service_network.go:1427 (waitUntilPortIsOpenWithTimeout) ---\nCaused by: An error occurred while calling network address '172.16.0.21:8551' with port protocol 'TCP' and using time out '200ms'\n --- at /home/circleci/project/core/server/api_container/server/service_network/default_service_network.go:1459 (scanPort) ---\nCaused by: dial tcp 172.16.0.21:8551: i/o timeout\nexit status 1\n```", "2025-02-14T13:09:32Z", "2025-02-14T13:09:32Z", "mateusfigmelo", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6emQq1", "I_kwDODjvEJM6p01mL", "Application failed: error in pipeline stage apply: failed to deploy superchain: failed to load DeploySuperchain.s.sol script: could not load script artifact: failed to open artifact \"DeploySuperchain.s.sol/DeploySuperchain.json\": open DeploySuperchain.s.sol/DeploySuperchain.json: no such file or directory", "2025-02-15T09:07:21Z", "2025-02-15T09:07:21Z", "Chomtana", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6eqvX_", "I_kwDODjvEJM6p01mL", "This issue is in discussing [here](https://github.com/ethereum-optimism/developers/discussions/726).", "2025-02-17T04:56:06Z", "2025-02-17T04:56:06Z", "mateusfigmelo", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6evpmB", "I_kwDODjvEJM6p01mL", "> Application failed: error in pipeline stage apply: failed to deploy superchain: failed to load DeploySuperchain.s.sol script: could not load script artifact: failed to open artifact \"DeploySuperchain.s.sol/DeploySuperchain.json\": open DeploySuperchain.s.sol/DeploySuperchain.json: no such file or directory\n\n@Chomtana , have you resolved this error?", "2025-02-17T14:28:42Z", "2025-02-17T14:28:42Z", "mateusfigmelo", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6ewB5Z", "I_kwDODjvEJM6p01mL", "I've covered the root cause of this error [here](https://github.com/ethereum-optimism/developers/discussions/726#discussioncomment-12226002).\nI think we can close this issue.", "2025-02-17T15:06:49Z", "2025-02-17T15:07:45Z", "mateusfigmelo", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6ezGoZ", "I_kwDODjvEJM6p01mL", "@mateusfigmelo Thanks! The weird thing is that I have latest foundry (my forge version is 1.0.0) installed, but still get the same error.", "2025-02-17T22:58:22Z", "2025-02-18T00:44:17Z", "dailinsubjam", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6ezp9u", "I_kwDODjvEJM6p01mL", "Try foundry version specified in https://github.com/ethereum-optimism/optimism/blob/develop/mise.toml", "2025-02-18T01:17:01Z", "2025-02-18T01:17:01Z", "Chomtana", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6ez30c", "I_kwDODjvEJM6p01mL", "@Chomtana Thanks! I'm able to run it now.", "2025-02-18T01:43:02Z", "2025-02-18T01:43:02Z", "dailinsubjam", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6ewLVG", "I_kwDODjvEJM6ptnav", "Closing due to resolving issue by https://github.com/ethereum-optimism/developers/discussions/726#discussioncomment-12226002", "2025-02-17T15:22:33Z", "2025-02-17T15:22:33Z", "Kourin1996", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6e6iMw", "I_kwDODjvEJM6oxd1T", "Done.  See: https://github.com/ethereum-optimism/superchain-ops/pull/594/files", "2025-02-18T15:59:28Z", "2025-02-18T15:59:28Z", "mbaxter", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6dix7Q", "I_kwDODjvEJM6olsxD", "superseded by: https://github.com/ethereum-optimism/optimism/pull/14186/files", "2025-02-07T14:46:35Z", "2025-02-07T14:46:35Z", "mbaxter", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6ebB3z", "I_kwDODjvEJM6olsxD", "We actually do want to code freeze these files - the PR referenced above removes them from code freeze.  Reopening. ", "2025-02-13T23:00:25Z", "2025-02-13T23:00:25Z", "mbaxter", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6dSRik", "I_kwDODjvEJM6oNjna", "Keeping open until we improve the error handling so the superchain client detects the not found string and returns a nice typed error rather than having to do string matching in challenger (which is likely to be duplicated in dispute-mon and proposer as well).", "2025-02-06T04:45:26Z", "2025-02-06T04:45:26Z", "ajsutton", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6ccs3H", "I_kwDODjvEJM6n5j8w", "@pauldowman is this good to be closed now?", "2025-01-30T15:12:32Z", "2025-01-30T15:12:32Z", "tynes", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6celvG", "I_kwDODjvEJM6n5j8w", "@tynes Not yet, but maybe soon. I assume you mean because of https://github.com/ethereum-optimism/optimism/pull/13523 ? That one only does the counter metrics so it doesn't fully address this. But it doesn't make sense to set non-counter metrics to 0 the way it does for a counter so I'm going to follow up about the original problem. If there's a different solution I'll change this, otherwise will close it if it's not doable.\n\n", "2025-01-30T18:28:36Z", "2025-01-30T18:28:36Z", "pauldowman", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6fVnMZ", "I_kwDODjvEJM6n5j8w", "Closing. https://github.com/ethereum-optimism/optimism/pull/13523 did this for the counter metrics and it doesn't make sense to do the others.", "2025-02-21T02:57:54Z", "2025-02-21T02:57:54Z", "pauldowman", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6fT0dL", "I_kwDODjvEJM6nwPQi", "No longer needed", "2025-02-20T21:54:22Z", "2025-02-20T21:54:22Z", "pauldowman", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6d-UVR", "I_kwDODjvEJM6nmnj5", "[Loom](https://www.loom.com/share/73713e1624294281a1b079df4c208dc6) from @maurelian explaining what needs to be done in more detail.\n\nTLDR; Drop beta and rc versioning, too much churn. Make the OPCM the source of truth for releases.", "2025-02-11T10:15:22Z", "2025-02-11T10:15:22Z", "alcueca", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6e-Hi8", "I_kwDODjvEJM6nFMIl", "Is it still worthwhile to have the op-challenger proposer super roots?", "2025-02-18T22:40:54Z", "2025-02-18T22:40:54Z", "Inphi", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6e_PId", "I_kwDODjvEJM6nFMIl", "I think at some point I'd like to get there, but there's a few behaviours in op-proposer like the way it waits for nodes to be in-sync after startup, particular metrics etc plus the deployment changes required for devnets etc that makes me think it's expanding scope too much to try and convert everything to op-challenger proposing.", "2025-02-19T02:43:10Z", "2025-02-19T02:43:10Z", "ajsutton", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6fT1_s", "I_kwDODjvEJM6mNX0F", "done", "2025-02-20T21:57:42Z", "2025-02-20T21:57:42Z", "pauldowman", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6dBvof", "I_kwDODjvEJM6dMCyo", "@geoknee is taking a closer look at this part of the code base. Feel free George to close this one if no more relevant.", "2025-02-04T16:28:55Z", "2025-02-04T16:28:55Z", "sebastianst", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6dVPQB", "I_kwDODjvEJM6dMCyo", "We should use only two triggers for the throttling behaviour: \n* when an active sequencer changes (and let's decrease the poll interval for determining that to say 10s)\n* when the pending_bytes changes", "2025-02-06T11:41:51Z", "2025-02-06T11:41:51Z", "geoknee", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6dWHtP", "I_kwDODjvEJM6dMCyo", "* we should then be able to remove the duration-based exit from the tx publish function", "2025-02-06T13:24:06Z", "2025-02-06T13:24:06Z", "sebastianst", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6e9KSB", "I_kwDODjvEJM6Tue5o", "Going to reopen this one - the support for avoiding db_debugGet is still experimental - we need to fully test it and make it the only option before we actually close this.", "2025-02-18T20:09:53Z", "2025-02-18T20:09:53Z", "ajsutton", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM57ElrQ", "I_kwDODjvEJM6GGEtP", "Can you give a reproducible case that shows this error? Imports using this style are part of our style guide. They make refactoring much easier internally to the project. Perhaps we could raise this as an issue with remix?", "2024-04-18T18:10:07Z", "2024-04-18T18:10:07Z", "tynes", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM57GfW_", "I_kwDODjvEJM6GGEtP", "Yeah, I can submit a foundry repo and a remix project example.\r\n\r\nIMO, that's a weird style guide recommendation, as the file is only refactored once.  Every importer pays the price of translating project-relative includes.\r\n\r\nFor example, all [OpenZeppelin](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/access/Ownable.sol) and [forge-std](https://github.com/foundry-rs/forge-std/blob/master/src/StdCheats.sol) includes are file-relative.", "2024-04-18T20:48:37Z", "2024-04-18T20:49:31Z", "adraffy", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM57Qt8n", "I_kwDODjvEJM6GGEtP", "https://github.com/adraffy/optimism-project-relative-issue/blob/main/src/Contract.sol\r\n\r\n1. `foundryup`\r\n1. `forge build`\r\n\r\n> Compiler run failed:\r\n> Error (9553): Invalid type for argument in function call. Invalid implicit conversion from `struct Types.OutputRootProof memory` to `struct Types.OutputRootProof memory` requested.", "2024-04-21T09:38:53Z", "2024-04-21T09:50:44Z", "adraffy", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM57d9tj", "I_kwDODjvEJM6GGEtP", "Thank you for the repro case", "2024-04-23T05:41:54Z", "2024-04-23T05:41:54Z", "tynes", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM57d_6n", "I_kwDODjvEJM6GGEtP", "You can use the following one-liner to repro in [Remix](https://remix.ethereum.org):\r\n\r\n```\r\nimport \"@eth-optimism/contracts-bedrock/src/libraries/Encoding.sol\";\r\n```\r\n\r\nThis will correctly import via NPM then fail with with: `Error: not found src/libraries/rlp/RLPWriter.sol`", "2024-04-23T05:51:25Z", "2024-04-23T05:51:39Z", "adraffy", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM57nktS", "I_kwDODjvEJM6GGEtP", "I see, so relative paths are going to be more portable. I am curious if the following comment helps for the foundry case: https://github.com/foundry-rs/foundry/issues/3440#issuecomment-1472396987", "2024-04-24T03:50:09Z", "2024-04-24T03:50:18Z", "tynes", "2025-08-31 01:51:31"]
["IC_kwDODjvEJM6ZFBrS", "I_kwDODjvEJM6GGEtP", "I ran into similar issues and I think context aware remappings can solve this. If the remappings are changed to the below, the example should compile.\n\n```\n  remappings = [\n    \"lib/optimism/packages/contracts-bedrock/:src=lib/optimism/packages/contracts-bedrock/src/\",\n    \"@eth-optimism/=lib/optimism/packages/contracts-bedrock/\",\n  ]\n```", "2025-01-02T19:00:08Z", "2025-02-18T14:47:19Z", "alexkeating", "2025-08-31 01:51:31"]
["IC_kwDOKSJyfM6f-fyJ", "I_kwDOKSJyfM6nFTXO", "done", "2025-02-26T05:47:17Z", "2025-02-26T05:47:17Z", "jakim929", "2025-08-31 01:52:39"]
["IC_kwDOEf1bQc6ktcu0", "I_kwDOEf1bQc5w-W89", "\u6211\u4e5f\u60f3\u77e5\u9053\u600e\u4e48\u83b7\u5f97\u5e2e\u52a9", "2025-03-29T13:56:46Z", "2025-03-29T13:56:46Z", "gemini090401", "2025-08-31 03:24:15"]
["IC_kwDOKSJyfM6lsCbg", "I_kwDOKSJyfM6wfoRA", "Done with https://github.com/ethereum-optimism/design-docs/pull/246", "2025-04-04T21:48:03Z", "2025-04-04T21:48:03Z", "tremarkley", "2025-08-31 03:24:16"]
["IC_kwDOKSJyfM6j4ViF", "I_kwDOKSJyfM6veH8u", "Closed with https://github.com/ethereum-optimism/supersim/pull/360", "2025-03-24T21:35:14Z", "2025-03-24T21:35:14Z", "tremarkley", "2025-08-31 03:24:17"]
["IC_kwDOKSJyfM6Y-ueS", "I_kwDOKSJyfM6j3y8o", "@fainashalts can we tighten the scope of this issue or perhaps we split it into tighter child issues? Based on conversations prior to break the first order of business should be scoped explicitly to the crossdomain bridges to support relayer work. We potentially want three separate issues:\n\n1. Index transactions on the L1<>L2 bridge for all support L2s on sepolia and mainnet (in support of migrating the superchainrelayer.xyz into console\n2. Index transactions on the L2<>L2 inbox for sepolia when devnet launches, then standard testnet/mainnet support\n3. Index contracts deployed through CLI for contract management dashboard\n", "2024-12-31T16:36:49Z", "2024-12-31T16:36:49Z", "pharger", "2025-08-31 03:24:17"]
["IC_kwDOKSJyfM6ZkCCk", "I_kwDOKSJyfM6j3y8o", "@pharger I think we might need to dive a bit deeper here -- the relayer doesn't strictly need indexing, as its used on a per-transaction basis. I know when we spoke earlier I may have said something different but we were moving fast so I think I misspoke. \ud83d\ude04 \n\nWe had talked separately about wanting to index interop transactions specifically for observability and potentially to back up other future tools like automatic relaying. We need to do more scoping to figure out what that looks like. I propose as a first step we move the existing relayer into the console (which entails a rewrite as well) and then make it able to do L2-L2 relaying in prep for interop. The auto relay + indexing piece we can do more legwork on defining. \n\nIts hard to narrow indexing needs for contract management which is why we had discussed using a third party service. We could index individual contracts but I'm not sure we should. Curious to know more about your thoughts here.", "2025-01-07T22:36:53Z", "2025-01-07T22:36:53Z", "fainashalts", "2025-08-31 03:24:17"]
["IC_kwDOKSJyfM6Z2YXB", "I_kwDOKSJyfM6j3y8o", "Hmm maybe I'm missing something - in the scope of this issue, does it need to be more than simply indexing the cross domain messenger for the tx hashs, etc. as specified in the PRD? Im assuming we are carving out the contract indexing pice from this ticket. I guess I'm curious what the MVP really entails if we keep the scope here tight", "2025-01-09T20:15:55Z", "2025-01-09T20:16:29Z", "pharger", "2025-08-31 03:24:17"]
["IC_kwDOKSJyfM6Z2Y9h", "I_kwDOKSJyfM6j3y8o", "Also curious if we can't just use a third party solution to get this off the ground adn build the front end capabilities before persuing the full indexer write?", "2025-01-09T20:17:31Z", "2025-01-09T20:17:31Z", "pharger", "2025-08-31 03:24:17"]
["IC_kwDOKSJyfM6aCbBj", "I_kwDOKSJyfM6j3y8o", "Hi @pharger! Have had more convos about this since last we spoke, and I propose we reduce the scope here way down: let's index interop transactioon _only_ (so those that go through the L2toL2CDM) - this will allow us to provide a frontend for these if we want, and also will make the L2 to L2 Relaying UX much nicer. \n\nFor contract dashboard, I agree we should find a 3rd party solution, at least until we see if our dashboard has any PMF. Does that align with your thinking?", "2025-01-10T21:37:11Z", "2025-01-10T21:37:11Z", "fainashalts", "2025-08-31 03:24:17"]
["IC_kwDOKSJyfM6aZTxJ", "I_kwDOKSJyfM6j3y8o", "Confirming we discussed Monday 1/13 and have a path forward for Hamdi to take ownership with the goal fo shipping for ETH Denver. Moving to ready.", "2025-01-14T15:56:29Z", "2025-01-14T15:56:29Z", "pharger", "2025-08-31 03:24:17"]
["IC_kwDOKSJyfM6i_hEr", "I_kwDOKSJyfM6j3y8o", "@hamdiallam is this issue the one you're working off of, or is it ok to close?", "2025-03-18T19:58:47Z", "2025-03-18T19:58:47Z", "fainashalts", "2025-08-31 03:24:17"]
["IC_kwDOKSJyfM6j20bd", "I_kwDOKSJyfM6j3y8o", "I'm going to close this issue as the this was more for the auto relayer in the devnet. I'll create a new one for the incentivized relays", "2025-03-24T18:29:02Z", "2025-03-24T18:29:02Z", "hamdiallam", "2025-08-31 03:24:17"]
["IC_kwDOL-xLQ86juwA5", "I_kwDOL-xLQ86n5AUp", "Still working. Tested with Cheetah on 24 March 2025.\n\n<img width=\"1932\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/75e288b7-4e90-4c38-a75f-576e88aff002\" />", "2025-03-24T05:34:27Z", "2025-03-24T05:34:27Z", "scharissis", "2025-08-31 03:24:17"]
["IC_kwDOL-xLQ86k-pZa", "I_kwDOL-xLQ86vZPpH", "No longer required; Adrian will do this with Proto instead.", "2025-04-01T02:23:04Z", "2025-04-01T02:23:04Z", "scharissis", "2025-08-31 03:24:18"]
["IC_kwDOKIwiaM6e7-t-", "I_kwDOKIwiaM6pDFSI", "This is being addressed in: https://github.com/ethereum-optimism/docs/pull/1383", "2025-02-18T18:17:32Z", "2025-02-18T18:17:32Z", "sbvegan", "2025-08-31 03:24:18"]
["IC_kwDOFpg0Ns6ZsMYv", "I_kwDOFpg0Ns6UaKHl", "Hi @hamdiallam I see this is a P0 - can you confirm if so?", "2025-01-08T19:47:17Z", "2025-01-08T19:47:17Z", "fainashalts", "2025-08-31 03:24:19"]
["IC_kwDOFpg0Ns6bf74t", "I_kwDOFpg0Ns6UaKHl", "Hi @hamdiallam just following up! Is this a P0 task?", "2025-01-23T04:41:10Z", "2025-01-23T04:41:10Z", "fainashalts", "2025-08-31 03:24:19"]
["IC_kwDOFpg0Ns6lqoKg", "I_kwDOFpg0Ns6UaKHl", "ah whoops missed these comments! definitely not a p0", "2025-04-04T17:53:55Z", "2025-04-04T17:53:55Z", "hamdiallam", "2025-08-31 03:24:19"]
["IC_kwDOKIwiaM6iEmdx", "I_kwDOKIwiaM6tv0r9", "Hi @Cryptobos44 , looks like this was open by mistake. Closing for now", "2025-03-12T20:59:29Z", "2025-03-12T20:59:29Z", "bradleycamacho", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6j0poL", "I_kwDOH2Qg5s6pDETK", "Not relevant anymore with acceslist-changes", "2025-03-24T15:14:44Z", "2025-03-24T15:14:44Z", "protolambda", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6Bhl_f", "I_kwDOH2Qg5s6Me-Je", "Please add additional information: description of component upgrade process, full log after op-geth startup, other warning logs for op-node (p2p issues are usually not critical issues)", "2024-06-17T10:50:34Z", "2024-06-17T10:50:34Z", "opfocus", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6BhoGx", "I_kwDOH2Qg5s6Me-Je", "Description of component upgrade process:\r\n- Compile op-node and op-geth from source\r\n- Stop op-node and op-geth\r\n- Start op-geth \r\n- Wait a few seconds\r\n- Start op-node\r\n\r\nNode is now slowly recovering and showing logs like:\r\n- \"Resuming state snapshot generation\"\r\n- \"Imported new potential chain segment\"\r\n- \"Chain head was updated\"\r\n\r\nOp-geth logs:\r\n```\r\nJun 17 10:54:13 m-optimism-02 geth[3431]: INFO [06-17|10:54:13.740] Starting work on payload                 id=0x034d324ac0411490\r\nJun 17 10:54:13 m-optimism-02 geth[3431]: INFO [06-17|10:54:13.759] Aborting state snapshot generation       root=c5204c..ef90dc at=203350..a2bfba accounts=17,495,088 slots=29,517,607 storage=3.29GiB    dangling=0 elapsed=33m44.383s  eta=4h31m5.091s\r\nJun 17 10:54:13 m-optimism-02 geth[3431]: INFO [06-17|10:54:13.760] Resuming state snapshot generation       root=ca74fd..49a798 at=203350..a2bfba accounts=17,495,088 slots=29,517,607 storage=3.29GiB    dangling=0 elapsed=33m44.384s  eta=4h31m5.099s\r\nJun 17 10:54:13 m-optimism-02 geth[3431]: INFO [06-17|10:54:13.766] Imported new potential chain segment     number=121,504,439 hash=008c08..454d3c blocks=1 txs=13 mgas=0.884  elapsed=20.687ms    mgasps=42.733  age=3h53m18s snapdiffs=80.09KiB  triedirty=0.00B\r\nJun 17 10:54:13 m-optimism-02 geth[3431]: INFO [06-17|10:54:13.768] Chain head was updated                   number=121,504,439 hash=008c08..454d3c root=390e44..4c6b24 elapsed=1.223119ms  age=3h53m18s\r\nJun 17 10:54:13 m-optimism-02 geth[3431]: INFO [06-17|10:54:13.972] Starting work on payload                 id=0x03e1ad5ab32d2b49\r\nJun 17 10:54:13 m-optimism-02 geth[3431]: INFO [06-17|10:54:13.999] Aborting state snapshot generation       root=ca74fd..49a798 at=20348a..4c7bcf accounts=17,497,643 slots=29,518,485 storage=3.29GiB    dangling=0 elapsed=33m44.623s  eta=4h31m3.881s\r\nJun 17 10:54:14 m-optimism-02 geth[3431]: INFO [06-17|10:54:14.000] Resuming state snapshot generation       root=2814ad..3125a9 at=20348a..4c7bcf accounts=17,497,643 slots=29,518,485 storage=3.29GiB    dangling=0 elapsed=33m44.624s  eta=4h31m3.889s\r\nJun 17 10:54:14 m-optimism-02 geth[3431]: INFO [06-17|10:54:14.014] Imported new potential chain segment     number=121,504,440 hash=2cf535..9bd990 blocks=1 txs=14 mgas=3.154  elapsed=34.805ms    mgasps=90.624  age=3h53m17s snapdiffs=83.60KiB  triedirty=0.00B\r\nJun 17 10:54:14 m-optimism-02 geth[3431]: INFO [06-17|10:54:14.017] Chain head was updated                   number=121,504,440 hash=2cf535..9bd990 root=88bdd1..75cf2f elapsed=1.714696ms  age=3h53m17s\r\n\r\n```\r\n\r\n\r\nAdditionally, I tried to disable snapshot (`--snapshot=false`) but op-geth is still entering in the generating snapshot", "2024-06-17T10:55:27Z", "2024-06-17T10:55:27Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6BhqQU", "I_kwDOH2Qg5s6Me-Je", "I think snapshot here is not a same concept. \r\nIn addition: \r\n```\r\nJun 17 10:54:14 m-optimism-02 geth[3431]: INFO [06-17|10:54:14.014] Imported new potential chain segment number=121,504,440 hash=2cf535..9bd990 blocks=1 txs=14 mgas=3.154 elapsed=34.805ms mgasps=90.624 age=3h53m17s snapdiffs=83.60KiB triedirty=0.00B\r\nJun 17 10:54:14 m-optimism-02 geth[3431]: INFO [06-17|10:54:14.017] Chain head was updated number=121,504,440 hash=2cf535..9bd990 root=88bdd1..75cf2f elapsed= 1.714696ms age=3h53m17s\r\n```\r\nIt may mean that it has returned to normal", "2024-06-17T11:00:52Z", "2024-06-17T11:00:52Z", "opfocus", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6BiOxn", "I_kwDOH2Qg5s6Me-Je", "btw, snapshot needs to be enabled with we use snap-sync: https://github.com/ethereum-optimism/op-geth/blob/5e9cb8176cf953e09208ffca77c6c6ea7ee07015/cmd/utils/flags.go#L1812", "2024-06-17T12:11:27Z", "2024-06-17T12:11:27Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6Bjsxv", "I_kwDOH2Qg5s6Me-Je", "> btw, snapshot needs to be enabled with we use snap-sync:\r\n> \r\n> https://github.com/ethereum-optimism/op-geth/blob/5e9cb8176cf953e09208ffca77c6c6ea7ee07015/cmd/utils/flags.go#L1812\r\n\r\n\r\nSorry, I don't know much about this. Here may be some relevant information:\r\n1. Log related:\r\nhttps://github.com/ethereum-optimism/op-geth/blob/5e9cb8176cf953e09208ffca77c6c6ea7ee07015/core/state/snapshot/generate.go#L663\r\n2. flag default value\r\n    --snapshot (default: true) ($GETH_SNAPSHOT)\r\n          Enables snapshot-database mode (default = enable)\r\n\r\n3. geth documentation part:\r\nhttps://geth.ethereum.org/docs/fundamentals/logs#syncing\r\n\r\nThe crux of the matter is why Generating state snapshot has to happen and is so time consuming, right?\r\nI'm looking for help from others\r\n\r\n\r\n", "2024-06-17T14:47:25Z", "2024-06-17T14:47:25Z", "opfocus", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C3fD8", "I_kwDOH2Qg5s6Me-Je", "the issue happen again in mainnet...\r\n\r\n#### Description\r\n\"Generating state snapshot\" is affecting node, which gets out of sync.\r\n\r\n#### System information\r\n- op_geth: \"v1.101315.2\"\r\n- op_node: \"op-node/v1.7.7\"\r\n- Ubuntu\r\n\r\n#### Expected behaviour\r\nNode should not get out of sync.\r\n\r\n#### Actual behaviour\r\nNode gets out of sync by more than 6000 blocks.\r\n\r\n#### Logs\r\n\r\nThis are the latest logs where the node is also processing blocks (\"Imported new potential chain segment\" / \"Chain head was updated\"). However for some time the node is only generating state snapshots and getting further behind the public network.\r\n\r\nop-geth\r\n````\r\nJun 27 19:57:35 m-optimism-02 geth[1263]: INFO [06-27|19:57:35.352] Generating state snapshot                root=04f327..4a689f in=da96cf..e517af at=b5e103..ba5cef accounts=119,253,539 slots=318,285,026 storage=30.36GiB  dangling=0 elapsed=3h22m40.833s eta=34m41.291s\r\nJun 27 19:57:35 m-optimism-02 geth[1263]: INFO [06-27|19:57:35.413] Imported new potential chain segment     number=121,953,090 hash=20b718..e1933a blocks=1 txs=7  mgas=2.669  elapsed=477.707ms    mgasps=5.587   age=3h41m38s triedirty=0.00B\r\nJun 27 19:57:35 m-optimism-02 geth[1263]: INFO [06-27|19:57:35.415] Chain head was updated                   number=121,953,090 hash=20b718..e1933a root=7b0c63..b5761f elapsed=1.015623ms   age=3h41m38s\r\nJun 27 19:57:35 m-optimism-02 geth[1263]: INFO [06-27|19:57:35.783] Imported new potential chain segment     number=121,953,091 hash=8fea53..930873 blocks=1 txs=7  mgas=2.111  elapsed=365.674ms    mgasps=5.774   age=3h41m36s triedirty=0.00B\r\nJun 27 19:57:35 m-optimism-02 geth[1263]: INFO [06-27|19:57:35.785] Chain head was updated                   number=121,953,091 hash=8fea53..930873 root=2d1371..874032 elapsed=\"781.358\u00b5s\"  age=3h41m36s\r\nJun 27 19:57:43 m-optimism-02 geth[1263]: INFO [06-27|19:57:43.353] Generating state snapshot                root=04f327..4a689f in=da96cf..e517af at=c11751..b4c505 accounts=119,253,539 slots=318,541,051 storage=30.38GiB  dangling=0 elapsed=3h22m48.834s eta=34m42.66s\r\nJun 27 19:57:44 m-optimism-02 geth[1263]: INFO [06-27|19:57:44.433] Imported new potential chain segment     number=121,953,092 hash=22d54a..34b31b blocks=1 txs=16 mgas=9.373  elapsed=8.646s       mgasps=1.084   age=3h41m43s triedirty=0.00B\r\nJun 27 19:57:44 m-optimism-02 geth[1263]: WARN [06-27|19:57:44.434] Ignoring already known beacon payload    number=121,953,092 hash=22d54a..34b31b age=3h41m43s\r\nJun 27 19:57:44 m-optimism-02 geth[1263]: INFO [06-27|19:57:44.437] Chain head was updated                   number=121,953,092 hash=22d54a..34b31b root=44874c..81f566 elapsed=1.917279ms   age=3h41m43s\r\nJun 27 19:57:45 m-optimism-02 geth[1263]: INFO [06-27|19:57:45.134] Imported new potential chain segment     number=121,953,093 hash=c321c6..d76921 blocks=1 txs=14 mgas=2.868  elapsed=693.580ms    mgasps=4.134   age=3h41m42s triedirty=0.00B\r\nJun 27 19:57:45 m-optimism-02 geth[1263]: INFO [06-27|19:57:45.137] Chain head was updated                   number=121,953,093 hash=c321c6..d76921 root=56df25..90bc84 elapsed=1.345763ms   age=3h41m42s\r\nJun 27 19:57:51 m-optimism-02 geth[1263]: INFO [06-27|19:57:51.371] Generating state snapshot                root=04f327..4a689f in=da96cf..e517af at=cce8b6..5d55bb accounts=119,253,539 slots=318,810,785 storage=30.40GiB  dangling=0 elapsed=3h22m56.851s eta=34m44.032s\r\nJun 27 19:57:51 m-optimism-02 geth[1263]: INFO [06-27|19:57:51.650] Imported new potential chain segment     number=121,953,094 hash=d20d58..9e51b9 blocks=1 txs=14 mgas=9.753  elapsed=6.508s       mgasps=1.499   age=3h41m46s triedirty=0.00B\r\nJun 27 19:57:52 m-optimism-02 geth[1263]: WARN [06-27|19:57:52.178] Ignoring already known beacon payload    number=121,953,094 hash=d20d58..9e51b9 age=3h41m47s\r\nJun 27 19:57:52 m-optimism-02 geth[1263]: INFO [06-27|19:57:52.182] Chain head was updated                   number=121,953,094 hash=d20d58..9e51b9 root=0b5694..997667 elapsed=2.497365ms   age=3h41m47s\r\nJun 27 19:57:52 m-optimism-02 geth[1263]: INFO [06-27|19:57:52.268] Imported new potential chain segment     number=121,953,095 hash=7217c4..12e630 blocks=1 txs=13 mgas=1.444  elapsed=83.602ms     mgasps=17.268  age=3h41m45s triedirty=0.00B\r\nJun 27 19:57:52 m-optimism-02 geth[1263]: INFO [06-27|19:57:52.270] Chain head was updated                   number=121,953,095 hash=7217c4..12e630 root=be08ae..a83b79 elapsed=\"797.269\u00b5s\"  age=3h41m45\r\n```\r\n\r\nop-node\r\n```\r\nJun 27 19:58:14 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:14+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xac234f9146c810dea6121552580a4ed4cd864ccccde907b12a09c5120b1ba426:121959758\r\nJun 27 19:58:15 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:15+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAkxHXmw5iw5amSc32mTxoTo2R2gZKrAfx143uBbZ9P6QQr\r\nJun 27 19:58:15 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:15+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xed2040c95470d3ab3c66b398b2d66387185658a2898a9151fab17a6b8b4ca553:121959759 peer=16Uiu2HAkzc1eByUyVc8tmu97dXYwehh6QTo5MxK5VFTjHH4wkj6J\r\nJun 27 19:58:15 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:15+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xed2040c95470d3ab3c66b398b2d66387185658a2898a9151fab17a6b8b4ca553:121959759\r\nJun 27 19:58:17 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:17+0000 lvl=info msg=\"Sync progress\" reason=\"new chain head block\" l2_finalized=0x22f88dea6338b4e91c6e68a4e71424b00079c36e7751a7e801a7c7499d3690f7:121952848 l2_safe=0x22f88dea6338b4e91c6e68a4e71424b00079c36e7751a7e801a7c7499d3690f7:121952848 l2_pending_safe=0x87dfd4a049bc84d2c31bd4a7dfa0b84e95ef9b7c0537253ec9277151b3b305f7:121952940 l2_unsafe=0xc97b9699f06ee17ac114e94ce2755c5aae861c6130affe754ad5883b02b7188c:121953102 l2_backup_unsafe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_time=1719504981\r\nJun 27 19:58:17 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:17+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x999e1d918794702d659d4fa32e79beff68322863fe94385d14add76198f6de1b:121959760 peer=16Uiu2HAkzc1eByUyVc8tmu97dXYwehh6QTo5MxK5VFTjHH4wkj6J\r\nJun 27 19:58:17 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:17+0000 lvl=info msg=\"Sync progress\" reason=\"new chain head block\" l2_finalized=0x22f88dea6338b4e91c6e68a4e71424b00079c36e7751a7e801a7c7499d3690f7:121952848 l2_safe=0x22f88dea6338b4e91c6e68a4e71424b00079c36e7751a7e801a7c7499d3690f7:121952848 l2_pending_safe=0x87dfd4a049bc84d2c31bd4a7dfa0b84e95ef9b7c0537253ec9277151b3b305f7:121952940 l2_unsafe=0x55f4f8e3e37df26f8c43d58ee8c5dd7ac206d03a62a9314d08354047ae843a59:121953103 l2_backup_unsafe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_time=1719504983\r\nJun 27 19:58:17 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:17+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x999e1d918794702d659d4fa32e79beff68322863fe94385d14add76198f6de1b:121959760\r\nJun 27 19:58:18 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:18+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAkzg2h8aN2gAq497tkHmN5hDy9EaKAkhbMScmCAbBPxNwJ\r\nJun 27 19:58:18 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:18+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm1zB9TgXxvtxjLXXJfhRw68ZMeCoiJ7BwSXKY2AbDAtVU\r\nJun 27 19:58:19 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:19+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x7ab504f8fb4452ca8c1fd749fec06de95a94ac93cfee6eab02b375a5bdb485d9:121959761 peer=16Uiu2HAkzc1eByUyVc8tmu97dXYwehh6QTo5MxK5VFTjHH4wkj6J\r\nJun 27 19:58:20 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:20+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm7v25THFJZJvkoK3bH4Wqjga9P96uWgBLRGQSYqggDA2N\r\nJun 27 19:58:20 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:20+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm1ftqsNaMCUi6JhQ6RdZrGvQ3Vt1d69xv1ZqzVMac8Cki\r\nJun 27 19:58:20 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:20+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm5Yo4tL1PnjeiVAvgKtr8vWzMTLSQs9xd7vYUC2fS4GJC\r\nJun 27 19:58:21 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:21+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmNytWmvGV1UHuUBngGFxyNsLkquzzCLeUoTLUGUrjW5Ju\r\nJun 27 19:58:21 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:21+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmFBzRPMeAkNUipyJ1DEN5d9XyvvNvEP41xotCiPhCb6Kz\r\nJun 27 19:58:21 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:21+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xaac5384d7406582c361803169b19e1f1fde1ff67f118bfe01662253675302340:121959762 peer=16Uiu2HAkzc1eByUyVc8tmu97dXYwehh6QTo5MxK5VFTjHH4wkj6J\r\nJun 27 19:58:22 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:22+0000 lvl=error msg=\"Payload execution failed\" block_hash=0xf3e2cf9db49412ae31e6d9c579c1cbd96529e9f3e3452df27eaa07a78ee084b7 err=\"Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 27 19:58:22 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:22+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=1 err=\"temp: failed to update insert payload: failed to execute payload: Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 27 19:58:22 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:22+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x7ab504f8fb4452ca8c1fd749fec06de95a94ac93cfee6eab02b375a5bdb485d9:121959761\r\nJun 27 19:58:22 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:22+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xaac5384d7406582c361803169b19e1f1fde1ff67f118bfe01662253675302340:121959762\r\nJun 27 19:58:23 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:23+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xe3607546f535e3b7b62bffecf741b044f08dac778afefe7c98f6a365349f325f:121959763 peer=16Uiu2HAkzc1eByUyVc8tmu97dXYwehh6QTo5MxK5VFTjHH4wkj6J\r\nJun 27 19:58:23 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:23+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xe3607546f535e3b7b62bffecf741b044f08dac778afefe7c98f6a365349f325f:121959763\r\nJun 27 19:58:25 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:25+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm2vXWR3ooDT3dnM9JuaHzeNysTmQV3tsqhXC8p1aFn3V7\r\nJun 27 19:58:25 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:25+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xcde7b7508a24f4e123734b96e724ff84d17f0afc73eb6f35ba77e87e3413345b:121959764 peer=16Uiu2HAkzc1eByUyVc8tmu97dXYwehh6QTo5MxK5VFTjHH4wkj6J\r\nJun 27 19:58:27 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:27+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xd4c6edd585941760e8ffc698763ed40acf3964c8176aa92ef104b39f9881ce4a:121959765 peer=16Uiu2HAkzc1eByUyVc8tmu97dXYwehh6QTo5MxK5VFTjHH4wkj6J\r\nJun 27 19:58:28 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:28+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmAPxaqqrjA8p2yUD2rma2pZoaJJaMory5af7x5vfKqBLG\r\nJun 27 19:58:29 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:29+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xb1b26df341b66bb777847bc5138605f6f222fab57d195cb6fa12a0f2ab52de57:121959766 peer=16Uiu2HAkzc1eByUyVc8tmu97dXYwehh6QTo5MxK5VFTjHH4wkj6J\r\nJun 27 19:58:29 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:29+0000 lvl=error msg=\"Payload execution failed\" block_hash=0xf3e2cf9db49412ae31e6d9c579c1cbd96529e9f3e3452df27eaa07a78ee084b7 err=\"Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 27 19:58:29 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:29+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=2 err=\"temp: failed to update insert payload: failed to execute payload: Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 27 19:58:29 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:29+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xcde7b7508a24f4e123734b96e724ff84d17f0afc73eb6f35ba77e87e3413345b:121959764\r\nJun 27 19:58:29 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:29+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xd4c6edd585941760e8ffc698763ed40acf3964c8176aa92ef104b39f9881ce4a:121959765\r\nJun 27 19:58:29 m-optimism-02 op-node[1264]: t=2024-06-27T19:58:29+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xb1b26df341b66bb777847bc5138605f6f222fab57d195cb6fa12a0f2ab52de57:121959766\r\n```\r\n\r\n#### CLIs\r\n\r\nop-geth:\r\n\r\n```\r\n/usr/local/bin/geth \\\r\n    --op-network=op-mainnet \\\r\n    --datadir=/var/lib/optimism/data \\\r\n    --syncmode=snap \\\r\n    --gcmode=archive \\\r\n    --http \\\r\n    --http.addr=0.0.0.0 \\\r\n    --http.port=8545 \\\r\n    --http.vhosts=\"*\" \\\r\n    --http.corsdomain=\"*\" \\\r\n    --http.api=web3,debug,eth,net,engine,geth \\\r\n    --ws \\\r\n    --ws.port=8546 \\\r\n    --ws.addr=0.0.0.0 \\\r\n    --ws.origins=\"*\" \\\r\n    --ws.api=debug,eth,net,engine \\\r\n    --authrpc.addr=127.0.0.1 \\\r\n    --authrpc.port=8551 \\\r\n    --authrpc.jwtsecret=/var/lib/optimism/jwt.txt \\\r\n    --authrpc.vhosts=\"*\" \\\r\n    --metrics \\\r\n    --metrics.addr=0.0.0.0 \\\r\n    --metrics.port=6060 \\\r\n    --port=30303 \\\r\n    --discovery.port=30303 \\\r\n    --maxpeers=500 \\\r\n    --rollup.historicalrpc=https://mainnet.optimism.io \\\r\n    --rollup.disabletxpoolgossip=true \\\r\n    --rollup.sequencerhttp=https://mainnet-sequencer.optimism.io \\\r\n    --rollup.superchain-upgrades \\\r\n    --rollup.halt=major \\\r\n    --verbosity=3\r\n```\r\n\r\nop-node\r\n\r\n```\r\n/usr/local/bin/op-node \\\r\n    --syncmode=execution-layer \\\r\n    --l1.trustrpc \\\r\n    --l1.rpckind=basic \\\r\n    --l1=http://ETH_ADDRESS:8545 \\\r\n    --l1.beacon=http://ETH_ADDRESS:5052 \\\r\n    --l2=http://127.0.0.1:8551 \\\r\n    --rpc.addr=127.0.0.1 \\\r\n    --rpc.port=9545 \\\r\n    --l2.jwt-secret=/var/lib/optimism/jwt.txt \\\r\n    --network=op-mainnet \\\r\n    --p2p.peerstore.path=/var/lib/optimism/data/opnode_peerstore_db \\\r\n    --p2p.priv.path=/var/lib/optimism/data/opnode_p2p_priv.txt \\\r\n    --p2p.discovery.path=/var/lib/optimism/data/opnode_discovery_db \\\r\n    --rollup.load-protocol-versions=true \\\r\n    --rollup.halt=major\r\n```", "2024-06-27T20:06:40Z", "2024-06-27T20:06:40Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C3lxp", "I_kwDOH2Qg5s6Me-Je", "When I restart de node it starts again \"Generating state snapshot\" without processing new blocks. Logs:\r\n\r\n```\r\nJun 27 20:23:34 m-optimism-02 geth[1441]: INFO [06-27|20:23:34.008] Generating state snapshot                root=cd7db8..47e256 at=000bb4..5adcf6 accounts=25092 slots=14909 storage=2.76MiB dangling=0 elapsed=8.004s  eta=12h26m55.013s\r\nJun 27 20:23:36 m-optimism-02 geth[1441]: INFO [06-27|20:23:36.608] Looking for peers                        peercount=3 tried=223 static=0\r\nJun 27 20:23:43 m-optimism-02 geth[1441]: INFO [06-27|20:23:43.244] Generating state snapshot                root=cd7db8..47e256 at=002561..0a9945 accounts=80007 slots=54874 storage=9.15MiB dangling=0 elapsed=16.009s eta=7h47m34.094s\r\nJun 27 20:23:51 m-optimism-02 geth[1441]: INFO [06-27|20:23:51.247] Generating state snapshot                root=cd7db8..47e256 at=00450f..aa0c3d accounts=147,852 slots=86056 storage=15.94MiB dangling=0 elapsed=24.012s eta=6h19m23.436s\r\nJun 27 20:23:59 m-optimism-02 geth[1441]: INFO [06-27|20:23:59.248] Generating state snapshot                root=cd7db8..47e256 in=005784..87caff at=338140..f1fe8b accounts=187,139 slots=250,057 storage=31.38MiB dangling=0 elapsed=32.013s eta=6h39m0.586s\r\nJun 27 20:24:07 m-optimism-02 geth[1441]: INFO [06-27|20:24:07.255] Generating state snapshot                root=cd7db8..47e256 in=0067b0..13c55c at=c035c1..a94276 accounts=221,556 slots=389,421 storage=44.41MiB dangling=0 elapsed=40.021s eta=7h0m55.19s\r\nJun 27 20:24:15 m-optimism-02 geth[1441]: INFO [06-27|20:24:15.259] Generating state snapshot                root=cd7db8..47e256 in=008a8f..c61aab at=1fbf5b..09ee45 accounts=295,951 slots=437,341 storage=52.91MiB dangling=0 elapsed=48.025s eta=6h17m47.152s\r\nJun 27 20:24:23 m-optimism-02 geth[1441]: INFO [06-27|20:24:23.276] Generating state snapshot                root=cd7db8..47e256 in=00acea..20c5cb at=0025e0..faec1c accounts=369,615 slots=544,266 storage=65.75MiB dangling=0 elapsed=56.041s eta=5h53m3.985s\r\nJun 27 20:24:31 m-optimism-02 geth[1441]: INFO [06-27|20:24:31.279] Generating state snapshot                root=cd7db8..47e256 in=00b43f..448241 at=d36e15..676750 accounts=385,416 slots=936,255 storage=98.46MiB dangling=0 elapsed=1m4.044s eta=6h27m2.078s\r\nJun 27 20:24:39 m-optimism-02 geth[1441]: INFO [06-27|20:24:39.279] Generating state snapshot                root=cd7db8..47e256 at=00d252..21f97f accounts=449,693 slots=1,038,518 storage=110.85MiB dangling=0 elapsed=1m12.045s eta=6h12m57.351s\r\n```", "2024-06-27T20:25:28Z", "2024-06-27T20:25:28Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C3zvY", "I_kwDOH2Qg5s6Me-Je", "@opfocus , is this an issue that needs to be fixed? \r\n\r\nCan the \"generating snapshot\" process be the cause of the node to run out of-sync???\r\n\r\nshould we use \"--snapshot=false\" to avoid this issue? ", "2024-06-27T21:07:49Z", "2024-06-27T21:07:49Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C5MbS", "I_kwDOH2Qg5s6Me-Je", "\r\nCan you investigate the performance of your server, which appears in the op-node log above. \"context deadline exceeded\" messages that indicate things are taking too long (hence deadline exceeded).   I am not sure", "2024-06-28T03:19:11Z", "2024-06-28T03:19:11Z", "opfocus", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C6db8", "I_kwDOH2Qg5s6Me-Je", "I have increased resources allocation (cpu and memory) and disk is an io2 ebs volume with 20k iops, but I still see \"context deadline exceeded\" in the op-node logs. Op-geth finished \"generating snapshot\" but node is still 2000 block behind.\r\n\r\nop-geth:\r\n\r\n```\r\nJun 28 07:59:44 m-optimism-02 geth[4648]: INFO [06-28|07:59:44.509] Imported new potential chain segment     number=121,978,515 hash=44e58f..fae752 blocks=1 txs=12  mgas=0.830  elapsed=203.124ms    mgasps=4.084   age=1h36m17s triedirty=0.00B\r\nJun 28 07:59:44 m-optimism-02 geth[4648]: INFO [06-28|07:59:44.510] Chain head was updated                   number=121,978,515 hash=44e58f..fae752 root=e362f9..0372a0 elapsed=\"637.619\u00b5s\"  age=1h36m17s\r\nJun 28 07:59:55 m-optimism-02 geth[4648]: INFO [06-28|07:59:55.194] Imported new potential chain segment     number=121,978,516 hash=4099bd..b1623d blocks=1 txs=8   mgas=13.746 elapsed=10.681s      mgasps=1.287   age=1h36m26s triedirty=0.00B\r\nJun 28 07:59:55 m-optimism-02 geth[4648]: WARN [06-28|07:59:55.194] Ignoring already known beacon payload    number=121,978,516 hash=4099bd..b1623d age=1h36m26s\r\nJun 28 07:59:55 m-optimism-02 geth[4648]: INFO [06-28|07:59:55.198] Chain head was updated                   number=121,978,516 hash=4099bd..b1623d root=3eea7c..c216ee elapsed=2.786871ms   age=1h36m26s\r\nJun 28 07:59:55 m-optimism-02 geth[4648]: INFO [06-28|07:59:55.663] Imported new potential chain segment     number=121,978,517 hash=ffc39b..26367f blocks=1 txs=10  mgas=2.271  elapsed=463.523ms    mgasps=4.899   age=1h36m24s triedirty=0.00B\r\nJun 28 07:59:55 m-optimism-02 geth[4648]: INFO [06-28|07:59:55.665] Chain head was updated                   number=121,978,517 hash=ffc39b..26367f root=f5841e..da457c elapsed=\"775.678\u00b5s\"  age=1h36m24s\r\n```\r\n\r\nop-node:\r\n```\r\nJun 28 08:00:19 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:19+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x77407905f683b97277d76c35c37ecf23835ec5ddf972726b97f4b18bec728dfd:121981421 peer=16Uiu2HAm9XhrnZKYNd4bpUpjuWbEPix3kJwMpMGSpcjwcydKRcp8\r\nJun 28 08:00:20 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:20+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm3eqrDVcH2pFaBuoFPgh7ncYhpTEUNEHYc1gKnsW21B7z\r\nJun 28 08:00:21 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:21+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x30c756021473fef0ddb7b803cd4ec3702437a3c5a0068e4577e65b743ab07cfd:121981422 peer=16Uiu2HAm9XhrnZKYNd4bpUpjuWbEPix3kJwMpMGSpcjwcydKRcp8\r\nJun 28 08:00:21 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:21+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm4pBud6PmDkq72TTydHYzNAZbvibaPebNvboABjAS8CJv\r\nJun 28 08:00:22 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:22+0000 lvl=error msg=\"Payload execution failed\" block_hash=0x6b34a50f5654a4bbfe5a0039a47b360ddb64d9888225f492f3cd2bec2600813b err=\"Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 28 08:00:22 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:22+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=1 err=\"temp: failed to update insert payload: failed to execute payload: Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 28 08:00:22 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:22+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x77407905f683b97277d76c35c37ecf23835ec5ddf972726b97f4b18bec728dfd:121981421\r\nJun 28 08:00:22 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:22+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x30c756021473fef0ddb7b803cd4ec3702437a3c5a0068e4577e65b743ab07cfd:121981422\r\nJun 28 08:00:23 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:23+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x8a792649348d9768140b555a7dc4ba33ae938328d400434573a6da817083cf31:121981423 peer=16Uiu2HAm9XhrnZKYNd4bpUpjuWbEPix3kJwMpMGSpcjwcydKRcp8\r\nJun 28 08:00:23 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:23+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x8a792649348d9768140b555a7dc4ba33ae938328d400434573a6da817083cf31:121981423\r\nJun 28 08:00:24 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:24+0000 lvl=info msg=\"Sync progress\" reason=\"new chain head block\" l2_finalized=0x6acd813fd568864eafbde7e98f1648a8e91884eb152ce6291fa687327aa33991:121973468 l2_safe=0x6acd813fd568864eafbde7e98f1648a8e91884eb152ce6291fa687327aa33991:121973468 l2_pending_safe=0x6acd813fd568864eafbde7e98f1648a8e91884eb152ce6291fa687327aa33991:121973468 l2_unsafe=0x6b34a50f5654a4bbfe5a0039a47b360ddb64d9888225f492f3cd2bec2600813b:121978525 l2_backup_unsafe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_time=1719555827\r\nJun 28 08:00:25 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:25+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x3a361ec79f6b119b454cb322565691e4c6e49f2143375e8a94058a142802086e:121981424 peer=16Uiu2HAm9XhrnZKYNd4bpUpjuWbEPix3kJwMpMGSpcjwcydKRcp8\r\nJun 28 08:00:26 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:26+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmEAGuCYWNsqWR8E4LPqGvfCBR222nUDqr81Un1Psm4roM\r\nJun 28 08:00:27 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:27+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmDUGWtUgtExBhrNCmvkGJWgpZRKMbV68nkGhYCNtoW1NV\r\nJun 28 08:00:27 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:27+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0x8f7fbfbc0e34edc0311b1909da007c64d5ae556615c87225352a417857b09857:121981425 peer=16Uiu2HAm9XhrnZKYNd4bpUpjuWbEPix3kJwMpMGSpcjwcydKRcp8\r\nJun 28 08:00:29 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:29+0000 lvl=info msg=\"Received signed execution payload from p2p\" id=0xd527eb96fd7a380bdcc921b75c0bdd05ddd6181e778aa15f4711d5a0facca53c:121981426 peer=16Uiu2HAm9XhrnZKYNd4bpUpjuWbEPix3kJwMpMGSpcjwcydKRcp8\r\nJun 28 08:00:29 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:29+0000 lvl=error msg=\"Payload execution failed\" block_hash=0x017b5234e2e0caf6f66ff49ae90758da353056c778e561fa20dd6b36852638c6 err=\"Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 28 08:00:29 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:29+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=1 err=\"temp: failed to update insert payload: failed to execute payload: Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 28 08:00:29 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:29+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x3a361ec79f6b119b454cb322565691e4c6e49f2143375e8a94058a142802086e:121981424\r\nJun 28 08:00:29 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:29+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0x8f7fbfbc0e34edc0311b1909da007c64d5ae556615c87225352a417857b09857:121981425\r\nJun 28 08:00:29 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:29+0000 lvl=info msg=\"Optimistically queueing unsafe L2 execution payload\" id=0xd527eb96fd7a380bdcc921b75c0bdd05ddd6181e778aa15f4711d5a0facca53c:121981426\r\nJun 28 08:00:30 m-optimism-02 op-node[4649]: t=2024-06-28T08:00:30+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmLMff8TyjBXV22hfiaPWiYCp5t7dztkNeF4qdSeczPhEh\r\n```", "2024-06-28T08:03:26Z", "2024-06-28T08:03:26Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C6ecX", "I_kwDOH2Qg5s6Me-Je", "Every time I restart op-geth, it starts to generate snapshots:\r\n\r\n```\r\nJun 28 08:03:57 m-optimism-02 geth[1333040]: INFO [06-28|08:03:57.042] Generating state snapshot                root=f37bb2..ca4435 in=005784..87caff at=03a62f..3a611d accounts=187,467 slots=233,195 storage=30.04MiB dangling=0 elapsed=32.043s  eta=6h39m23.02s\r\nJun 28 08:04:05 m-optimism-02 geth[1333040]: INFO [06-28|08:04:05.043] Generating state snapshot                root=f37bb2..ca4435 at=005c86..10f356 accounts=198,112 slots=331,944 storage=38.71MiB dangling=0 elapsed=40.044s  eta=7h52m4.109s\r\nJun 28 08:04:07 m-optimism-02 geth[1333040]: INFO [06-28|08:04:07.572] New local node record                    seq=1,717,531,279,945 id=4857b91b76a03e11 ip=108.128.34.94 udp=30303 tcp=30303\r\nJun 28 08:04:13 m-optimism-02 geth[1333040]: INFO [06-28|08:04:13.052] Generating state snapshot                root=f37bb2..ca4435 at=007bf3..ac002a accounts=265,159 slots=417,557 storage=49.38MiB dangling=0 elapsed=48.054s  eta=7h2m40.094s\r\nJun 28 08:04:21 m-optimism-02 geth[1333040]: INFO [06-28|08:04:21.055] Generating state snapshot                root=f37bb2..ca4435 at=009859..93b546 accounts=326,266 slots=520,921 storage=61.19MiB dangling=0 elapsed=56.056s  eta=6h40m58.05s\r\nJun 28 08:04:29 m-optimism-02 geth[1333040]: INFO [06-28|08:04:29.056] Generating state snapshot                root=f37bb2..ca4435 at=00b425..8207dd accounts=385,846 slots=554,450 storage=67.56MiB dangling=0 elapsed=1m4.057s eta=6h27m19.945s\r\nJun 28 08:04:37 m-optimism-02 geth[1333040]: INFO [06-28|08:04:37.060] Generating state snapshot                root=f37bb2..ca4435 at=00bccc..933349 accounts=404,529 slots=1,022,520 storage=106.67MiB dangling=0 elapsed=1m12.062s eta=6h55m42.248s\r\nJun 28 08:04:45 m-optimism-02 geth[1333040]: INFO [06-28|08:04:45.062] Generating state snapshot                root=f37bb2..ca4435 at=00e0cf..0ab337 accounts=481,493 slots=1,149,376 storage=120.55MiB dangling=0 elapsed=1m20.063s eta=6h27m40.115s\r\nJun 28 08:04:53 m-optimism-02 geth[1333040]: INFO [06-28|08:04:53.064] Generating state snapshot                root=f37bb2..ca4435 at=010102..2d7a89 accounts=550,072 slots=1,263,094 storage=133.40MiB dangling=0 elapsed=1m28.065s eta=6h12m48.312s\r\nJun 28 08:05:01 m-optimism-02 geth[1333040]: INFO [06-28|08:05:01.064] Generating state snapshot                root=f37bb2..ca4435 at=0126ca..4e7220 accounts=630,803 slots=1,348,877 storage=145.33MiB dangling=0 elapsed=1m36.066s eta=5h54m21.044s\r\n```\r\n\r\nis this related with snap sync? ", "2024-06-28T08:05:56Z", "2024-06-28T08:05:56Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C6ibE", "I_kwDOH2Qg5s6Me-Je", "@opfocus , can you ask a developer to have a look at this issue and logs? \r\n\r\nIMO, this is not related with server resources... We are currently using a VM with 16 cpu cores and 64Gb (m6a.4xlarge), and still getting these warning messages:\r\n\r\n```\r\nJun 28 08:15:18 m-optimism-02 op-node[1334149]: t=2024-06-28T08:15:18+0000 lvl=warn msg=\"Failed to share forkchoice-updated signal\" state=\"&{HeadBlockHash:0xbce3d67769cdd4063a2abbc49032df9f90df22f57c1d7ee98eb225313f7ffec3 SafeBlockHash:0xc8e677bd7e1667e0f9644906e55c5a0a203f995aa39119f7bfebe0a9238e5fd7 FinalizedBlockHash:0xc8e677bd7e1667e0f9644906e55c5a0a203f995aa39119f7bfebe0a9238e5fd7}\" err=\"Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\nJun 28 08:15:18 m-optimism-02 op-node[1334149]: t=2024-06-28T08:15:18+0000 lvl=warn msg=\"Derivation process temporary error\" attempts=1 err=\"engine stage failed: temp: temporarily cannot insert new safe block: failed to create new block via forkchoice: Post \\\"http://127.0.0.1:8551\\\": context deadline exceeded\"\r\n```\r\n\r\nand the node is still running behind and unable to catch up... ", "2024-06-28T08:16:31Z", "2024-06-28T08:16:31Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C70w8", "I_kwDOH2Qg5s6Me-Je", "I will share it on Discord, as it seems I cannot tag others here. There should be three issues so far:\r\n\r\n1. Is the lag of 2000 blocks a persistent issue? According to the synchronization speed in the op-geth logs, it seems it will catch up soon.\r\n2. The process of \"Generating state snapshot\" after a restart seems to be very time-consuming.\r\n3. The issue of \"context deadline exceeded.\"\r\n\r\nPerhaps you should clarify the first issue: Will it always lag for some reason, making it impossible to sync to the latest block?", "2024-06-28T11:45:29Z", "2024-06-28T11:45:29Z", "opfocus", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C78_L", "I_kwDOH2Qg5s6Me-Je", "Now is unable to catch up... the recent chain head update age is `3h4m52s`\r\n\r\n```\r\nJun 28 12:01:07 m-optimism-02 geth[1333040]: INFO [06-28|12:01:07.858] Chain head was updated                   number=121,983,099 hash=2d6448..5d5f4b root=f66e82..c0682e elapsed=\"809.109\u00b5s\"  age=3h4m52s\r\n```\r\n\r\nand it is still generating snapshots:\r\n```\r\nJun 28 12:02:22 m-optimism-02 geth[1333040]: INFO [06-28|12:02:22.858] Generating state snapshot                root=f37bb2..ca4435 at=c74cb7..7c5f48 accounts=108,920,752 slots=290,736,814 storage=27.63GiB  dangling=0 elapsed=3h58m57.859s eta=1h7m59.09s\r\n```\r\n\r\nEvery time I see this lagging, the node is generating the state snapshot...  If I restart the node it seems to start generating the state snapshot from the begging....\r\n\r\nI have increased all the resources allocation (cpu, memory and storage settings) but it does not help... :/ ", "2024-06-28T12:02:38Z", "2024-06-28T12:04:47Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C8RZ7", "I_kwDOH2Qg5s6Me-Je", "can you use `top` to see your `wa` status?want to ensure this isn't disk io issue\r\n![image](https://github.com/ethereum-optimism/op-geth/assets/27230181/1978c2fd-5772-4150-9f49-d1752ac2fb9f)\r\n", "2024-06-28T12:52:10Z", "2024-06-28T12:52:10Z", "imtipi", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C8TjE", "I_kwDOH2Qg5s6Me-Je", "![image](https://github.com/ethereum-optimism/op-geth/assets/33039405/2ef063e7-56f7-4688-962a-b851a3ce72a5)\r\n\r\n![image](https://github.com/ethereum-optimism/op-geth/assets/33039405/850f2e95-3674-4f57-a6aa-09e2ccc0a675)\r\n\r\n", "2024-06-28T12:57:49Z", "2024-06-28T12:57:49Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C8XCh", "I_kwDOH2Qg5s6Me-Je", "can you also run `iostat -xm`?want to check the actual iops", "2024-06-28T13:04:59Z", "2024-06-28T13:04:59Z", "imtipi", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C8YSl", "I_kwDOH2Qg5s6Me-Je", "```\r\nandre@m-optimism-02:~$ iostat -xm nvme1n1\r\nLinux 6.2.0-1012-aws (m-optimism-02) \t06/28/24 \t_x86_64_\t(16 CPU)\r\n\r\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\r\n          11.04    0.00    3.03    8.65    0.00   77.27\r\n\r\nDevice            r/s     rMB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wMB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dMB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util\r\nnvme1n1       1953.46    181.42     0.42   0.02    0.76    95.10  278.68     19.48     0.00   0.00    0.50    71.59    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.63  96.77\r\n```", "2024-06-28T13:07:11Z", "2024-06-28T13:07:11Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C8fbc", "I_kwDOH2Qg5s6Me-Je", "seems io is ok,looks like Generating state snapshot is required in snap sync,I will recommend try with Archive sync,so that you don't need to \r\n\r\n> rebuild any intermediate state \"on-the-fly\"\r\n\r\nbecause you already \r\n\r\n> executes every block in the chain\r\n\r\nhttps://docs.optimism.io/builders/node-operators/management/snap-sync\r\n\r\nwe can also wait developer to check this thread,because I didn't test it before.", "2024-06-28T13:23:23Z", "2024-06-28T13:24:59Z", "imtipi", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C8gGP", "I_kwDOH2Qg5s6Me-Je", "Can I change the node syncmode from `snap` to `full` now? \ud83e\udd14 \r\n\r\nYes, we need a developer to look into this issue... this shouldn't be the normal behavior...", "2024-06-28T13:25:05Z", "2024-06-28T15:24:02Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C9o70", "I_kwDOH2Qg5s6Me-Je", "> Can I change the node syncmode from `snap` to `full` now? \ud83e\udd14\r\n\r\nbetter delete the datadir to start again\r\n\r\n", "2024-06-28T15:38:32Z", "2024-06-28T15:38:32Z", "imtipi", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C9pYc", "I_kwDOH2Qg5s6Me-Je", "it is important to add that we don't see this problem with base mainnet... and both networks have the same configuration.\r\nI see base mainnet also generating state snapshots but no impact in the node height. \r\n\r\nI'm not sure if the generating state snapshot is the issue... but the optimism mainnet node is unable to catch up.. it is now 4h9m3s behind...  :/ \r\n\r\n```\r\nJun 28 15:40:28 m-optimism-02 geth[1333040]: INFO [06-28|15:40:28.459] Chain head was updated                   number=121,987,754 hash=4496bf..24c345 root=5766d9..2d1d1f elapsed=\"788.357\u00b5s\"  age=4h9m3s\r\n```", "2024-06-28T15:39:25Z", "2024-06-28T15:41:02Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6C-RWN", "I_kwDOH2Qg5s6Me-Je", "Try resyncing again but delete\r\n\r\n```\r\n--gcmode=archive\r\n```\r\n\r\nI don't think snap sync works with archive mode\r\n\r\nWith archive mode, you must sync from a snapshot. Otherwise, it will take forever (months or even year).", "2024-06-28T17:22:39Z", "2024-06-28T17:22:39Z", "Chomtana", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6DdWpw", "I_kwDOH2Qg5s6Me-Je", "we are now observing the same issue with optimism sepolia node: \r\n\r\n```\r\nJul 03 09:12:27 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:12:27.995] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a790be..9c4263 accounts=3,107,081 slots=103,483,294 storage=7.58GiB   dangling=0 elapsed=40m10.097s eta=1m4.212s\r\nJul 03 09:12:35 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:12:35.998] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a79956..920d49 accounts=3,107,081 slots=103,487,411 storage=7.58GiB   dangling=0 elapsed=40m18.100s eta=1m4.426s\r\nJul 03 09:12:43 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:12:43.999] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7a271..8e966c accounts=3,107,081 slots=103,491,709 storage=7.58GiB   dangling=0 elapsed=40m26.101s eta=1m4.639s\r\nJul 03 09:12:52 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:12:52.000] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7ab68..bb3b96 accounts=3,107,081 slots=103,496,012 storage=7.58GiB   dangling=0 elapsed=40m34.102s eta=1m4.852s\r\nJul 03 09:13:00 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:13:00.001] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7b4ee..4c9522 accounts=3,107,081 slots=103,500,509 storage=7.58GiB   dangling=0 elapsed=40m42.103s eta=1m5.065s\r\nJul 03 09:13:08 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:13:08.002] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7bd74..86044c accounts=3,107,081 slots=103,504,711 storage=7.58GiB   dangling=0 elapsed=40m50.104s eta=1m5.278s\r\nJul 03 09:13:16 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:13:16.003] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7c66f..7a4136 accounts=3,107,081 slots=103,508,873 storage=7.58GiB   dangling=0 elapsed=40m58.105s eta=1m5.491s\r\nJul 03 09:13:24 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:13:24.005] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7cf1e..fb3212 accounts=3,107,081 slots=103,513,032 storage=7.58GiB   dangling=0 elapsed=41m6.107s  eta=1m5.705s\r\nJul 03 09:13:32 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:13:32.007] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7d7ba..de0794 accounts=3,107,081 slots=103,517,143 storage=7.58GiB   dangling=0 elapsed=41m14.109s eta=1m5.918s\r\nJul 03 09:13:40 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:13:40.016] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7e07d..89d2fe accounts=3,107,081 slots=103,521,303 storage=7.58GiB   dangling=0 elapsed=41m22.118s eta=1m6.131s\r\nJul 03 09:13:48 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:13:48.016] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7e960..3fab59 accounts=3,107,081 slots=103,525,560 storage=7.58GiB   dangling=0 elapsed=41m30.118s eta=1m6.344s\r\nJul 03 09:13:56 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:13:56.018] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7f22c..2096c9 accounts=3,107,081 slots=103,529,838 storage=7.58GiB   dangling=0 elapsed=41m38.120s eta=1m6.558s\r\nJul 03 09:14:04 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:14:04.021] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a7fa73..b26d99 accounts=3,107,081 slots=103,533,718 storage=7.58GiB   dangling=0 elapsed=41m46.123s eta=1m6.771s\r\nJul 03 09:14:12 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:14:12.023] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a80308..ceebdd accounts=3,107,081 slots=103,537,755 storage=7.58GiB   dangling=0 elapsed=41m54.125s eta=1m6.984s\r\nJul 03 09:14:20 t-optimism-sepolia-02 geth[775]: INFO [07-03|09:14:20.023] Generating state snapshot                root=d90958..08bb3c in=f95b39..df95f6 at=a80b8a..a83bcb accounts=3,107,081 slots=103,541,866 storage=7.58GiB   dangling=0 elapsed=42m2.125s  eta=1m7.197s\r\n```", "2024-07-03T09:14:46Z", "2024-07-03T09:14:46Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6DfLWx", "I_kwDOH2Qg5s6Me-Je", "@imtipi @@opfocus , I do think developers should look into these issues.... \ud83d\ude4f\ud83c\udffd  ", "2024-07-03T12:42:36Z", "2024-07-03T12:42:36Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6DfWGi", "I_kwDOH2Qg5s6Me-Je", "> @imtipi @@opfocus , I do think developers should look into these issues.... \ud83d\ude4f\ud83c\udffd\r\n\r\nsoyboy(from dev team) also think snap sync isn't for archive node\r\n\r\n> I came to the same conclusion on snapsync. Probably not for archive nodes. Seems like it just does archiving once it catches up and then moving forward. Not really useful imo\r\n", "2024-07-03T13:04:31Z", "2024-07-03T13:04:31Z", "imtipi", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6DpBkM", "I_kwDOH2Qg5s6Me-Je", "I will keep monitoring our optimism and base archive nodes using snap sync, and let you know if I see more issues.\r\n\r\nBTW, Base nodes have been running without issues.", "2024-07-04T09:49:41Z", "2024-07-04T09:49:41Z", "andreclaro", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6Ek7x_", "I_kwDOH2Qg5s6Me-Je", "i hava the same question,", "2024-07-12T01:53:23Z", "2024-07-12T01:53:23Z", "leihot", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6SA_65", "I_kwDOH2Qg5s6Me-Je", "We're having the same issue in our Base mainnet node:\r\n```\r\nOct 31 11:55:19 geth[1042058]: INFO [10-31|11:55:19.714] Generating state snapshot                root=2a9837..c27089 in=52d154..be8cda at=2800a1..f945bb accounts=53,044,801 slots=209,596,375 storage=18.02GiB   dangling=0 elapsed=20h49m47.246s eta=43h33m27.83s\r\n```\r\n\r\nGenerating snapshots takes a looong time and the node has been falling behind since it started.\r\nWe're using the following flags on `op-geth` (v1.101408.0):\r\n```\r\n/usr/local/bin/geth \\\r\n    --op-network=base-mainnet \\\r\n    --datadir=/var/lib/base/data \\\r\n    --syncmode=snap \\\r\n    --gcmode=archive \\\r\n    --networkid=\"8453\" \\\r\n    --http \\\r\n    --http.addr=0.0.0.0 \\\r\n    --http.port=8545 \\\r\n    --http.vhosts=\"*\" \\\r\n    --http.corsdomain=\"*\" \\\r\n    --http.api=web3,debug,eth,net,engine,geth \\\r\n    --ws \\\r\n    --ws.port=8546 \\\r\n    --ws.addr=0.0.0.0 \\\r\n    --ws.origins=\"*\" \\\r\n    --ws.api=debug,eth,net,engine \\\r\n    --authrpc.addr=127.0.0.1 \\\r\n    --authrpc.port=8551 \\\r\n    --authrpc.jwtsecret=/var/lib/base/jwt.txt \\\r\n    --authrpc.vhosts=\"*\" \\\r\n    --metrics \\\r\n    --metrics.addr=0.0.0.0 \\\r\n    --metrics.port=6060 \\\r\n    --port=30303 \\\r\n    --discovery.port=30303 \\\r\n    --maxpeers=500 \\\r\n    --rollup.disabletxpoolgossip=true \\\r\n    --rollup.sequencerhttp=https://mainnet-sequencer.base.org \\\r\n    --rollup.superchain-upgrades \\\r\n    --rollup.halt=major \\\r\n    --verbosity=3\r\n```\r\n\r\nAnd these on `op-node` (v1.9.1):\r\n```\r\n/usr/local/bin/op-node \\\r\n    --syncmode=execution-layer \\\r\n    --l2.enginekind=geth \\\r\n    --l1.trustrpc \\\r\n    --l1.rpckind=basic \\\r\n    --l1=http://<ip_address>:8545 \\\r\n    --l1.beacon=http://<ip_address>:5052 \\\r\n    --l2=http://127.0.0.1:8551 \\\r\n    --rpc.addr=127.0.0.1 \\\r\n    --rpc.port=9545 \\\r\n    --l2.jwt-secret=/var/lib/base/jwt.txt \\\r\n    --network=base-mainnet \\\r\n    --p2p.peerstore.path=/var/lib/base/data/opnode_peerstore_db \\\r\n    --p2p.priv.path=/var/lib/base/data/opnode_p2p_priv.txt \\\r\n    --p2p.discovery.path=/var/lib/base/data/opnode_discovery_db \\\r\n    --rollup.load-protocol-versions=true \\\r\n    --rollup.halt=major\r\n```\r\n", "2024-10-31T12:31:42Z", "2024-10-31T12:31:42Z", "ncavedale-xlabs", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6kqwKc", "I_kwDOH2Qg5s6Me-Je", "any update?", "2025-03-28T22:00:42Z", "2025-03-28T22:00:42Z", "askucher", "2025-08-31 03:24:20"]
["IC_kwDOH2Qg5s6llDjP", "I_kwDOH2Qg5s6xIBj-", "duplicate of https://github.com/ethereum-optimism/optimism/issues/15241", "2025-04-04T08:39:33Z", "2025-04-04T08:39:33Z", "emhane", "2025-08-31 03:24:21"]
["IC_kwDOH2Qg5s6k5bMF", "I_kwDOH2Qg5s6uTo50", "Via https://github.com/ethereum-optimism/optimism/issues/15033, where we now do not load the config over RPC but require it to be in the chain config file itself. ", "2025-03-31T15:00:36Z", "2025-03-31T15:00:36Z", "geoknee", "2025-08-31 03:24:21"]
["IC_kwDOLB-lzc6kmrF0", "I_kwDOLB-lzc6vrcKr", "It has been added to the FMA. Still worth adding to the specs.", "2025-03-28T14:53:26Z", "2025-03-28T14:53:26Z", "sebastianst", "2025-08-31 03:24:22"]
["IC_kwDOLB-lzc6XARei", "I_kwDOLB-lzc6i0Nap", "Allowing the sequencer to control the timestamp will mean the timestamp will need to be batch submitted. This could be optimized by submitting a timestamp diff rather than the full timestamp. This would require a new batch version and we would want to keep the semantics of the current batch version the same, where the timestamp auto increments by a hardcoded amount specific to the network", "2024-12-11T01:43:27Z", "2024-12-11T01:43:27Z", "tynes", "2025-08-31 03:24:22"]
["IC_kwDOLB-lzc6XAgpc", "I_kwDOLB-lzc6i0Nap", "Note that using an epoch level gas limit would mean restricting the total gas for the epoch to something that can be proven in a single cannon/op-program run (ie the ephoch gas limit could not be larger than the maximum block gas limit that could be safely supported).  This is because the sequencer may choose to create a single block that uses the entire epoch gas limit so in effect the epoch limit is the gas limit.\n\nNot a blocker, just something to factor in to designs to ensure it remains fault provable.", "2024-12-11T02:36:23Z", "2024-12-11T02:36:23Z", "ajsutton", "2025-08-31 03:24:22"]
["IC_kwDOLB-lzc6XOOSJ", "I_kwDOLB-lzc6i0Nap", "Another consideration here is \"lazy block production\". It should be possible for a sequencer to build blocks more slowly if they are not seeing a lot of throughput. Decoupling the timestamps helps enable this. To be able to build blocks slower than the base chain, we would need https://github.com/ethereum-optimism/specs/issues/416. The tradeoff with allowing the sequencer to build blocks arbitrarily slowly is that it could delay deposits. There would need to be a rule where blocks can be produced lazily when there are no deposits and then progression is forced when a deposit is observed.\n\nAfter thinking about this more, I am much more in favor of a timestamp based gas limit than an epoch based gas limit. We could keep the exact same timestamp rules with sequencer drift and have a cap of 4 blocks with the same timestamp and overload the definition of gas limit to mean \"gas limit of blocks with same timestamp\". This guarantees progression of the timestamp and opens up innovation at the block builder layer, which we are already seeing with rbuilder/rollup boost. Its a complex optimization problem to build blocks that are cheap to batch submit and also earn more fees.\n\nWe would also need a way to determine the total ordering of blocks with the same timestamp, to which @protolambda has previously suggested we could overload the block header's nonce field for this.\n\nThe `op-node` could at first implement a naive block building process that updates the timestamp in a very simple way and we can then build blocks faster in the future without needing to introduce any protocol changes.", "2024-12-11T20:37:00Z", "2024-12-11T20:37:00Z", "tynes", "2025-08-31 03:24:22"]
["IC_kwDOKIsnqM6VvRit", "I_kwDOKIsnqM6f_JRw", "@geoknee Do you know why this happens? I'm wondering who we can assign this issue to so that it can be resolved before the Holocene mainnet playbooks", "2024-12-02T17:13:12Z", "2024-12-02T17:13:12Z", "mds1", "2025-08-31 03:24:31"]
["IC_kwDOKIsnqM6V3_Cc", "I_kwDOKIsnqM6f_JRw", "I had the same error message as above (in a clone repo using the same scripts) and also running on a M3 macbook. \nI rebuilding eip712signer with the flags and copied the binaries and worked as expected. Thanks for sharing the fix @geoknee ", "2024-12-03T12:52:27Z", "2024-12-03T12:52:27Z", "PierreOssun", "2025-08-31 03:24:31"]
["IC_kwDOKIsnqM6V-Gub", "I_kwDOKIsnqM6f_JRw", "@geoknee @PierreOssun do you know what go and macOS versions you compiled with? I was on `go1.23.0 darwin/arm64` with `Sequoia 15.1.1`. I upgraded to the latest go version, `go1.23.4 darwin/arm64`, and will re-test with the default build steps tomorrow to see if that affects anything\n\ncc @mdehoog as well since it looks like you are the maintainer of https://github.com/base-org/eip712sign", "2024-12-04T03:20:53Z", "2024-12-04T03:20:53Z", "mds1", "2025-08-31 03:24:31"]
["IC_kwDOKIsnqM6WA0k4", "I_kwDOKIsnqM6f_JRw", "I'm on `Sonoma 14.7.1` and `go1.21.13 darwin/arm64`.\n\nSeems like a compatibility issue with ARM architecture, but I haven't dug a lot more deeply. ", "2024-12-04T10:07:05Z", "2024-12-04T10:07:05Z", "geoknee", "2025-08-31 03:24:31"]
["IC_kwDOKIsnqM6WGVVN", "I_kwDOKIsnqM6f_JRw", "I'm unable to reproduce this (on M2 with OS `15.1 (24B83)` and `go version go1.22.6 darwin/arm64`).\n\nBut did notice the utility was lagging behind go-ethereum versions, updated and released a version here: https://github.com/base-org/eip712sign/releases/tag/v0.0.8\n\n@geoknee could you please check if that resolves anything for you? Otherwise we can always switch the `justfile` to build locally with those parameters.", "2024-12-04T18:35:20Z", "2024-12-04T18:35:20Z", "mdehoog", "2025-08-31 03:24:31"]
["IC_kwDOKIsnqM6XH905", "I_kwDOKIsnqM6f_JRw", "> I'm unable to reproduce this (on M2 with OS `15.1 (24B83)` and `go version go1.22.6 darwin/arm64`).\n> \n> But did notice the utility was lagging behind go-ethereum versions, updated and released a version here: https://github.com/base-org/eip712sign/releases/tag/v0.0.8\n> \n> [@geoknee](https://github.com/geoknee) could you please check if that resolves anything for you? Otherwise we can always switch the `justfile` to build locally with those parameters.\n\n@mdehoog  yes that seems to do the trick for me #399 . Thanks for the help!", "2024-12-11T10:23:28Z", "2024-12-11T10:23:28Z", "geoknee", "2025-08-31 03:24:31"]
["IC_kwDOKIsnqM6iuJpP", "I_kwDOKIsnqM6f_JRw", "I've reopened this as some signers have recently began experience this issue again \u2014 @mdehoog is it possible we just need another geth version bump in eip712sign?", "2025-03-17T15:44:17Z", "2025-03-17T15:44:17Z", "mds1", "2025-08-31 03:24:31"]
["IC_kwDOKIsnqM6kls-k", "I_kwDOKIsnqM6f_JRw", "This has been fixed with the latest release of eip712sign: https://github.com/ethereum-optimism/superchain-ops/pull/794", "2025-03-28T13:19:58Z", "2025-03-28T13:19:58Z", "mds1", "2025-08-31 03:24:31"]
["IC_kwDOLB-lzc6j0fEB", "I_kwDOLB-lzc6vcuyi", "Whoops wrong repo, moving to monorepo: https://github.com/ethereum-optimism/optimism/issues/15007", "2025-03-24T15:04:54Z", "2025-03-24T15:07:40Z", "pcw109550", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6djjAz", "I_kwDOLB-lzc6pDBAl", "WIP https://github.com/ethereum-optimism/specs/pull/533, would be good if somebody could take this PR over", "2025-02-07T16:04:41Z", "2025-02-07T16:04:41Z", "tynes", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6j0pLM", "I_kwDOLB-lzc6pDBAl", "Fixed by https://github.com/ethereum-optimism/specs/pull/533", "2025-03-24T15:14:07Z", "2025-03-24T15:14:07Z", "protolambda", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6j0edX", "I_kwDOLB-lzc6o56Tz", "Closed by https://github.com/ethereum-optimism/specs/pull/613", "2025-03-24T15:04:08Z", "2025-03-24T15:04:08Z", "refcell", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6dch7K", "I_kwDOLB-lzc6orfHJ", "discussed with @ajsutton in slack and don't _think_ that this is an issue, but would catch it quickly in kurtosis/devnet if it was\n\n![Image](https://github.com/user-attachments/assets/f6e37697-6e6c-4f23-885f-cd6136739029)", "2025-02-07T00:21:15Z", "2025-02-07T00:21:15Z", "elihaims", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6dgR78", "I_kwDOLB-lzc6orfHJ", "_From Slack_ Definitely a good callout. I see the max usage is 64k in @elihaims's statistics (can you post it here too?). The worst-case increased gas usage with EIP-7623 is by a factor of 10/4 (that's how non-zero calldata tokens are priced before/after, multiplied by 4). So even if those txs were calldata-only-txs, they would increase usage to 160k, still well below the limit. But in actuality, they aren't calldata-only txs, so their cost probably just stays the same. I think much more gas goes into storage writes etc.\n\nBut an end2end test/Kurtosis test will deliver the definitive proof.", "2025-02-07T10:06:05Z", "2025-02-07T10:06:05Z", "sebastianst", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6eD_Oy", "I_kwDOLB-lzc6orfHJ", "![Image](https://github.com/user-attachments/assets/1711c5ba-f31c-4749-af82-e224e65ae225)", "2025-02-11T19:18:11Z", "2025-02-11T19:18:11Z", "elihaims", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6eENmh", "I_kwDOLB-lzc6orfHJ", "cc @meyer9 in case you are working on an E2E test for this", "2025-02-11T19:45:36Z", "2025-02-11T19:45:36Z", "refcell", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6eMnns", "I_kwDOLB-lzc6orfHJ", "Not currently - my e2e test for network upgrade txs might cover this. It checks to make sure they were successfully executed, although not for older upgrade txs.", "2025-02-12T15:44:44Z", "2025-02-12T15:44:44Z", "meyer9", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6j1DqL", "I_kwDOLB-lzc6orfHJ", "The L1 attributes tx has 1 million gas limit, the calldata cost change will not cause it to use that much gas", "2025-03-24T15:48:50Z", "2025-03-24T15:48:50Z", "tynes", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6cLoYz", "I_kwDOLB-lzc6mfnj8", "For the system contracts being introduced, can we make sure we specify how they end up in the L2 state? We previously did it via special deposit transactions and also have used state setting directly in the EL. We prefer to not do the irregular state transition with state setting if possible. cc @refcell @emhane @meyer9 ", "2025-01-28T23:57:20Z", "2025-01-28T23:57:20Z", "tynes", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6dOpnG", "I_kwDOLB-lzc6mfnj8", "> For the system contracts being introduced, can we make sure we specify how they end up in the L2 state? We previously did it via special deposit transactions and also have used state setting directly in the EL. We prefer to not do the irregular state transition with state setting if possible. cc [@refcell](https://github.com/refcell) [@emhane](https://github.com/emhane) [@meyer9](https://github.com/meyer9)\n\nI believe I did this [here](https://specs.optimism.io/protocol/isthmus/derivation.html#eip-2935-contract-deployment). Since the only one is EIP-2935. Both EIP-7002 and EIP-7251 predeploys [are noted](https://specs.optimism.io/protocol/isthmus/overview.html#execution-layer) to not be included in isthmus.", "2025-02-05T19:38:11Z", "2025-02-05T19:38:11Z", "refcell", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6f5Mg1", "I_kwDOLB-lzc6mfnj8", "can this be closed @refcell ?", "2025-02-25T16:25:21Z", "2025-02-25T16:25:21Z", "emhane", "2025-08-31 03:24:32"]
["IC_kwDOLB-lzc6j1EKV", "I_kwDOLB-lzc6mfnj8", "Complete!", "2025-03-24T15:49:30Z", "2025-03-24T15:49:30Z", "refcell", "2025-08-31 03:24:32"]
["IC_kwDODjvEJM6kbhyS", "I_kwDODjvEJM6wDV08", "In progress: https://github.com/ethereum-optimism/specs/pull/625", "2025-03-27T16:34:49Z", "2025-03-27T16:34:49Z", "smartcontracts", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6km2IH", "I_kwDODjvEJM6wDV08", "Done!", "2025-03-28T15:08:51Z", "2025-03-28T15:08:51Z", "smartcontracts", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kbhLn", "I_kwDODjvEJM6wDVx9", "https://github.com/ethereum-optimism/design-docs/pull/202 Done!", "2025-03-27T16:34:10Z", "2025-03-27T16:34:10Z", "smartcontracts", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kbheN", "I_kwDODjvEJM6wDVvI", "Done! https://github.com/ethereum-optimism/design-docs/pull/202", "2025-03-27T16:34:30Z", "2025-03-27T16:34:30Z", "smartcontracts", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kbf-8", "I_kwDODjvEJM6wDTsP", "dupe", "2025-03-27T16:32:31Z", "2025-03-27T16:32:31Z", "smartcontracts", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kYAxP", "I_kwDODjvEJM6v_jOf", "https://github.com/ethereum-optimism/optimism/blob/db2ee470c834358b048ffad7c84f5462218ea38a/op-service/eth/account_proof_test.go#L128", "2025-03-27T11:49:42Z", "2025-03-27T11:49:42Z", "geoknee", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6juQkO", "I_kwDODjvEJM6vWdtk", "It's not possible to create a proposal prior to the anchor state.  So there's no way the challenger can win a game where the proposal is before genesis as to create that game, you must have an invalid anchor state.\n\nOr do you mean the challenger is using the timestamp from in the proposed superchain root (not sure how it would get it since it only has the hash)?", "2025-03-24T03:51:05Z", "2025-03-24T03:51:05Z", "ajsutton", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6juQ9k", "I_kwDODjvEJM6vWdtk", "> It's not possible to create a proposal prior to the anchor state. So there's no way the challenger can win a game where the proposal is before genesis as to create that game, you must have an invalid anchor state.\n> \n> Or do you mean the challenger is using the timestamp from in the proposed superchain root (not sure how it would get it since it only has the hash)?\n\nYeah I just realized this. I ran into this issue because my OPCM setup configures the wrong timestamp anchor state. It shouldn't be possible in a properly configured chain.\n\nClosing.", "2025-03-24T03:52:51Z", "2025-03-24T03:52:51Z", "Inphi", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6jhv_u", "I_kwDODjvEJM6vJvpF", "We should be able to provide a few other assertions, like the latest expected fork for this chain and its activation time.", "2025-03-21T14:27:23Z", "2025-03-21T14:27:23Z", "sebastianst", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6js91b", "I_kwDODjvEJM6vJvpF", "See https://github.com/ethereum-optimism/optimism/pull/14997", "2025-03-23T21:23:17Z", "2025-03-23T21:23:17Z", "pauldowman", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6j5ITZ", "I_kwDODjvEJM6vJvpF", "Logged https://github.com/ethereum-optimism/optimism/issues/15017 to track further automation of this.  Otherwise I think we can close this once https://github.com/ethereum-optimism/optimism/pull/15003 is merged. At that point we can try using the tool to verify a few upgrades and see how helpful it is / what other checks we need to implement.", "2025-03-24T23:53:09Z", "2025-03-24T23:53:09Z", "ajsutton", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6jKltj", "I_kwDODjvEJM6up-QA", "Note critical for the release", "2025-03-19T17:20:16Z", "2025-03-19T17:20:16Z", "yoonchung8822", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kc0O0", "I_kwDODjvEJM6up-QA", "I've added these @yoonchung8822. See here: https://github.com/ethereum-optimism/optimism/blob/develop/op-acceptance-tests/tests/isthmus/pectra_features_test.go", "2025-03-27T18:15:08Z", "2025-03-27T18:15:08Z", "clabby", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6j0oHz", "I_kwDODjvEJM6up8K3", "Depends on https://github.com/ethereum-optimism/infra/issues/261", "2025-03-24T15:12:42Z", "2025-03-24T15:12:42Z", "teddyknox", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6j0rdv", "I_kwDODjvEJM6up8K3", "Draft PR: https://github.com/ethereum-optimism/optimism/pull/14972 (blocked on the above)", "2025-03-24T15:17:10Z", "2025-03-24T15:17:10Z", "teddyknox", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6j01pj", "I_kwDODjvEJM6up8K3", "Closing as duplicate of https://github.com/ethereum-optimism/pm/issues/46", "2025-03-24T15:30:49Z", "2025-03-24T15:30:49Z", "teddyknox", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kczX9", "I_kwDODjvEJM6up7DM", "Coverage: https://github.com/ethereum-optimism/optimism/blob/develop/op-acceptance-tests/tests/isthmus/pectra_features_test.go#L98-L170", "2025-03-27T18:13:46Z", "2025-03-27T18:13:46Z", "clabby", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6jlVEv", "I_kwDODjvEJM6uoxZV", "https://github.com/ethereum-optimism/optimism/pull/14989", "2025-03-21T21:21:26Z", "2025-03-21T21:21:26Z", "axelKingsley", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6i7S3P", "I_kwDODjvEJM6tl-OY", "Any more updates on this one @pauldowman since I know that Alpha is planned for next week so we should probably start planning execution on any remaining tasks for OPCM and Proofs work to enable that release?", "2025-03-18T14:25:41Z", "2025-03-18T14:25:41Z", "BlocksOnAChain", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6i7UcZ", "I_kwDODjvEJM6tl-OY", "Yes, this is planned for Thursday with @ControlCplusControlV  @ajsutton @mslipper and @smartcontracts ", "2025-03-18T14:27:43Z", "2025-03-18T14:27:43Z", "pauldowman", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6i7lch", "I_kwDODjvEJM6tl-OY", "Great stuff, I will keep an eye on this issue and relevant threads to get more information about it. Thank you Paul.", "2025-03-18T14:48:18Z", "2025-03-18T14:48:18Z", "BlocksOnAChain", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kA6xm", "I_kwDODjvEJM6tBr-7", "superseded by different encapsulated devnet-sdk github issues", "2025-03-25T15:41:42Z", "2025-03-25T15:41:42Z", "protolambda", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kYvL5", "I_kwDODjvEJM6tBpIz", "This is done, with the devnet-sdk `system2` typing\n", "2025-03-27T12:59:19Z", "2025-03-27T12:59:19Z", "protolambda", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6iDAsu", "I_kwDODjvEJM6tBlzR", "@protolambda would you mind adding a bit more detail to this? This issue seems similar to #14711.", "2025-03-12T18:04:37Z", "2025-03-12T18:04:37Z", "teddyknox", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kYyBG", "I_kwDODjvEJM6tBlzR", "Closing, duplicate of #15026 \n", "2025-03-27T13:03:41Z", "2025-03-27T13:03:41Z", "protolambda", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6keXxo", "I_kwDODjvEJM6sxgxe", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/14977", "2025-03-27T20:39:42Z", "2025-03-27T20:39:42Z", "bitwiseguy", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6gwuL2", "I_kwDODjvEJM6sfMw-", "@pcw109550 is working on this", "2025-03-04T11:07:25Z", "2025-03-04T11:07:25Z", "protolambda", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6j0mn2", "I_kwDODjvEJM6sfMw-", "This is being solved by the `system2` devnet SDK work, which is tracked by #14909 and the already merged interfaces #14714 ", "2025-03-24T15:11:08Z", "2025-03-24T15:11:56Z", "protolambda", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6gY98t", "I_kwDODjvEJM6r-0B6", "That's odd, I  can't reproduce the issue.\nAlso, the error message seems to suggest that the RPC endpoint might be down? (unless the port number is somehow incorrect of course).\nCould you check that the corresponding op-el-1-op-geth-op-node-op-kurtosis service is indeed running? (and if so what host port is mapped to the rpc port?)", "2025-02-28T15:15:24Z", "2025-02-28T15:15:24Z", "sigma", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6hw6iZ", "I_kwDODjvEJM6oM10Y", "@smartcontracts can we use Clabby's pr for this one and maybe extend the work: https://github.com/ethereum-optimism/optimism/pull/14349 or we need to start this from 0?", "2025-03-11T12:13:13Z", "2025-03-11T12:13:13Z", "BlocksOnAChain", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6a6A_H", "I_kwDODjvEJM6lXezZ", "@protolambda regarding your proposed future work for generic receipts, I'm not sure it makes sense for now given how I specified handling of future receipts in https://github.com/ethereum-optimism/specs/pull/518 - they are expected to decode exactly the same as type 0-4 txs, so the current `*types.Receipt` type should be fine for now.", "2025-01-17T17:57:28Z", "2025-01-17T17:57:28Z", "sebastianst", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6a6EqW", "I_kwDODjvEJM6lXezZ", "@sebastianst see code here: https://github.com/ethereum-optimism/op-geth/blob/6c1047a2d9db86d03f635bb24704b77675189227/core/types/receipt.go#L351\n\nGeth will not decode the receipt binary data (as passed via pre-image oracle) if not configured with a recognized type. The receipt bytes are prefixed with the type byte just like txs, and thus distinguished and not automatically compatible.\n\nAnd currently type 4 is not supported (until we merge in later geth work with 7702 tx type). And this problem will repeat for future tx types if we don't change the receipt decoding code or implement separate custom receipt decoding here in the monorepo.\n", "2025-01-17T18:05:47Z", "2025-01-17T18:05:47Z", "protolambda", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6a6JD4", "I_kwDODjvEJM6lXezZ", "> [@sebastianst](https://github.com/sebastianst) see code here: https://github.com/ethereum-optimism/op-geth/blob/6c1047a2d9db86d03f635bb24704b77675189227/core/types/receipt.go#L351\n> \n> Geth will not decode the receipt binary data (as passed via pre-image oracle) if not configured with a recognized type. The receipt bytes are prefixed with the type byte just like txs, and thus distinguished and not automatically compatible.\n> \n> And currently type 4 is not supported (until we merge in later geth work with 7702 tx type). And this problem will repeat for future tx types if we don't change the receipt decoding code or implement separate custom receipt decoding here in the monorepo.\n\nAh makes sense, thanks! I was falsely assuming that the decoding code wouldn't check the types, which of course was a strange assumption to make \ud83d\ude05 ", "2025-01-17T18:15:19Z", "2025-01-17T18:15:19Z", "sebastianst", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6bSLRk", "I_kwDODjvEJM6lXezZ", "We hit a roadblock when trying to have `op-service` handle transactions generically https://github.com/ethereum-optimism/optimism/pull/13816#discussion_r1923476414.  We likely will need to just adopt a strategy of  explicitly supporting EIP-7702 transactions rather than trying to be forward compatible with any future tx types. ", "2025-01-21T16:41:52Z", "2025-01-21T16:41:52Z", "geoknee", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6bTy_O", "I_kwDODjvEJM6lXezZ", "> We hit a roadblock when trying to have `op-service` handle transactions generically [#13816 (comment)](https://github.com/ethereum-optimism/optimism/pull/13816#discussion_r1923476414). We likely will need to just adopt a strategy of explicitly supporting EIP-7702 transactions rather than trying to be forward compatible with any future tx types.\n\nJust wondering if any decisions have been made on the new scope\n\nI am generally in favor of just rebasing `op-geth` and then supporting reading of calldata from 7702 txs if that unblocks this. It may not be possible to decouple at this point", "2025-01-21T20:19:27Z", "2025-01-21T20:19:27Z", "tynes", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6bT5Dr", "I_kwDODjvEJM6lXezZ", "> > We hit a roadblock when trying to have `op-service` handle transactions generically [#13816 (comment)](https://github.com/ethereum-optimism/optimism/pull/13816#discussion_r1923476414). We likely will need to just adopt a strategy of explicitly supporting EIP-7702 transactions rather than trying to be forward compatible with any future tx types.\n> \n> Just wondering if any decisions have been made on the new scope\n> \n> I am generally in favor of just rebasing `op-geth` and then supporting reading of calldata from 7702 txs if that unblocks this. It may not be possible to decouple at this point\n\n@tynes yes we've decided to abandon the generic future-proofing work, and instead just pull 7702 txs into op-geth and then add the support to the monorepo. I'm working on this right now.\n\nQuoting summary from Discord\n> No matter how we spin it, we loose the ability when using JSON RPC (like in op-node or op-program host) to trustlessly verify the block hash integrity. We may be able to make it work with even more changes to how we pull data from an RPC, e.g. using some debug RPCs to get the binary encoding, but it seems unwieldy and the more we tried to make it work, the more issues came up, creating more complexity. And in the end, we don't even know whether this rather special kind of forwards-compatibility would even be enough for the next L1 upgrade. It would only proof against new EIP-2718 txs, that use the same receipt encoding as the existing non-legacy txs, which is quite the assumption to make.\n", "2025-01-21T20:33:50Z", "2025-01-21T20:33:50Z", "sebastianst", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6bbb7l", "I_kwDODjvEJM6lXezZ", "~~@mslipper should we be tracking op-deployer updates in here so net new chains support Pectra at L1 contract deployment?~~\n\nnvm I thought about it and realized the absolute prestate is the only thing that will need to be updated ", "2025-01-22T16:10:20Z", "2025-01-24T23:13:34Z", "sbvegan", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6cmiMM", "I_kwDODjvEJM6lXezZ", "Release candidates are ready:\n- `op-node/v1.10.4-rc.1`\n- `op-geth/v1.101500.0-rc.1`\n- `op-program/v1.5.0-rc.1` - absolute prestate hashes:\n  - default: `0x03dfa3b3ac66e8fae9f338824237ebacff616df928cf7dada0e14be2531bc1f4`\n  - mt64: `0x03f83792f653160f3274b0888e998077a27e1f74cb35bcb20d86021e769340aa`\n  - interop: `0x03b7658b889796c1e372f57439e48eb46a5b008f6e6a4b7e5c8c2d3bddffa797`", "2025-01-31T13:30:47Z", "2025-01-31T13:30:47Z", "sebastianst", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6cqXr_", "I_kwDODjvEJM6lXezZ", "Also \n* `op-batcher/v1.11.0-rc.1`", "2025-01-31T20:33:17Z", "2025-01-31T20:33:17Z", "geoknee", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6eWcmj", "I_kwDODjvEJM6lXezZ", "We have finalized the releases:\n* op-node: [v1.11.0](https://github.com/ethereum-optimism/optimism/releases/tag/op-node%2Fv1.11.0)\n* op-geth: [v1.101500.0](https://github.com/ethereum-optimism/op-geth/releases/tag/v1.101500.0)\n    * if updating a full node, intermediate step via [v1.101411.8](https://github.com/ethereum-optimism/op-geth/releases/tag/v1.101411.8) needed (see release notes)\n* op-batcher: [v1.11.1](https://github.com/ethereum-optimism/optimism/releases/tag/op-batcher%2Fv1.11.1)\n* op-challenger: [v1.3.1](https://github.com/ethereum-optimism/optimism/releases/tag/op-challenger%2Fv1.3.1)\n* op-program: `v1.5.0-rc.2`\n    * _default_: `0x035ac388b5cb22acf52a2063cfde108d09b1888655d21f02f595f9c3ea6cbdcd`\n     * _mt64_: `0x03a7d967025dc434a9ca65154acdb88a7b658147b9b049f0b2f5ecfb9179b0fe`\n    * _interop_: `0x0379d61de1833af6766f07b4ed931d85b3f6282508bbcbf9f4637398d97b61c1`", "2025-02-13T13:59:40Z", "2025-02-13T13:59:40Z", "sebastianst", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6kdZ-h", "I_kwDODjvEJM6UwDr_", "This is no longer relevant given that we moved to an alternative approach using the access lists. The only todo here is to follow up with adding a notice to the FMA document that it is replaced", "2025-03-27T19:22:56Z", "2025-03-27T19:22:56Z", "tynes", "2025-08-31 03:24:51"]
["IC_kwDODjvEJM6lrfx4", "I_kwDODjvEJM6xODYg", "false alarm. The tests were already fixed by https://github.com/ethereum-optimism/optimism/pull/14831", "2025-04-04T20:09:07Z", "2025-04-04T20:09:07Z", "Inphi", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lfhbx", "I_kwDODjvEJM6xCFe-", "@BlocksOnAChain not valid. The documentation referenced is not the real spec. We should close this.", "2025-04-03T17:39:23Z", "2025-04-03T17:39:23Z", "Inphi", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lflNe", "I_kwDODjvEJM6xCFe-", "closing as not valid. ", "2025-04-03T17:46:46Z", "2025-04-03T17:46:46Z", "BlocksOnAChain", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lpaek", "I_kwDODjvEJM6xBRSJ", "full list here: https://github.com/ethereum-optimism/optimism/issues?q=state%3Aopen%20label%3A%22op-program%20audit%20findings%22", "2025-04-04T15:30:12Z", "2025-04-04T15:30:12Z", "BlocksOnAChain", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lf1jL", "I_kwDODjvEJM6xA6D7", "The number of chains cannot exceed 127 as that implies an invalid prestate. So this case cannot happen.\nBut it's worthwhile to sanity check this is indeed the case.\n\nNote that the number of chains shouldn't exceed 126, not just 127.", "2025-04-03T18:18:00Z", "2025-04-03T18:18:00Z", "Inphi", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lfp19", "I_kwDODjvEJM6xA3NJ", "Won't fix. Because:\n- A sorted `SuperRoot.Chains` is a core invariant of the interop program as it's in the prestate. \n- sanity checking this invariant introduces a smidgen of complexity and compute that it's not worthwhile.", "2025-04-03T17:55:23Z", "2025-04-03T17:55:23Z", "Inphi", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lfr_2", "I_kwDODjvEJM6xAwR6", "This is a useful informational to consider. But:\nIn the above example, a super root containing a block at timestamp 99 would be an invalid super root. And thus cannot be used as an agreed prestate. This means we know that the agreed prestate must have the Randomchain block at t >= 100. And we can be sure that the checkChainCanExecute function won't return an error in this case. So no action is needed here.", "2025-04-03T17:59:22Z", "2025-04-03T17:59:22Z", "Inphi", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lftNx", "I_kwDODjvEJM6xAuCs", "The op-program maintains an in-memory cache of block data fetched from the oracle. I don't think it's necessary for `HazardSet.entries` maintain its own cache.\nIn the op-supervisor, the log db is fast enough that this isn't a issue. Closing.", "2025-04-03T18:01:42Z", "2025-04-03T18:01:42Z", "Inphi", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lpzEG", "I_kwDODjvEJM6w_CJR", "Fixed with https://github.com/ethereum-optimism/optimism/pull/15239", "2025-04-04T16:10:15Z", "2025-04-04T16:10:15Z", "teddyknox", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6leTsI", "I_kwDODjvEJM6w9Gu4", "This data would need to be both posted to DA and via the p2p network so that full nodes can fully recreate the blocks. Right now there is a requirement that the extradata is the exact size required to encode consensus critical information, if we allowed it to be up to a certain length past that then I believe the batch format would need to be updated to include it as well as the p2p types. Does this change your thinking on the difficulties of using a noop transaction for this? ", "2025-04-03T15:28:05Z", "2025-04-03T15:28:05Z", "tynes", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lHzHW", "I_kwDODjvEJM6wfxS8", "Closing this issue since it is the only sub-issue of its parent and is redundant with that parent. Please refer to https://github.com/ethereum-optimism/optimism/issues/15128 for this issue going forward.", "2025-04-01T18:04:13Z", "2025-04-01T18:04:13Z", "teddyknox", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6kpiyw", "I_kwDODjvEJM6wP0z1", "Paraphrasing with my understanding: \n\n\n1. I create an L2 native token on OP Mainnet at `0x1234`\n2. I bridge it to an L1 token, which needs to be an `OptimismMintableERC20`, meaning it has a `remoteToken()` getter.\n3. I then try to bridge that L1 token to a token on Base at 0xabcd. In doing so, the Base bridge will call `_isCorrectTokenPair(0xabcd)`, because `0xabcd != 0x1234` it will return false and the deposit will fail.\n4. So there is an edge case where if the L2 token had the same address on both Base and OP Mainnet, the bridge would work, but this is often untrue.\n\nThis is an artifact of the bridge having been designed prior to the superchain vision being well formed. Tokens created by the OptimismERC20TokenFactory now have the same address on each chain, but that wasn't always the case, and not all tokens are created that way. \n\nTBH I don't think we're likely to prioritize fixing this directly, but once we launch interop, the recommended way to do this would be to move coins directly between L2s rather than via L1. And if the token is already bridged to L1, you'd have to move it back to its home L2, before transferring it to another one.", "2025-03-28T20:14:19Z", "2025-03-28T20:14:44Z", "maurelian", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6k482z", "I_kwDODjvEJM6wP0z1", ">the recommended way to do this would be to move coins directly between L2s rather than via L1. And if the token is already bridged to L1, you'd have to move it back to its home L2, before transferring it to another one.\n\nOne small clarification: this is the recommendation / current design for the initial release of interop since deposited tokens wont benefit from interop out of the gate (due to the shared lockbox problem). ", "2025-03-31T14:18:32Z", "2025-03-31T14:18:32Z", "zainbacchus", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6k-FIi", "I_kwDODjvEJM6vp62r", "There's a todo referencing this somewhere. Reopening until I hunt it down.", "2025-04-01T00:42:51Z", "2025-04-01T00:42:51Z", "ajsutton", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6k574m", "I_kwDODjvEJM6vdonU", "With @geoknee's https://github.com/ethereum-optimism/optimism/pull/15102 we have assurance no chain fork appears, closing.", "2025-03-31T15:48:21Z", "2025-03-31T15:48:21Z", "teddyknox", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6k5tKd", "I_kwDODjvEJM6up9wx", "Closing, as the NAT tests already take 1-2h to complete and executing individual tests multiple times in the same test run would be impractical. Realistically, we should look to whether any tests fail across different post-merge CI runs as a way of detecting flakiness.", "2025-03-31T15:25:25Z", "2025-03-31T15:25:25Z", "teddyknox", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6hnfiF", "I_kwDODjvEJM6tCQPu", "Hello,\n\nI\u2019ve reviewed the issue regarding the missing operator fee parameters in the setL1BlockValuesInterop function. Here are some suggestions that might help address this problem:\n\n\t1.\tUpdate Function Parameters: Ensure that the setL1BlockValuesInterop function includes all necessary parameters related to the operator fee. This may involve adding new parameters or modifying existing ones to accommodate the operator fee.\n\t2.\tModify Function Implementation: After updating the function parameters, adjust the function\u2019s implementation to handle the operator fee correctly. This includes calculating, applying, and validating the operator fee within the function.\n\t3.\tUpdate Calling Code: Review all instances where setL1BlockValuesInterop is called and ensure that the new operator fee parameters are provided. This may involve updating function calls throughout the codebase to pass the correct arguments.\n\t4.\tTest Thoroughly: Implement comprehensive tests to verify that the operator fee is correctly integrated into the setL1BlockValuesInterop function. This includes unit tests for the function itself and integration tests to ensure that the changes work seamlessly within the larger system.\n\t5.\tReview Documentation: Update any relevant documentation to reflect the changes made to the setL1BlockValuesInterop function. This ensures that other developers are aware of the new parameters and understand how to use the function correctly.\n\nBy following these steps, you should be able to integrate the operator fee parameters into the setL1BlockValuesInterop function effectively. If you have further questions or need additional assistance, feel free to ask.\n\nBest regards.", "2025-03-10T18:33:25Z", "2025-03-10T18:33:25Z", "bentrnr21", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6knGpP", "I_kwDODjvEJM6tCQPu", "The function `setL1BlockValuesInterop` has been removed, now there is no diff to the isthmus contracts. https://github.com/ethereum-optimism/optimism/blob/9f480717e640591768939ef4cd97bc010f4e2ff0/packages/contracts-bedrock/src/L2/L1Block.sol#L191", "2025-03-28T15:36:27Z", "2025-03-28T15:36:27Z", "tynes", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6d3CYK", "I_kwDODjvEJM6pcesb", "include @tyler-smith as stake-holder, this overlaps a lot with interop NAT work", "2025-02-10T15:45:16Z", "2025-02-10T15:45:16Z", "protolambda", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6fT1_h", "I_kwDODjvEJM6pcesb", "Linking @protolambda's withdrawals root head NAT test: https://github.com/ethereum-optimism/optimism/pull/14353", "2025-02-20T21:57:42Z", "2025-02-20T21:57:52Z", "teddyknox", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6k58ZR", "I_kwDODjvEJM6pcesb", "Marking as done and closed as all subissues have been completed.", "2025-03-31T15:49:11Z", "2025-03-31T15:49:11Z", "teddyknox", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6d74yH", "I_kwDODjvEJM6nFKQl", "https://github.com/ethereum-optimism/optimism/pull/14266 starts to flesh out more tests around different interactions between chains/messages, but definitely still more cases to cover - should review the spec to ensure we have tests for each reason that a message might be invalid at minimum.", "2025-02-11T03:21:59Z", "2025-02-11T03:21:59Z", "ajsutton", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6leNgq", "I_kwDODjvEJM6nFKQl", "@Inphi @BlocksOnAChain  - already covered, we can close it", "2025-04-03T15:22:33Z", "2025-04-03T15:22:33Z", "BlocksOnAChain", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6d3K8d", "I_kwDODjvEJM6jeNn_", "Two questions:\n\n1. Would this go in a new section in the Optimism Docs (\"Chain Upgrades\")?\n\n<img width=\"261\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d2d03981-b7d4-4ad3-8931-8e853535017d\" />\n\n2. Is all the information that we need the four items in this issue?", "2025-02-10T15:57:03Z", "2025-02-10T15:57:03Z", "alcueca", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6lEK_L", "I_kwDODjvEJM6jeNn_", "This is addressed in https://devdocs.optimism.io/contracts-bedrock/contributing/opcm.html", "2025-04-01T13:17:59Z", "2025-04-01T13:17:59Z", "maurelian", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6XxHHA", "I_kwDODjvEJM6iyXSo", "Work in progress @ https://github.com/ethereum-optimism/optimism/pull/13406", "2024-12-16T17:31:26Z", "2024-12-16T17:31:26Z", "protolambda", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6kCSyY", "I_kwDODjvEJM6iyXSo", "I believe this is invalidated by the Standard Mode tasks, plus the fact that this task hasn't been meaningfully updated since early December. @protolambda is there still some valid information in here or shall we close this?", "2025-03-25T17:44:48Z", "2025-03-25T17:44:48Z", "axelKingsley", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6k2Wyv", "I_kwDODjvEJM6iyXSo", "Let's close it. I checked the work in the ticket and believe it's already done. The standard-mode work is tracked separately.\n", "2025-03-31T09:45:11Z", "2025-03-31T09:45:11Z", "protolambda", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6Vz2bU", "I_kwDODjvEJM6fXWC5", "This should be essentially the same as the 1.3.0 to 1.6.0 runbook but updated to deploy the 1.8.0 contracts (need to review which contracts it deploys need to come from which release...)", "2024-12-03T02:17:58Z", "2024-12-03T02:17:58Z", "ajsutton", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6Vz3lz", "I_kwDODjvEJM6fXWC5", "Most notable `OptimismPortal2` and `DisputeGameFactory` are not included in the 1.8.0 release so will still need to come from the 1.3.0 release, but `MIPS`, `FaultDisputeGame` and `PermissionedDisputeGame` are updated so need to come from 1.8.0.  We will therefore need something that can deploy contracts from multiple different releases.", "2024-12-03T02:21:26Z", "2024-12-03T02:21:26Z", "ajsutton", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6Vz7OT", "I_kwDODjvEJM6fXWC5", "We should be able to just point the proxy to the existing deployments, rather than redeploying to get around this versioning problem.", "2024-12-03T02:35:34Z", "2024-12-03T02:35:34Z", "ajsutton", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6V-lB7", "I_kwDODjvEJM6fXWC5", "But we do need to deploy a new `AnchorStateRegistry` from 1.6.0.  May be able to combine the existing 1.3->1.6 tool and the holocene deployer to get the job done.", "2024-12-04T05:10:27Z", "2024-12-04T05:10:27Z", "ajsutton", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6fVhc8", "I_kwDODjvEJM6fXWC5", "@sbvegan I'm removing this from the Proofs team board but don't want it to fall through the cracks, who should I assign it to?", "2025-02-21T02:47:31Z", "2025-02-21T02:47:31Z", "pauldowman", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6fVmOd", "I_kwDODjvEJM6fXWC5", "@pauldowman  you can assign it to me, I'll put it in my project board's backlog", "2025-02-21T02:55:24Z", "2025-02-21T02:55:24Z", "sbvegan", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6dBToe", "I_kwDODjvEJM6UmjIm", "@mbaxter is this still something we plan to work on or we can close this issue?", "2025-02-04T15:46:01Z", "2025-02-04T15:46:01Z", "BlocksOnAChain", "2025-08-31 03:25:11"]
["IC_kwDODjvEJM6dFryY", "I_kwDODjvEJM6UmjIm", "We should keep it open. I believe there's someone exploring working on it.", "2025-02-05T01:40:33Z", "2025-02-05T01:40:33Z", "pauldowman", "2025-08-31 03:25:11"]
["IC_kwDOKSJyfM6l_eM8", "I_kwDOKSJyfM6xgofT", "Links are working now! Fix was to update package.json with a link to the repo: https://github.com/ethereum-optimism/ecosystem/blob/bf197a0537ec0192b6dc3786f7bbac77a4fb1d13/packages/viem/package.json#L3-L7", "2025-04-07T23:50:30Z", "2025-04-07T23:50:30Z", "tremarkley", "2025-08-31 03:26:17"]
["IC_kwDOKSJyfM6j2zOQ", "I_kwDOKSJyfM6ve7gG", "https://github.com/ethereum-optimism/design-docs/pull/226", "2025-03-24T18:26:50Z", "2025-03-24T18:26:50Z", "hamdiallam", "2025-08-31 03:26:17"]
["IC_kwDOKSJyfM6l4WD6", "I_kwDOKSJyfM6veI7j", "Closing in favor of individual issues", "2025-04-07T11:31:43Z", "2025-04-07T11:31:43Z", "hamdiallam", "2025-08-31 03:26:17"]
["IC_kwDOKSJyfM6l4VJf", "I_kwDOKSJyfM6uyNmG", "Closing in favor of individual issues", "2025-04-07T11:30:23Z", "2025-04-07T11:30:23Z", "hamdiallam", "2025-08-31 03:26:17"]
["IC_kwDOKIwiaM6ZKiiV", "I_kwDOKIwiaM6kxTTW", "Hey @Maar-io, thanks for opening this issue. The EOA that is resolving games is account that is associated with the op-challenger services that guards the fault proof system. This is different from the privileged role, also called the challenger: https://docs.optimism.io/chain/security/privileged-roles#challenger. I'm going to keep this issue open and add some clarification around the docs on the difference between these two roles. The information below is for our tech writers:\n\n---\n\n## Description\n\nThere is confusion around the different accounts that are associated with the hot wallets that fund the op-challenger and the challenger role. We need to update the docs to clarify the difference.\n\n### Resources\n\n- Existing docs that need further clarification:\n    - https://docs.optimism.io/chain/security/privileged-roles#challenger\n    - https://docs.optimism.io/builders/chain-operators/tools/op-challenger\n    - https://docs.optimism.io/stack/fault-proofs/challenger\n- Internal support, you can ask in #noob-questions and tag in @proofs-support for open questions\n\n### Acceptance criteria\n\n- The different sections of the documentation that reference the challenger role and the op-challenger should have a description that separates the two concepts.\n\n### Action items\n\n1. Reach out to proofs support if you need information on understanding these two roles\n2. Open a PR to add this additional context to the docs\n3. Get a review from `@proofs-support`\n4. Get a peer review\n5. Merge\n", "2025-01-03T18:55:19Z", "2025-01-03T18:55:19Z", "sbvegan", "2025-08-31 03:26:19"]
["IC_kwDOKIwiaM6mnZme", "I_kwDOKIwiaM6kxTTW", "docs have been updated with this difference", "2025-04-10T22:54:41Z", "2025-04-10T22:54:41Z", "sbvegan", "2025-08-31 03:26:19"]
["IC_kwDOKIwiaM6mvQOO", "I_kwDOKIwiaM6kBFPj", "Didn't get a chance to start on this one", "2025-04-11T16:26:40Z", "2025-04-11T16:26:40Z", "bradleycamacho", "2025-08-31 03:26:19"]
["IC_kwDOH2Qg5s6kErgV", "I_kwDOH2Qg5s6pDFH8", "If we assume that we are able to filter at the ingress, then I think that we can close this ticket, and push complexity to the edge and reduce the diff in the EL software", "2025-03-25T22:29:22Z", "2025-03-25T22:29:22Z", "tynes", "2025-08-31 03:26:19"]
["IC_kwDOH2Qg5s6mVVx1", "I_kwDOH2Qg5s6pDFH8", "Closing this, can reopen if we decide that its part of the hot path", "2025-04-09T18:36:57Z", "2025-04-09T18:36:57Z", "tynes", "2025-08-31 03:26:19"]
["IC_kwDOLB-lzc6lqJvH", "I_kwDOLB-lzc6xMQNH", "I am in favor of this and have attempted to propose this before. I would like to see the `op-node` fetch these values dynamically rather than having them hardcoded into the config. This does not imply that the values are mutable. That would break things without making the `op-node` be able to handle them changing.", "2025-04-04T16:56:27Z", "2025-04-04T16:56:27Z", "tynes", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6lqROn", "I_kwDOLB-lzc6xMQNH", "For me one major advantage (other than better visibility) of `op-program` fetching the value from SystemConfig is that the absolute prestate would need to change less often. We review a ton of OP stack chains and we currently don\u2019t assess `op-program` because it\u2019s a lot of work, but we want to start doing so, and we would maintain a list of absolute prestates that we know and trust. If a new chain uses an absolute prestate that we\u2019ve already assessed we don\u2019t need to do any more work, and unknown prestates are by default untrusted. I know that this might sound like an L2BEAT specific issue, but it\u2019s actually about self verification in general. ", "2025-04-04T17:12:49Z", "2025-04-07T13:23:23Z", "lucadonnoh", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6mAQ-M", "I_kwDOLB-lzc6nWoxu", "Covered in https://specs.optimism.io/interop/fault-proof.html#local-safe-block-derivation", "2025-04-08T02:46:00Z", "2025-04-08T02:46:00Z", "ajsutton", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6mAQ6S", "I_kwDOLB-lzc6nWoLK", "Covered in https://specs.optimism.io/interop/fault-proof.html#local-safe-block-derivation", "2025-04-08T02:45:45Z", "2025-04-08T02:45:45Z", "ajsutton", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6mASMe", "I_kwDOLB-lzc6nWkox", "Closing this as its out of scope for the initial version of interop (which doesn't support adding chains).  I'm not sure the fault proof system itself will actually ever support this as we'll likely just bootstrap a new `DisputeGameFactory` when adding new chains with an anchor state that already has the added chain and let it run from there. But will work out the details when we do the work to support adding new chains.", "2025-04-08T02:50:36Z", "2025-04-08T02:50:36Z", "ajsutton", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6mARoG", "I_kwDOLB-lzc6nWiMT", "The honest challenger algorithm doesn't actually change - it never discusses what the valid claims are, just how to interact with  the game given a valid trace. And we apparently forgot to add the l2 block number challenge to it so that doesn't need to be removed.", "2025-04-08T02:48:38Z", "2025-04-08T02:48:38Z", "ajsutton", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6mAR2M", "I_kwDOLB-lzc6nWgpC", "https://specs.optimism.io/interop/fault-proof.html describes the difference to the pre-interop game.", "2025-04-08T02:49:17Z", "2025-04-08T02:49:17Z", "ajsutton", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6mAQ0x", "I_kwDOLB-lzc6nWgMQ", "https://specs.optimism.io/interop/fault-proof.html#transition-state", "2025-04-08T02:45:24Z", "2025-04-08T02:45:24Z", "ajsutton", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6fKFs9", "I_kwDOLB-lzc6nWfR6", "Adding one more link from [a duplicate issue](https://github.com/ethereum-optimism/optimism/issues/10894#issuecomment-2669961980): \nhttps://oplabs.notion.site/External-Interop-Fault-Dispute-Game-Notes-1537bf9fad054bcfb2245dea88d48d16", "2025-02-20T01:53:25Z", "2025-02-20T01:53:25Z", "pauldowman", "2025-08-31 03:26:34"]
["IC_kwDOLB-lzc6fKGmR", "I_kwDOLB-lzc6nWfR6", "I can't set an interop milestone on here because those are defined on the optimism monorepo. It will show up on the proofs interop project board as \"no milestone\".", "2025-02-20T01:55:50Z", "2025-02-20T01:55:50Z", "pauldowman", "2025-08-31 03:26:34"]
["IC_kwDOJ_r-bs6hmHE1", "I_kwDOJ_r-bs6m51h0", "How to Tell If a Chain Runs Fault proofs: [link](https://www.notion.so/oplabs/Fault-Proof-Answers-12cf153ee162802b84b7c9cb1054e818?pvs=4#19ff153ee16280e89762eddc4218a039)", "2025-03-10T16:14:04Z", "2025-03-10T16:14:04Z", "bitwiseguy", "2025-08-31 03:26:38"]
["IC_kwDOJ_r-bs6iYclB", "I_kwDOJ_r-bs6m51h0", "Reassigned to @yashvardhan-kukreja based on our discussion earlier this week.", "2025-03-14T10:54:30Z", "2025-03-14T10:54:30Z", "alfonso-op", "2025-08-31 03:26:38"]
["IC_kwDOJ_r-bs6iY1at", "I_kwDOJ_r-bs6m51h0", "@bitwiseguy are you on the same page? Not exactly sure about the fact that the discussion being mentioned above included you as well explicitly.\n\nI am happy to take over this but just mentioning this as a heads up so as to not step over any toes and do any duplicated work.\n\nThanks folks!", "2025-03-14T11:37:32Z", "2025-03-14T11:37:32Z", "yashvardhan-kukreja", "2025-08-31 03:26:38"]
["IC_kwDOJ_r-bs6iaey_", "I_kwDOJ_r-bs6m51h0", "@yashvardhan-kukreja just synced with Sam and he clarified he'll take this. Sorry for the noise!", "2025-03-14T14:21:34Z", "2025-03-14T14:21:34Z", "alfonso-op", "2025-08-31 03:26:38"]
["IC_kwDOJ_r-bs6idMgZ", "I_kwDOJ_r-bs6m51h0", "No problemo, cheers! Thanks for the clarification :)", "2025-03-14T19:21:21Z", "2025-03-14T19:21:21Z", "yashvardhan-kukreja", "2025-08-31 03:26:38"]
["IC_kwDOKIsnqM6l95p8", "I_kwDOKIsnqM6q1f-h", "We should use `forge coverage` to get a lcov file. We can open that in an lcov reader. `lcov` has a cli.", "2025-04-07T19:49:48Z", "2025-04-07T19:49:48Z", "blmalone", "2025-08-31 03:27:01"]
["IC_kwDOKIsnqM6ayB8A", "I_kwDOKIsnqM6Yuefi", "Pull out part 1 into a separate issue. ", "2025-01-16T20:13:39Z", "2025-01-16T20:13:39Z", "blmalone", "2025-08-31 03:27:01"]
["IC_kwDODjvEJM6mG7NT", "I_kwDODjvEJM6xm5dv", "[Example failing pipeline](https://app.circleci.com/pipelines/github/ethereum-optimism/optimism/86609/workflows/cd06b086-f379-40e9-8d44-fd432f7fa397/jobs/3422561)", "2025-04-08T15:27:14Z", "2025-04-08T15:27:14Z", "janjakubnanista", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mADFj", "I_kwDODjvEJM6xhe0v", "Proof of concept: https://github.com/defi-wonderland/optimism/pull/358", "2025-04-08T01:55:09Z", "2025-04-08T01:55:09Z", "tynes", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6l8WLS", "I_kwDODjvEJM6xc7Bi", "One solution is to run a git command at the start to find all changed and untracked files and print a warning that those may influence the prestate hash.", "2025-04-07T17:20:44Z", "2025-04-07T17:20:44Z", "sebastianst", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6l_x2M", "I_kwDODjvEJM6xc7Bi", "Just to bring other conversations together here - we do expect that untracked files will cause the prestate to differ. We should:\n1. ensure that anything that is git ignored doesn't affect the result\n2. Update any instructions for building prestates to ensure they tell people to confirm they have no outstanding changes (ie `git status` reports no changes and no untracked files).  A lot of instructions already start by saying to check out the repo so that's not an issue.\n3. Consider Seb's suggestion above of logging a warning if there are changes.  We should do that at the *end* of the build so it's output next to the printed absolute prestates. It will get lost if we print it at the start.", "2025-04-08T00:49:10Z", "2025-04-08T00:49:10Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6l8uYY", "I_kwDODjvEJM6w2JCL", "Met with @zhwrd @yashvardhan-kukreja @jelias2 to discuss items they can take on.\n\nProxyd:\n- 1 `send raw tx` which has an Interop access list : 1 call to checkAccessLIst\n- (existing srtx rate limit is 90 per second)\n  - Limit is already pretty low, are we sure an additional RL will make a difference?\n  - At least looking to have two different liveness controls (so interop doesn't interrupt typical traffic)\n- Should set up a dedicated Supervisor (again, do we need rate limit?)\n- 3 tx ingress rpcs, so 3 supervisors (and implied nodes)\nAI:\n- Put together design with proposed design flow (@jelias2 or @yashvardhan-kukreja ) - how specifically will proxyd route? how will it rate limit?\n\nConductor:\n- Might need separate ZDD process for Supervisor\n- Leadership transfer based on Supervisor going down\n- Want to be able to selectively turn Supervisor Conductor Triggers off\n- Tracking things like the cross-unsafe head for leadership transfer is maybe too tricky", "2025-04-07T18:05:36Z", "2025-04-07T18:05:36Z", "axelKingsley", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6l4Rs4", "I_kwDODjvEJM6wij7o", "Implemented some improvements here already, and some more in the op-deployer integration PR.\nWill keep this issue open for now, but I think with the gradual improvements of the other PRs it can soon be closed.\n", "2025-04-07T11:26:06Z", "2025-04-07T11:26:06Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mH4Gt", "I_kwDODjvEJM6wij7o", "I believe this is fixed now that the deployer integration PR, with its fixes, has been merged", "2025-04-08T16:48:56Z", "2025-04-08T16:48:56Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6kn0ML", "I_kwDODjvEJM6wOsfg", "We should send an RPC error response, with some error-message that the supervisor is still initializing.\nIt's zero until the DB contents have been loaded.", "2025-03-28T16:46:15Z", "2025-03-28T16:46:15Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mdPfS", "I_kwDODjvEJM6vULad", "`core-utils` is no longer part of the monorepo, and has not been here for quite a while. We are not changing the behavior of the package anymore.\n", "2025-04-10T12:57:53Z", "2025-04-10T12:57:53Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6ka7BI", "I_kwDODjvEJM6udT-x", "This one should be fun! I can just calculate it. I realize that we are looking for a *dashboard*, but it won't show realistic growth until there is realistic traffic. I will calculate the theoretical and expected growth rates, and then will make a dashboard.\n\n**Sources of Database Growth in Supervisor**\n- Log Database per Chain\n- Derivation Database per Chain\n\n\n### Log Database Growth\n\n**Size of an entry:** 24 bytes (but some items have multiple entries)\n- Search Checkpoint: 1 entry\n- Padding: up to 2 entries if Executing Messages would intersect a Search Checkpoint\n- Executing Messages: 3 entries\n- Initiating Message: 1 entry\n\n**Search Checkpoint Frequency**: Every 256 entries, a search checkpoint entry is added. AND at the end of every block.\n\n**Formula for Growth**\nSo then the size of the database for a given block is\n```\n(i + 2e + 2c + 1) <- 1 per initiating message, 2 (additional) per executing message, 2 per padding, 1 for the block seal\n* 257/256 <- overhead from inserting checkpoints every 256\n* 24 <- bytes\n```\nFrom here on I'm going to discount padding bytes from executing message collisions with the search checkpoint, because its too convoluted to expect this random chance.\n\n** Realistic Growth Numbers**\n\nOP Mainnet sees between 4-4.5M Logs per day\n\n![Image](https://github.com/user-attachments/assets/e30bfb46-c491-46aa-915f-20f0c75de98c)\n\nPlugging in 4,250,000 in as our `initiating messages`, and assuming a `10%` rate of logs being executing gets us:\n\n```\n4.25M initiating messages x1 entry per\n.425M executing messages x2 additional entries per\n43200 block seals\n*257/256 overhead\n\n-> 5163290 entries daily\nx24 bytes\n\n-> 124Mb per day\n```\n\n### Derivation Database Growth\n\n**Size of an entry:** 100 bytes\n\n**Frequency of an entry:**\n- One Entry per L1 source block\n- One Entry per L2 derived block\nThis includes blocks derived which are invalidated and replaced from later L1 sources.\nThis is *per chain*\n\nRate of Growth:\n- 1 L1 block per 12 seconds\n- 6 L2 blocks per 12 seconds\n-> 7 entries per 12 seconds, or 35 entries per minute\n\n35 * 100 bytes -> 3500 bytes per minute -> 210000 bytes per hour -> 5040000 bytes per hour\n**Per Chain**\n\nTotal: 5.04Mb per Chain per Day per Derivation DB\n\nThere are 2x Derivation DBs: Cross and Local Safe: 10.08Mb per Chain per Day.\n\n## Total Calculated Growth Rate\n\nAssuming:\n- Log Volume Similar to OPM\n- 10% Executing Message Rate\n\nWe can expect the Supervisor DB to grow by 134Mb per day per chain.", "2025-03-27T15:42:18Z", "2025-03-31T19:39:13Z", "axelKingsley", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mS7ol", "I_kwDODjvEJM6udT-x", "Because this rate of growth estimate is aggressive and *still* not a concern, I am closing this investigation as \"no action required\". We will naturally track the disk fullness of our hosts and will need to respond if for some reason storage is running out quicker than calculated.", "2025-04-09T14:42:28Z", "2025-04-09T14:42:28Z", "axelKingsley", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mlLAp", "I_kwDODjvEJM6uVuow", "A solution for this was implemented in https://github.com/paradigmxyz/reth/pull/15313. We should be good to close this issue", "2025-04-10T18:21:59Z", "2025-04-10T18:21:59Z", "tynes", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6ij1HO", "I_kwDODjvEJM6uOA0k", "https://github.com/ethereum-optimism/developers/issues/466", "2025-03-16T09:43:07Z", "2025-03-16T09:43:07Z", "EthanBlockson", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mdV7h", "I_kwDODjvEJM6uOA0k", "The above linked developers repo, as discussion, is probably better to find help with operating issues. The errors here alone just seem like DB corruption, which can happen after a brute shutdown that does not properly persist buffered in-memory changes.\n", "2025-04-10T13:02:04Z", "2025-04-10T13:02:04Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6l4PFm", "I_kwDODjvEJM6tsKe_", "This issue is stale, it's already fixed in the new `devstack` package, which I think we can complete moving to this cycle.", "2025-04-07T11:21:41Z", "2025-04-07T11:21:41Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6hngk1", "I_kwDODjvEJM6tU2mz", "Hi, I came across this issue and wanted to share some potential solutions that might help.\n\nIt seems like the setL1BlockValuesInterop function does not currently include the operator fee parameters. Here are some possible steps to address this issue:\n\n\t1.\tCheck the existing function implementation \u2013 It would be useful to inspect how setL1BlockValuesInterop is currently handling L1 block values and determine where the operator fee should be added.\n\t2.\tVerify recent changes \u2013 If this issue appeared recently, it might be worth checking recent commits to see if any modifications accidentally removed or altered the operator fee handling.\n\t3.\tUpdate function parameters \u2013 If the function needs to include additional parameters for operator fees, modifying the function signature and updating its logic accordingly could be a solution.\n\t4.\tTest the changes \u2013 Adding unit tests to ensure that the new operator fee logic is correctly applied would help validate the fix.\n\t5.\tCheck related functions \u2013 If setL1BlockValuesInterop relies on other functions, they might also need adjustments to correctly pass and process the operator fee parameters.\n\nWould be great to hear any insights from the team on whether this approach makes sense. Looking forward to updates!\n\nBest regards.", "2025-03-10T18:35:17Z", "2025-03-10T18:35:17Z", "bentrnr21", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mdZJU", "I_kwDODjvEJM6tRq1S", "Fixed in https://github.com/ethereum-optimism/optimism/pull/14748\n", "2025-04-10T13:03:34Z", "2025-04-10T13:03:34Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6hXkSX", "I_kwDODjvEJM6sugdN", "Done in https://github.com/ethereum-optimism/optimism/pull/14662", "2025-03-07T20:03:16Z", "2025-03-07T20:03:16Z", "leruaa", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mnROS", "I_kwDODjvEJM6qXKhz", "All set #15357 ", "2025-04-10T22:31:50Z", "2025-04-10T22:31:59Z", "stevennevins", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mc_GA", "I_kwDODjvEJM6qHiow", "Duplicate of https://github.com/ethereum-optimism/optimism/issues/15261\n", "2025-04-10T12:41:40Z", "2025-04-10T12:41:40Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mc03E", "I_kwDODjvEJM6pDUSw", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15325\n", "2025-04-10T12:26:15Z", "2025-04-10T12:26:15Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6dELes", "I_kwDODjvEJM6ov2Jv", "are you considering other changes that arent audit fixes? i dont want to accidentally merge something", "2025-02-04T21:29:59Z", "2025-02-04T21:29:59Z", "tynes", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6dN1iT", "I_kwDODjvEJM6ov2Jv", "Not considering any changes that are not audit fixes", "2025-02-05T18:01:31Z", "2025-02-05T18:01:31Z", "smartcontracts", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mm-eP", "I_kwDODjvEJM6nVX01", "Saying that this has been completed given that there are devnets live today running interop", "2025-04-10T21:45:38Z", "2025-04-10T21:45:38Z", "tynes", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6beJ7t", "I_kwDODjvEJM6nFNEB", "Started with some quick tests, without using the Pectra block history support.  Walking back 5000 L2 blocks took about 10 mins on my laptop with local source nodes - based on experience with block processing times on my laptop vs GCP that would be 2-3 hours in GCP.\n\nWalking back a week of L2 blocks (2s block time) is still running after over 15 hours on my laptop.\n\nPectra should make things about 8000 times faster since we can skip back 8124 blocks at a time (but with more overhead per request as I believe we'll have to walk down the state trie to get the value we need).\n\nHowever we then need to divide the maximum time window by the number of chains in the dependency set (as we may be forced to walk back the maximum distance on each of those chains).\n\nSo even _with pectra_ I think we'd be struggling to support longer than 1 week window. Once we grow the dependency set beyond two chains even one week looks unrealistic.\n\nWill need to do some better benchmarks and pull together a spreadsheet to forecast execution times for different histories and number of chains in the dep set, but this is an initial rough take.", "2025-01-22T22:09:37Z", "2025-01-22T22:10:55Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6be9e4", "I_kwDODjvEJM6nFNEB", "Fetching a receipt from a block that's a week old on my laptop took just over 16 hours (without pectra)", "2025-01-23T00:34:46Z", "2025-01-23T00:34:46Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6bgEbF", "I_kwDODjvEJM6nFNEB", "Created a spreadsheet to play with the numbers a bit more: https://docs.google.com/spreadsheets/d/1rCrFItkOWYG0KNAkkfmu07rH8Lw_Tve4JDogTYvLVsY/edit?usp=sharing (sadly OP Labs only due to sharing restrictions).\n\nBottom line is that with 2 chains in the dependency set, we could have an expiry window of 30 days (with pectra) and still execute cannon in an estimated 7 hours in GCP.  While we have 84 hours on the chess clock to start, I would be very nervous to push execution times much past that as it becomes too easy to overwhelm the honest challenger via multiple claims. Plus we need to leave ourselves time to actually post counter claims, including rerunning parts of the cannon trace to generate claims at specific steps.\n\nWith 10 chains in the dependency set we could do 7 days expiry and still execute in 7-8 hours in GCP.\n\nWithout pectra, even with just two chains in the dependency set we'd need to set the expiry window to about 5 hours at most.\n\nIf we wanted to extend the expiry window we could extend the number of block hashes retained in Pectra's block history contract.  Retaining 32768 blocks instead of 8192, would let us handle 30 day expiry with 10 chains in the dependency set and an execution time of about 8 hours which is manageable.", "2025-01-23T05:15:12Z", "2025-01-23T05:15:12Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6bgE3K", "I_kwDODjvEJM6nFNEB", "Key thing out of this though is Pectra on L2 is a hard requirement for interop being fault provable and we should start tracking it as a dependency.\n\nWe need to implement support for leveraging Petra's block hash history and benchmark that to know for sure what speed up it provides.  We can also then evaluate how much extra storage is required if we increase the pectra block hash range vs the increase in expiry window it could support.", "2025-01-23T05:16:53Z", "2025-01-23T05:16:53Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6bp01C", "I_kwDODjvEJM6nFNEB", "Logged https://github.com/ethereum-optimism/optimism/issues/13961 to implement support for the pectra block hash ring buffer.  Once we have that implementation we can benchmark it and select final numbers.", "2025-01-24T02:51:23Z", "2025-01-24T02:51:23Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6bvI9Q", "I_kwDODjvEJM6nFNEB", "> Key thing out of this though is Pectra on L2 is a hard requirement for interop being fault provable and we should start tracking it as a dependency.\n> \n> We need to implement support for leveraging Petra's block hash history and benchmark that to know for sure what speed up it provides. We can also then evaluate how much extra storage is required if we increase the pectra block hash range vs the increase in expiry window it could support.\n\nOngoing work in https://github.com/ethereum-optimism/op-geth/pull/474", "2025-01-24T15:40:11Z", "2025-01-24T15:40:11Z", "tynes", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6f-FVK", "I_kwDODjvEJM6nFNEB", "As an update on the above - we have fixed some issues with the GCP configuration, resulting in significantly faster cannon execution times there (the test box previously had fewer CPUs than intended leading to a lot of contention). So with pectra and two chains in the dependency set we could support a 90 day expiry window with cannon taking just over 5 hours to execute.\n\nHowever, we do still need to benchmark the actual speedup we get using Pectra's block hash history - it's been implemented now, we just haven't had time to benchmark it.", "2025-02-26T04:16:29Z", "2025-02-26T04:16:29Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6iT_FM", "I_kwDODjvEJM6nFNEB", "Important to note that since we're depending on the pectra block hash history contract, we will need an archive L2 node with world state data available back at least one expiry window prior to the current anchor state. Currently we need an L2 archive node with ~16.5 days of history before the anchor state in the worst case. We generally ensure we have 30 days of archive history available.\n\nSo with a 30 day expiry window we'd be extending the required archive history by at least 7 days (minimum time to finalise a new anchor state) plus whatever buffer we want for any periods where valid proposals aren't proposed or don't finalize. For op and unichain that's fine, but for chains with very large world states like base it could be problematic to maintain those nodes.", "2025-03-14T02:08:54Z", "2025-03-14T02:08:54Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6koC2q", "I_kwDODjvEJM6nFNEB", "Is the testnet milestone right? Or should this be for betanet?", "2025-03-28T17:13:37Z", "2025-03-28T17:13:37Z", "pauldowman", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6kyndY", "I_kwDODjvEJM6nFNEB", "Yeah switched to beta. ", "2025-03-30T20:29:22Z", "2025-03-30T20:29:22Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6lXifW", "I_kwDODjvEJM6nFNEB", "Some good news for a change, it turns out that accessing historical block hashes using the new pectra contract is way faster than I had estimated.  I think it will be slower on chains with larger world state (more state trie nodes need to be loaded to get the state) but on the interop-rc checking a log from a week ago takes 1m40s even with the high latency accessing the US based L2 nodes from Australia (running in cannon).  With data already fetched it's 18s.\n\nThere are other avenues for resource exhaution (having a lot of initiating messages to check in the one block, cascading invalidations etc), but we had expected the block by number look up required to be the slowest part and thus the limiting factor for how long the message expiry window can be.  I now think the limiting factor will be how much historical world state we are prepared to required for L2 nodes. Because the pectra block history is stored in world state - currently we need ~28 days of state history, with interop we will need ~28 days + message expiry window.", "2025-04-03T03:49:26Z", "2025-04-03T03:49:26Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mAQhu", "I_kwDODjvEJM6nFNEB", "We can be suitably confident that a 30 day expiry window will be suitable.  Logged https://github.com/ethereum-optimism/optimism/issues/15285 to stress test this to be certain but we have enough confidence to call this spike complete and run with the 30 day expiry window.", "2025-04-08T02:44:08Z", "2025-04-08T02:44:08Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mItNW", "I_kwDODjvEJM6nFMNR", "The migration to a shared DGF is now supported by OPCM. So we can close this.", "2025-04-08T18:14:40Z", "2025-04-08T18:14:40Z", "Inphi", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6fLpZV", "I_kwDODjvEJM6mL5Pd", "Reopening", "2025-02-20T07:04:14Z", "2025-02-20T07:04:14Z", "pauldowman", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6abcYq", "I_kwDODjvEJM6lTqBD", "The theory is that these issues should be resolved with the introduction of the Monorepo Event pattern, which eliminates many areas that could get stuck during a test.\n\nThat work is merging here, and we can optimistically close this ticket after observing for a short while that these flakes are gone. cc @mslipper\n\nhttps://github.com/ethereum-optimism/optimism/pull/13752", "2025-01-14T18:59:52Z", "2025-01-14T18:59:52Z", "axelKingsley", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mcJtR", "I_kwDODjvEJM6lTqBD", "This issue seems stale. Not aware of any current timeouts that are regular in CI. And above change was implemented. Closing.", "2025-04-10T11:13:46Z", "2025-04-10T11:13:46Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mdrDp", "I_kwDODjvEJM6i3M6b", "Closing, seems like the reth issue has been resolved\n", "2025-04-10T13:10:32Z", "2025-04-10T13:10:32Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6SIVEA", "I_kwDODjvEJM6crxfj", "Sounds good to me, I'll take this one ", "2024-11-01T10:28:17Z", "2024-11-01T10:28:17Z", "jsvisa", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6SJABC", "I_kwDODjvEJM6crxfj", "Awesome, thank you!", "2024-11-01T12:59:09Z", "2024-11-01T12:59:13Z", "smartcontracts", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6SXXuR", "I_kwDODjvEJM6crxfj", "I mentioned it in the [design-doc](https://github.com/ethereum-optimism/design-docs/commit/91a6d3ba145d05afc7d6a8e989566c79c17afbdb) for custom errors, but do we actually just want a semgrep rule to blanket outlaw `require` and then use `assertEq` and `assert` in tests/scripts", "2024-11-04T20:05:37Z", "2024-11-04T20:05:37Z", "ControlCplusControlV", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6SZgU7", "I_kwDODjvEJM6crxfj", "> I mentioned it in the [design-doc](https://github.com/ethereum-optimism/design-docs/commit/91a6d3ba145d05afc7d6a8e989566c79c17afbdb) for custom errors, but do we actually just want a semgrep rule to blanket outlaw `require` and then use `assertEq` and `assert` in tests/scripts\r\n\r\nYes", "2024-11-05T03:33:58Z", "2024-11-05T03:33:58Z", "smartcontracts", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mcxR8", "I_kwDODjvEJM6coFn4", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15337\n", "2025-04-10T12:21:05Z", "2025-04-10T12:21:05Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6am9zj", "I_kwDODjvEJM6cR51N", "@op-aaron this work appears to be specific to the solidity development, which is obviously important, but I'm not sure the Stable Devnet milestone is the right tracker for it.\n\nI am also not positive this is really a P1 ask for any given devnet, unless @mds1 feels very strongly about it being a security requirement.", "2025-01-15T20:49:00Z", "2025-01-15T20:49:00Z", "axelKingsley", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mnCAM", "I_kwDODjvEJM6cR51N", "closing this due to being stale", "2025-04-10T21:55:12Z", "2025-04-10T21:55:12Z", "tynes", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6XyDiA", "I_kwDODjvEJM6YNZuK", "I'm not sure this behavior is as described anymore and we should check it.", "2024-12-16T19:30:42Z", "2024-12-16T19:30:42Z", "axelKingsley", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mcuJQ", "I_kwDODjvEJM6YNZuK", "This code no longer exists, and the issue is stale, I am closing this.\n", "2025-04-10T12:16:21Z", "2025-04-10T12:16:21Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6md6Tg", "I_kwDODjvEJM6WY7LK", "Closing, this issue seems stale\n", "2025-04-10T13:16:01Z", "2025-04-10T13:16:01Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6am78I", "I_kwDODjvEJM6U4d2z", "I think the meaning of this task is blending into this other task we have that I recently edited:\n\nhttps://github.com/ethereum-optimism/optimism/issues/11027\n\nThis linked task has to do with *any time* a reset is drived into the op-node from the supervisor, and would include these invalid dependency reorg cases.", "2025-01-15T20:44:31Z", "2025-01-15T20:44:31Z", "axelKingsley", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6c6c8Y", "I_kwDODjvEJM6U4d2z", "This issue is stale, reorg work is complete, some follow-up integration testing is still in progress, but this old issue doesn't add anything to that.\n", "2025-02-04T01:42:07Z", "2025-02-04T01:42:07Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6g5dmm", "I_kwDODjvEJM6U4d2z", "The todo checker is flagging 4 TODOs that reference this issue - reopening to stop it failing:\n\n```\nethereum-optimism/optimism #11693        | Interop: check reorgs                                             | op-supervisor/supervisor/backend/db/query.go:44\nethereum-optimism/optimism #11693        | Interop: check reorgs                                             | op-supervisor/supervisor/backend/db/query.go:56\nethereum-optimism/optimism #11693        | Interop: check reorgs                                             | op-supervisor/supervisor/backend/cross/safe_update.go:39\nethereum-optimism/optimism #11693        | Interop: check reorgs                                             | op-supervisor/supervisor/backend/cross/unsafe_update.go:53\n```\nThe todo checker was broken for a fair while so these slipped through.  Could you please remove the TODOs if no longer relevant or update the issue number they reference if there's something else tracking that work now.", "2025-03-05T00:50:12Z", "2025-03-05T00:50:12Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mcJCQ", "I_kwDODjvEJM6U4d2z", "This issue was done, and the old TODO references were removed, closing now.", "2025-04-10T11:12:35Z", "2025-04-10T11:12:35Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6J4ieO", "I_kwDODjvEJM6TyhO_", "My recommendation:\r\n- op-bootnode: remove, the geth bootnode is sufficient for current purposes, and op-bootnode has some tech-debt of pulling in the whole op-stack p2p code. In the future a more minimal op-bootnode could be composed, with better more directed instrumentation. But the geth bootnode is the shorted path to cleanup.\r\n- op-conductor: move to its own repo or infra repo. Figure out a path for improved integration testing of the op-conductor using kurtosis.\r\n- op-heartbeat: either fix the versioning allow-list, or just remove it altogether. It is not providing any meaningful data to production or test networks\r\n- op-chain-ops: keep in the monorepo; I strongly believe we need it as integration point for the protocol, including active protocol changes.\r\n- op-wheel: move this to its own repository. It is very useful to recover corrupted nodes, and potentially do shadow-forks with. But it does not need to be in the monorepo.\r\n- contracts-bedrock: mixed opinions here. It can be valuable to encapsulate, but without integration-test plan it feels more like escapism than meaningful encapsulation.\r\n", "2024-08-27T19:00:11Z", "2024-08-27T19:00:11Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6md_z4", "I_kwDODjvEJM6TyhO_", "This issue is inactive and stale. The remaining move of op-wheel is tracked here: https://github.com/ethereum-optimism/optimism/issues/14115\nThe rest has already moved out or improved. And the contracts package is a whole decision on its own, not actively tracked here in this issue.\n\n", "2025-04-10T13:18:08Z", "2025-04-10T13:18:08Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mcs2R", "I_kwDODjvEJM6Miq_4", "This issue is stale, a lot changed since, and we'll likely prioritize more targeted flashblocks integration over more general log-db builder forks.\n", "2025-04-10T12:14:10Z", "2025-04-10T12:14:10Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6meFNE", "I_kwDODjvEJM6HyvXi", "Closing, we no longer use a `versions.json`, it's all via mise now (not a fan, but it works for CI)\n", "2025-04-10T13:20:08Z", "2025-04-10T13:20:08Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6lqn2w", "I_kwDODjvEJM5z1TJB", "These changes were implemented", "2025-04-04T17:53:11Z", "2025-04-04T17:53:11Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6lzdzt", "I_kwDODjvEJM5z1TJB", "@protolambda Reopening this one as there's still a couple of todos.  I think they are done so have put up https://github.com/ethereum-optimism/optimism/pull/15254 to remove them (which will close this again).", "2025-04-06T23:44:54Z", "2025-04-06T23:44:54Z", "ajsutton", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM5kRWWY", "I_kwDODjvEJM5uFg-J", "Hi! A friend (@0xR360) and I want to contribute in this issue, we have a couple of questions, where are implemented the RPC transport options now? and we have a question related to this item\r\n\r\n> \"attaching in-process: this can make tests faster, by not depending on http requests/servers.\" \r\n\r\n We would like to know which tests would run faster and why?\r\n \r\n Thanks in advance!\r\n\r\n", "2023-08-17T13:16:22Z", "2023-08-17T13:16:22Z", "AN-Drew207", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM5kRY_s", "I_kwDODjvEJM5uFg-J", "Yes! Thanks @AN-Drew207 . Would like to contribute here too", "2023-08-17T13:23:04Z", "2023-08-17T13:23:04Z", "Rcontre360", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM5kTO6E", "I_kwDODjvEJM5uFg-J", "@AN-Drew207 in op-geth, see https://github.com/ethereum-optimism/op-geth  if you're unsure how to support the transports, please go through the op-geth code and then post any proposed changes / usage here. This is a more advanced issue than some of the other good-first-issue items, so it may require more time to get right.\r\n\r\n@Rcontre360 thank you, @AN-Drew207 was first, but if you're really interested you can help them by outlining the geth RPC utils and options here.", "2023-08-17T18:26:02Z", "2023-08-17T18:26:02Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM5kc7ty", "I_kwDODjvEJM5uFg-J", "Hi @protolambda!\r\nThe following is the Idea we get from the issue and what we need to do. We reviewed the op-geth code to see how they do it and this are the points we think we need to work on. Let us know if we are missing anything or if we have the wrong understanding.\r\n\r\n - Create flags on command line to allow the user to configure ws and ipc. https://github.com/ethereum-optimism/optimism/blob/develop/op-node/flags/flags.go \r\n\r\n - Update the Config or RPCConfig struct to have this the ipc/ws config available: https://github.com/ethereum-optimism/optimism/blob/develop/op-node/node/config.go#L15-L48\r\n\r\n- Modify rpcServer to hold server instances of ipc, ws https://github.com/ethereum-optimism/optimism/blob/develop/op-node/node/server.go#L21-L29 . In op-geth they seem to hold the server instances of all 3 transports in the Node config object.\r\n\r\n - Create a WebsocketHandler and IPCHandler (ws: https://github.com/ethereum-optimism/op-geth/blob/optimism/rpc/websocket.go). Then configure properly the rpcServer on creation and start https://github.com/ethereum-optimism/optimism/blob/develop/op-node/node/server.go#L31-L46\r\n\r\nQuestions:\r\n - Let us know if we are understanding well and we are on track :).\r\n - About the \"attaching in-process\" we need more clarification about what it is exactly\r\n - Can we work first on \"move op-node client package\", make the PR so that you start verifying our code? or we must do the PR after finishing everything?", "2023-08-20T15:01:16Z", "2023-08-20T15:01:16Z", "Rcontre360", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM5kjnbr", "I_kwDODjvEJM5uFg-J", "Thanks @Rcontre360!\r\nI've split out the code-organization related cleanup into its own issue: https://github.com/ethereum-optimism/optimism/issues/6936\r\nIf you're able to tackle that first then go for it, I appreciate the contributions :+1: ", "2023-08-21T21:18:41Z", "2023-08-21T21:18:41Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM5k4nOM", "I_kwDODjvEJM5uFg-J", "gRPC would enable streaming, which would be helpful for large RPC responses like debug_* etc.", "2023-08-24T23:56:45Z", "2023-08-24T23:56:45Z", "sambacha", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6Bt9Le", "I_kwDODjvEJM5uFg-J", "@protolambda ok to close this issue, now?", "2024-06-18T14:53:52Z", "2024-06-18T14:53:52Z", "BlocksOnAChain", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mfYMC", "I_kwDODjvEJM5uFg-J", "Closing, websockets are supported on the server, the rest isn't needed.\n", "2025-04-10T13:49:58Z", "2025-04-10T13:49:58Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM5Z7BtF", "I_kwDODjvEJM5jSDnf", "I believe this came about because prior to regolith the system tx was unmetered, but now it is metered and so affects the gas limit for a regular tx.\r\n\r\nIn regolith the max gas is 1m for the system tx and it is metered so correct fix is to offset the blockGasLimit by 1m (in DoEstimateGas) to ensure the system tx and a max gas tx would be included", "2023-04-14T14:28:02Z", "2023-04-14T14:45:57Z", "sidhujag", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM5Z8PbX", "I_kwDODjvEJM5jSDnf", "Thank you for the detailed report, we'll look into improving the gas estimation to account for this max-gas case better, and evict this type of transaction that cannot fit into a block.", "2023-04-14T17:00:59Z", "2023-04-14T17:00:59Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM51-_hO", "I_kwDODjvEJM5jSDnf", "Is this still an issue? cc @protolambda ", "2024-03-05T19:01:36Z", "2024-03-05T19:01:36Z", "smartcontracts", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6Bt86C", "I_kwDODjvEJM5jSDnf", "@protolambda ok to close this one?", "2024-06-18T14:53:25Z", "2024-06-18T14:53:25Z", "BlocksOnAChain", "2025-08-31 03:27:16"]
["IC_kwDODjvEJM6mfHk7", "I_kwDODjvEJM5jSDnf", "Going through old github issues to clean up, apologies about this getting lost.\nI didn't hear any other reports of this, I am not sure if this is ever an issue in practice.\n\nThe tx-pool does remove transactions like that and so they should not get stuck, but it makes sense to reflect that in the estimate-gas function too (in cases where a tx really does consume so much of the full gas-limit).\n\nPR: https://github.com/ethereum-optimism/op-geth/pull/578\n\n", "2025-04-10T13:44:10Z", "2025-04-10T13:44:10Z", "protolambda", "2025-08-31 03:27:16"]
["IC_kwDOL-xLQ86nEBC4", "I_kwDOL-xLQ86yBzXU", "Closing in favor of using https://github.com/ethereum-optimism/optimism/issues/15358 to track.", "2025-04-14T19:58:22Z", "2025-04-14T19:58:22Z", "teddyknox", "2025-08-31 03:28:40"]
["IC_kwDOL-xLQ86mLieV", "I_kwDOL-xLQ86wiWTB", "This is very likely fixed by https://github.com/ethereum-optimism/optimism/pull/15308", "2025-04-09T01:17:38Z", "2025-04-09T01:17:38Z", "scharissis", "2025-08-31 03:28:40"]
["IC_kwDOL-xLQ86nkl5M", "I_kwDOL-xLQ86wiWTB", "This appears to have been fixed. I've just tested this locally to confirm.\n\nDevnet: isthmus\nGate: isthmus (modified to only run `TestERC20`, `TestPectra` and `TestWithdrawalsRoot`)\n\nop-acceptor  = 1m4s\ngo-test = 1m8s\n\nClosing, but will keep an eye on things.", "2025-04-17T01:11:24Z", "2025-04-17T01:11:24Z", "scharissis", "2025-08-31 03:28:40"]
["IC_kwDOKIwiaM6nFkCL", "I_kwDOKIwiaM6yX4BL", "We no longer support deploying the legacy L2OutputOracle version of the smart contracts. You can use op-deployer to deploy the system with Fault Proofs. The initial deployment of using op-deployer utilizes the PermissionedDisputeGame, so you need to run op-challenger as well to reclaim bonds. However, using the PermissionedDisputeGames is the same security model as the L2OO system. ", "2025-04-14T23:00:00Z", "2025-04-14T23:00:00Z", "sbvegan", "2025-08-31 03:28:41"]
["IC_kwDOKIwiaM6nCIbB", "I_kwDOKIwiaM6tDWxo", "Target to close today (Apr 14th). PR #1573 for reference. [Managing the proposer address #1573](https://github.com/ethereum-optimism/docs/pull/1573)", "2025-04-14T17:08:21Z", "2025-04-14T17:08:21Z", "yoonchung8822", "2025-08-31 03:28:41"]
["IC_kwDOKIwiaM6nFl-p", "I_kwDOKIwiaM6MhIFT", "This is being tracked in an internal issue, that I actually assigned to you lol", "2025-04-14T23:04:38Z", "2025-04-14T23:04:38Z", "sbvegan", "2025-08-31 03:28:41"]
["IC_kwDOKIwiaM6nFlvT", "I_kwDOKIwiaM6JnQb-", "This is being tracked in an internal issue", "2025-04-14T23:03:58Z", "2025-04-14T23:03:58Z", "sbvegan", "2025-08-31 03:28:41"]
["IC_kwDOKIwiaM5-i3zp", "I_kwDOKIwiaM6JnIUr", "@trianglesphere @sbvegan for suggestions? \ud83d\ude4f\ud83c\udffe ", "2024-05-21T17:06:33Z", "2024-05-21T17:06:33Z", "cpengilly", "2025-08-31 03:28:41"]
["IC_kwDOKIwiaM6nFlpa", "I_kwDOKIwiaM6JnIUr", "This is being tracked an internal issue", "2025-04-14T23:03:43Z", "2025-04-14T23:03:43Z", "sbvegan", "2025-08-31 03:28:41"]
["IC_kwDOKIwiaM6nFlU0", "I_kwDOKIwiaM6It4R-", "I updated this along with our pause testing notice", "2025-04-14T23:02:59Z", "2025-04-14T23:02:59Z", "sbvegan", "2025-08-31 03:28:41"]
["IC_kwDOH2Qg5s6nnvBm", "I_kwDOH2Qg5s6y4N4F", "Thanks for reporting! Yes the Prague time and Isthmus time need to be set to the same value. I consider this a missing configuration check and documentation bug. We will add this to the docs as a requirement for custom chains for all of our three forks that implicitly activate an Ethereum fork (Canyon->Shanghai, Ecotone->Cancun, Isthmus->Prague). I will also add a check to op-geth to require this. Note that for chains loading their config from the superchain-registry via the network flag, this is already automatically happening:\nhttps://github.com/ethereum-optimism/op-geth/blob/d1b66c4722e7fb5525efbcadda4c1ab3f7dacaa8/params/superchain.go#L47-L49\nand when using the override flags, this is also already happening:\nhttps://github.com/ethereum-optimism/op-geth/blob/d1b66c4722e7fb5525efbcadda4c1ab3f7dacaa8/core/genesis.go#L345-L370", "2025-04-17T08:47:29Z", "2025-04-17T08:47:29Z", "sebastianst", "2025-08-31 03:28:42"]
["IC_kwDOH2Qg5s6m0bk1", "I_kwDOH2Qg5s6yM1Ft", "Apple ships this ancient version of bash and I didn't realize when I modified the sync superchain-registry script that I introduced the non-compatible `declare -A`. I have locally installed a recent version of bash (`5.2.37`) and Linux CIs also use recent bash versions, which is why this wasn't caught in CI.\n\nI'll change the script nonetheless to not use `declare -A`. This will help with future runs of the script.", "2025-04-12T09:46:10Z", "2025-04-14T15:54:08Z", "sebastianst", "2025-08-31 03:28:42"]
["IC_kwDOH2Qg5s6nAqXD", "I_kwDOH2Qg5s6xo8sh", "An update:\n\nI have traced this to some type of issue with the [prestate_tracer.js](https://github.com/ethereum-optimism/optimism-legacy/blob/develop/l2geth/eth/tracers/internal/tracers/prestate_tracer.js) in `l2geth`. Using that tracer, included in the final 0.5.3 version of `l2geth` before the Bedrock release, as a custom tracer I get the same issue as in the example curl above. I've tried making a number of modifications to the tracer to get around the null issues but I either still get them, or crash the node with a nil pointer dereference. Might be a me-fault though, I'm not a Javascript coder.\n\nAnyway, this means the issue doesn't lie with `op-geth` and this post should perhaps go elsewhere. I'll leave it up for a little while in case someone sees this and has a suggestion on what to do \ud83d\ude4f ", "2025-04-14T14:46:31Z", "2025-04-14T14:46:31Z", "jkbrsn", "2025-08-31 03:28:42"]
["IC_kwDOH2Qg5s6n2aQt", "I_kwDOH2Qg5s6rnROq", "I believe this was completed as part of https://github.com/ethereum-optimism/op-geth/pull/540", "2025-04-18T19:21:21Z", "2025-04-18T19:21:21Z", "tynes", "2025-08-31 03:28:42"]
["IC_kwDOH2Qg5s6n2hIm", "I_kwDOH2Qg5s6rnROq", "The `supervisor_checkMessages` API has been removed for quite some time. I'm not sure what component might still have this fallback, but I think they're all gone.", "2025-04-18T19:37:52Z", "2025-04-18T19:37:52Z", "axelKingsley", "2025-08-31 03:28:42"]
["IC_kwDOH2Qg5s6nW0Gq", "I_kwDOH2Qg5s587UNt", "Hey @welkin22 did you ever figure out support for pruning in op-geth? I'm facing this issue as well when trying to prune an archive node", "2025-04-15T23:40:18Z", "2025-04-15T23:40:18Z", "tobidae-cb", "2025-08-31 03:28:42"]
["IC_kwDOLB-lzc6nCdRv", "I_kwDOLB-lzc6vgQtO", "Closing this because it does not address the top failure mode of leaked batcher key that includes a 0 day in the batch", "2025-04-14T17:46:15Z", "2025-04-14T17:46:15Z", "tynes", "2025-08-31 03:28:45"]
["IC_kwDODjvEJM6nuKBG", "I_kwDODjvEJM6zAs5x", "This ticket exists in https://github.com/ethereum-optimism/optimism/issues/15324", "2025-04-17T20:00:11Z", "2025-04-17T20:00:11Z", "tynes", "2025-08-31 03:29:09"]
["IC_kwDODjvEJM6nuVer", "I_kwDODjvEJM6zAs5x", "Got it - I will close, sorry for the dupe!", "2025-04-17T20:21:32Z", "2025-04-17T20:21:32Z", "op-aaron", "2025-08-31 03:29:09"]
["IC_kwDODjvEJM6nhwz9", "I_kwDODjvEJM6yyKdN", "The key likely doesn't correspond to the proposer in the `L2OutputOracle` contract", "2025-04-16T20:08:12Z", "2025-04-16T20:08:12Z", "tynes", "2025-08-31 03:29:09"]
["IC_kwDODjvEJM6nmFQq", "I_kwDODjvEJM6yyKdN", "@tynes which key ?\nand how to set that\n", "2025-04-17T05:26:00Z", "2025-04-17T05:26:15Z", "DSHIVAAY-23", "2025-08-31 03:29:09"]
["IC_kwDODjvEJM6m8CE4", "I_kwDODjvEJM6yWFr3", "The spec is wrong in this case - the L1 block values are set in every L2 block because that's how the L1 origin information is encoded in the block. If we only added that extra transaction on the first L2 block of each epoch, block processing (and anything else that needs to map from an L2 block to its L1 origin) would need to scan back to find the previous epoch boundary and parse the deposit tx from there which adds a lot of complexity.", "2025-04-14T07:42:55Z", "2025-04-14T07:42:55Z", "ajsutton", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6m8H1G", "I_kwDODjvEJM6yWFr3", "Is it possible to just forward the information to subsequent L2 blocks, instead of creating a new transaction for every L2 block? What are the risks in this case?", "2025-04-14T07:53:02Z", "2025-04-14T07:53:02Z", "LampardNguyen234", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6nEQZU", "I_kwDODjvEJM6yWFr3", "How would you \"forward\" the information so its available from the block itself without including a transaction?", "2025-04-14T20:28:52Z", "2025-04-14T20:28:52Z", "ajsutton", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6nFMMC", "I_kwDODjvEJM6yWFr3", "One possible way to do this has been explored here: https://github.com/ethereum-optimism/specs/issues/122\n\nThe idea is that the proof would consume the values from storage and/or logs rather than the L1 attributes transaction calldata and the L1 attributes tx calldata would only include a diff rather than repeat data", "2025-04-14T22:08:10Z", "2025-04-14T22:08:10Z", "tynes", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6nFpDZ", "I_kwDODjvEJM6yWFr3", "Technically that works, but we'd have to find every place that parses the L1 attributes transaction and ensure it had access to the world state to load the older data - in a lot of cases we currently only have the block itself.  I'd be very much leaning towards keeping at least the L1 origin block info in every block since that's pretty much always what is being pulled out.  Using a diff based approach for system config info works well.\n\nBut the starting point would be to properly review places that are parsing that L1 info tx and see what it would take to update them to have access to the world state. Maybe its easier than I'm expecting.", "2025-04-14T23:11:20Z", "2025-04-14T23:11:20Z", "ajsutton", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6nuVFL", "I_kwDODjvEJM6xCABC", "This was fixed by https://github.com/ethereum-optimism/optimism/issues/15149.\nI've confirmed that this bug doesn't affect the op-program. The issue only occurs when attempting to execute an _existing_ message, but in the future. In Fault Proof terms, this is when consolidating such a message.\nSince the FP's view of the superchain is bounded by the L1 head, the future message that's referenced won't be visible to the fault proof. And so the supervisor logic that [checks](https://github.com/ethereum-optimism/optimism/blob/0124d213c845aa630268480da98634241792fe75/op-supervisor/supervisor/backend/cross/hazard_set.go#L181) for future timestamp is unreachable in this case, as the init message existence [check](https://github.com/ethereum-optimism/optimism/blob/0124d213c845aa630268480da98634241792fe75/op-supervisor/supervisor/backend/cross/hazard_set.go#L157) fails because the message is not visible to the canonical block oracle. \nFor this reason, it's not possible to write a test case that triggers this specific bug in the supervisor.\n\nTo note; we already have tests that cover broken message references in the FP, particularly this [one](https://github.com/ethereum-optimism/optimism/blob/0124d213c845aa630268480da98634241792fe75/op-program/client/interop/interop_test.go#L292C2-L305C5). So no further work is needed.", "2025-04-17T20:20:34Z", "2025-04-17T20:20:34Z", "Inphi", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6kBZbr", "I_kwDODjvEJM6vCRe8", "this is now unblocked since https://github.com/ethereum-optimism/optimism/pull/14858 is merged", "2025-03-25T16:23:28Z", "2025-03-25T16:23:28Z", "BlocksOnAChain", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6nD-8d", "I_kwDODjvEJM6ttLk6", "is this done? @maurelian @smartcontracts ", "2025-04-14T19:54:01Z", "2025-04-14T19:54:01Z", "lewejahlil", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6knNE5", "I_kwDODjvEJM6tFw27", "Is this something we need to cover for MT cannon release for mainnet - @mbaxter ?", "2025-03-28T15:47:43Z", "2025-03-28T15:47:43Z", "BlocksOnAChain", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6kotRK", "I_kwDODjvEJM6tFw27", "No - this is part of the Go 1.23 work", "2025-03-28T18:31:41Z", "2025-03-28T18:31:41Z", "mbaxter", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6nsDlj", "I_kwDODjvEJM6tFw27", "This is now irrelevant with the new [feature toggling approach](https://github.com/ethereum-optimism/optimism/pull/15409) - closing.", "2025-04-17T15:35:22Z", "2025-04-17T15:35:22Z", "mbaxter", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6m6w0h", "I_kwDODjvEJM6qxgaY", "This was reviewed, some fixes went in and we're happy with where things landed.", "2025-04-14T04:07:16Z", "2025-04-14T04:07:16Z", "ajsutton", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6W7kdv", "I_kwDODjvEJM6ivr2g", "I am generally not in favor of reading in the state from a json file for the solidity unit tests. This is going to cause a massive devex headache, needing to regenerate the state dump before every little test. This is going to surprise solidity developers as its a very non standard flow. The abstractions should be set up so that whatever solidity file `op-deployer` calls into to do a deployment can be called in memory during a `foundry` invocation.", "2024-12-10T16:20:33Z", "2024-12-10T16:20:33Z", "tynes", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6XihgW", "I_kwDODjvEJM6ivr2g", "I tend to agree that resorting to FFI to call a go util is poor devex. Something that I'm starting to land on that might be a good middle ground: \n\nMy thinking is that `op-deployer` appears to have standardized on this [`WithScript()`](https://github.com/ethereum-optimism/optimism/blob/upgrade-against-registry-2/op-deployer/pkg/deployer/opcm/implementations.go#L80) method that takes a `DeployFoo` solidity script. Those scripts have to satisfy the same constraints of the existing scripts (ie. `DeploySuperchain.s.sol`, `DeployImplementations.s.sol` etc.) scripts. \n\nSo what we can do is implement similar logic in solidity, which handles those scripts in a similar way by:\n\n1. etching the I/O contracts (DeployFooInput /DeployFooOutput )\n2. reading the intent file to get and pass the intput to DeployFooInput\n3. running the DeployFoo script\n4. reading the output from DeployFooOutput\n5. labelling the resulting addresses\n\nThis keeps the deploy/upgrade dev fully in solidity, while making it safe and easy to add new bootstrap commands.", "2024-12-13T21:27:25Z", "2024-12-13T21:27:25Z", "maurelian", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6X1ve0", "I_kwDODjvEJM6ivr2g", "> question: can we kill hundreds of lines of code by deleting the whole set interface and just use foundry vm.store + deterministic unstructured storage? seems overly complicated to need to write all this boilerplate in solidity. you can just write directly to StateDB in go and use vm.store in solidity if necessary\n\nIt could be way less work than it is to implement similar logic in solidity", "2024-12-17T03:49:36Z", "2024-12-17T03:49:36Z", "tynes", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6YbxhF", "I_kwDODjvEJM6ivr2g", "I don't think the Solidity should call out to OP Deployer at all. Instead, we should compose the various scripts that OP Deployer uses under the hood to create networks for the purpose of testing.", "2024-12-20T17:19:16Z", "2024-12-20T17:19:16Z", "mslipper", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6iMFnH", "I_kwDODjvEJM6ivr2g", "A comment from @protolambda:\n\n> Can we remove the legacy DeployConfig.s.sol, Deploy.s.sol, Deployer.sol and ChainAssertions.sol, now that OPCM + op-deployer are there to do new deployments with? We haven't ported over all hardfork functionality into these legacy contracts, it's missing Isthmus fully, and the Jovian config changes are in flight, but don't seem worthwhile to backport to this old config system. The L2Genesis.s.sol script uses some workarounds and still kind of relies on a subset of Deployer.sol, but it would be great to clean up. I am happy to help with that if nobody is actively relying on the legacy code there.\n", "2025-03-13T12:23:26Z", "2025-03-13T12:23:26Z", "maurelian", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6nuQuS", "I_kwDODjvEJM6ivr2g", "This issue is a pretty convoluted, I'm going to close it in favor of this one: https://github.com/ethereum-optimism/optimism/issues/15455", "2025-04-17T20:11:05Z", "2025-04-17T20:11:05Z", "maurelian", "2025-08-31 03:29:10"]
["IC_kwDODjvEJM6m6wfX", "I_kwDODjvEJM6bDAus", "Closing this as won't fix - with OPCM preimage oracles wind up being shared far more so there is far less need to deploy a new one.", "2025-04-14T04:06:09Z", "2025-04-14T04:06:09Z", "ajsutton", "2025-08-31 03:29:10"]
["IC_kwDOKIsnqM6nRQxL", "I_kwDOKIsnqM6yoEL2", "@maurelian as I know you're looking into StandardValidator improvements. ", "2025-04-15T15:35:10Z", "2025-04-15T15:35:10Z", "blmalone", "2025-08-31 03:29:10"]
["IC_kwDOKIsnqM6nSe1b", "I_kwDOKIsnqM6yoEL2", "Thanks yes, my plan here is that, for these checks which require more flexibility, I would allow for the expected value to be provided as input to the `validate()` call, and then do option 2: \n\n> E.g. in the above example, the error code PROXYA-10 could be extended to include the actually found owner in parenthesis, like PROXYA-10(0x<actual-address>). This would probably work for most errors and ensure when we expect errors, we also implicitly assert on the alternative value.\n\n", "2025-04-15T16:05:21Z", "2025-04-15T16:05:21Z", "maurelian", "2025-08-31 03:29:10"]
["IC_kwDOKIsnqM6iGfxU", "I_kwDOKIsnqM6tgm9t", "Track tasks [here](https://www.notion.so/oplabs/DO-NOT-SHARE-Superchain-Version-Sync-Tasks-and-Playbook-Coordination-Draft-0-6-197f153ee1628068928dfbf5ace32c31)", "2025-03-13T02:11:59Z", "2025-03-13T02:11:59Z", "pauldowman", "2025-08-31 03:29:10"]
["IC_kwDOKIsnqM6mFi0A", "I_kwDOKIsnqM6tgm9t", "@ControlCplusControlV is this now done or we still have items to cover here?", "2025-04-08T13:37:29Z", "2025-04-08T13:37:29Z", "BlocksOnAChain", "2025-08-31 03:29:10"]
["IC_kwDOKIsnqM6nQHPt", "I_kwDOKIsnqM6tgm9t", "Closing in favour of https://github.com/ethereum-optimism/release-management/issues/109", "2025-04-15T15:11:47Z", "2025-04-15T15:11:47Z", "pauldowman", "2025-08-31 03:29:10"]
["IC_kwDOKIsnqM6iGfr_", "I_kwDOKIsnqM6tgmeL", "Track tasks [here](https://www.notion.so/oplabs/DO-NOT-SHARE-Superchain-Version-Sync-Tasks-and-Playbook-Coordination-Draft-0-6-197f153ee1628068928dfbf5ace32c31)", "2025-03-13T02:11:51Z", "2025-03-13T02:11:51Z", "pauldowman", "2025-08-31 03:29:10"]
["IC_kwDOMMiGhs6l8ggv", "I_kwDOMMiGhs6xdIVD", "Hi @OscBacon I've added this issue to our backlog and we'll keep you posted! Thanks for bringing it to our attention.", "2025-04-07T17:40:22Z", "2025-04-07T17:40:22Z", "fainashalts", "2025-08-31 03:29:22"]
["IC_kwDOMMiGhs6l9ReB", "I_kwDOMMiGhs6xdIVD", "Hey @fainashalts any chance you could prioritize this more please?\nMy project is part of the superchain incubator and this is forcing us for now to stay on three anvil chains since we can't test subscribing to events.\nSince supersim is using anvil under the hood it should be a matter of properly exposing both http and ws RPCs", "2025-04-07T19:09:42Z", "2025-04-07T19:09:42Z", "OscBacon", "2025-08-31 03:29:22"]
["IC_kwDOMMiGhs6l98Ir", "I_kwDOMMiGhs6xdIVD", "@OscBacon yes sorry I wasn't more clear! our team will talk about this later today and figure out priority. \ud83d\ude04 backlog is just a pit stop. thank you for clarifying the need, very helpful for our discussion.", "2025-04-07T19:54:51Z", "2025-04-07T19:54:51Z", "fainashalts", "2025-08-31 03:29:22"]
["IC_kwDOMMiGhs6l_bS6", "I_kwDOMMiGhs6xdIVD", "@OscBacon I believe that websockets should work for all the supersim chains. Do you mind sending me some steps I can follow to reproduce this issue?", "2025-04-07T23:40:00Z", "2025-04-07T23:40:00Z", "tremarkley", "2025-08-31 03:29:22"]
["IC_kwDOMMiGhs6mDhOo", "I_kwDOMMiGhs6xdIVD", "Hey @tremarkley for sure!\n```\nsupersim\n# In another shell\nwscat -c localhost:9546\n```\n\n`wscat` is a nifty too to send out requests to a WS server, like `cat`\nInstall with `npm i -g wscat`\n\nFor comparison, run `wscat -c localhost:8545` to connect to the L1.\n\nAs you will see, expected behavior:\n```\nwscat -c localhost:8545\nConnected (press CTRL+C to quit)\n```\n\nbut with the L2:\n```\nwscat -c localhost:9546\nerror: Unexpected server response: 400\n```", "2025-04-08T10:14:40Z", "2025-04-08T10:16:14Z", "OscBacon", "2025-08-31 03:29:22"]
["IC_kwDOMMiGhs6nhdhx", "I_kwDOMMiGhs6xdIVD", "@OscBacon this fix is included on supersim version 0.1.0-alpha.48", "2025-04-16T19:33:18Z", "2025-04-16T19:33:18Z", "tremarkley", "2025-08-31 03:29:22"]
["IC_kwDOL-xLQ86oN1Vt", "I_kwDOL-xLQ86zhWCZ", "Baseline (April 8)\n<img width=\"646\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/476328a6-0c04-4941-849a-12f6ee0e59a5\" />", "2025-04-22T18:52:47Z", "2025-04-22T18:52:47Z", "scharissis", "2025-08-31 03:31:05"]
["IC_kwDOL-xLQ86oN1rR", "I_kwDOL-xLQ86zhWCZ", "Progress (April 18)\n<img width=\"971\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6a0d43c3-77da-42d5-af6e-0340849b410f\" />", "2025-04-22T18:53:28Z", "2025-04-22T18:53:28Z", "scharissis", "2025-08-31 03:31:05"]
["IC_kwDOL-xLQ86oN-rb", "I_kwDOL-xLQ86zhWCZ", "Update: 85% success rate over the 7d period up till April 22.\n\n<img width=\"985\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eaaadf20-5dde-4419-bd99-28e987c08865\" />\n", "2025-04-22T19:08:36Z", "2025-04-22T19:08:36Z", "scharissis", "2025-08-31 03:31:05"]
["IC_kwDOL-xLQ86oeTJH", "I_kwDOL-xLQ86zhWCZ", "Update: 100% success rate over the 7d period up till April 24.\n\n<img width=\"839\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9f86a373-80f4-44e4-8eee-bcdf89002eea\" />", "2025-04-24T06:26:06Z", "2025-04-24T06:26:06Z", "scharissis", "2025-08-31 03:31:05"]
["IC_kwDOL-xLQ86hETzk", "I_kwDOL-xLQ86sxzn_", "cc: @tessr  (FYI)", "2025-03-05T22:53:43Z", "2025-03-05T22:53:43Z", "scharissis", "2025-08-31 03:31:05"]
["IC_kwDOL-xLQ86oNsW7", "I_kwDOL-xLQ86sxzn_", "Done here: https://github.com/ethereum-optimism/pm/pull/47", "2025-04-22T18:35:09Z", "2025-04-22T18:35:09Z", "scharissis", "2025-08-31 03:31:05"]
["IC_kwDOKSJyfM6nhKe0", "I_kwDOKSJyfM6ybyMF", "Code Change: https://github.com/ethereum-optimism/optimism/pull/15433\nSpecs: https://github.com/ethereum-optimism/specs/pull/665", "2025-04-16T19:02:08Z", "2025-04-16T19:02:08Z", "hamdiallam", "2025-08-31 03:31:05"]
["IC_kwDOKSJyfM6nCMQU", "I_kwDOKSJyfM6yakEe", "From Zain: might make sense to call out ETH as well here.", "2025-04-14T17:14:44Z", "2025-04-14T17:14:44Z", "fainashalts", "2025-08-31 03:31:05"]
["IC_kwDOKSJyfM6YWs6d", "I_kwDOKSJyfM6kBDot", "@mayanksayshi design is needed for this issue when you have bandwidth. Happy to chat more about the flow if you'd like! \ud83d\ude04 ", "2024-12-20T01:35:31Z", "2024-12-20T01:35:31Z", "fainashalts", "2025-08-31 03:31:05"]
["IC_kwDOKSJyfM6Yb9b3", "I_kwDOKSJyfM6kBDot", "> [@mayanksayshi](https://github.com/mayanksayshi) design is needed for this issue when you have bandwidth. Happy to chat more about the flow if you'd like! \ud83d\ude04\n\nHey @fainashalts, I did some early Lofi designs; however, I did them without any detailed requirements, so I am unsure how correct they are. If you can, please review the Lofi designs and let me know what is missing or needs to be corrected. That would be helpful. \n\nhttps://www.figma.com/design/WmHJErF0zjIgoR8IYWOVFT/Dev-Console---CLI-Features-lofi?node-id=6723-4846&t=4v8BYPFQ00xX2TcB-4\n\n\n", "2024-12-20T17:53:28Z", "2024-12-20T17:53:28Z", "mayanksayshi", "2025-08-31 03:31:05"]
["IC_kwDOKSJyfM6ZtLMx", "I_kwDOKSJyfM6kBDot", "Hey @fainashalts @jvmi7 @pharger @nitaliano \n\nPlease see the first version of the Hi-Fi design for the API key functionality. Looking forward to your feedback. \n\nhttps://www.figma.com/design/YCjUV6asLLEna8yISmFLlw/API-Key?node-id=0-1", "2025-01-08T22:04:02Z", "2025-01-08T22:04:02Z", "mayanksayshi", "2025-08-31 03:31:05"]
["IC_kwDOKSJyfM6al_Fg", "I_kwDOKSJyfM6kBDot", "Thanks @mayanksayshi! The design looks great - I just had one question about whether we need to separate out read and write permissions.", "2025-01-15T18:20:35Z", "2025-01-15T18:20:35Z", "fainashalts", "2025-08-31 03:31:05"]
["IC_kwDOLB-lzc6miNMY", "I_kwDOLB-lzc6x-WuH", "makes sense if all errors that have to do with minimum safety check failing have some shared prefix sequence", "2025-04-10T14:47:13Z", "2025-04-10T14:47:13Z", "emhane", "2025-08-31 03:31:11"]
["IC_kwDOLB-lzc6miSkj", "I_kwDOLB-lzc6x-WuH", "the error codes should only be specd for errors related to protocol, i.e. from the permalink errors, leaving out errors such as\nhttps://github.com/ethereum-optimism/optimism/blob/e22cdd9af00698d2fd04683a5bbddd27292c4c98/op-supervisor/supervisor/types/error.go#L36-L37", "2025-04-10T14:49:03Z", "2025-04-10T14:49:22Z", "emhane", "2025-08-31 03:31:11"]
["IC_kwDOLB-lzc6mkVC4", "I_kwDOLB-lzc6x-WuH", "hey @emhane may I take this? I am new to Ethereum-Optimism and specs, but would like to give it a try.would be great if you can share any resources I can use to prepare the specs. Once the specs are done I would also like to take up - [Parse supervisor RPC errors #15671](https://github.com/paradigmxyz/reth/issues/15671)\n\n", "2025-04-10T16:55:16Z", "2025-04-10T16:55:16Z", "07Vaishnavi-Singh", "2025-08-31 03:31:11"]
["IC_kwDOLB-lzc6ml5iQ", "I_kwDOLB-lzc6x-WuH", "check out how it's done here as reference https://ethereum-json-rpc.com/errors @07Vaishnavi-Singh \nadd this new section as a sub heading to\nhttps://github.com/ethereum-optimism/specs/blob/main/specs/interop/supervisor.md#rpc-api\ni.e. after the \"Methods\" section, make an \"Errors\" section", "2025-04-10T19:30:55Z", "2025-04-10T19:30:55Z", "emhane", "2025-08-31 03:31:11"]
["IC_kwDOLB-lzc6mnNpe", "I_kwDOLB-lzc6x-WuH", "@emhane added a PR. Please let me know the suggestions will fix the codes accordingly. ty!", "2025-04-10T22:23:21Z", "2025-04-10T22:23:21Z", "07Vaishnavi-Singh", "2025-08-31 03:31:11"]
["IC_kwDOLB-lzc6okHCT", "I_kwDOLB-lzc6x-WuH", "closed in https://github.com/ethereum-optimism/specs/pull/661", "2025-04-24T15:25:11Z", "2025-04-24T15:25:11Z", "emhane", "2025-08-31 03:31:11"]
["IC_kwDOLB-lzc6mnCaC", "I_kwDOLB-lzc6uY6Sl", "This is important for the audit", "2025-04-10T21:56:26Z", "2025-04-10T21:56:26Z", "tynes", "2025-08-31 03:31:11"]
["IC_kwDOKIsnqM6T8EkQ", "I_kwDOKIsnqM6YuoBq", "Looks like the oz foundry-upgrades package is running the oz-upgrades npm package under the hood https://github.com/OpenZeppelin/openzeppelin-foundry-upgrades/blob/main/src/internal/Core.sol#L371-L376", "2024-11-18T05:37:47Z", "2024-11-18T05:37:47Z", "ElliotFriedman", "2025-08-31 03:31:21"]
["IC_kwDOKIsnqM6hy8qE", "I_kwDOKIsnqM6YuoBq", "Before this, we should consider another check that lives in the monorepo since that is closer to the source and we can improve the storage layout collision detection there", "2025-03-11T14:19:05Z", "2025-03-11T14:19:05Z", "mds1", "2025-08-31 03:31:21"]
["IC_kwDOKIsnqM6k5zkz", "I_kwDOKIsnqM6YuoBq", "Some research added to this Discord thread: https://discord.com/channels/1244729134312198194/1354110492460711986/1354110500484681819 - Implementation still needs to be completed for this issue. ", "2025-03-31T15:35:17Z", "2025-03-31T15:35:17Z", "blmalone", "2025-08-31 03:31:21"]
["IC_kwDOKIsnqM6oCNmp", "I_kwDOKIsnqM6YuoBq", "Some thoughts:\n1. Check if the slot being updated is an implementation address update (3 proxy types)\n2. For the current state diff address, get the implementation contract address (check this is equal to the OLD value in the current state diff)\n3. Use this address to find the old op-contracts release in the superchain-registry\n4. Using the release, grab the storage layout for the old implementation.\n5. Now do the same for the NEW value in the current state diff.\n\n\nWhich storage layout do I use for a task that doesn\u2019t update any implementations? Seems like the storageLayout should be dynamically looked up for each contract. i.e. get the contracts impl then reverse lookup the storage layout that way. If it's not a proxy, then simply try to find the address in the superchain-registry standard versions [toml file](https://github.com/ethereum-optimism/superchain-registry/blob/main/validation/standard/standard-versions-mainnet.toml). In failing both of those options, we can default to the latest storageLayout on main. I believe it's safe to do this e.g. contracts like [GnosisSafe](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts-bedrock/snapshots/storageLayout/GnosisSafe.json). ", "2025-04-21T18:05:47Z", "2025-04-21T18:05:56Z", "blmalone", "2025-08-31 03:31:21"]
["IC_kwDOMMiGhs6oMzoa", "I_kwDOMMiGhs6zgDo6", "> From what I understand, the websocket you expose for op chain a and b doesn't fully behave like the underlying anvil rpc websockets\n\nhey @OscBacon can you elaborate more on what you mean by this and what the expected behavior is vs the behavior you're seeing?\n", "2025-04-22T16:54:18Z", "2025-04-22T16:54:18Z", "tremarkley", "2025-08-31 03:31:41"]
["IC_kwDOMMiGhs6oM14j", "I_kwDOMMiGhs6zgDo6", "Hey, expected behavior is as documented here: https://docs.alchemy.com/reference/eth-subscribe\nIf I run `wscat -c ws://localhost:9545` and then a subscribe method, I should get a subscription ID and then periodically receive the logs that match my filter.\nThe rpc does not send me any logs even though the contract has emitted the event I subscribed to\nAFK but can send an example subscription later ", "2025-04-22T16:58:14Z", "2025-04-22T16:58:14Z", "OscBacon", "2025-08-31 03:31:41"]
["IC_kwDOMMiGhs6oV_MS", "I_kwDOMMiGhs6zgDo6", "To follow up,  after running `wscat`, I should be able to run this, and get event logs when my message has been relayed.\nNote that when bridging from op chain a or b to op chain L1, this does work. Only op chain a and b are not behaving properly\n```\n{\"jsonrpc\":\"2.0\",\"id\":3355,\"method\":\"eth_subscribe\",\"params\":[\"logs\",{\"address\":\"0x4200000000000000000000000000000000000023\",\"topics\":[\"0x5948076590932b9d173029c7df03fe386e755a61c86c7fe2671011a2faa2a379\",null,null,\"YOUR_MESSAGE_ID\"]}]}\n```", "2025-04-23T13:36:34Z", "2025-04-23T13:37:53Z", "OscBacon", "2025-08-31 03:31:41"]
["IC_kwDOMMiGhs6oWH05", "I_kwDOMMiGhs6zgDo6", "This is how `eth_subscribe` is expected to work. For example here I am watching a Hyperlane event instead:\n```\n> {\"jsonrpc\":\"2.0\",\"id\":11644,\"method\":\"eth_subscribe\",\"params\":[\"logs\",{\"address\":\"0x51ed66FdF1072f9C2bD5d8858675f46BA4a6EfcB\",\"topics\":[\"0x1cae38cdd3d3919489272725a5ae62a4f48b2989b0dae843d3c279fee18073a9\",null]}]}\n< {\"jsonrpc\":\"2.0\",\"id\":11644,\"result\":\"0x7079426a526342566a4c366279396847\"}\n< {\"jsonrpc\":\"2.0\",\"method\":\"eth_subscription\",\"params\":{\"subscription\":\"0x7079426a526342566a4c366279396847\",\"result\":{\"address\":\"0x51ed66fdf1072f9c2bd5d8858675f46ba4a6efcb\",\"topics\":[\"0x1cae38cdd3d3919489272725a5ae62a4f48b2989b0dae843d3c279fee18073a9\",\"0x90fc9f32908f44d508a261321ddc1d866f968827b4663dcbf31e078c59ae4460\"],\"data\":\"0x\",\"blockHash\":\"0x70f4c48a5c9fba17164ef6be89aa8b740832249ce49cdc864b3f89f1c5fde53e\",\"blockNumber\":\"0x451\",\"blockTimestamp\":\"0x6808ef56\",\"transactionHash\":\"0x667fd8c542fbd825ca4c79848859b12823633a07952ca3004073d985b6cb7237\",\"transactionIndex\":\"0x0\",\"logIndex\":\"0x1\",\"removed\":false}}}\n```", "2025-04-23T13:48:21Z", "2025-04-23T13:48:21Z", "OscBacon", "2025-08-31 03:31:41"]
["IC_kwDOMMiGhs6n16tI", "I_kwDOMMiGhs6zI1iM", "Note, this is specifically when running `./scoutup supersim` at the same time, and having the interop autorelay on", "2025-04-18T17:53:36Z", "2025-04-18T17:53:36Z", "OscBacon", "2025-08-31 03:31:41"]
["IC_kwDOMMiGhs6oN5X8", "I_kwDOMMiGhs6zI1iM", "@OscBacon fix for this is available in supersim [0.1.0-alpha.49](https://github.com/ethereum-optimism/supersim/releases/tag/0.1.0-alpha.49)", "2025-04-22T19:00:15Z", "2025-04-22T19:00:15Z", "tremarkley", "2025-08-31 03:31:41"]
["IC_kwDOMMiGhs6oO_Uz", "I_kwDOMMiGhs6zI1iM", "Thanks @tremarkley ! Was using your branch in the meantime ", "2025-04-22T21:15:45Z", "2025-04-22T21:15:45Z", "OscBacon", "2025-08-31 03:31:41"]
["IC_kwDODjvEJM6oyNZF", "I_kwDODjvEJM60FeIc", "<img width=\"736\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2d46d3db-02a3-4920-8db4-f15ea567bed2\" />\n\nok, it is resolve by adding above code. \n\nIt is caused by doing the `synctarget ` from few days ago", "2025-04-26T01:57:47Z", "2025-04-26T01:57:47Z", "unclezoro", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6n0z7d", "I_kwDODjvEJM6zFmTh", "I wonder if this is the reason why https://github.com/ethereum-optimism/optimism/pull/15419 needed to be reverted. cc @sebastianst @geoknee ", "2025-04-18T15:24:32Z", "2025-04-18T15:24:32Z", "tynes", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6n00EZ", "I_kwDODjvEJM6zFmTh", "I am also confused as to why this was not caught on devnets/testing", "2025-04-18T15:24:56Z", "2025-04-18T15:24:56Z", "tynes", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oJXfm", "I_kwDODjvEJM6zFmTh", "Here in op-geth, when creating the genesis block we fall back to using the empty withdrawals hash if there is no contract storage:\nhttps://github.com/ethereum-optimism/op-geth/blob/c31cb465d1cec1f00d4c47cae51b7b5fab613b3b/core/genesis.go#L646-L653\n\nThis is according to [spec.](https://specs.optimism.io/protocol/isthmus/exec-engine.html#backwards-compatibility-considerations) . Although this rule does make things difficult because it means Isthmus is not equivalent to a non-nil, non-empty withdrawals hash. And we are clearly leaning on this incorrect assumption in the p2p part of the stack.\n\nIf the empty withdrawals hash were to be used at genesis with isthmus active, we would then use BlockV3 when marshalling the execution payload as described above. \n[According to the spec, we should use BlockV4 for all Isthmus blocks](https://specs.optimism.io/protocol/isthmus/exec-engine.html#genesis-block), so this is a spec violation. However,  think it may be simpler to change the spec to use BlockV3 for edge cases such as this.\n\nI have been unable to reproduce this so far, since on kurtosis as well as for out devnets when activating Isthmus at genesis, the `L2toL1MessagePasser` has a non nil, non empty value `0x8ed4baae3a927be3dea54996b4d5899f8c01e7594bf50b17dc1e741388ce3d12` (this is as expected, it stores the proxy owner and the implementation address).\n\nEDIT: to clarify, it seems impossible to get an empty storage object for this contract when rolling an OPStack chain in the usual way. Therefore it seems impossible to run into this bug (we will probably still address it, but want to understand how you triggered it). \n\n@kien-rise can you provide any more details to allow us to reproduce? Did you do any modifications to the genesis allocations?", "2025-04-22T11:28:32Z", "2025-04-22T17:28:01Z", "geoknee", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oR47F", "I_kwDODjvEJM6zFmTh", "> [@kien-rise](https://github.com/kien-rise) can you provide any more details to allow us to reproduce? Did you do any modifications to the genesis allocations?\n\nThanks for the insightful comment! In RISE, we're currently building a custom Execution Layer on top of Reth. At the moment, we're hard-coding the `withdrawals_root` to `EMPTY_ROOT_HASH`, which works correctly for the range `>= Canyon, < Isthmus`.\n\nI will change the logic in our EL to: `if >= Isthmus { calculate withdrawals_root } else { EMPTY_ROOT_HASH }`", "2025-04-23T06:57:40Z", "2025-04-23T06:57:40Z", "kien-rise", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6ojtDA", "I_kwDODjvEJM6zFmTh", "Closing this as it should only occur with custom execution clients which break the protocol rules. We clarified the spec here https://github.com/ethereum-optimism/specs/pull/676", "2025-04-24T14:52:15Z", "2025-04-24T14:52:15Z", "geoknee", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oFwdk", "I_kwDODjvEJM6y5JGe", "Not doing this now.", "2025-04-22T05:03:52Z", "2025-04-22T05:03:52Z", "ajsutton", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oKgFl", "I_kwDODjvEJM6y5G-E", "Closing this because instead of this we're going to select the output roots and generate the super root in a permissioned way, state it for governance approval, and provide a script to verify it:  https://github.com/ethereum-optimism/optimism/issues/15491", "2025-04-22T13:24:46Z", "2025-04-22T13:24:46Z", "pauldowman", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oR0Xg", "I_kwDODjvEJM6yBzmB", "Thanks for this @teddyknox !\n\nThe netchef generator largely uses devnet-sdk for generating the environment JSON. But, because we were barely using it until now, the devnet-sdk dependency wasn't being kept up to date with the latest (hence the divergence).\nNow we'll be starting to use it - often. So we expect them to remain in sync and, if not, we'll quickly notice and remediate it.\n\nIf more work is required, we (@sigma , @zhwrd and I) don't believe a JSON Schema checker is the way to go. But we can chat about it if/when the time comes.\n\n(Here's [a recent PR in which the dependency is updated](https://github.com/ethereum-optimism/infrastructure-services/pull/420/files#diff-c7d053389aaadf47a2764e0b630596b328db831b2eab3fe568f68cbcd3e8b239L9-R9) (amongst other things))", "2025-04-23T06:48:34Z", "2025-04-23T06:48:34Z", "scharissis", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oKfsN", "I_kwDODjvEJM6xqOpl", "Closing this and it's sub-issues because instead of this we're going to select the output roots and generate the super root in a permissioned way, state it for governance approval, and provide a script to verify it:  https://github.com/ethereum-optimism/optimism/issues/15491", "2025-04-22T13:24:10Z", "2025-04-22T13:24:10Z", "pauldowman", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oJyCX", "I_kwDODjvEJM6xawWh", "Closing this as duplicate. I linked the issues with the right parent-issue and context (moved over some context from description we put together in the last call). \n\nThe integrate test-sequencer PR should link to issue https://github.com/ethereum-optimism/optimism/issues/15330\n", "2025-04-22T12:13:38Z", "2025-04-22T12:13:38Z", "protolambda", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6miVyt", "I_kwDODjvEJM6xCHAg", "@Inphi - can you check if the PR above(https://github.com/ethereum-optimism/optimism/pull/15249) is the only thing we need to close this issue? ", "2025-04-10T14:50:12Z", "2025-04-10T14:50:12Z", "BlocksOnAChain", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6k7EcR", "I_kwDODjvEJM6wfXQC", "> Some work should be done to determine the feasibility of all supervisor functionality being supported with this backend. The minimal amount of functionality that should be supported are the RPC endpoints `supervisor_checkMessagesV2` and `supervisor_checkAccessList`. This would enable the alternative backend to be used at transaction ingress. It may be arbitrarily resource intensive to do finality using the alternative backend.\n\nWe decided that the MVP is just supporting `supervisor_checkAccessList`", "2025-03-31T17:47:39Z", "2025-03-31T17:47:39Z", "tynes", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6ob6zV", "I_kwDODjvEJM6v4O5i", "Closing this - didn't get as much done as I would have liked but have well and truly exceeded the time box for this work. The sys go work by Proto has improved the usability of testing a lot to unblock a lot of the issues that prevented progress on this spike and as part of that a DSL/testlib has been started. It can keep being fleshed out as we continue building tests.", "2025-04-24T01:01:43Z", "2025-04-24T01:01:43Z", "ajsutton", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6kfrQT", "I_kwDODjvEJM6vz84k", "Running `just isthmus-devnet` failed with:\n```\nAdding service with name 'fileserver' and image 'nginx:latest'\n2025/03/28 09:06:32 Error: error deploying environment: error deploying fileserver: error deploying kurtosis package: execution error: An error occurred executing instruction (number 3) at github.com/ethereum-optimism/optimism/kurtosis-devnet/fileserver/main.star[23:21]:\nadd_service(name=\"fileserver\", config=ServiceConfig(image=\"nginx:latest\", ports={\"http\": PortSpec(number=80)}, files={\"/content\": \"fileserver-content\", \"/etc/nginx/conf.d\": \"fileserver-nginx-conf\"}, cmd=[\"nginx\", \"-g\", \"daemon off;\"]))\n --- at /home/circleci/project/core/server/api_container/server/startosis_engine/startosis_executor.go:157 (sendErrorAndFail) ---\nCaused by: Unexpected error occurred starting service 'fileserver'\n --- at /home/circleci/project/core/server/api_container/server/startosis_engine/kurtosis_instruction/add_service/add_service.go:212 (AddServiceCapabilities.Execute) ---\nCaused by: An error occurred starting services in enclave '5089e11a09ce48e69050107a46899583' with the following service ids: map[03d1a368989d4fb089d7f094cdda1d23:0x40208aa168]\n --- at /home/circleci/project/container-engine-lib/lib/backend_impls/metrics_reporting/metrics_reporting_kurtosis_backend.go:279 (MetricsReportingKurtosisBackend.StartRegisteredUserServices) ---\nCaused by: The user services can't be started because no logs collector is running for it to send logs to.\n --- at /home/circleci/project/container-engine-lib/lib/backend_impls/docker/docker_kurtosis_backend/docker_kurtosis_backend.go:229 (DockerKurtosisBackend.StartRegisteredUserServices) ---\nexit status 1\n```\n\nbut then after I ran `kurtosis enclave rm isthmus-devnet --force` it started working.", "2025-03-27T23:15:35Z", "2025-03-27T23:15:35Z", "ajsutton", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6k1oia", "I_kwDODjvEJM6vz84k", "**Environment**\nLocal / Laptop (Apple M4 Max, Sequoia 15.3.2)\n\n**Commit**\ndevelop @ 7e063d29b2b493a3de83f8294ef9a48760178998 (March 31)\n\n**Error log**\n```\ncd kurtosis-devnet\njust isthmus-devnet\n```\n\n```\nPrinting a message\nLaunching CL network\n\nAdding service with name 'cl-1-teku-geth' and image 'consensys/teku:latest'\n2025/03/31 16:25:28 Error: error deploying environment: error deploying kurtosis package: execution error: An error occurred executing instruction (number 29) at github.com/ethpandaops/ethereum-package/src/cl/teku/teku_launcher.star[79:38]:\nadd_service(name=\"cl-1-teku-geth\", config=ServiceConfig(image=\"consensys/teku:latest\", ports={\"http\": PortSpec(number=4000, transport_protocol=\"TCP\", application_protocol=\"http\"), \"metrics\": PortSpec(number=8008, transport_protocol=\"TCP\", application_protocol=\"http\"), \"tcp-discovery\": PortSpec(number=9000, transport_protocol=\"TCP\", application_protocol=\"\"), \"udp-discovery\": PortSpec(number=9000, transport_protocol=\"UDP\", application_protocol=\"\")}, public_ports={}, files={\"/jwt\": \"jwt_file\", \"/network-configs\": \"el_cl_genesis_data\", \"/validator-keys\": \"1-teku-geth-0-63\"}, cmd=[\"--logging=INFO\", \"--log-destination=CONSOLE\", \"--network=/network-configs/config.yaml\", \"--data-path=/data/teku/teku-beacon-data\", \"--data-storage-mode=ARCHIVE\", \"--p2p-enabled=true\", \"--p2p-peer-lower-bound=1\", \"--p2p-advertised-ip=KURTOSIS_IP_ADDR_PLACEHOLDER\", \"--p2p-discovery-site-local-addresses-enabled=true\", \"--p2p-port=9000\", \"--rest-api-enabled=true\", \"--rest-api-docs-enabled=true\", \"--rest-api-interface=0.0.0.0\", \"--rest-api-port=4000\", \"--rest-api-host-allowlist=*\", \"--data-storage-non-canonical-blocks-enabled=true\", \"--ee-jwt-secret-file=/jwt/jwtsecret\", \"--ee-endpoint=http://{{kurtosis:6f741702ab3849228e589dda98c712e4:ip_address.runtime_value}}:8551\", \"--metrics-enabled\", \"--metrics-interface=0.0.0.0\", \"--metrics-host-allowlist=*\", \"--metrics-categories=BEACON,PROCESS,LIBP2P,JVM,NETWORK,PROCESS\", \"--metrics-port=8008\", \"--ignore-weak-subjectivity-period-enabled=true\", \"--initial-state=/network-configs/genesis.ssz\", \"--validator-keys=/validator-keys/teku-keys:/validator-keys/teku-secrets\", \"--validators-proposer-default-fee-recipient=0x8943545177806ED17B9F23F0a21ee5948eCaa776\", \"--validators-graffiti=1-geth-teku\"], env_vars={}, private_ip_address_placeholder=\"KURTOSIS_IP_ADDR_PLACEHOLDER\", ready_conditions=ReadyCondition(recipe=GetHttpRequestRecipe(port_id=\"http\", endpoint=\"/eth/v1/node/health\"), field=\"code\", assertion=\"IN\", target_value=[200, 206], timeout=\"15m\"), labels={\"ethereum-package.client\": \"teku\", \"ethereum-package.client-image\": \"consensys-teku_latest\", \"ethereum-package.client-type\": \"beacon\", \"ethereum-package.connected-client\": \"geth\", \"ethereum-package.sha256\": \"\"}, user=User(uid=0, gid=0), tolerations=[], node_selectors={}))\n --- at /home/circleci/project/core/server/api_container/server/startosis_engine/startosis_executor.go:157 (sendErrorAndFail) ---\nCaused by: Unexpected error occurred starting service 'cl-1-teku-geth'\n --- at /home/circleci/project/core/server/api_container/server/startosis_engine/kurtosis_instruction/add_service/add_service.go:212 (AddServiceCapabilities.Execute) ---\nCaused by: An error occurred waiting for all TCP and UDP ports to be open for service 'cl-1-teku-geth' with private IP '172.16.0.12'; this is usually due to a misconfiguration in the service itself, so here are the logs:\n== SERVICE 'cl-1-teku-geth' LOGS ===================================\n#\n# A fatal error has been detected by the Java Runtime Environment:\n#\n#  SIGILL (0x4) at pc=0x0000ffff78467c5c, pid=7, tid=23\n#\n# JRE version:  (21.0.6+7) (build )\n# Java VM: OpenJDK 64-Bit Server VM (21.0.6+7-LTS, mixed mode, tiered, compressed oops, compressed class ptrs, g1 gc, linux-aarch64)\n# Problematic frame:\n# j  java.lang.System.registerNatives()V+0 java.base\n#\n# No core dump will be written. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n#\n# An error report file with more information is saved as:\n# /opt/teku/hs_err_pid7.log\n[0.017s][warning][os] Loading hsdis library failed\n#\n# The crash happened outside the Java Virtual Machine in native code.\n# See problematic frame for where to report the bug.\n#\n\n== FINISHED SERVICE 'cl-1-teku-geth' LOGS ===================================\n --- at /home/circleci/project/core/server/api_container/server/service_network/default_service_network.go:1016 (DefaultServiceNetwork.startRegisteredService) ---\nCaused by: An error occurred while waiting for all TCP and UDP ports to be open\n --- at /home/circleci/project/core/server/api_container/server/service_network/default_service_network.go:1397 (waitUntilAllTCPAndUDPPortsAreOpen) ---\nCaused by: Unsuccessful ports check for IP '172.16.0.12' and port spec '{privatePortSpec:0x4000b71cb0}', even after '240' retries with '500' milliseconds in between retries. Timeout '2m0s' has been reached\n --- at /home/circleci/project/core/server/api_container/server/service_network/default_service_network.go:1427 (waitUntilPortIsOpenWithTimeout) ---\nCaused by: An error occurred while calling network address '172.16.0.12:8008' with port protocol 'TCP' and using time out '200ms'\n --- at /home/circleci/project/core/server/api_container/server/service_network/default_service_network.go:1459 (scanPort) ---\nCaused by: dial tcp 172.16.0.12:8008: connect: no route to host\nexit status 1\nerror: Recipe `devnet` failed with exit code 1\n```", "2025-03-31T08:39:21Z", "2025-03-31T08:39:34Z", "scharissis", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6k-Har", "I_kwDODjvEJM6vz84k", "`SIGILL` usually indicates that the processor was reported incorrectly somehow and so the optimised version of blst was used instead of the one that's compatible with older CPUs.  https://docs.teku.consensys.io/how-to/troubleshoot/network#teku-crashes-with-sigill has a method to force it to use the compatibility library - the performance difference won't be noticeable with the very small number of validators in our test setup.", "2025-04-01T00:51:04Z", "2025-04-01T00:51:04Z", "ajsutton", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oEn-5", "I_kwDODjvEJM6vp_g4", "Reopening as there are still two TODOs in the code for this issue:\n```\nethereum-optimism/optimism #15027        | devnet-sdk: add minimal in-process Faucet implementation          | devnet-sdk/devstack/sysgo/system.go:82            \nethereum-optimism/optimism #15027        | devnet-sdk: add minimal in-process Faucet implementation          | devnet-sdk/devstack/sysgo/system.go:87            \n```", "2025-04-22T00:45:11Z", "2025-04-22T00:45:11Z", "ajsutton", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6omgi3", "I_kwDODjvEJM6vp_g4", "https://github.com/ethereum-optimism/optimism/pull/15556", "2025-04-24T19:34:49Z", "2025-04-24T19:34:49Z", "protolambda", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6mFkFM", "I_kwDODjvEJM6q6K7c", "Hey @pauldowman @ajsutton this issue is \"in-progress\" status for a while now. Should we move it to reviews or maybe create sub-issue to track the work in a more granular way for E2E testing for super-cannon games?", "2025-04-08T13:39:14Z", "2025-04-08T13:39:14Z", "BlocksOnAChain", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6mKAU4", "I_kwDODjvEJM6q6K7c", "Adrian is away this week, let's wait for him to return.", "2025-04-08T20:55:07Z", "2025-04-08T20:55:07Z", "pauldowman", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6oJZ33", "I_kwDODjvEJM6onMm0", "The tx-endpoints are now in #14126 \nClosing this, the test-sequencer does have a pretty extensive RPC already.\n", "2025-04-22T11:32:46Z", "2025-04-22T11:32:46Z", "protolambda", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6hlTr7", "I_kwDODjvEJM6onL6k", "Fixed by https://github.com/ethereum-optimism/optimism/pull/14604", "2025-03-10T15:05:22Z", "2025-03-10T15:05:38Z", "protolambda", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6jjqd-", "I_kwDODjvEJM6onL6k", "This makes no sense until the devnet-SDK is in a good place. I'm going to pull this out of RC-Alpha, and put it in Beta, where I would like to make the full devnet-SDK and test-sequencer a blocker.\n", "2025-03-21T17:39:54Z", "2025-03-21T17:39:54Z", "protolambda", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6Za7o1", "I_kwDODjvEJM6gdUNY", "Sharing Product context on the importance of OPCM:\n\n\n**Problem:**\nThere are two major pain points with upgrading the L1 contracts for the chains in the Superchain\n1. Because many chains in the superchain currently have different versions of their L1 contracts, upgrading contracts requires a time consuming process of tracking every different version of the contracts for different chains and handling each upgrade differently.\nIf we do nothing, the contract changes for Isthmus would force us to perform the same time consuming manual tasks to track down all versions of the contracts and require more work to get all contracts to the same version.\n2. If we keep adding chains to the SC with our upgrades process as-is, there is an enormous amount of increased manual labor for the OP Labs security team and for the Security Council. This includes manual validation of every storage slot, for every upgrade, for every chain\u2014along with the ability to track any subtle variations that may exist for chains that are on different versions. This creates a lot of surface area for something to go wrong. \n**Solution**\n1. OPCM upgrades will allow us to easily get all chains in the Superchain to the same version of the contracts with the Isthmus upgrade(although some chains *will* be permissioned vs. permissionless), making this version hell a thing of the past.\n2. OPCM streamlines the security council signing process \n\n**Risks**\nIf we do not thoroughly test out the OPCM upgrade for Isthmus on devnets before the Isthmus testnet upgrade we could risk errors in the upgrade process.\n", "2025-01-06T21:39:20Z", "2025-01-06T21:39:20Z", "K-Ho", "2025-08-31 03:32:01"]
["IC_kwDODjvEJM6nEDze", "I_kwDODjvEJM6gdUNY", "Closing this as the main project is complete. I will keep the sub tasks open for now to help guide maintenance work.", "2025-04-14T20:04:01Z", "2025-04-14T20:04:01Z", "maurelian", "2025-08-31 03:32:01"]
["IC_kwDOL-xLQ86oeVp9", "I_kwDOL-xLQ86zhWpy", "Fail-fast when devnet doesn't spin up:\nhttps://github.com/ethereum-optimism/optimism/pull/15442", "2025-04-24T06:32:08Z", "2025-04-24T06:32:08Z", "scharissis", "2025-08-31 03:33:10"]
["IC_kwDOL-xLQ86oooH9", "I_kwDOL-xLQ86zhWpy", "Fail even faster when devnet doesn't spin up, clearly show whether the devnet as spun up successfully and more targeted notifications:\nhttps://github.com/ethereum-optimism/optimism/pull/15564", "2025-04-25T02:12:31Z", "2025-04-25T02:12:31Z", "scharissis", "2025-08-31 03:33:10"]
["IC_kwDOL-xLQ86izwyX", "I_kwDOL-xLQ86rgF8S", "Started a draft here: https://www.notion.so/oplabs/Acceptance-Criteria-176f153ee162802db9aafd717078a2f4", "2025-03-18T02:57:50Z", "2025-03-18T02:57:50Z", "scharissis", "2025-08-31 03:33:10"]
["IC_kwDOL-xLQ86oNxJG", "I_kwDOL-xLQ86rgF8S", "Started in https://github.com/ethereum-optimism/pm/pull/47", "2025-04-22T18:44:13Z", "2025-04-22T18:44:13Z", "scharissis", "2025-08-31 03:33:10"]
["IC_kwDOL-xLQ86o5w-h", "I_kwDOL-xLQ86rgF8S", "The published checklist can be found here: https://devdocs.optimism.io/pm/acceptance-testing/release-checklist.html", "2025-04-28T00:13:33Z", "2025-04-28T00:13:33Z", "scharissis", "2025-08-31 03:33:10"]
["IC_kwDOL-xLQ86jwzQ3", "I_kwDOL-xLQ86qR_LF", "A few to include here: https://www.notion.so/oplabs/Devnet-Spinup-Runbook-17af153ee16280f3b610e808143cd02a#1a6f153ee1628038aab0e6f834830bed", "2025-03-24T09:38:35Z", "2025-03-24T09:38:35Z", "scharissis", "2025-08-31 03:33:10"]
["IC_kwDOKIwiaM6iahzy", "I_kwDOKIwiaM6uEIqU", "Thanks @seromenho, i completely agree with you, we will see to it next week.", "2025-03-14T14:24:16Z", "2025-03-14T14:24:16Z", "krofax", "2025-08-31 04:54:36"]
["IC_kwDOKIwiaM6pW22T", "I_kwDOKIwiaM6uEIqU", "@seromenho i updated the op deployer docs to reflect this, [here](https://docs.optimism.io/operators/chain-operators/tools/op-deployer#understanding-the-intenttoml-fields).", "2025-04-30T09:19:24Z", "2025-04-30T09:19:24Z", "krofax", "2025-08-31 04:54:36"]
["IC_kwDOKIwiaM6p-XyC", "I_kwDOKIwiaM6uEIqU", "@krofax Thanks, it\u2019s looking much better now. I appreciate the updates to the op-deployer documentation!\nThat said, my original request is still partially unanswered. While the op-deployer is well explained, the intent file itself lacks documentation, especially around how to override individual parameters.\n\nFor example, I\u2019d like to override baseFeeVaultMinimumWithdrawalAmount.\nIt would be very helpful if each deploy-config field could be cross-referenced in the docs, either:\n- showing how it can be set via the intent file, or\n- explicitly stating if it can\u2019t be overridden (and the reason)\n\nWhat do you think?\n", "2025-05-05T17:15:39Z", "2025-05-05T17:15:39Z", "seromenho", "2025-08-31 04:54:36"]
["IC_kwDOL-xLQ86qw2eI", "I_kwDOL-xLQ8611Ccy", "https://github.com/ethereum-optimism/optimism/pull/15696", "2025-05-09T02:46:07Z", "2025-05-09T02:46:07Z", "scharissis", "2025-08-31 04:54:36"]
["IC_kwDOL-xLQ86o55nf", "I_kwDOL-xLQ86zhVX_", "NETWORK=`interop-rc-alpha`\nGATE=`base`\n\ncommand:\n```\nDEVNET_ENV_URL=$(realpath ../../infrastructure-services/netchef/testdata/interop-permissionless/outputs/kustomize/op-acceptor/oplabs-dev-infra/interop-rc-alpha-op-acceptor/devnet-env.json) \\\n  go run cmd/main.go \\\n  --testdir ../../optimism/op-acceptance-tests/tests \\\n  --gate base \\\n  --validators ../../optimism/op-acceptance-tests/acceptance-tests.yaml \\\n  --log.level INFO\n```\n\noutput:\n<img width=\"918\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2740e3fc-8f41-4eae-b4f2-d5a6b106f7dd\" />", "2025-04-28T00:57:46Z", "2025-04-28T00:57:46Z", "scharissis", "2025-08-31 04:54:36"]
["IC_kwDOL-xLQ86o56Wg", "I_kwDOL-xLQ86zhVX_", "NETWORK=`interop-rc-alpha`\nGATE=`interop`\n\n\ncommand:\n```\nDEVNET_ENV_URL=$(realpath ../../infrastructure-services/netchef/testdata/interop-permissionless/outputs/kustomize/op-acceptor/oplabs-dev-infra/interop-rc-alpha-op-acceptor/devnet-env.json) \\\n  go run cmd/main.go \\\n  --testdir ../../optimism/op-acceptance-tests/tests \\\n  --gate interop \\\n  --validators ../../optimism/op-acceptance-tests/acceptance-tests.yaml \\\n  --log.level INFO\n```\n\noutput:\n\n<img width=\"1370\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9e450274-7233-475f-98c1-6e51da02c1a6\" />\n\n\nerrors:\n```\n\u251c\u2500\u2500 Test: github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop (14.3s) [status=fail]\n\u2502       \u2514\u2500\u2500 Error: TestSystemWrapETH: === RUN   TestSystemWrapETH\n    systest.go:185: precondition not met: failed to acquire l2 wallet: no available wallet with balance of at least of 1000000000000000000\n```\n```\nTestInteropSystemNoop: === RUN   TestInteropSystemNoop\n    systest.go:142: precondition not met: interop test requested, but system is not an interop system\n--- FAIL: TestInteropSystemNoop (0.00s)\n```\n\nFull log attached.\n\n[inrerop-rc-alpha-280425.log](https://github.com/user-attachments/files/19933718/inrerop-rc-alpha-280425.log)", "2025-04-28T01:01:23Z", "2025-04-28T03:41:42Z", "scharissis", "2025-08-31 04:54:36"]
["IC_kwDOL-xLQ86pHbmf", "I_kwDOL-xLQ86zhVX_", "For the Acceptance Tests to pass on the persistent devnets we will need:\n- The new faucet deployed\n- The new faucet enabled\n- The devnet-sdk tests to be re-written to use the new frontend/interface which allows them to use the faucet and fund wallets\n\nThis work is being undertaken by Proto:\n- ethereum-optimism/optimism#15574\n- ethereum-optimism/optimism#15267", "2025-04-29T02:47:52Z", "2025-04-29T02:47:52Z", "scharissis", "2025-08-31 04:54:36"]
["IC_kwDOL-xLQ86qw1Pd", "I_kwDOL-xLQ86rhVeF", "op-acceptor can be pointed at a netchef-produced devnet-env.json.", "2025-05-09T02:41:28Z", "2025-05-09T02:41:28Z", "scharissis", "2025-08-31 04:54:36"]
["IC_kwDOFpg0Ns6rgw9j", "I_kwDOFpg0Ns62a3pm", "@bstchow Thanks for flagging. Looks like this is because of the \u20ae character (Tugrik sign) in USD\u20ae0 that was added in a separate, unrelated PR.", "2025-05-13T18:04:24Z", "2025-05-13T18:04:24Z", "wbnns", "2025-08-31 04:54:37"]
["IC_kwDOKSJyfM6j20tP", "I_kwDOKSJyfM6ve7Ll", "Draft notion doc on a rough design: https://www.notion.so/oplabs/Incentivized-Message-Relays-1b2f153ee162800e9f36ceab34efa7d9?pvs=4\n\nTLDR: async settlement framework based on gas receipts emitted by the L2ToL2CDM", "2025-03-24T18:29:32Z", "2025-03-24T18:30:06Z", "hamdiallam", "2025-08-31 04:54:39"]
["IC_kwDOKSJyfM6nfYOv", "I_kwDOKSJyfM6ve7Ll", "Design doc on the messenger changes: https://github.com/ethereum-optimism/design-docs/pull/266", "2025-04-16T15:54:31Z", "2025-04-16T15:54:31Z", "hamdiallam", "2025-08-31 04:54:39"]
["IC_kwDOKSJyfM6r7rn6", "I_kwDOKSJyfM6mV0JV", "done", "2025-05-15T17:20:57Z", "2025-05-15T17:20:57Z", "hamdiallam", "2025-08-31 04:54:39"]
["IC_kwDOKSJyfM6r7rrh", "I_kwDOKSJyfM6mV0IB", "done", "2025-05-15T17:21:02Z", "2025-05-15T17:21:02Z", "hamdiallam", "2025-08-31 04:54:39"]
["IC_kwDOKSJyfM6r7rPU", "I_kwDOKSJyfM6mUK4O", "Not needed as viem has support", "2025-05-15T17:20:18Z", "2025-05-15T17:20:18Z", "hamdiallam", "2025-08-31 04:54:39"]
["IC_kwDOKSJyfM6YWc2X", "I_kwDOKSJyfM6kAw3a", "@nitaliano to add additional details to this issue.", "2024-12-20T00:37:07Z", "2024-12-20T00:37:07Z", "fainashalts", "2025-08-31 04:54:39"]
["IC_kwDOKSJyfM6gIfZ6", "I_kwDOKSJyfM6kAw3a", "A couple other comments on this one, additional sdk surface:\n\n- [ ] Bring down https://sdk.optimism.io/\n- [ ] Update the npm page saying the optimism sdk has been deprecated: https://www.npmjs.com/package/@eth-optimism/sdk\n\nWe also need help addressing distribution our fork of viem because upstream viem doesn't have have some optimism specific functionality. How do we differentiate and get these releases into our developers hands?", "2025-02-27T01:31:14Z", "2025-02-27T01:31:14Z", "sbvegan", "2025-08-31 04:54:39"]
["IC_kwDOKSJyfM6gKYZV", "I_kwDOKSJyfM6kAw3a", "> because upstream viem doesn't have have some optimism specific functionality.\n\nFeel free to create PRs to Viem. :)", "2025-02-27T07:00:22Z", "2025-02-27T07:00:22Z", "jxom", "2025-08-31 04:54:39"]
["IC_kwDOL-xLQ86sD_fT", "I_kwDOL-xLQ862UEU1", "Fixed by https://github.com/ethereum-optimism/optimism/pull/15970", "2025-05-16T13:27:21Z", "2025-05-16T13:27:21Z", "scharissis", "2025-08-31 04:54:40"]
["IC_kwDOH2Qg5s6sHRVU", "I_kwDOH2Qg5s6246d_", "op-geth doesn't have a rate limit built in.  If you're using the op-mainnet public RPC endpoint, that is not intended for production or heavy usage and you should migrate to an alternate provider or run your own nodes.  If you're already using another rpc provider please contact them.", "2025-05-16T20:07:37Z", "2025-05-16T20:07:37Z", "ajsutton", "2025-08-31 04:54:43"]
["IC_kwDOH2Qg5s6rctmV", "I_kwDOH2Qg5s61sOwe", "I propose that we merge just all at once, so we change the targets of the `.11` PRs to the trunk branches when all is reviewed and approved.\n\nThe current revisions are running as a Sepolia sync test and on dev cluster canary nodes.", "2025-05-13T13:01:49Z", "2025-05-13T13:01:49Z", "sebastianst", "2025-08-31 04:54:43"]
["IC_kwDOH2Qg5s6ptnyY", "I_kwDOH2Qg5s60iZ9f", "Seb already made a draft PR taking us all the way to `1.15.10` https://github.com/ethereum-optimism/op-geth/pull/593 ", "2025-05-02T14:12:25Z", "2025-05-02T14:12:25Z", "geoknee", "2025-08-31 04:54:43"]
["IC_kwDOH2Qg5s6nMkKd", "I_kwDOH2Qg5s6vcuEA", "Next release .8 is out https://github.com/ethereum/go-ethereum/releases/tag/v1.15.8", "2025-04-15T13:37:29Z", "2025-04-15T13:37:29Z", "sebastianst", "2025-08-31 04:54:43"]
["IC_kwDODjvEJM6r9vev", "I_kwDODjvEJM62iN8v", "We have a metric for peer \"unbans\" but no metric for peer bans: \n\nhttps://github.com/ethereum-optimism/optimism/blob/b7175209565ac373b07a50351dccbe637ba0ab18/op-node/p2p/gating/expiry.go#L48-L59", "2025-05-15T21:25:45Z", "2025-05-15T21:25:45Z", "geoknee", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r6zzB", "I_kwDODjvEJM62d-PN", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:52:17Z", "2025-05-15T15:52:17Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r6z2u", "I_kwDODjvEJM62d-OR", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:52:22Z", "2025-05-15T15:52:22Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r6z8o", "I_kwDODjvEJM62d-NZ", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:52:31Z", "2025-05-15T15:52:31Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6ri1eJ", "I_kwDODjvEJM62d-Me", "Arguably this is only a load test at scale, and may not pose much of a DoS risk since block building checks may be removed: https://www.notion.so/oplabs/Interop-Workstreams-1ecf153ee1628076b6f9e2dc38006641?pvs=4#1f1f153ee162800b9656d19f3556842b", "2025-05-13T21:50:57Z", "2025-05-13T21:50:57Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r60Mx", "I_kwDODjvEJM62d-Me", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:52:51Z", "2025-05-15T15:52:51Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r60RU", "I_kwDODjvEJM62d9t6", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:52:56Z", "2025-05-15T15:52:56Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r60kb", "I_kwDODjvEJM62d9tC", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:53:19Z", "2025-05-15T15:53:19Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r60U7", "I_kwDODjvEJM62d9rP", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:53:01Z", "2025-05-15T15:53:01Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r60aQ", "I_kwDODjvEJM62d9qS", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:53:07Z", "2025-05-15T15:53:07Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r60eJ", "I_kwDODjvEJM62d9o7", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15941.", "2025-05-15T15:53:12Z", "2025-05-15T15:53:12Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rud0D", "I_kwDODjvEJM62bC5b", "Closing as a pass through FMA docs testing action items shows that the current testing checklist provides greater coverage.", "2025-05-14T18:00:20Z", "2025-05-14T18:00:20Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rsMB_", "I_kwDODjvEJM62azM2", "Decision is to ship a new OPCM with or soon after U17 (depending on when it's ready) that deploys new chains into their own dependency set of 1.  No support for deploying chains into a shared dependency set.", "2025-05-14T14:35:30Z", "2025-05-14T14:35:30Z", "ajsutton", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rwDp-", "I_kwDODjvEJM62aYBI", "See interop-permissionless [inventory](https://github.com/ethereum-optimism/infrastructure-services/blob/main/netchef/testdata/interop-permissionless/inputs/inventory.yaml)", "2025-05-14T20:37:27Z", "2025-05-14T20:37:27Z", "jelias2", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rqILT", "I_kwDODjvEJM62aUA4", "@jelias2 this reads like a DoD - what's the actual work that needs to be completed to get to <3 hours? Is all the required work already covered by the other tasks under the parent ticket?", "2025-05-14T12:05:16Z", "2025-05-14T12:05:16Z", "alfonso-op", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6sGjL6", "I_kwDODjvEJM62aUA4", "Will close it out, I guess its the workstream DoD", "2025-05-16T18:19:22Z", "2025-05-16T18:19:22Z", "jelias2", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rUtAl", "I_kwDODjvEJM62STca", "Fixed by https://github.com/ethereum-optimism/optimism/pull/15828", "2025-05-12T22:21:22Z", "2025-05-12T22:21:22Z", "protolambda", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rPUZo", "I_kwDODjvEJM62OQ2O", "Temporal fix included here: https://github.com/ethereum-optimism/optimism/pull/15840", "2025-05-12T14:58:17Z", "2025-05-12T14:58:17Z", "pcw109550", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6ro-fX", "I_kwDODjvEJM61y02W", "@mbaxter when you say for permissionless chains, `The root is not equal to 0xdead or 0x0`, do you mean that the root is not equal to 0xdead and it is not equal to 0x0 or it is not equal to 0xdead or it is equal to 0x0?", "2025-05-14T10:20:06Z", "2025-05-14T10:20:06Z", "AmadiMichael", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rRJl1", "I_kwDODjvEJM60C0At", "TODO: Enumerate all tests to be migrated.", "2025-05-12T17:32:33Z", "2025-05-12T17:32:33Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rRISC", "I_kwDODjvEJM6zYoyv", "Closing as duplicate of #15846.", "2025-05-12T17:31:06Z", "2025-05-12T17:31:06Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rO7rg", "I_kwDODjvEJM6zYoCh", "Closing as duplicate checklist of #13855 ", "2025-05-12T14:31:15Z", "2025-05-12T14:31:15Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6oNtf-", "I_kwDODjvEJM6zX18q", "kurtosis side: https://github.com/ethpandaops/optimism-package/pull/228", "2025-04-22T18:37:27Z", "2025-04-22T18:37:27Z", "sigma", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6oAlUt", "I_kwDODjvEJM6zJp2m", "@pcw109550 to tackle this down with teddy", "2025-04-21T15:13:21Z", "2025-04-21T15:13:21Z", "pcw109550", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6qmzgk", "I_kwDODjvEJM6zJp2m", "https://github.com/ethereum-optimism/optimism/pull/15720 included the core interop-upgrade test setup and pre/post check split. Any more in-depth pre/post checks I am splitting off into the existing pre/post acceptance test issues. Closing this one.\n\n", "2025-05-08T09:00:47Z", "2025-05-08T09:00:47Z", "protolambda", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6ncZt1", "I_kwDODjvEJM6yxZsC", "This may resolve https://github.com/ethereum-optimism/optimism/issues/15243", "2025-04-16T11:08:46Z", "2025-04-16T11:08:46Z", "pcw109550", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rhi0Y", "I_kwDODjvEJM6x8e8p", "According to @protolambda we may have some supersystem E2E mempool related testing already, so extending that testing may be the easiest way to add coverage here. Race conditions like these are difficult to reproduce.", "2025-05-13T19:24:59Z", "2025-05-13T19:25:26Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rhrUE", "I_kwDODjvEJM6x8V4G", "We implicitly test this in other action/acceptance tests but not directly.", "2025-05-13T19:36:15Z", "2025-05-13T19:36:15Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rO5Rr", "I_kwDODjvEJM6x8M3K", "Closing to use #13855 as the general testing checklist.", "2025-05-12T14:29:26Z", "2025-05-12T14:29:26Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6l6h__", "I_kwDODjvEJM6xcp7A", "Possible duplicate of: https://github.com/ethereum-optimism/optimism/issues/15101\n\nI think it would make sense to focus this issue @sigma created on the kurtosis/devnet-env side.\nAnd focus the issue @pcw109550 created on the frontend integration and sysgo support.\n", "2025-04-07T14:41:42Z", "2025-04-07T14:41:42Z", "protolambda", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6mEl9x", "I_kwDODjvEJM6xcp7A", "Yup, we can see this as a low level implementation for the devnet-sdk to start/stop services. ", "2025-04-08T12:07:57Z", "2025-04-08T12:07:57Z", "pcw109550", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6n0emq", "I_kwDODjvEJM6xcp7A", "re-opening to track missing control plane interface implementation in sysext", "2025-04-18T14:26:27Z", "2025-04-18T14:26:27Z", "sigma", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6r6uS3", "I_kwDODjvEJM6xapCE", "Closing this checklist ticket as all outstanding tasks appear to be done. cc @protolambda ", "2025-05-15T15:44:59Z", "2025-05-15T15:44:59Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6mJMAY", "I_kwDODjvEJM6wRwDu", "@smartcontracts I've added the pull request, hope it's going to be useful and will live up to your expectations ", "2025-04-08T19:09:41Z", "2025-04-08T19:09:41Z", "GarmashAlex", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6krBBF", "I_kwDODjvEJM6wPzpV", "[Here is the PR in rollup-boost](https://github.com/flashbots/rollup-boost/pull/142) which returns status codes from the `/healthz` endpoint.\n\n`/healthz` responses:\n\n200 OK - healthy\n206 Partial Content - l2 creating blocks, builder is down\n503 Service Unavailable - We're not building any blocks", "2025-03-28T22:37:25Z", "2025-03-28T22:37:25Z", "0xForerunner", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6k_vT5", "I_kwDODjvEJM6wPzpV", "As far as configuration goes, I think it makes sense to add a boolean flag like this:\n```\nOP_CONDUCTOR_EXECUTION_ENABLE_HEALTH_CHECK: 'true'\n```\n\nIf there is support for this feature here, I'd be happy to start working on a PR.", "2025-04-01T05:53:49Z", "2025-04-01T05:53:49Z", "0xForerunner", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6olBx6", "I_kwDODjvEJM6wPzpV", "Implements this: https://github.com/ethereum-optimism/optimism/pull/15316.", "2025-04-24T16:57:20Z", "2025-04-24T16:57:30Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6l4QzM", "I_kwDODjvEJM6wOXUA", "Let's work on this together this week", "2025-04-07T11:24:31Z", "2025-04-07T11:24:31Z", "protolambda", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6mEmaw", "I_kwDODjvEJM6wOXUA", "High level implementation(sysgo, frontend devstack) will be tracked at here, and low level impl(kurtosis api calls) is tracked at https://github.com/ethereum-optimism/optimism/issues/15270.", "2025-04-08T12:08:42Z", "2025-04-08T12:08:42Z", "pcw109550", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6rtQs-", "I_kwDODjvEJM6wNKvh", "Running this test against kurtosis devnets is blocked by https://github.com/ethereum-optimism/optimism/issues/15928.", "2025-05-14T15:58:43Z", "2025-05-14T15:58:43Z", "teddyknox", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6lgceV", "I_kwDODjvEJM6wDWKx", "Same ticket https://github.com/ethereum-optimism/release-management/issues/150", "2025-04-03T19:34:31Z", "2025-04-03T19:34:41Z", "JosepBove", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6lglxo", "I_kwDODjvEJM6wDV4W", "FMA Template: https://github.com/ethereum-optimism/design-docs/blob/main/assets/fma-template.md\nFMA Review Request: https://github.com/ethereum-optimism/security-pod/issues/new?template=support-ticket.md\n@JosepBove ", "2025-04-03T19:55:13Z", "2025-04-03T19:55:13Z", "lewejahlil", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6lyorn", "I_kwDODjvEJM6wDV4W", "Draft PR to track the FMA: https://github.com/ethereum-optimism/design-docs/pull/258", "2025-04-06T17:05:11Z", "2025-04-06T17:05:11Z", "JosepBove", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6lPRUq", "I_kwDODjvEJM6vcz6y", "https://github.com/ethereum-optimism/optimism/pull/15149 implements this.", "2025-04-02T11:19:06Z", "2025-04-02T11:19:06Z", "pcw109550", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6kCquf", "I_kwDODjvEJM6uUzk7", "@alessandromazza98 I discussed this with @sebastianst today. \n\nWe are minded to solve this problem by a different route. We want to remove the code which has op-node reaching out to op-geth for chain config, even for custom chains. The config should instead be attached to the rollup config and if it isn't this would be an error.  How does that sound to you?", "2025-03-25T18:21:55Z", "2025-03-25T18:21:55Z", "geoknee", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6kCtlv", "I_kwDODjvEJM6uUzk7", "> We are minded to solve this problem by a different route. We want to remove the code which has op-node reaching out to op-geth for chain config, even for custom chains. The config should instead be attached to the rollup config and if it isn't this would be an error. How does that sound to you?\n\nit sounds good to me, thanks (just want to note that I'm not part of reth team so I'm answering for my own).\n\nA good idea could also be to log an error in `op-node` to explain to add those information in the `rollup.json` file so that if it happens the user knows what to do to fix it", "2025-03-25T18:27:06Z", "2025-03-25T18:27:06Z", "alessandromazza98", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6kCvqI", "I_kwDODjvEJM6uUzk7", "> A good idea could also be to log an error in `op-node` to explain to add those information in the `rollup.json` file so that if it happens the user knows what to do to fix it\n\nAgree! When we remove the `loadOrFetch...` code, we will do that check instead and error if the field is not present.\n\nWe also described how the new parameters should be added to the rollup config in the [op-node/v1.11.0](https://github.com/ethereum-optimism/optimism/releases/tag/op-node%2Fv1.11.0) release and somewhere in the docs.", "2025-03-25T18:30:57Z", "2025-03-25T18:30:57Z", "sebastianst", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6STC0S", "I_kwDODjvEJM6c1k41", "Importing `CommonTest` and inheriting it (instead of `Test`) to a test that currently inherits `Test` adds ~ 8 secs to compilation time on my end. Tested this with `packages/contracts-bedrock/test/dispute/lib/LibClock.t.sol` and `packages/contracts-bedrock/test/cannon/PreimageOracle.t.sol`", "2024-11-04T11:28:03Z", "2024-11-04T11:28:03Z", "AmadiMichael", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6SUupG", "I_kwDODjvEJM6c1k41", "We should not be using `CommonTest` for testing libraries that require no contract deployments", "2024-11-04T14:46:33Z", "2024-11-04T14:46:33Z", "tynes", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6SVq98", "I_kwDODjvEJM6c1k41", "> We should not be using `CommonTest` for testing libraries that require no contract deployments\r\n\r\nArgument here is that using a single `CommonTest` for everything is more braindead than trying to understand if your test suite will need access to these things or not. If there's no significant impact on testing time (especially when you don't actually call `super.setUp()`) then having everything use `CommonTest` is an improvement because less brain cells go into deciding how to write your tests.\r\n\r\nBasically, smart contract development should be as simple as possible and we should reduce the number of options down to 1 whenever we can.", "2024-11-04T16:24:50Z", "2024-11-04T16:25:52Z", "smartcontracts", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6SZp_Z", "I_kwDODjvEJM6c1k41", "Generally agree with the direction that you are going for but if you are a developer that cannot determine if you should use `CommonTest` or `Test` then I don't think you would be a good fit to work on this project", "2024-11-05T04:18:08Z", "2024-11-05T04:18:08Z", "tynes", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6SdrVx", "I_kwDODjvEJM6c1k41", "I think it's mostly just that we want to keep things simple. `CommonTest` is the same thing as `Test` except for the setup function so if you don't call `super.setUp` then it's identical. By using `CommonTest` everywhere we just standardize a bit more and have less to think about. Less brain power on boilerplate when writing tests is good, because it means more brainpower goes to writing tests. It seems minor but these things add up.", "2024-11-05T13:58:42Z", "2024-11-05T13:58:42Z", "smartcontracts", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6SgcbF", "I_kwDODjvEJM6c1k41", "I agree with Mark that it doesn't make sense to test a library with an entire L1 and L2 deployed. Aggregated over hundreds of tests that will definitely slow down the tests.\r\n\r\nIMO this is most an issue of naming, ie. `CommonTest` and `Test` are very different things that just share a similar name. \r\n\r\nI'd propose combining `CommonTest` and `BridgeInitializer` into one contract `Foo`. All concrete contracts in the system should be tested against `Foo`. \r\n\r\nIf we organize our tests correctly, then we _should_ be able to have semgrep tests per path, so that all tests in `test/L1`,`test/L2`, `test/dispute`, etc. inherit from `Foo`, and all others can inherit `Test`.\r\n", "2024-11-05T19:21:50Z", "2024-11-05T19:21:50Z", "maurelian", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6Shxyk", "I_kwDODjvEJM6c1k41", "> I'd propose combining CommonTest and BridgeInitializer into one contract Foo. All concrete contracts in the system should be tested against Foo.\r\n\r\nThis PR https://github.com/ethereum-optimism/optimism/pull/12795 merges CommonTest and BridgeInitializer \ud83d\ude42", "2024-11-05T22:56:57Z", "2024-11-05T22:56:57Z", "AmadiMichael", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6SjEJW", "I_kwDODjvEJM6c1k41", "@maurelian the key point here is that CommonTest doesn't actually deploy all of the contracts unless you explicitly tell it to - if you don't tell it to do that deployment then it behaves identically to Test. If we merged the two contracts then we basically have a single unified testing base that you can use to explicitly request deployments of other contracts if needed.", "2024-11-06T03:40:17Z", "2024-11-06T03:40:17Z", "smartcontracts", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6SjE22", "I_kwDODjvEJM6c1k41", "If you don't call `CommonTest.setUp()` then `CommonTest` and `Test` are identical. \r\n\r\nSo my proposal here would be that we only use `CommonTest` and rename `CommonTest.setUp` to `CommonTest.deployFullSystem` or something similar ", "2024-11-06T03:44:11Z", "2024-11-06T03:44:11Z", "smartcontracts", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6SjGBv", "I_kwDODjvEJM6c1k41", "I see, I actually think that's a reasonable idea then, esp. with renaming `setUp()`.", "2024-11-06T03:50:41Z", "2024-11-06T03:50:41Z", "maurelian", "2025-08-31 04:55:29"]
["IC_kwDODjvEJM6quPa9", "I_kwDODjvEJM61y3sz", "Cutting this scope from U16.", "2025-05-08T20:35:36Z", "2025-05-08T20:35:36Z", "maurelian", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6p8Dmo", "I_kwDODjvEJM61KwGV", "Example L2EL logs\n```\nINFO [05-05|13:49:48.066] Config environment variable found        envvar=GETH_ROLLUP_INTEROPRPC\nINFO [05-05|13:49:48.171] Maximum peer count                       ETH=50 total=50\nINFO [05-05|13:49:48.173] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\nINFO [05-05|13:49:48.175] Set global gas cap                       cap=50,000,000\nINFO [05-05|13:49:48.175] Initializing the KZG library             backend=gokzg\nINFO [05-05|13:49:48.196] Defaulting to pebble as the backing database\nINFO [05-05|13:49:48.196] Allocated cache and file handles         database=/data/geth/execution-data/geth/chaindata cache=16.00MiB handles=16\nINFO [05-05|13:49:48.239] Opened ancient database                  database=/data/geth/execution-data/geth/chaindata/ancient/chain readonly=false\nERROR[05-05|13:49:48.239] Head block is not reachable\nINFO [05-05|13:49:48.239] State scheme set by user                 scheme=hash\nINFO [05-05|13:49:48.239] Writing custom genesis block\nINFO [05-05|13:49:48.293] Persisted trie from memory database      nodes=3203 size=462.69KiB time=4.217482ms gcnodes=0 gcsize=0.00B gctime=0s livenodes=0 livesize=0.00B\nINFO [05-05|13:49:48.365] Successfully wrote genesis state         database=chaindata hash=704d9d..13e95b\nINFO [05-05|13:49:48.399] Config environment variable found        envvar=GETH_ROLLUP_INTEROPRPC\nINFO [05-05|13:49:48.405] Maximum peer count                       ETH=50 total=50\nINFO [05-05|13:49:48.405] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\nWARN [05-05|13:49:48.405] Option \"allow-insecure-unlock\" is deprecated and has no effect\nINFO [05-05|13:49:48.407] Enabling recording of key preimages since archive mode is used\nWARN [05-05|13:49:48.407] Disabled transaction unindexing for archive node\nWARN [05-05|13:49:48.407] Forcing hash state-scheme for archive mode\nINFO [05-05|13:49:48.407] Set global gas cap                       cap=50,000,000\nINFO [05-05|13:49:48.407] Initializing the KZG library             backend=gokzg\nINFO [05-05|13:49:48.424] Enabling metrics collection\nINFO [05-05|13:49:48.424] Enabling stand-alone metrics HTTP endpoint address=0.0.0.0:9001\nINFO [05-05|13:49:48.424] Starting metrics server                  addr=http://0.0.0.0:9001/debug/metrics\nINFO [05-05|13:49:48.424] Allocated trie memory caches             clean=307.00MiB dirty=0.00B\nINFO [05-05|13:49:48.424] Using pebble as the backing database\nINFO [05-05|13:49:48.424] Allocated cache and file handles         database=/data/geth/execution-data/geth/chaindata cache=512.00MiB handles=524,288\nINFO [05-05|13:49:48.443] Opened ancient database                  database=/data/geth/execution-data/geth/chaindata/ancient/chain readonly=false\nINFO [05-05|13:49:48.443] State scheme set by user                 scheme=hash\nINFO [05-05|13:49:48.445] Genesis hash                             hash=704d9d..13e95b\nWARN [05-05|13:49:48.445] failed to load chain config from superchain-registry, skipping override err=\"unknown chain ID: 2151908\" chain_id=2,151,908\nINFO [05-05|13:49:48.445] Checking compatibility                   height=0 time=1,746,452,981 error=<nil>\nINFO [05-05|13:49:48.445] Configured chain config matches existing chain config in storage.\nINFO [05-05|13:49:48.445]\nINFO [05-05|13:49:48.445] ---------------------------------------------------------------------------------------------------------------------------------------------------------\nINFO [05-05|13:49:48.445] Chain ID:  2151908 (unknown)\nINFO [05-05|13:49:48.445] Consensus: Optimism\nINFO [05-05|13:49:48.445]\nINFO [05-05|13:49:48.445] Pre-Merge hard forks (block based):\nINFO [05-05|13:49:48.445]  - Homestead:                   #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/homestead.md)\nINFO [05-05|13:49:48.445]  - Tangerine Whistle (EIP 150): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/tangerine-whistle.md)\nINFO [05-05|13:49:48.445]  - Spurious Dragon/1 (EIP 155): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/spurious-dragon.md)\nINFO [05-05|13:49:48.445]  - Spurious Dragon/2 (EIP 158): #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/spurious-dragon.md)\nINFO [05-05|13:49:48.445]  - Byzantium:                   #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/byzantium.md)\nINFO [05-05|13:49:48.445]  - Constantinople:              #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/constantinople.md)\nINFO [05-05|13:49:48.445]  - Petersburg:                  #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/petersburg.md)\nINFO [05-05|13:49:48.445]  - Istanbul:                    #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/istanbul.md)\nINFO [05-05|13:49:48.445]  - Muir Glacier:                #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/muir-glacier.md)\nINFO [05-05|13:49:48.445]  - Berlin:                      #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/berlin.md)\nINFO [05-05|13:49:48.445]  - London:                      #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/london.md)\nINFO [05-05|13:49:48.445]  - Arrow Glacier:               #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/arrow-glacier.md)\nINFO [05-05|13:49:48.445]  - Gray Glacier:                #0        (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/gray-glacier.md)\nINFO [05-05|13:49:48.445]\nINFO [05-05|13:49:48.445] Merge configured:\nINFO [05-05|13:49:48.445]  - Hard-fork specification:    https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/paris.md\nINFO [05-05|13:49:48.445]  - Network known to be merged\nINFO [05-05|13:49:48.445]  - Total terminal difficulty:  0\nINFO [05-05|13:49:48.445]  - Merge netsplit block:       #0\nINFO [05-05|13:49:48.445]\nINFO [05-05|13:49:48.445] Post-Merge hard forks (timestamp based):\nINFO [05-05|13:49:48.445]  - Shanghai:                    @0          (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/shanghai.md)\nINFO [05-05|13:49:48.445]  - Cancun:                      @0          (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/cancun.md)\nINFO [05-05|13:49:48.445]  - Prague:                      @0\nINFO [05-05|13:49:48.445]  - Regolith:                    @0\nINFO [05-05|13:49:48.445]  - Canyon:                      @0\nINFO [05-05|13:49:48.445]  - Ecotone:                     @0\nINFO [05-05|13:49:48.445]  - Fjord:                       @0\nINFO [05-05|13:49:48.445]  - Granite:                     @0\nINFO [05-05|13:49:48.445]  - Holocene:                    @0\nINFO [05-05|13:49:48.445]  - Isthmus:                     @0\nINFO [05-05|13:49:48.445]  - Interop:                     @0\nINFO [05-05|13:49:48.445]\nINFO [05-05|13:49:48.445] ---------------------------------------------------------------------------------------------------------------------------------------------------------\nINFO [05-05|13:49:48.445]\nINFO [05-05|13:49:48.445] Loaded most recent local block           number=0 hash=704d9d..13e95b age=7s\nWARN [05-05|13:49:48.445] Failed to load snapshot                  err=\"missing or corrupted snapshot\"\nINFO [05-05|13:49:48.445] Rebuilding state snapshot\nINFO [05-05|13:49:48.446] Initialized transaction indexer          range=\"entire chain\"\nINFO [05-05|13:49:48.446] Initialising Ethereum protocol           network=2,151,908 dbversion=<nil>\nINFO [05-05|13:49:48.446] Resuming state snapshot generation       root=851c5c..dd2881 accounts=0 slots=0 storage=0.00B dangling=0 elapsed=\"688.08\u00b5s\"\nINFO [05-05|13:49:48.446] Unprotected transactions allowed\nINFO [05-05|13:49:48.446] Gasprice oracle is ignoring threshold set threshold=2\nWARN [05-05|13:49:48.447] Engine API enabled                       protocol=eth\nINFO [05-05|13:49:48.447] Starting peer-to-peer node               instance=Geth/v1.101503.4-stable-2b9abb39/linux-arm64/go1.24.2\nINFO [05-05|13:49:48.463] New local node record                    seq=1,746,452,988,462 id=b579fb9b2ce3d5ee ip=172.16.0.23 udp=30303 tcp=30303\nINFO [05-05|13:49:48.463] Started P2P networking                   self=enode://21cca11d7b2e1b739ec49f2dad8217cd5e7b06fa971f61267b3f4b0cda05b8888df01788cd7b484754d1fdcd1f44bf58c91531356c454a40708f27fdfaa51c13@172.16.0.23:30303\nINFO [05-05|13:49:48.463] IPC endpoint opened                      url=/data/geth/execution-data/geth.ipc\nINFO [05-05|13:49:48.463] Loaded JWT secret file                   path=/jwt/jwtsecret crc32=0xec7a0f2c\nINFO [05-05|13:49:48.464] HTTP server started                      endpoint=[::]:8545 auth=false prefix= cors=* vhosts=*\nINFO [05-05|13:49:48.464] WebSocket enabled                        url=ws://[::]:8546\nINFO [05-05|13:49:48.464] WebSocket enabled                        url=ws://[::]:8551\nINFO [05-05|13:49:48.464] HTTP server started                      endpoint=[::]:8551 auth=true  prefix= cors=localhost vhosts=*\nINFO [05-05|13:49:48.473] Generated state snapshot                 accounts=2374 slots=2090 storage=395.07KiB dangling=0 elapsed=28.217ms\nINFO [05-05|13:49:58.471] Looking for peers                        peercount=0 tried=0 static=0\nINFO [05-05|13:50:08.499] Looking for peers                        peercount=0 tried=101 static=0\nINFO [05-05|13:50:18.612] Looking for peers                        peercount=0 tried=96  static=0\nWARN [05-05|13:50:19.263] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=4 duration=\"249.04\u00b5s\" err=\"finalized block not found\"\nWARN [05-05|13:50:19.453] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=5 duration=\"37.083\u00b5s\" err=\"finalized block not found\"\nWARN [05-05|13:50:27.564] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=7 duration=\"88.625\u00b5s\" err=\"finalized block not found\"\nINFO [05-05|13:50:29.939] Looking for peers                        peercount=0 tried=72  static=0\nWARN [05-05|13:50:37.909] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=9 duration=3.630917ms err=\"finalized block not found\"\nINFO [05-05|13:50:40.033] Looking for peers                        peercount=1 tried=104 static=0\nWARN [05-05|13:50:47.944] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=11 duration=\"147.5\u00b5s\" err=\"finalized block not found\"\nINFO [05-05|13:50:50.042] Looking for peers                        peercount=0 tried=59  static=0\nWARN [05-05|13:50:58.044] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=13 duration=\"177.709\u00b5s\" err=\"finalized block not found\"\nINFO [05-05|13:51:00.103] Looking for peers                        peercount=1 tried=122 static=0\nWARN [05-05|13:51:08.090] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=15 duration=\"137.417\u00b5s\" err=\"finalized block not found\"\nINFO [05-05|13:51:10.140] Looking for peers                        peercount=0 tried=90  static=0\nWARN [05-05|13:51:18.200] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=17 duration=\"185.208\u00b5s\" err=\"finalized block not found\"\nINFO [05-05|13:51:20.449] Looking for peers                        peercount=0 tried=83  static=0\nWARN [05-05|13:51:28.318] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=19 duration=\"156.417\u00b5s\" err=\"finalized block not found\"\nINFO [05-05|13:51:31.072] Looking for peers                        peercount=0 tried=69  static=0\nINFO [05-05|13:51:34.370] using bedrock l1 cost func for first Ecotone block time=1,746,452,981\nINFO [05-05|13:51:34.370] Submitted transaction                    hash=0x9cff1aacccc5eed28965653719f3a476731ce2cf81ddfe9e4306a53af9420b43 from=0x3C44CdDdB6a900fa2b585dd299e03d12FA4293BC nonce=0 recipient=0x4200000000000000000000000000000000000023 value=0\nWARN [05-05|13:51:34.370] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=1 duration=\"120.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:34.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=2 duration=\"112.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:34.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=3 duration=\"144.917\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:34.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=4 duration=\"141.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:34.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=5 duration=\"122.75\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:34.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=6 duration=\"163.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:34.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=7 duration=\"135.959\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.073] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=8 duration=\"86.166\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.171] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=9 duration=\"112.084\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=10 duration=\"126.084\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.371] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=11 duration=\"123\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=12 duration=\"111.917\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=13 duration=\"125.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.671] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=14 duration=\"163.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=15 duration=\"112.583\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=16 duration=\"104\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:35.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=17 duration=\"117.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=18 duration=\"102.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.171] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=19 duration=\"95.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=20 duration=\"115.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=21 duration=\"110.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=22 duration=\"178.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.581] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=23 duration=\"712.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.673] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=24 duration=\"169.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=25 duration=\"133.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=26 duration=\"157.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:36.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=27 duration=\"145.084\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=28 duration=\"203.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=29 duration=\"193.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=30 duration=\"135.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=31 duration=\"287.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=32 duration=\"205.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=33 duration=\"231.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.671] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=34 duration=\"322\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=35 duration=\"413.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=36 duration=\"234.208\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:37.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=37 duration=\"339\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.075] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=38 duration=\"119.958\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.174] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=39 duration=\"167.792\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=40 duration=\"165.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=41 duration=\"129.75\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.425] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=21 duration=\"52\u00b5s\" err=\"finalized block not found\"\nWARN [05-05|13:51:38.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=42 duration=\"95.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=43 duration=\"269.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=44 duration=\"483.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=45 duration=\"393.166\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=46 duration=\"265.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:38.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=47 duration=\"138.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.070] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=48 duration=\"87.208\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=49 duration=\"131.459\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=50 duration=\"398.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=51 duration=\"436.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=52 duration=\"286.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=53 duration=\"140.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=54 duration=\"118.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=55 duration=\"146.041\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=56 duration=\"146.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:39.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=57 duration=\"167.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=58 duration=\"223.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=59 duration=\"188.792\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=60 duration=\"154.708\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=61 duration=\"210.792\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=62 duration=\"159.709\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=63 duration=\"178.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=64 duration=\"195.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=65 duration=\"167.166\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=66 duration=\"130.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:40.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=67 duration=\"193.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=68 duration=\"125.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nINFO [05-05|13:51:41.095] Looking for peers                        peercount=0 tried=67  static=0\nWARN [05-05|13:51:41.175] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=69 duration=\"229.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=70 duration=\"250.791\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=71 duration=\"147.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=72 duration=\"121.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=73 duration=\"117.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=74 duration=\"254.417\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=75 duration=\"211.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=76 duration=\"113.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:41.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=77 duration=\"78.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=78 duration=\"173.458\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=79 duration=\"141.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=80 duration=\"194.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=81 duration=\"187.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=82 duration=\"141.792\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=83 duration=\"155.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=84 duration=\"307.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=85 duration=\"184.666\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=86 duration=\"141.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:42.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=87 duration=\"222.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=88 duration=\"116.791\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.171] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=89 duration=\"145.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=90 duration=\"278.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=91 duration=\"146.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=92 duration=\"168\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=93 duration=\"151.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=94 duration=\"160.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=95 duration=\"106.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=96 duration=\"353.583\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:43.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=97 duration=\"179.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=98 duration=\"171.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=99 duration=\"141.791\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.274] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=100 duration=\"176.541\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=101 duration=\"142.917\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=102 duration=\"220.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=103 duration=\"103.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=104 duration=\"256.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=105 duration=\"259.084\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=106 duration=\"301.458\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:44.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=107 duration=\"143.291\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=108 duration=\"124.084\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.176] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=109 duration=\"151.917\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=110 duration=\"165.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.371] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=111 duration=\"226.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.473] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=112 duration=\"955.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.571] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=113 duration=\"82.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.676] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=114 duration=\"912.916\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=115 duration=\"129.458\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=116 duration=\"104.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:45.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=117 duration=\"103.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=118 duration=\"134.917\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=119 duration=\"135.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=120 duration=\"207.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=121 duration=\"257.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=122 duration=\"279\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.571] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=123 duration=\"150.417\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.674] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=124 duration=\"305.917\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.773] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=125 duration=\"201.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=126 duration=\"175.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:46.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=127 duration=\"187.708\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=128 duration=\"158.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=129 duration=\"158.709\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=130 duration=\"168.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=131 duration=\"311.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=132 duration=\"174.584\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.573] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=133 duration=\"131.584\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.671] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=134 duration=\"133.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=135 duration=\"138.334\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=136 duration=\"175.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:47.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=137 duration=\"149.209\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=138 duration=\"171.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=139 duration=\"283.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=140 duration=\"219.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.371] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=141 duration=\"154.792\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=142 duration=\"135.583\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=143 duration=\"175.166\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.590] Served eth_getBlockByNumber              conn=172.16.0.25:48344 reqid=23 duration=\"67.208\u00b5s\" err=\"finalized block not found\"\nWARN [05-05|13:51:48.674] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=144 duration=\"709.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=145 duration=\"173.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=146 duration=\"243.958\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:48.973] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=147 duration=\"158.792\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=148 duration=\"130.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.171] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=149 duration=\"195.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=150 duration=\"134.291\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=151 duration=\"150.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=152 duration=\"200.666\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=153 duration=\"168.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=154 duration=\"178.666\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=155 duration=\"204.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=156 duration=\"199.166\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:49.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=157 duration=\"176.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=158 duration=\"192.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=159 duration=\"305.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=160 duration=\"177.458\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=161 duration=\"145.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=162 duration=\"165.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.571] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=163 duration=\"168.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=164 duration=\"193.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=165 duration=\"147.208\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=166 duration=\"166.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:50.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=167 duration=\"199.792\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=168 duration=\"145.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nINFO [05-05|13:51:51.104] Looking for peers                        peercount=0 tried=89  static=0\nWARN [05-05|13:51:51.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=169 duration=\"146.584\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=170 duration=\"143.291\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.371] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=171 duration=\"167.709\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=172 duration=\"108.459\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.575] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=173 duration=\"134.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=174 duration=\"339.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=175 duration=\"253.417\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.873] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=176 duration=\"270.541\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:51.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=177 duration=\"258.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.073] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=178 duration=\"154.958\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.182] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=179 duration=\"187.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=180 duration=\"142.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.371] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=181 duration=\"230.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=182 duration=\"148.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=183 duration=\"177.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=184 duration=\"160.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.773] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=185 duration=\"319.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=186 duration=\"192.291\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:52.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=187 duration=\"142.959\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=188 duration=\"118.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=189 duration=\"155.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=190 duration=\"168.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=191 duration=\"216.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=192 duration=\"198.708\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=193 duration=\"180.208\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=194 duration=\"135.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=195 duration=\"126.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=196 duration=\"291.666\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:53.973] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=197 duration=\"172.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=198 duration=\"243.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.173] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=199 duration=\"238.791\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=200 duration=\"230.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.371] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=201 duration=\"98.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=202 duration=\"106.291\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=203 duration=\"173.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=204 duration=\"350\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=205 duration=\"308.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=206 duration=\"187.209\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:54.975] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=207 duration=\"693.084\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=208 duration=\"399.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=209 duration=\"215.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=210 duration=\"255.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=211 duration=\"384.709\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=212 duration=\"229.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=213 duration=\"328.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.671] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=214 duration=\"285.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=215 duration=\"477.916\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=216 duration=\"309.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:55.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=217 duration=\"228.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=218 duration=\"164.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=219 duration=\"235.334\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=220 duration=\"238.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.375] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=221 duration=\"252.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.473] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=222 duration=\"470.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=223 duration=\"238.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.671] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=224 duration=\"230.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=225 duration=\"372.75\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=226 duration=\"248.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:56.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=227 duration=\"118.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=228 duration=\"98.334\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.171] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=229 duration=\"127.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=230 duration=\"331.542\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=231 duration=\"344.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=232 duration=\"246.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=233 duration=\"320.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.671] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=234 duration=\"297.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=235 duration=\"270.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=236 duration=\"367.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:57.975] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=237 duration=\"401.084\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=238 duration=\"315.209\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.173] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=239 duration=\"432.584\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=240 duration=\"254.666\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=241 duration=\"250.208\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=242 duration=\"291.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=243 duration=\"423.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=244 duration=\"205\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=245 duration=\"100.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=246 duration=\"313.709\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:58.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=247 duration=\"175.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=248 duration=\"110.042\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=249 duration=\"193.666\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=250 duration=\"185.291\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=251 duration=\"367.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=252 duration=\"204.333\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=253 duration=\"209.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=254 duration=\"169\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=255 duration=\"210.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=256 duration=\"315.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:51:59.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=257 duration=\"416.417\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=258 duration=\"190.791\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=259 duration=\"264.958\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=260 duration=\"278\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=261 duration=\"350.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=262 duration=\"149.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=263 duration=\"282.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=264 duration=\"349.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=265 duration=\"321.584\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=266 duration=\"492.083\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:00.972] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=267 duration=\"172.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=268 duration=\"200.458\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nINFO [05-05|13:52:01.118] Looking for peers                        peercount=0 tried=83  static=0\nWARN [05-05|13:52:01.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=269 duration=\"276.667\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.271] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=270 duration=\"158.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=271 duration=\"552.459\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.473] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=272 duration=\"518.916\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=273 duration=\"164.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.671] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=274 duration=\"279\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=275 duration=\"442.917\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.872] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=276 duration=\"487.75\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:01.973] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=277 duration=\"422.167\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=278 duration=\"205.417\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.171] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=279 duration=\"278.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.280] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=280 duration=\"346.334\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.371] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=281 duration=\"348.375\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.471] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=282 duration=\"442.583\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.571] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=283 duration=\"206.417\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.673] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=284 duration=\"545.625\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=285 duration=\"161\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=286 duration=\"327.166\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:02.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=287 duration=\"189.041\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.071] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=288 duration=\"111.875\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=289 duration=\"165\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=290 duration=\"278.833\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=291 duration=\"232.708\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.472] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=292 duration=\"244.5\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=293 duration=\"213.917\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=294 duration=\"317.541\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.771] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=295 duration=\"278.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.871] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=296 duration=\"374.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:03.971] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=297 duration=\"226.208\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:04.072] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=298 duration=\"170.125\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:04.172] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=299 duration=\"172.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:04.272] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=300 duration=\"262.834\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:04.372] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=301 duration=\"289.958\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:04.473] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=302 duration=\"188.292\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:04.572] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=303 duration=\"307\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:04.672] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=304 duration=\"324.25\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nWARN [05-05|13:52:04.772] Served eth_getTransactionReceipt         conn=172.16.0.9:32630 reqid=305 duration=\"320.458\u00b5s\" err=\"transaction indexing is in progress\" errdata=\"\\\"transaction indexing is in progress\\\"\"\nINFO [05-05|13:52:04.783] Starting work on payload                 id=0x0342ca1ae22e4401\nINFO [05-05|13:52:04.790] Block building got interrupted by payload resolution\nINFO [05-05|13:52:04.795] Updated payload                          id=0x0342ca1ae22e4401 number=1 hash=3c61a4..9b2655 txs=1 withdrawals=0 gas=162,720 fees=0 root=2ea019..06e810 elapsed=11.627ms\nINFO [05-05|13:52:04.795] Stopping work on payload                 id=0x0342ca1ae22e4401 reason=delivery elapsed=11.911ms\nINFO [05-05|13:52:04.804] Imported new potential chain segment     number=1 hash=3c61a4..9b2655 blocks=1 txs=1 mgas=0.163 elapsed=2.886ms    mgasps=56.370 age=2m21s snapdiffs=790.00B triedirty=0.00B\nINFO [05-05|13:52:04.805] Chain head was updated                   number=1 hash=3c61a4..9b2655 root=2ea019..06e810 elapsed=\"97.541\u00b5s\" age=2m21s\nINFO [05-05|13:52:04.806] Indexed transactions                     blocks=2 txs=1 tail=0 elapsed=\"458.291\u00b5s\"\nINFO [05-05|13:52:04.808] Starting work on payload                 id=0x030dba3d10f4df04\nINFO [05-05|13:52:04.810] Updated payload                          id=0x030dba3d10f4df04 number=2 hash=e98f3f..f394f2 txs=2 withdrawals=0 gas=122,266 fees=7.6222146e-05 root=67a772..d4fc5c elapsed=2.480ms\nINFO [05-05|13:52:04.811] Stopping work on payload                 id=0x030dba3d10f4df04 reason=delivery elapsed=2.865ms\nINFO [05-05|13:52:04.813] Imported new potential chain segment     number=2 hash=e98f3f..f394f2 blocks=1 txs=2 mgas=0.122 elapsed=1.437ms     mgasps=85.077 age=2m19s snapdiffs=1.84KiB triedirty=0.00B\nINFO [05-05|13:52:04.813] Chain head was updated                   number=2 hash=e98f3f..f394f2 root=67a772..d4fc5c elapsed=\"145.125\u00b5s\" age=2m19s\nINFO [05-05|13:52:04.817] Starting work on payload                 id=0x0388c8e541d2f42b\nINFO [05-05|13:52:04.818] Updated payload                          id=0x0388c8e541d2f42b number=3 hash=fad826..c12a90 txs=1 withdrawals=0 gas=54508   fees=0             root=23e93c..31eedc elapsed=\"892.083\u00b5s\"\nINFO [05-05|13:52:04.818] Stopping work on payload                 id=0x0388c8e541d2f42b reason=delivery elapsed=1.070ms\nINFO [05-05|13:52:04.823] Imported new potential chain segment     number=3 hash=fad826..c12a90 blocks=1 txs=1 mgas=0.055 elapsed=4.207ms     mgasps=12.954 age=2m17s snapdiffs=2.53KiB triedirty=0.00B\nINFO [05-05|13:52:04.823] Chain head was updated                   number=3 hash=fad826..c12a90 root=23e93c..31eedc elapsed=\"56.875\u00b5s\"  age=2m17s\nINFO [05-05|13:52:04.825] Starting work on payload                 id=0x037914e6e17fede2\nINFO [05-05|13:52:04.828] Updated payload                          id=0x037914e6e17fede2 number=4 hash=9346bc..bbe364 txs=1 withdrawals=0 gas=46120   fees=0             root=1b2b69..271231 elapsed=2.715ms\nINFO [05-05|13:52:04.829] Stopping work on payload                 id=0x037914e6e17fede2 reason=delivery elapsed=3.005ms\nINFO [05-05|13:52:04.831] Imported new potential chain segment     number=4 hash=9346bc..bbe364 blocks=1 txs=1 mgas=0.046 elapsed=1.438ms     mgasps=32.065 age=2m15s snapdiffs=3.07KiB triedirty=0.00B\nINFO [05-05|13:52:04.832] Chain head was updated                   number=4 hash=9346bc..bbe364 root=1b2b69..271231 elapsed=\"21.917\u00b5s\"  age=2m15s\nINFO [05-05|13:52:04.833] Starting work on payload                 id=0x03794b707eeb8142\nINFO [05-05|13:52:04.834] Updated payload                          id=0x03794b707eeb8142 number=5 hash=8c6e9a..4d16b4 txs=1 withdrawals=0 gas=46120   fees=0             root=b374fb..e65a26 elapsed=\"329.875\u00b5s\"\nINFO [05-05|13:52:04.834] Stopping work on payload                 id=0x03794b707eeb8142 reason=delivery elapsed=\"702.834\u00b5s\"\nINFO [05-05|13:52:04.836] Imported new potential chain segment     number=5 hash=8c6e9a..4d16b4 blocks=1 txs=1 mgas=0.046 elapsed=\"781.125\u00b5s\" mgasps=59.043 age=2m13s snapdiffs=3.62KiB triedirty=0.00B\nINFO [05-05|13:52:04.836] Chain head was updated                   number=5 hash=8c6e9a..4d16b4 root=b374fb..e65a26 elapsed=\"33.584\u00b5s\"  age=2m13s\nINFO [05-05|13:52:04.838] Starting work on payload                 id=0x0359489e0d2ff326\nINFO [05-05|13:52:04.838] Updated payload                          id=0x0359489e0d2ff326 number=6 hash=284f4f..9f28ab txs=1 withdrawals=0 gas=54508   fees=0             root=b50673..d7bc97 elapsed=\"330.833\u00b5s\"\nINFO [05-05|13:52:04.839] Stopping work on payload                 id=0x0359489e0d2ff326 reason=delivery elapsed=\"749.667\u00b5s\"\nINFO [05-05|13:52:04.840] Imported new potential chain segment     number=6 hash=284f4f..9f28ab blocks=1 txs=1 mgas=0.055 elapsed=1.224ms     mgasps=44.533 age=2m11s snapdiffs=4.30KiB triedirty=0.00B\nINFO [05-05|13:52:04.841] Chain head was updated                   number=6 hash=284f4f..9f28ab root=b50673..d7bc97 elapsed=\"31.25\u00b5s\"   age=2m11s\nINFO [05-05|13:52:04.842] Starting work on payload                 id=0x039637fb3864ea3c\nINFO [05-05|13:52:04.843] Updated payload                          id=0x039637fb3864ea3c number=7 hash=4b4d73..004728 txs=1 withdrawals=0 gas=46120   fees=0             root=804f3c..06cd6c elapsed=\"264.125\u00b5s\"\nINFO [05-05|13:52:04.843] Stopping work on payload                 id=0x039637fb3864ea3c reason=delivery elapsed=\"444.375\u00b5s\"\nINFO [05-05|13:52:04.844] Imported new potential chain segment     number=7 hash=4b4d73..004728 blocks=1 txs=1 mgas=0.046 elapsed=1.060ms     mgasps=43.475 age=2m9s  snapdiffs=4.85KiB triedirty=0.00B\nINFO [05-05|13:52:04.845] Chain head was updated                   number=7 hash=4b4d73..004728 root=804f3c..06cd6c elapsed=\"20.291\u00b5s\"  age=2m9s\nINFO [05-05|13:52:04.846] Starting work on payload                 id=0x031b1c2c4f6819fb\nINFO [05-05|13:52:04.846] Updated payload                          id=0x031b1c2c4f6819fb number=8 hash=2e4c97..69c907 txs=1 withdrawals=0 gas=46120   fees=0             root=190e25..7d8ea5 elapsed=\"296.792\u00b5s\"\nINFO [05-05|13:52:04.846] Stopping work on payload                 id=0x031b1c2c4f6819fb reason=delivery elapsed=\"424.625\u00b5s\"\nINFO [05-05|13:52:04.848] Imported new potential chain segment     number=8 hash=2e4c97..69c907 blocks=1 txs=1 mgas=0.046 elapsed=\"748.375\u00b5s\" mgasps=61.627 age=2m7s  snapdiffs=5.39KiB triedirty=0.00B\nINFO [05-05|13:52:04.848] Chain head was updated                   number=8 hash=2e4c97..69c907 root=190e25..7d8ea5 elapsed=\"95.875\u00b5s\"  age=2m7s\nINFO [05-05|13:52:04.850] Starting work on payload                 id=0x03c5d043659d934f\nINFO [05-05|13:52:04.850] Updated payload                          id=0x03c5d043659d934f number=9 hash=b5fed7..535ab0 txs=1 withdrawals=0 gas=54496   fees=0             root=56f1c0..9bc616 elapsed=\"338.5\u00b5s\"\nINFO [05-05|13:52:04.851] Stopping work on payload                 id=0x03c5d043659d934f reason=delivery elapsed=\"685.667\u00b5s\"\nINFO [05-05|13:52:04.852] Imported new potential chain segment     number=9 hash=b5fed7..535ab0 blocks=1 txs=1 mgas=0.054 elapsed=\"494.125\u00b5s\" mgasps=110.288 age=2m5s  snapdiffs=6.08KiB triedirty=0.00B\nINFO [05-05|13:52:04.852] Chain head was updated                   number=9 hash=b5fed7..535ab0 root=56f1c0..9bc616 elapsed=\"25.25\u00b5s\"   age=2m5s\nINFO [05-05|13:52:04.854] Starting work on payload                 id=0x03a8d22b26f435f8\nINFO [05-05|13:52:04.855] Updated payload                          id=0x03a8d22b26f435f8 number=10 hash=f117b8..3b4dbf txs=1 withdrawals=0 gas=46108   fees=0             root=43b90f..016dfa elapsed=\"549.291\u00b5s\"\nINFO [05-05|13:52:04.856] Stopping work on payload                 id=0x03a8d22b26f435f8 reason=delivery elapsed=1.317ms\nINFO [05-05|13:52:04.857] Imported new potential chain segment     number=10 hash=f117b8..3b4dbf blocks=1 txs=1 mgas=0.046 elapsed=\"547.166\u00b5s\" mgasps=84.267  age=2m3s  snapdiffs=6.62KiB triedirty=0.00B\nINFO [05-05|13:52:04.857] Chain head was updated                   number=10 hash=f117b8..3b4dbf root=43b90f..016dfa elapsed=\"32.667\u00b5s\"  age=2m3s\nINFO [05-05|13:52:04.858] Starting work on payload                 id=0x0356fbd9c2ada05f\nINFO [05-05|13:52:04.859] Updated payload                          id=0x0356fbd9c2ada05f number=11 hash=06abf6..ee3ff5 txs=1 withdrawals=0 gas=46108   fees=0             root=f01533..a89d2a elapsed=\"265.625\u00b5s\"\nINFO [05-05|13:52:04.859] Stopping work on payload                 id=0x0356fbd9c2ada05f reason=delivery elapsed=\"570.583\u00b5s\"\nINFO [05-05|13:52:04.860] Imported new potential chain segment     number=11 hash=06abf6..ee3ff5 blocks=1 txs=1 mgas=0.046 elapsed=\"711.584\u00b5s\" mgasps=64.796  age=2m1s  snapdiffs=7.17KiB triedirty=0.00B\nINFO [05-05|13:52:04.861] Chain head was updated                   number=11 hash=06abf6..ee3ff5 root=f01533..a89d2a elapsed=\"31.291\u00b5s\"  age=2m1s\nINFO [05-05|13:52:04.862] Starting work on payload                 id=0x033225a7f8ccb4de\nINFO [05-05|13:52:04.863] Updated payload                          id=0x033225a7f8ccb4de number=12 hash=872276..6cfe72 txs=1 withdrawals=0 gas=54508   fees=0             root=f4aae7..c363b9 elapsed=\"292.459\u00b5s\"\nINFO [05-05|13:52:04.863] Stopping work on payload                 id=0x033225a7f8ccb4de reason=delivery elapsed=\"518.125\u00b5s\"\nINFO [05-05|13:52:04.864] Imported new potential chain segment     number=12 hash=872276..6cfe72 blocks=1 txs=1 mgas=0.055 elapsed=\"533.292\u00b5s\" mgasps=102.210 age=1m59s snapdiffs=7.85KiB triedirty=0.00B\nINFO [05-05|13:52:04.864] Chain head was updated                   number=12 hash=872276..6cfe72 root=f4aae7..c363b9 elapsed=\"29.834\u00b5s\"  age=1m59s\nINFO [05-05|13:52:04.866] Starting work on payload                 id=0x034da8f53a436139\nINFO [05-05|13:52:04.867] Updated payload                          id=0x034da8f53a436139 number=13 hash=deddc2..0a4ebf txs=1 withdrawals=0 gas=46120   fees=0             root=69721b..b88405 elapsed=\"298.042\u00b5s\"\nINFO [05-05|13:52:04.867] Stopping work on payload                 id=0x034da8f53a436139 reason=delivery elapsed=\"618.334\u00b5s\"\nINFO [05-05|13:52:04.868] Imported new potential chain segment     number=13 hash=deddc2..0a4ebf blocks=1 txs=1 mgas=0.046 elapsed=\"540.834\u00b5s\" mgasps=85.276  age=1m57s snapdiffs=8.40KiB triedirty=0.00B\nINFO [05-05|13:52:04.869] Chain head was updated                   number=13 hash=deddc2..0a4ebf root=69721b..b88405 elapsed=\"27.833\u00b5s\"  age=1m57s\nINFO [05-05|13:52:04.870] Starting work on payload                 id=0x0346270d1dc82e13\nINFO [05-05|13:52:04.870] Updated payload                          id=0x0346270d1dc82e13 number=14 hash=b76ef6..43a6bb txs=1 withdrawals=0 gas=46120   fees=0             root=baf636..ab8dc7 elapsed=\"678.5\u00b5s\"\nINFO [05-05|13:52:04.871] Stopping work on payload                 id=0x0346270d1dc82e13 reason=delivery elapsed=\"836.584\u00b5s\"\nINFO [05-05|13:52:04.872] Imported new potential chain segment     number=14 hash=b76ef6..43a6bb blocks=1 txs=1 mgas=0.046 elapsed=\"862.5\u00b5s\"   mgasps=53.472  age=1m55s snapdiffs=8.94KiB triedirty=0.00B\nINFO [05-05|13:52:04.873] Chain head was updated                   number=14 hash=b76ef6..43a6bb root=baf636..ab8dc7 elapsed=\"23.584\u00b5s\"  age=1m55s\nINFO [05-05|13:52:04.875] Starting work on payload                 id=0x03ed86ff482800ed\nINFO [05-05|13:52:04.876] Updated payload                          id=0x03ed86ff482800ed number=15 hash=fe8716..3286f9 txs=1 withdrawals=0 gas=54508   fees=0             root=6a071a..88c23d elapsed=\"428.084\u00b5s\"\nINFO [05-05|13:52:04.876] Stopping work on payload                 id=0x03ed86ff482800ed reason=delivery elapsed=\"951.291\u00b5s\"\nINFO [05-05|13:52:04.878] Imported new potential chain segment     number=15 hash=fe8716..3286f9 blocks=1 txs=1 mgas=0.055 elapsed=1.268ms     mgasps=42.982  age=1m53s snapdiffs=9.63KiB triedirty=0.00B\nINFO [05-05|13:52:04.880] Chain head was updated                   number=15 hash=fe8716..3286f9 root=6a071a..88c23d elapsed=\"45.083\u00b5s\"  age=1m53s\nINFO [05-05|13:52:04.884] Starting work on payload                 id=0x033cd87ce1b0fe30\nINFO [05-05|13:52:04.884] Updated payload                          id=0x033cd87ce1b0fe30 number=16 hash=97e42f..7b595c txs=1 withdrawals=0 gas=46120   fees=0             root=3792c5..b151d1 elapsed=\"333.166\u00b5s\"\nINFO [05-05|13:52:04.885] Stopping work on payload                 id=0x033cd87ce1b0fe30 reason=delivery elapsed=\"745.666\u00b5s\"\nINFO [05-05|13:52:04.886] Imported new potential chain segment     number=16 hash=97e42f..7b595c blocks=1 txs=1 mgas=0.046 elapsed=\"559.958\u00b5s\" mgasps=82.363  age=1m51s snapdiffs=10.17KiB triedirty=0.00B\nINFO [05-05|13:52:04.886] Chain head was updated                   number=16 hash=97e42f..7b595c root=3792c5..b151d1 elapsed=\"27.125\u00b5s\"  age=1m51s\nINFO [05-05|13:52:04.887] Starting work on payload                 id=0x0335b646e529923b\nINFO [05-05|13:52:04.888] Updated payload                          id=0x0335b646e529923b number=17 hash=9b9ba8..1fd2cc txs=1 withdrawals=0 gas=46120   fees=0             root=e0c085..c4d57c elapsed=\"304.833\u00b5s\"\nINFO [05-05|13:52:04.888] Stopping work on payload                 id=0x0335b646e529923b reason=delivery elapsed=\"400.875\u00b5s\"\nINFO [05-05|13:52:04.889] Imported new potential chain segment     number=17 hash=9b9ba8..1fd2cc blocks=1 txs=1 mgas=0.046 elapsed=\"524.916\u00b5s\" mgasps=87.862  age=1m49s snapdiffs=10.72KiB triedirty=0.00B\nINFO [05-05|13:52:04.889] Chain head was updated                   number=17 hash=9b9ba8..1fd2cc root=e0c085..c4d57c elapsed=\"22.291\u00b5s\"  age=1m49s\nINFO [05-05|13:52:04.892] Starting work on payload                 id=0x03008a8ddc13d31a\nINFO [05-05|13:52:04.892] Updated payload                          id=0x03008a8ddc13d31a number=18 hash=3c828a..029a2d txs=1 withdrawals=0 gas=54508   fees=0             root=beb830..20b1d3 elapsed=\"465.125\u00b5s\"\nINFO [05-05|13:52:04.893] Stopping work on payload                 id=0x03008a8ddc13d31a reason=delivery elapsed=\"660.917\u00b5s\"\nINFO [05-05|13:52:04.894] Imported new potential chain segment     number=18 hash=3c828a..029a2d blocks=1 txs=1 mgas=0.055 elapsed=\"447.333\u00b5s\" mgasps=121.851 age=1m47s snapdiffs=11.40KiB triedirty=0.00B\nINFO [05-05|13:52:04.895] Chain head was updated                   number=18 hash=3c828a..029a2d root=beb830..20b1d3 elapsed=\"23.375\u00b5s\"  age=1m47s\nINFO [05-05|13:52:04.896] Starting work on payload                 id=0x03106a23199c8ad3\nINFO [05-05|13:52:04.897] Updated payload                          id=0x03106a23199c8ad3 number=19 hash=4b7d16..00b4dd txs=1 withdrawals=0 gas=46120   fees=0             root=e8c680..a58250 elapsed=\"340.709\u00b5s\"\nINFO [05-05|13:52:04.897] Stopping work on payload                 id=0x03106a23199c8ad3 reason=delivery elapsed=\"488.125\u00b5s\"\nINFO [05-05|13:52:04.898] Imported new potential chain segment     number=19 hash=4b7d16..00b4dd blocks=1 txs=1 mgas=0.046 elapsed=\"399.375\u00b5s\" mgasps=115.480 age=1m45s snapdiffs=11.95KiB triedirty=0.00B\nINFO [05-05|13:52:04.898] Chain head was updated                   number=19 hash=4b7d16..00b4dd root=e8c680..a58250 elapsed=\"67.583\u00b5s\"  age=1m45s\nINFO [05-05|13:52:04.899] Starting work on payload                 id=0x036654579ce79060\nINFO [05-05|13:52:04.900] Updated payload                          id=0x036654579ce79060 number=20 hash=f69cb5..1e0e4e txs=1 withdrawals=0 gas=46120   fees=0             root=ff45c0..6c0dc0 elapsed=\"715.167\u00b5s\"\nINFO [05-05|13:52:04.900] Stopping work on payload                 id=0x036654579ce79060 reason=delivery elapsed=\"814.167\u00b5s\"\nINFO [05-05|13:52:04.902] Imported new potential chain segment     number=20 hash=f69cb5..1e0e4e blocks=1 txs=1 mgas=0.046 elapsed=\"507.792\u00b5s\" mgasps=90.825  age=1m43s snapdiffs=12.49KiB triedirty=0.00B\nINFO [05-05|13:52:04.902] Chain head was updated                   number=20 hash=f69cb5..1e0e4e root=ff45c0..6c0dc0 elapsed=\"30.917\u00b5s\"  age=1m43s\nINFO [05-05|13:52:04.904] Starting work on payload                 id=0x03b0876c6aeeea48\nINFO [05-05|13:52:04.904] Updated payload                          id=0x03b0876c6aeeea48 number=21 hash=8c8ffe..bf0ba3 txs=1 withdrawals=0 gas=54508   fees=0             root=2986f2..70225f elapsed=\"412.875\u00b5s\"\nINFO [05-05|13:52:04.904] Stopping work on payload                 id=0x03b0876c6aeeea48 reason=delivery elapsed=\"667.042\u00b5s\"\nINFO [05-05|13:52:04.906] Imported new potential chain segment     number=21 hash=8c8ffe..bf0ba3 blocks=1 txs=1 mgas=0.055 elapsed=1.043ms     mgasps=52.246  age=1m41s snapdiffs=13.18KiB triedirty=0.00B\nINFO [05-05|13:52:04.906] Chain head was updated                   number=21 hash=8c8ffe..bf0ba3 root=2986f2..70225f elapsed=\"33.208\u00b5s\"  age=1m41s\nINFO [05-05|13:52:04.908] Starting work on payload                 id=0x034ba2b8caf15b38\nINFO [05-05|13:52:04.908] Updated payload                          id=0x034ba2b8caf15b38 number=22 hash=56da55..d5a904 txs=1 withdrawals=0 gas=46120   fees=0             root=732cc0..0ba24b elapsed=\"361.542\u00b5s\"\nINFO [05-05|13:52:04.909] Stopping work on payload                 id=0x034ba2b8caf15b38 reason=delivery elapsed=\"604\u00b5s\"\nINFO [05-05|13:52:04.910] Imported new potential chain segment     number=22 hash=56da55..d5a904 blocks=1 txs=1 mgas=0.046 elapsed=\"808.166\u00b5s\" mgasps=57.067  age=1m39s snapdiffs=13.72KiB triedirty=0.00B\nINFO [05-05|13:52:04.910] Chain head was updated                   number=22 hash=56da55..d5a904 root=732cc0..0ba24b elapsed=\"24.583\u00b5s\"  age=1m39s\nINFO [05-05|13:52:04.912] Starting work on payload                 id=0x03e9404cf2c57aa0\nINFO [05-05|13:52:04.912] Updated payload                          id=0x03e9404cf2c57aa0 number=23 hash=ad9545..3b7d75 txs=1 withdrawals=0 gas=46120   fees=0             root=7f89ae..561b06 elapsed=\"420.875\u00b5s\"\nINFO [05-05|13:52:04.912] Stopping work on payload                 id=0x03e9404cf2c57aa0 reason=delivery elapsed=\"763.167\u00b5s\"\nINFO [05-05|13:52:04.914] Imported new potential chain segment     number=23 hash=ad9545..3b7d75 blocks=1 txs=1 mgas=0.046 elapsed=1.364ms     mgasps=33.796  age=1m37s snapdiffs=14.27KiB triedirty=0.00B\nINFO [05-05|13:52:04.915] Chain head was updated                   number=23 hash=ad9545..3b7d75 root=7f89ae..561b06 elapsed=\"26.958\u00b5s\"  age=1m37s\nINFO [05-05|13:52:04.918] Starting work on payload                 id=0x0397b550af4cdc17\nINFO [05-05|13:52:04.919] Updated payload                          id=0x0397b550af4cdc17 number=24 hash=334979..f9f52f txs=1 withdrawals=0 gas=54508   fees=0             root=8c47d6..81bf3f elapsed=1.085ms\nINFO [05-05|13:52:04.919] Stopping work on payload                 id=0x0397b550af4cdc17 reason=delivery elapsed=1.248ms\nINFO [05-05|13:52:04.921] Imported new potential chain segment     number=24 hash=334979..f9f52f blocks=1 txs=1 mgas=0.055 elapsed=\"611.375\u00b5s\" mgasps=89.156  age=1m35s snapdiffs=14.96KiB triedirty=0.00B\nINFO [05-05|13:52:04.921] Chain head was updated                   number=24 hash=334979..f9f52f root=8c47d6..81bf3f elapsed=\"28.167\u00b5s\"  age=1m35s\nINFO [05-05|13:52:04.924] Starting work on payload                 id=0x03574ebe52e874ac\nINFO [05-05|13:52:04.924] Updated payload                          id=0x03574ebe52e874ac number=25 hash=c9fd05..bfb7d0 txs=1 withdrawals=0 gas=46120   fees=0             root=f9a39a..6d906e elapsed=\"532.459\u00b5s\"\nINFO [05-05|13:52:04.924] Stopping work on payload                 id=0x03574ebe52e874ac reason=delivery elapsed=\"694.833\u00b5s\"\nINFO [05-05|13:52:04.927] Imported new potential chain segment     number=25 hash=c9fd05..bfb7d0 blocks=1 txs=1 mgas=0.046 elapsed=1.257ms     mgasps=36.676  age=1m33s snapdiffs=15.50KiB triedirty=0.00B\nINFO [05-05|13:52:04.927] Chain head was updated                   number=25 hash=c9fd05..bfb7d0 root=f9a39a..6d906e elapsed=\"29.958\u00b5s\"  age=1m33s\nINFO [05-05|13:52:04.928] Starting work on payload                 id=0x0345c0054e8d2d57\nINFO [05-05|13:52:04.929] Updated payload                          id=0x0345c0054e8d2d57 number=26 hash=e11808..2b1e66 txs=1 withdrawals=0 gas=46120   fees=0             root=c6a879..3e331c elapsed=\"367.083\u00b5s\"\nINFO [05-05|13:52:04.929] Stopping work on payload                 id=0x0345c0054e8d2d57 reason=delivery elapsed=\"862.25\u00b5s\"\nINFO [05-05|13:52:04.931] Imported new potential chain segment     number=26 hash=e11808..2b1e66 blocks=1 txs=1 mgas=0.046 elapsed=\"595.542\u00b5s\" mgasps=77.442  age=1m31s snapdiffs=16.04KiB triedirty=0.00B\nINFO [05-05|13:52:04.932] Chain head was updated                   number=26 hash=e11808..2b1e66 root=c6a879..3e331c elapsed=\"31.541\u00b5s\"  age=1m31s\nINFO [05-05|13:52:04.934] Starting work on payload                 id=0x0313c99ebbbe57a1\nINFO [05-05|13:52:04.935] Updated payload                          id=0x0313c99ebbbe57a1 number=27 hash=13bb27..49606a txs=1 withdrawals=0 gas=54508   fees=0             root=e28bb1..743df9 elapsed=\"411.959\u00b5s\"\nINFO [05-05|13:52:04.935] Stopping work on payload                 id=0x0313c99ebbbe57a1 reason=delivery elapsed=\"574.375\u00b5s\"\nINFO [05-05|13:52:04.939] Imported new potential chain segment     number=27 hash=13bb27..49606a blocks=1 txs=1 mgas=0.055 elapsed=2.301ms     mgasps=23.685  age=1m29s snapdiffs=16.73KiB triedirty=0.00B\nINFO [05-05|13:52:04.940] Chain head was updated                   number=27 hash=13bb27..49606a root=e28bb1..743df9 elapsed=\"33.209\u00b5s\"  age=1m29s\nINFO [05-05|13:52:04.942] Starting work on payload                 id=0x030059017bcf5b50\nINFO [05-05|13:52:04.942] Updated payload                          id=0x030059017bcf5b50 number=28 hash=e5d233..3a6081 txs=1 withdrawals=0 gas=46120   fees=0             root=3e97f9..102a83 elapsed=\"387.834\u00b5s\"\nINFO [05-05|13:52:04.943] Stopping work on payload                 id=0x030059017bcf5b50 reason=delivery elapsed=\"547.084\u00b5s\"\nINFO [05-05|13:52:04.944] Imported new potential chain segment     number=28 hash=e5d233..3a6081 blocks=1 txs=1 mgas=0.046 elapsed=\"584.125\u00b5s\" mgasps=78.956  age=1m27s snapdiffs=17.27KiB triedirty=0.00B\nINFO [05-05|13:52:04.944] Chain head was updated                   number=28 hash=e5d233..3a6081 root=3e97f9..102a83 elapsed=\"23.917\u00b5s\"  age=1m27s\nINFO [05-05|13:52:04.946] Starting work on payload                 id=0x032ad36fc12ebc1a\nINFO [05-05|13:52:04.947] Updated payload                          id=0x032ad36fc12ebc1a number=29 hash=172cf6..248104 txs=1 withdrawals=0 gas=46120   fees=0             root=65aae4..62f2ad elapsed=\"380\u00b5s\"\nINFO [05-05|13:52:04.947] Stopping work on payload                 id=0x032ad36fc12ebc1a reason=delivery elapsed=\"718.958\u00b5s\"\nINFO [05-05|13:52:04.948] Imported new potential chain segment     number=29 hash=172cf6..248104 blocks=1 txs=1 mgas=0.046 elapsed=\"494.084\u00b5s\" mgasps=93.344  age=1m25s snapdiffs=17.82KiB triedirty=0.00B\nINFO [05-05|13:52:04.949] Chain head was updated                   number=29 hash=172cf6..248104 root=65aae4..62f2ad elapsed=\"23.458\u00b5s\"  age=1m25s\nINFO [05-05|13:52:04.951] Starting work on payload                 id=0x03ac65e83b48875b\nINFO [05-05|13:52:04.952] Updated payload                          id=0x03ac65e83b48875b number=30 hash=c40758..65b4fd txs=1 withdrawals=0 gas=54508   fees=0             root=98019f..9a302b elapsed=\"500.5\u00b5s\"\nINFO [05-05|13:52:04.952] Stopping work on payload                 id=0x03ac65e83b48875b reason=delivery elapsed=\"729.792\u00b5s\"\nINFO [05-05|13:52:04.954] Imported new potential chain segment     number=30 hash=c40758..65b4fd blocks=1 txs=1 mgas=0.055 elapsed=\"720.625\u00b5s\" mgasps=75.640  age=1m23s snapdiffs=18.51KiB triedirty=0.00B\nINFO [05-05|13:52:04.954] Chain head was updated                   number=30 hash=c40758..65b4fd root=98019f..9a302b elapsed=\"40.375\u00b5s\"  age=1m23s\nINFO [05-05|13:52:04.957] Starting work on payload                 id=0x0348a32d128c2f37\nINFO [05-05|13:52:04.958] Updated payload                          id=0x0348a32d128c2f37 number=31 hash=bd4008..3f9eac txs=1 withdrawals=0 gas=46120   fees=0             root=2179a0..06695d elapsed=\"450.041\u00b5s\"\nINFO [05-05|13:52:04.958] Stopping work on payload                 id=0x0348a32d128c2f37 reason=delivery elapsed=\"847.334\u00b5s\"\nINFO [05-05|13:52:04.961] Imported new potential chain segment     number=31 hash=bd4008..3f9eac blocks=1 txs=1 mgas=0.046 elapsed=1.221ms     mgasps=37.765  age=1m21s snapdiffs=19.05KiB triedirty=0.00B\nINFO [05-05|13:52:04.962] Chain head was updated                   number=31 hash=bd4008..3f9eac root=2179a0..06695d elapsed=\"51.708\u00b5s\"  age=1m21s\nINFO [05-05|13:52:04.964] Starting work on payload                 id=0x033ddc294c528f64\nINFO [05-05|13:52:04.965] Updated payload                          id=0x033ddc294c528f64 number=32 hash=bb20ad..8c9162 txs=1 withdrawals=0 gas=46120   fees=0             root=d9f21b..7cd419 elapsed=\"596.958\u00b5s\"\nINFO [05-05|13:52:04.965] Stopping work on payload                 id=0x033ddc294c528f64 reason=delivery elapsed=\"829.375\u00b5s\"\nINFO [05-05|13:52:04.966] Imported new potential chain segment     number=32 hash=bb20ad..8c9162 blocks=1 txs=1 mgas=0.046 elapsed=\"761.042\u00b5s\" mgasps=60.601  age=1m19s snapdiffs=19.59KiB triedirty=0.00B\nINFO [05-05|13:52:04.967] Chain head was updated                   number=32 hash=bb20ad..8c9162 root=d9f21b..7cd419 elapsed=\"53.083\u00b5s\"  age=1m19s\nINFO [05-05|13:52:04.970] Starting work on payload                 id=0x038d2651e0636e16\nINFO [05-05|13:52:04.971] Updated payload                          id=0x038d2651e0636e16 number=33 hash=1356f9..cd84bc txs=1 withdrawals=0 gas=54508   fees=0             root=ecde8c..1421c3 elapsed=\"341.292\u00b5s\"\nINFO [05-05|13:52:04.971] Stopping work on payload                 id=0x038d2651e0636e16 reason=delivery elapsed=1.005ms\nINFO [05-05|13:52:04.973] Imported new potential chain segment     number=33 hash=1356f9..cd84bc blocks=1 txs=1 mgas=0.055 elapsed=\"651.625\u00b5s\" mgasps=83.649  age=1m17s snapdiffs=20.28KiB triedirty=0.00B\nINFO [05-05|13:52:04.974] Chain head was updated                   number=33 hash=1356f9..cd84bc root=ecde8c..1421c3 elapsed=\"42.959\u00b5s\"  age=1m17s\nINFO [05-05|13:52:04.976] Starting work on payload                 id=0x032a2d3964d4bc09\nINFO [05-05|13:52:04.977] Updated payload                          id=0x032a2d3964d4bc09 number=34 hash=451a63..1e3441 txs=1 withdrawals=0 gas=46120   fees=0             root=d31067..69746a elapsed=\"686.25\u00b5s\"\nINFO [05-05|13:52:04.977] Stopping work on payload                 id=0x032a2d3964d4bc09 reason=delivery elapsed=\"919.291\u00b5s\"\nINFO [05-05|13:52:04.979] Imported new potential chain segment     number=34 hash=451a63..1e3441 blocks=1 txs=1 mgas=0.046 elapsed=\"626.667\u00b5s\" mgasps=73.596  age=1m15s snapdiffs=20.83KiB triedirty=0.00B\nINFO [05-05|13:52:04.979] Chain head was updated                   number=34 hash=451a63..1e3441 root=d31067..69746a elapsed=\"38.875\u00b5s\"  age=1m15s\nINFO [05-05|13:52:04.981] Starting work on payload                 id=0x03f39a110dc8a485\nINFO [05-05|13:52:04.982] Updated payload                          id=0x03f39a110dc8a485 number=35 hash=5c0728..be3fca txs=1 withdrawals=0 gas=46120   fees=0             root=50a5b7..766190 elapsed=\"427.083\u00b5s\"\nINFO [05-05|13:52:04.982] Stopping work on payload                 id=0x03f39a110dc8a485 reason=delivery elapsed=\"912.542\u00b5s\"\nINFO [05-05|13:52:04.984] Imported new potential chain segment     number=35 hash=5c0728..be3fca blocks=1 txs=1 mgas=0.046 elapsed=\"487.917\u00b5s\" mgasps=94.524  age=1m13s snapdiffs=21.37KiB triedirty=0.00B\nINFO [05-05|13:52:04.984] Chain head was updated                   number=35 hash=5c0728..be3fca root=50a5b7..766190 elapsed=\"36.209\u00b5s\"  age=1m13s\nINFO [05-05|13:52:04.986] Starting work on payload                 id=0x03b38510af0ac616\nINFO [05-05|13:52:04.987] Updated payload                          id=0x03b38510af0ac616 number=36 hash=749cf8..a4e8c6 txs=1 withdrawals=0 gas=54508   fees=0             root=cc6df4..9fb5b1 elapsed=\"277.834\u00b5s\"\nINFO [05-05|13:52:04.987] Stopping work on payload                 id=0x03b38510af0ac616 reason=delivery elapsed=\"618.875\u00b5s\"\nINFO [05-05|13:52:04.988] Imported new potential chain segment     number=36 hash=749cf8..a4e8c6 blocks=1 txs=1 mgas=0.055 elapsed=\"575.542\u00b5s\" mgasps=94.707  age=1m11s snapdiffs=22.06KiB triedirty=0.00B\nINFO [05-05|13:52:04.989] Chain head was updated                   number=36 hash=749cf8..a4e8c6 root=cc6df4..9fb5b1 elapsed=\"32.5\u00b5s\"    age=1m11s\nINFO [05-05|13:52:04.990] Starting work on payload                 id=0x03a395975f25c5fd\nINFO [05-05|13:52:04.991] Updated payload                          id=0x03a395975f25c5fd number=37 hash=677670..d9f243 txs=1 withdrawals=0 gas=46120   fees=0             root=3108bf..17b74b elapsed=\"909.291\u00b5s\"\nINFO [05-05|13:52:04.991] Stopping work on payload                 id=0x03a395975f25c5fd reason=delivery elapsed=1.170ms\nINFO [05-05|13:52:04.993] Imported new potential chain segment     number=37 hash=677670..d9f243 blocks=1 txs=1 mgas=0.046 elapsed=\"827.459\u00b5s\" mgasps=55.737  age=1m9s  snapdiffs=22.60KiB triedirty=0.00B\nINFO [05-05|13:52:04.993] Chain head was updated                   number=37 hash=677670..d9f243 root=3108bf..17b74b elapsed=\"30.25\u00b5s\"   age=1m9s\nINFO [05-05|13:52:04.995] Starting work on payload                 id=0x03dfda29b777337e\nINFO [05-05|13:52:04.996] Updated payload                          id=0x03dfda29b777337e number=38 hash=2af1c5..6594da txs=1 withdrawals=0 gas=46120   fees=0             root=fd1485..c04ea2 elapsed=\"343.583\u00b5s\"\nINFO [05-05|13:52:04.996] Stopping work on payload                 id=0x03dfda29b777337e reason=delivery elapsed=\"821\u00b5s\"\nINFO [05-05|13:52:04.997] Imported new potential chain segment     number=38 hash=2af1c5..6594da blocks=1 txs=1 mgas=0.046 elapsed=\"596.916\u00b5s\" mgasps=77.264  age=1m7s  snapdiffs=23.14KiB triedirty=0.00B\nINFO [05-05|13:52:04.998] Chain head was updated                   number=38 hash=2af1c5..6594da root=fd1485..c04ea2 elapsed=\"41.334\u00b5s\"  age=1m7s\nINFO [05-05|13:52:05.000] Starting work on payload                 id=0x03a70060bf75dff3\nINFO [05-05|13:52:05.001] Updated payload                          id=0x03a70060bf75dff3 number=39 hash=039cc0..941033 txs=1 withdrawals=0 gas=54508   fees=0             root=a770fb..70ed14 elapsed=\"672.791\u00b5s\"\nINFO [05-05|13:52:05.002] Stopping work on payload                 id=0x03a70060bf75dff3 reason=delivery elapsed=1.699ms\nINFO [05-05|13:52:05.007] Imported new potential chain segment     number=39 hash=039cc0..941033 blocks=1 txs=1 mgas=0.055 elapsed=1.354ms     mgasps=40.238  age=1m6s  snapdiffs=23.83KiB triedirty=0.00B\nINFO [05-05|13:52:05.008] Chain head was updated                   number=39 hash=039cc0..941033 root=a770fb..70ed14 elapsed=\"50.334\u00b5s\"  age=1m6s\nINFO [05-05|13:52:05.011] Starting work on payload                 id=0x03747460063c957c\nINFO [05-05|13:52:05.012] Updated payload                          id=0x03747460063c957c number=40 hash=88e876..7bab29 txs=1 withdrawals=0 gas=46120   fees=0             root=af857c..7cc89a elapsed=\"904.666\u00b5s\"\nINFO [05-05|13:52:05.012] Stopping work on payload                 id=0x03747460063c957c reason=delivery elapsed=1.033ms\nINFO [05-05|13:52:05.014] Imported new potential chain segment     number=40 hash=88e876..7bab29 blocks=1 txs=1 mgas=0.046 elapsed=\"683.625\u00b5s\" mgasps=67.464  age=1m4s  snapdiffs=24.38KiB triedirty=0.00B\nINFO [05-05|13:52:05.014] Chain head was updated                   number=40 hash=88e876..7bab29 root=af857c..7cc89a elapsed=\"52.042\u00b5s\"  age=1m4s\nINFO [05-05|13:52:05.017] Starting work on payload                 id=0x0379c3f0cd55c5b3\nINFO [05-05|13:52:05.019] Updated payload                          id=0x0379c3f0cd55c5b3 number=41 hash=ec5115..7b6c17 txs=1 withdrawals=0 gas=46120   fees=0             root=09a4f3..bc3e63 elapsed=1.070ms\nINFO [05-05|13:52:05.019] Stopping work on payload                 id=0x0379c3f0cd55c5b3 reason=delivery elapsed=1.562ms\nINFO [05-05|13:52:05.021] Imported new potential chain segment     number=41 hash=ec5115..7b6c17 blocks=1 txs=1 mgas=0.046 elapsed=1.116ms     mgasps=41.312  age=1m2s  snapdiffs=24.92KiB triedirty=0.00B\nINFO [05-05|13:52:05.021] Chain head was updated                   number=41 hash=ec5115..7b6c17 root=09a4f3..bc3e63 elapsed=\"44.583\u00b5s\"  age=1m2s\nINFO [05-05|13:52:05.025] Starting work on payload                 id=0x03142ff6f6888389\nINFO [05-05|13:52:05.025] Updated payload                          id=0x03142ff6f6888389 number=42 hash=f83c43..de3528 txs=1 withdrawals=0 gas=54496   fees=0             root=a7169e..87bda2 elapsed=\"578.5\u00b5s\"\nINFO [05-05|13:52:05.025] Stopping work on payload                 id=0x03142ff6f6888389 reason=delivery elapsed=\"749.709\u00b5s\"\nINFO [05-05|13:52:05.027] Imported new potential chain segment     number=42 hash=f83c43..de3528 blocks=1 txs=1 mgas=0.054 elapsed=\"825.125\u00b5s\" mgasps=66.046  age=1m    snapdiffs=25.61KiB triedirty=0.00B\nINFO [05-05|13:52:05.028] Chain head was updated                   number=42 hash=f83c43..de3528 root=a7169e..87bda2 elapsed=\"36.583\u00b5s\"  age=1m\nINFO [05-05|13:52:05.040] Starting work on payload                 id=0x03b50de815c53a84\nINFO [05-05|13:52:05.041] Updated payload                          id=0x03b50de815c53a84 number=43 hash=72c240..d4f7e5 txs=1 withdrawals=0 gas=46108   fees=0             root=fcd24e..de376d elapsed=\"368.75\u00b5s\"\nINFO [05-05|13:52:05.042] Stopping work on payload                 id=0x03b50de815c53a84 reason=delivery elapsed=1.308ms\nINFO [05-05|13:52:05.044] Imported new potential chain segment     number=43 hash=72c240..d4f7e5 blocks=1 txs=1 mgas=0.046 elapsed=\"768.25\u00b5s\"  mgasps=60.017  snapdiffs=26.15KiB triedirty=0.00B\nINFO [05-05|13:52:05.046] Chain head was updated                   number=43 hash=72c240..d4f7e5 root=fcd24e..de376d elapsed=\"71.084\u00b5s\"\nINFO [05-05|13:52:05.050] Starting work on payload                 id=0x035109ccc535d266\nINFO [05-05|13:52:05.050] Updated payload                          id=0x035109ccc535d266 number=44 hash=beea01..c293be txs=1 withdrawals=0 gas=46108   fees=0             root=6e74cb..4a50e6 elapsed=\"645.709\u00b5s\"\nINFO [05-05|13:52:05.051] Stopping work on payload                 id=0x035109ccc535d266 reason=delivery elapsed=1.298ms\nINFO [05-05|13:52:05.053] Imported new potential chain segment     number=44 hash=beea01..c293be blocks=1 txs=1 mgas=0.046 elapsed=\"759.458\u00b5s\" mgasps=60.712  snapdiffs=26.70KiB triedirty=0.00B\nINFO [05-05|13:52:05.053] Chain head was updated                   number=44 hash=beea01..c293be root=6e74cb..4a50e6 elapsed=\"31.416\u00b5s\"\nINFO [05-05|13:52:05.056] Starting work on payload                 id=0x0358f86ba26aeebe\nINFO [05-05|13:52:05.056] Updated payload                          id=0x0358f86ba26aeebe number=45 hash=04aaaa..c2e835 txs=1 withdrawals=0 gas=54496   fees=0             root=3ef041..52a574 elapsed=\"411.291\u00b5s\"\nINFO [05-05|13:52:05.057] Stopping work on payload                 id=0x0358f86ba26aeebe reason=delivery elapsed=\"965.25\u00b5s\"\nINFO [05-05|13:52:05.058] Imported new potential chain segment     number=45 hash=04aaaa..c2e835 blocks=1 txs=1 mgas=0.054 elapsed=\"673.125\u00b5s\" mgasps=80.960  snapdiffs=27.38KiB triedirty=0.00B\nINFO [05-05|13:52:05.059] Chain head was updated                   number=45 hash=04aaaa..c2e835 root=3ef041..52a574 elapsed=\"55.25\u00b5s\"\nINFO [05-05|13:52:05.061] Starting work on payload                 id=0x03626541576ddc29\nINFO [05-05|13:52:05.061] Updated payload                          id=0x03626541576ddc29 number=46 hash=a95cf3..bbf4ea txs=1 withdrawals=0 gas=46108   fees=0             root=98c5bd..966499 elapsed=\"356\u00b5s\"\nINFO [05-05|13:52:05.062] Stopping work on payload                 id=0x03626541576ddc29 reason=delivery elapsed=\"732.458\u00b5s\"\nINFO [05-05|13:52:05.063] Imported new potential chain segment     number=46 hash=a95cf3..bbf4ea blocks=1 txs=1 mgas=0.046 elapsed=\"830.584\u00b5s\" mgasps=55.513  snapdiffs=27.93KiB triedirty=0.00B\nINFO [05-05|13:52:05.067] Chain head was updated                   number=46 hash=a95cf3..bbf4ea root=98c5bd..966499 elapsed=\"39.666\u00b5s\"\nINFO [05-05|13:52:05.068] Starting work on payload                 id=0x03f04b556f73af05\nINFO [05-05|13:52:05.069] Updated payload                          id=0x03f04b556f73af05 number=47 hash=34d8a5..0005b2 txs=1 withdrawals=0 gas=46108   fees=0             root=750dfd..ad4770 elapsed=\"852.208\u00b5s\"\nINFO [05-05|13:52:05.069] Stopping work on payload                 id=0x03f04b556f73af05 reason=delivery elapsed=1.015ms\nINFO [05-05|13:52:05.074] Imported new potential chain segment     number=47 hash=34d8a5..0005b2 blocks=1 txs=1 mgas=0.046 elapsed=\"819.583\u00b5s\" mgasps=56.258  snapdiffs=28.47KiB triedirty=0.00B\nINFO [05-05|13:52:05.075] Chain head was updated                   number=47 hash=34d8a5..0005b2 root=750dfd..ad4770 elapsed=\"42.334\u00b5s\"\nINFO [05-05|13:52:05.079] Starting work on payload                 id=0x03484142c2ae84a5\nINFO [05-05|13:52:05.081] Updated payload                          id=0x03484142c2ae84a5 number=48 hash=dc22e5..ebcb89 txs=1 withdrawals=0 gas=54508   fees=0             root=2ced29..4469d5 elapsed=2.300ms\nINFO [05-05|13:52:05.081] Stopping work on payload                 id=0x03484142c2ae84a5 reason=delivery elapsed=2.553ms\nINFO [05-05|13:52:05.084] Imported new potential chain segment     number=48 hash=dc22e5..ebcb89 blocks=1 txs=1 mgas=0.055 elapsed=1.712ms     mgasps=31.829  snapdiffs=29.16KiB triedirty=0.00B\nINFO [05-05|13:52:05.085] Chain head was updated                   number=48 hash=dc22e5..ebcb89 root=2ced29..4469d5 elapsed=\"46.834\u00b5s\"\nINFO [05-05|13:52:05.087] Starting work on payload                 id=0x0307832a9b399e05\nINFO [05-05|13:52:05.088] Updated payload                          id=0x0307832a9b399e05 number=49 hash=a5d8ff..efd6c3 txs=1 withdrawals=0 gas=46120   fees=0             root=3495a7..ab69d9 elapsed=\"854.709\u00b5s\"\nINFO [05-05|13:52:05.088] Stopping work on payload                 id=0x0307832a9b399e05 reason=delivery elapsed=1.051ms\nINFO [05-05|13:52:05.090] Imported new potential chain segment     number=49 hash=a5d8ff..efd6c3 blocks=1 txs=1 mgas=0.046 elapsed=\"776.667\u00b5s\" mgasps=59.382  snapdiffs=29.70KiB triedirty=0.00B\nINFO [05-05|13:52:05.091] Chain head was updated                   number=49 hash=a5d8ff..efd6c3 root=3495a7..ab69d9 elapsed=\"40.958\u00b5s\"\nINFO [05-05|13:52:05.092] Starting work on payload                 id=0x03a2fab36935df0c\nINFO [05-05|13:52:05.092] Updated payload                          id=0x03a2fab36935df0c number=50 hash=14e32d..6ac553 txs=1 withdrawals=0 gas=46120   fees=0             root=449403..4cfbae elapsed=\"345.291\u00b5s\"\nINFO [05-05|13:52:05.093] Stopping work on payload                 id=0x03a2fab36935df0c reason=delivery elapsed=\"596.666\u00b5s\"\nINFO [05-05|13:52:05.094] Imported new potential chain segment     number=50 hash=14e32d..6ac553 blocks=1 txs=1 mgas=0.046 elapsed=\"489.208\u00b5s\" mgasps=94.275  snapdiffs=30.25KiB triedirty=0.00B\nINFO [05-05|13:52:05.094] Chain head was updated                   number=50 hash=14e32d..6ac553 root=449403..4cfbae elapsed=\"22.709\u00b5s\"\nINFO [05-05|13:52:05.096] Starting work on payload                 id=0x0305858a36b29627\nINFO [05-05|13:52:05.097] Updated payload                          id=0x0305858a36b29627 number=51 hash=697cad..a03268 txs=1 withdrawals=0 gas=54508   fees=0             root=f8e2c0..be368d elapsed=\"465.375\u00b5s\"\nINFO [05-05|13:52:05.097] Stopping work on payload                 id=0x0305858a36b29627 reason=delivery elapsed=\"913.625\u00b5s\"\nINFO [05-05|13:52:05.099] Imported new potential chain segment     number=51 hash=697cad..a03268 blocks=1 txs=1 mgas=0.055 elapsed=\"767.542\u00b5s\" mgasps=71.016  snapdiffs=30.93KiB triedirty=0.00B\nINFO [05-05|13:52:05.100] Chain head was updated                   number=51 hash=697cad..a03268 root=f8e2c0..be368d elapsed=\"33.083\u00b5s\"\nINFO [05-05|13:52:05.101] Starting work on payload                 id=0x03ef35634c1e5124\nINFO [05-05|13:52:05.102] Updated payload                          id=0x03ef35634c1e5124 number=52 hash=ed4203..2350f0 txs=1 withdrawals=0 gas=46120   fees=0             root=dc67c7..f52ca8 elapsed=\"293.917\u00b5s\"\nINFO [05-05|13:52:05.102] Stopping work on payload                 id=0x03ef35634c1e5124 reason=delivery elapsed=\"480.833\u00b5s\"\nINFO [05-05|13:52:05.104] Imported new potential chain segment     number=52 hash=ed4203..2350f0 blocks=1 txs=1 mgas=0.046 elapsed=2.021ms     mgasps=22.811  snapdiffs=31.48KiB triedirty=0.00B\nINFO [05-05|13:52:05.105] Chain head was updated                   number=52 hash=ed4203..2350f0 root=dc67c7..f52ca8 elapsed=\"36.583\u00b5s\"\nINFO [05-05|13:52:05.106] Starting work on payload                 id=0x031420388be6aad0\nINFO [05-05|13:52:05.107] Updated payload                          id=0x031420388be6aad0 number=53 hash=23b2b2..9f25d5 txs=1 withdrawals=0 gas=46120   fees=0             root=0a3419..c7ac5c elapsed=\"488\u00b5s\"\nINFO [05-05|13:52:05.107] Stopping work on payload                 id=0x031420388be6aad0 reason=delivery elapsed=\"684.334\u00b5s\"\nINFO [05-05|13:52:05.108] Imported new potential chain segment     number=53 hash=23b2b2..9f25d5 blocks=1 txs=1 mgas=0.046 elapsed=\"459.667\u00b5s\" mgasps=100.334 snapdiffs=32.02KiB triedirty=0.00B\nINFO [05-05|13:52:05.108] Chain head was updated                   number=53 hash=23b2b2..9f25d5 root=0a3419..c7ac5c elapsed=\"29.5\u00b5s\"\nINFO [05-05|13:52:05.110] Starting work on payload                 id=0x030f1e4e78df7bd4\nINFO [05-05|13:52:05.111] Updated payload                          id=0x030f1e4e78df7bd4 number=54 hash=9bb7fc..a74a65 txs=1 withdrawals=0 gas=54508   fees=0             root=3ce97d..9b83ca elapsed=\"594.25\u00b5s\"\nINFO [05-05|13:52:05.111] Stopping work on payload                 id=0x030f1e4e78df7bd4 reason=delivery elapsed=\"745.583\u00b5s\"\nINFO [05-05|13:52:05.112] Imported new potential chain segment     number=54 hash=9bb7fc..a74a65 blocks=1 txs=1 mgas=0.055 elapsed=\"625.834\u00b5s\" mgasps=87.097  snapdiffs=32.71KiB triedirty=0.00B\nINFO [05-05|13:52:05.113] Chain head was updated                   number=54 hash=9bb7fc..a74a65 root=3ce97d..9b83ca elapsed=\"25.875\u00b5s\"\nINFO [05-05|13:52:05.114] Starting work on payload                 id=0x03e0806b3a22da47\nINFO [05-05|13:52:05.115] Updated payload                          id=0x03e0806b3a22da47 number=55 hash=0995fc..22de60 txs=1 withdrawals=0 gas=46120   fees=0             root=7c63e7..d095f8 elapsed=\"455.5\u00b5s\"\nINFO [05-05|13:52:05.115] Stopping work on payload                 id=0x03e0806b3a22da47 reason=delivery elapsed=\"633.709\u00b5s\"\nINFO [05-05|13:52:05.116] Imported new potential chain segment     number=55 hash=0995fc..22de60 blocks=1 txs=1 mgas=0.046 elapsed=\"426.667\u00b5s\" mgasps=108.094 snapdiffs=33.25KiB triedirty=0.00B\nINFO [05-05|13:52:05.116] Chain head was updated                   number=55 hash=0995fc..22de60 root=7c63e7..d095f8 elapsed=\"19.708\u00b5s\"\nINFO [05-05|13:52:05.117] Starting work on payload                 id=0x03f5502a1443c8e5\nINFO [05-05|13:52:05.118] Updated payload                          id=0x03f5502a1443c8e5 number=56 hash=01b679..51c020 txs=1 withdrawals=0 gas=46120   fees=0             root=001f7a..942ab7 elapsed=\"325.208\u00b5s\"\nINFO [05-05|13:52:05.118] Stopping work on payload                 id=0x03f5502a1443c8e5 reason=delivery elapsed=\"479.458\u00b5s\"\nINFO [05-05|13:52:05.119] Imported new potential chain segment     number=56 hash=01b679..51c020 blocks=1 txs=1 mgas=0.046 elapsed=\"384.958\u00b5s\" mgasps=119.805 snapdiffs=33.80KiB triedirty=0.00B\nINFO [05-05|13:52:05.119] Chain head was updated                   number=56 hash=01b679..51c020 root=001f7a..942ab7 elapsed=\"60.916\u00b5s\"\nINFO [05-05|13:52:05.121] Starting work on payload                 id=0x03b4dafcdd91b489\nINFO [05-05|13:52:05.121] Updated payload                          id=0x03b4dafcdd91b489 number=57 hash=a6cb0f..781115 txs=1 withdrawals=0 gas=54508   fees=0             root=9d6a27..cbd2a1 elapsed=\"329.708\u00b5s\"\nINFO [05-05|13:52:05.121] Stopping work on payload                 id=0x03b4dafcdd91b489 reason=delivery elapsed=\"498.792\u00b5s\"\nINFO [05-05|13:52:05.123] Imported new potential chain segment     number=57 hash=a6cb0f..781115 blocks=1 txs=1 mgas=0.055 elapsed=\"819.333\u00b5s\" mgasps=66.527  snapdiffs=34.48KiB triedirty=0.00B\nINFO [05-05|13:52:05.123] Chain head was updated                   number=57 hash=a6cb0f..781115 root=9d6a27..cbd2a1 elapsed=\"25.125\u00b5s\"\nINFO [05-05|13:52:05.125] Starting work on payload                 id=0x03c36c0561715abd\nINFO [05-05|13:52:05.125] Updated payload                          id=0x03c36c0561715abd number=58 hash=2e08b8..afc8fd txs=1 withdrawals=0 gas=46120   fees=0             root=ff5ec5..d360aa elapsed=\"231.208\u00b5s\"\nINFO [05-05|13:52:05.125] Stopping work on payload                 id=0x03c36c0561715abd reason=delivery elapsed=\"453.875\u00b5s\"\nINFO [05-05|13:52:05.126] Imported new potential chain segment     number=58 hash=2e08b8..afc8fd blocks=1 txs=1 mgas=0.046 elapsed=\"421.125\u00b5s\" mgasps=109.516 snapdiffs=35.03KiB triedirty=0.00B\nINFO [05-05|13:52:05.127] Chain head was updated                   number=58 hash=2e08b8..afc8fd root=ff5ec5..d360aa elapsed=\"22.625\u00b5s\"\nINFO [05-05|13:52:05.128] Starting work on payload                 id=0x03405cfb51496100\nINFO [05-05|13:52:05.128] Updated payload                          id=0x03405cfb51496100 number=59 hash=157c3f..62ad9a txs=1 withdrawals=0 gas=46120   fees=0             root=47f5b8..11a414 elapsed=\"357.083\u00b5s\"\nINFO [05-05|13:52:05.128] Stopping work on payload                 id=0x03405cfb51496100 reason=delivery elapsed=\"463.5\u00b5s\"\nINFO [05-05|13:52:05.129] Imported new potential chain segment     number=59 hash=157c3f..62ad9a blocks=1 txs=1 mgas=0.046 elapsed=\"380.917\u00b5s\" mgasps=121.076 snapdiffs=35.57KiB triedirty=0.00B\nINFO [05-05|13:52:05.131] Chain head was updated                   number=59 hash=157c3f..62ad9a root=47f5b8..11a414 elapsed=\"53.834\u00b5s\"\nINFO [05-05|13:52:05.132] Starting work on payload                 id=0x03ff617df2f6c57c\nINFO [05-05|13:52:05.133] Updated payload                          id=0x03ff617df2f6c57c number=60 hash=b5ab40..2dc29a txs=1 withdrawals=0 gas=54508   fees=0             root=e8ab7a..aefac5 elapsed=\"581.125\u00b5s\"\nINFO [05-05|13:52:05.134] Stopping work on payload                 id=0x03ff617df2f6c57c reason=delivery elapsed=\"895.917\u00b5s\"\nINFO [05-05|13:52:05.134] Imported new potential chain segment     number=60 hash=b5ab40..2dc29a blocks=1 txs=1 mgas=0.055 elapsed=\"530.166\u00b5s\" mgasps=102.813 snapdiffs=36.26KiB triedirty=0.00B\nINFO [05-05|13:52:05.135] Chain head was updated                   number=60 hash=b5ab40..2dc29a root=e8ab7a..aefac5 elapsed=\"30.375\u00b5s\"\nINFO [05-05|13:52:05.136] Starting work on payload                 id=0x039c385fe784bf9c\nINFO [05-05|13:52:05.136] Updated payload                          id=0x039c385fe784bf9c number=61 hash=32cccb..931fe2 txs=1 withdrawals=0 gas=46120   fees=0             root=1d1c68..ba5d87 elapsed=\"245.25\u00b5s\"\nINFO [05-05|13:52:05.137] Stopping work on payload                 id=0x039c385fe784bf9c reason=delivery elapsed=\"539\u00b5s\"\nINFO [05-05|13:52:05.138] Imported new potential chain segment     number=61 hash=32cccb..931fe2 blocks=1 txs=1 mgas=0.046 elapsed=\"664.708\u00b5s\" mgasps=69.384  snapdiffs=36.80KiB triedirty=0.00B\nINFO [05-05|13:52:05.138] Chain head was updated                   number=61 hash=32cccb..931fe2 root=1d1c68..ba5d87 elapsed=\"20.875\u00b5s\"\nINFO [05-05|13:52:05.139] Starting work on payload                 id=0x0311727e4c47236b\nINFO [05-05|13:52:05.139] Updated payload                          id=0x0311727e4c47236b number=62 hash=f45990..5be0d0 txs=1 withdrawals=0 gas=46120   fees=0             root=bf43a0..770fd7 elapsed=\"336.625\u00b5s\"\nINFO [05-05|13:52:05.139] Stopping work on payload                 id=0x0311727e4c47236b reason=delivery elapsed=\"507.792\u00b5s\"\nINFO [05-05|13:52:05.140] Imported new potential chain segment     number=62 hash=f45990..5be0d0 blocks=1 txs=1 mgas=0.046 elapsed=\"549.5\u00b5s\"   mgasps=83.931  snapdiffs=37.35KiB triedirty=0.00B\nINFO [05-05|13:52:05.141] Chain head was updated                   number=62 hash=f45990..5be0d0 root=bf43a0..770fd7 elapsed=\"25.375\u00b5s\"\nINFO [05-05|13:52:05.142] Starting work on payload                 id=0x03ef7968ab458e3f\nINFO [05-05|13:52:05.142] Updated payload                          id=0x03ef7968ab458e3f number=63 hash=d845e0..e8d577 txs=1 withdrawals=0 gas=54508   fees=0             root=7c2c51..0a03e5 elapsed=\"283.167\u00b5s\"\nINFO [05-05|13:52:05.142] Stopping work on payload                 id=0x03ef7968ab458e3f reason=delivery elapsed=\"391\u00b5s\"\nINFO [05-05|13:52:05.144] Imported new potential chain segment     number=63 hash=d845e0..e8d577 blocks=1 txs=1 mgas=0.055 elapsed=\"657.417\u00b5s\" mgasps=82.912  snapdiffs=38.04KiB triedirty=0.00B\nINFO [05-05|13:52:05.144] Chain head was updated                   number=63 hash=d845e0..e8d577 root=7c2c51..0a03e5 elapsed=\"23.958\u00b5s\"\nINFO [05-05|13:52:05.145] Starting work on payload                 id=0x039ba54065cf23cb\nINFO [05-05|13:52:05.146] Updated payload                          id=0x039ba54065cf23cb number=64 hash=adc94c..bd0606 txs=1 withdrawals=0 gas=46120   fees=0             root=8e715d..5042b9 elapsed=\"284.459\u00b5s\"\nINFO [05-05|13:52:05.146] Stopping work on payload                 id=0x039ba54065cf23cb reason=delivery elapsed=\"762.125\u00b5s\"\nINFO [05-05|13:52:05.147] Imported new potential chain segment     number=64 hash=adc94c..bd0606 blocks=1 txs=1 mgas=0.046 elapsed=\"560.083\u00b5s\" mgasps=82.345  snapdiffs=38.58KiB triedirty=0.00B\nINFO [05-05|13:52:05.147] Chain head was updated                   number=64 hash=adc94c..bd0606 root=8e715d..5042b9 elapsed=\"26.833\u00b5s\"\nINFO [05-05|13:52:05.148] Starting work on payload                 id=0x03a47f4277cc7732\nINFO [05-05|13:52:05.149] Updated payload                          id=0x03a47f4277cc7732 number=65 hash=8a406b..01ad64 txs=1 withdrawals=0 gas=46120   fees=0             root=c6b2e4..c5a300 elapsed=\"828.292\u00b5s\"\nINFO [05-05|13:52:05.150] Stopping work on payload                 id=0x03a47f4277cc7732 reason=delivery elapsed=1.353ms\nINFO [05-05|13:52:05.152] Imported new potential chain segment     number=65 hash=8a406b..01ad64 blocks=1 txs=1 mgas=0.046 elapsed=\"828.5\u00b5s\"   mgasps=55.667  snapdiffs=39.12KiB triedirty=0.00B\nINFO [05-05|13:52:05.153] Chain head was updated                   number=65 hash=8a406b..01ad64 root=c6b2e4..c5a300 elapsed=\"31\u00b5s\"\nINFO [05-05|13:52:05.154] Starting work on payload                 id=0x03f55a8b28789192\nINFO [05-05|13:52:05.155] Updated payload                          id=0x03f55a8b28789192 number=66 hash=202fbc..59f141 txs=1 withdrawals=0 gas=46120   fees=0             root=db099d..68117a elapsed=\"332\u00b5s\"\nINFO [05-05|13:52:05.155] Stopping work on payload                 id=0x03f55a8b28789192 reason=delivery elapsed=\"457.834\u00b5s\"\nINFO [05-05|13:52:05.156] Imported new potential chain segment     number=66 hash=202fbc..59f141 blocks=1 txs=1 mgas=0.046 elapsed=\"532.417\u00b5s\" mgasps=86.624  snapdiffs=39.67KiB triedirty=0.00B\nINFO [05-05|13:52:05.157] Chain head was updated                   number=66 hash=202fbc..59f141 root=db099d..68117a elapsed=\"37.458\u00b5s\"\nINFO [05-05|13:52:05.159] Starting work on payload                 id=0x030984ca22505146\nINFO [05-05|13:52:05.159] Updated payload                          id=0x030984ca22505146 number=67 hash=a6dfef..d714be txs=1 withdrawals=0 gas=46120   fees=0             root=091b52..d27218 elapsed=\"365.083\u00b5s\"\nINFO [05-05|13:52:05.159] Stopping work on payload                 id=0x030984ca22505146 reason=delivery elapsed=\"508.458\u00b5s\"\nINFO [05-05|13:52:05.160] Imported new potential chain segment     number=67 hash=a6dfef..d714be blocks=1 txs=1 mgas=0.046 elapsed=\"499.042\u00b5s\" mgasps=92.417  snapdiffs=40.21KiB triedirty=0.00B\nINFO [05-05|13:52:05.161] Chain head was updated                   number=67 hash=a6dfef..d714be root=091b52..d27218 elapsed=\"26.25\u00b5s\"\nINFO [05-05|13:52:05.162] Starting work on payload                 id=0x0316d17449478541\nINFO [05-05|13:52:05.163] Updated payload                          id=0x0316d17449478541 number=68 hash=121971..59f25f txs=1 withdrawals=0 gas=46120   fees=0             root=5d15a6..85bfe0 elapsed=\"347.917\u00b5s\"\nINFO [05-05|13:52:05.163] Stopping work on payload                 id=0x0316d17449478541 reason=delivery elapsed=\"512.167\u00b5s\"\nINFO [05-05|13:52:05.164] Imported new potential chain segment     number=68 hash=121971..59f25f blocks=1 txs=1 mgas=0.046 elapsed=\"463.375\u00b5s\" mgasps=99.531  snapdiffs=40.75KiB triedirty=0.00B\nINFO [05-05|13:52:05.164] Chain head was updated                   number=68 hash=121971..59f25f root=5d15a6..85bfe0 elapsed=\"30.709\u00b5s\"\nINFO [05-05|13:52:05.165] Starting work on payload                 id=0x03c8bf3ede9e277b\nINFO [05-05|13:52:05.166] Updated payload                          id=0x03c8bf3ede9e277b number=69 hash=1a06bb..098411 txs=1 withdrawals=0 gas=46120   fees=0             root=a2f09a..140290 elapsed=\"466.042\u00b5s\"\nINFO [05-05|13:52:05.166] Stopping work on payload                 id=0x03c8bf3ede9e277b reason=delivery elapsed=\"587.5\u00b5s\"\nINFO [05-05|13:52:05.167] Imported new potential chain segment     number=69 hash=1a06bb..098411 blocks=1 txs=1 mgas=0.046 elapsed=\"443.375\u00b5s\" mgasps=104.020 snapdiffs=41.30KiB triedirty=0.00B\nINFO [05-05|13:52:05.168] Chain head was updated                   number=69 hash=1a06bb..098411 root=a2f09a..140290 elapsed=\"35.375\u00b5s\"\nINFO [05-05|13:52:05.169] Starting work on payload                 id=0x031ff5ee8a6d0b3c\nINFO [05-05|13:52:05.169] Updated payload                          id=0x031ff5ee8a6d0b3c number=70 hash=113db8..b38c14 txs=1 withdrawals=0 gas=46120   fees=0             root=b43eb5..26a790 elapsed=\"385.875\u00b5s\"\nINFO [05-05|13:52:05.169] Stopping work on payload                 id=0x031ff5ee8a6d0b3c reason=delivery elapsed=\"529\u00b5s\"\nINFO [05-05|13:52:05.176] Imported new potential chain segment     number=70 hash=113db8..b38c14 blocks=1 txs=1 mgas=0.046 elapsed=6.683ms     mgasps=6.901   snapdiffs=41.84KiB triedirty=0.00B\nINFO [05-05|13:52:05.179] Chain head was updated                   number=70 hash=113db8..b38c14 root=b43eb5..26a790 elapsed=\"38.542\u00b5s\"\nINFO [05-05|13:52:05.180] Starting work on payload                 id=0x031e8fb43ac9057a\nINFO [05-05|13:52:05.180] Updated payload                          id=0x031e8fb43ac9057a number=71 hash=1a26ed..935fd7 txs=1 withdrawals=0 gas=46120   fees=0             root=c444ab..507b54 elapsed=\"373.542\u00b5s\"\nINFO [05-05|13:52:05.180] Stopping work on payload                 id=0x031e8fb43ac9057a reason=delivery elapsed=\"739.541\u00b5s\"\nINFO [05-05|13:52:05.182] Imported new potential chain segment     number=71 hash=1a26ed..935fd7 blocks=1 txs=1 mgas=0.046 elapsed=\"811.75\u00b5s\"  mgasps=56.816  snapdiffs=42.39KiB triedirty=0.00B\nINFO [05-05|13:52:05.183] Chain head was updated                   number=71 hash=1a26ed..935fd7 root=c444ab..507b54 elapsed=\"28.5\u00b5s\"\nINFO [05-05|13:52:05.184] Starting work on payload                 id=0x03082b719df3e28d\nINFO [05-05|13:52:05.185] Updated payload                          id=0x03082b719df3e28d number=72 hash=d5a487..bce388 txs=1 withdrawals=0 gas=46120   fees=0             root=6f3fad..d5d91b elapsed=\"320.125\u00b5s\"\nINFO [05-05|13:52:05.185] Stopping work on payload                 id=0x03082b719df3e28d reason=delivery elapsed=\"477.917\u00b5s\"\nINFO [05-05|13:52:05.186] Imported new potential chain segment     number=72 hash=d5a487..bce388 blocks=1 txs=1 mgas=0.046 elapsed=\"408.625\u00b5s\" mgasps=112.866 snapdiffs=42.93KiB triedirty=0.00B\nINFO [05-05|13:52:05.186] Chain head was updated                   number=72 hash=d5a487..bce388 root=6f3fad..d5d91b elapsed=\"36.167\u00b5s\"\nINFO [05-05|13:52:05.188] Starting work on payload                 id=0x03789d763b35ae22\nINFO [05-05|13:52:05.188] Updated payload                          id=0x03789d763b35ae22 number=73 hash=1e64ce..2fa935 txs=1 withdrawals=0 gas=46120   fees=0             root=918a4c..8fbd41 elapsed=\"311.959\u00b5s\"\nINFO [05-05|13:52:06.954] Stopping work on payload                 id=0x03789d763b35ae22 reason=delivery elapsed=1.766s\nINFO [05-05|13:52:06.960] Imported new potential chain segment     number=73 hash=1e64ce..2fa935 blocks=1 txs=1 mgas=0.046 elapsed=2.881ms     mgasps=16.007  snapdiffs=43.47KiB triedirty=0.00B\nINFO [05-05|13:52:06.963] Chain head was updated                   number=73 hash=1e64ce..2fa935 root=918a4c..8fbd41 elapsed=\"959.458\u00b5s\"\nINFO [05-05|13:52:07.003] Starting work on payload                 id=0x0311afae11ef3d33\nINFO [05-05|13:52:07.005] Updated payload                          id=0x0311afae11ef3d33 number=74 hash=c99f1a..95e08a txs=1 withdrawals=0 gas=46120   fees=0             root=378fe0..0962f5 elapsed=1.012ms\nINFO [05-05|13:52:08.952] Stopping work on payload                 id=0x0311afae11ef3d33 reason=delivery elapsed=1.948s\nINFO [05-05|13:52:08.958] Imported new potential chain segment     number=74 hash=c99f1a..95e08a blocks=1 txs=1 mgas=0.046 elapsed=1.381ms     mgasps=33.383  snapdiffs=44.02KiB triedirty=0.00B\nINFO [05-05|13:52:08.959] Chain head was updated                   number=74 hash=c99f1a..95e08a root=378fe0..0962f5 elapsed=\"59.375\u00b5s\"\nINFO [05-05|13:52:09.005] Starting work on payload                 id=0x03ceb7130a7e1f74\nINFO [05-05|13:52:09.006] Updated payload                          id=0x03ceb7130a7e1f74 number=75 hash=cd2a04..5a881e txs=1 withdrawals=0 gas=46120   fees=0             root=7e3bdf..f8c592 elapsed=1.237ms\nINFO [05-05|13:52:10.951] Stopping work on payload                 id=0x03ceb7130a7e1f74 reason=delivery elapsed=1.946s\nINFO [05-05|13:52:10.953] Imported new potential chain segment     number=75 hash=cd2a04..5a881e blocks=1 txs=1 mgas=0.046 elapsed=\"699.875\u00b5s\" mgasps=65.897  snapdiffs=44.56KiB triedirty=0.00B\nINFO [05-05|13:52:10.953] Chain head was updated                   number=75 hash=cd2a04..5a881e root=7e3bdf..f8c592 elapsed=\"51.208\u00b5s\"\nINFO [05-05|13:52:11.003] Starting work on payload                 id=0x03464be351ebf40d\nINFO [05-05|13:52:11.004] Updated payload                          id=0x03464be351ebf40d number=76 hash=224559..8f54d9 txs=1 withdrawals=0 gas=46120   fees=0             root=6808f7..18f328 elapsed=\"497.875\u00b5s\"\nINFO [05-05|13:52:11.126] Looking for peers                        peercount=1 tried=145 static=0\nINFO [05-05|13:52:12.955] Stopping work on payload                 id=0x03464be351ebf40d reason=delivery elapsed=1.951s\nINFO [05-05|13:52:12.959] Imported new potential chain segment     number=76 hash=224559..8f54d9 blocks=1 txs=1 mgas=0.046 elapsed=1.474ms     mgasps=31.275  snapdiffs=45.11KiB triedirty=0.00B\nINFO [05-05|13:52:12.960] Chain head was updated                   number=76 hash=224559..8f54d9 root=6808f7..18f328 elapsed=\"84.583\u00b5s\"\nINFO [05-05|13:52:13.004] Starting work on payload                 id=0x034b996be96e229f\nINFO [05-05|13:52:13.005] Updated payload                          id=0x034b996be96e229f number=77 hash=1cece2..81955d txs=1 withdrawals=0 gas=46120   fees=0             root=b2c993..12a585 elapsed=\"881.375\u00b5s\"\nINFO [05-05|13:52:14.952] Stopping work on payload                 id=0x034b996be96e229f reason=delivery elapsed=1.948s\nINFO [05-05|13:52:14.957] Imported new potential chain segment     number=77 hash=1cece2..81955d blocks=1 txs=1 mgas=0.046 elapsed=3.045ms     mgasps=15.141  snapdiffs=45.65KiB triedirty=0.00B\nINFO [05-05|13:52:14.960] Chain head was updated                   number=77 hash=1cece2..81955d root=b2c993..12a585 elapsed=\"115.542\u00b5s\"\nINFO [05-05|13:52:15.004] Starting work on payload                 id=0x03717dd8f5f67e4e\nINFO [05-05|13:52:15.005] Updated payload                          id=0x03717dd8f5f67e4e number=78 hash=af76e4..95b3a7 txs=1 withdrawals=0 gas=46120   fees=0             root=bbb235..e7e25e elapsed=1.061ms\nINFO [05-05|13:52:16.972] Stopping work on payload                 id=0x03717dd8f5f67e4e reason=delivery elapsed=1.967s\nINFO [05-05|13:52:16.975] Imported new potential chain segment     number=78 hash=af76e4..95b3a7 blocks=1 txs=1 mgas=0.046 elapsed=1.448ms     mgasps=31.831  snapdiffs=46.19KiB triedirty=0.00B\nINFO [05-05|13:52:16.975] Chain head was updated                   number=78 hash=af76e4..95b3a7 root=bbb235..e7e25e elapsed=\"58.458\u00b5s\"\nINFO [05-05|13:52:17.002] Starting work on payload                 id=0x035f0aaaf653f010\nINFO [05-05|13:52:17.003] Updated payload                          id=0x035f0aaaf653f010 number=79 hash=beefb0..52c0c6 txs=1 withdrawals=0 gas=54496   fees=0             root=b9bbb1..c9d40a elapsed=\"745.834\u00b5s\"\nINFO [05-05|13:52:18.965] Stopping work on payload                 id=0x035f0aaaf653f010 reason=delivery elapsed=1.962s\nINFO [05-05|13:52:18.971] Imported new potential chain segment     number=79 hash=beefb0..52c0c6 blocks=1 txs=1 mgas=0.054 elapsed=2.431ms     mgasps=22.413  snapdiffs=46.88KiB triedirty=0.00B\nINFO [05-05|13:52:18.973] Chain head was updated                   number=79 hash=beefb0..52c0c6 root=b9bbb1..c9d40a elapsed=\"104.5\u00b5s\"\nINFO [05-05|13:52:19.005] Starting work on payload                 id=0x03c5479d55795eb8\nINFO [05-05|13:52:19.008] Updated payload                          id=0x03c5479d55795eb8 number=80 hash=179853..534738 txs=1 withdrawals=0 gas=51696   fees=0             root=3fd0af..3f3b43 elapsed=2.356ms\nINFO [05-05|13:52:21.006] Stopping work on payload                 id=0x03c5479d55795eb8 reason=timeout  elapsed=2.000s\nINFO [05-05|13:52:21.044] Imported new potential chain segment     number=80 hash=179853..534738 blocks=1 txs=1 mgas=0.052 elapsed=1.632ms     mgasps=31.664  snapdiffs=47.52KiB triedirty=0.00B\nINFO [05-05|13:52:21.045] Chain head was updated                   number=80 hash=179853..534738 root=3fd0af..3f3b43 elapsed=\"45.542\u00b5s\"\nINFO [05-05|13:52:21.047] Starting work on payload                 id=0x03d6b259ff91e899\nINFO [05-05|13:52:21.048] Updated payload                          id=0x03d6b259ff91e899 number=81 hash=e410cd..f6c271 txs=1 withdrawals=0 gas=46108   fees=0             root=ae51fa..c646b9 elapsed=\"605.208\u00b5s\"\nINFO [05-05|13:52:21.139] Looking for peers                        peercount=0 tried=76  static=0\n```", "2025-05-05T13:54:56Z", "2025-05-05T13:54:56Z", "pcw109550", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6pnomF", "I_kwDODjvEJM604WGU", "This also causes https://github.com/ethereum-optimism/optimism/releases/tag/op-batcher%2Fv1.13.0-rc.1 to print a burst of warning logs every 5s. ", "2025-05-01T20:46:40Z", "2025-05-01T20:47:02Z", "geoknee", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6quslj", "I_kwDODjvEJM60ih3-", "@janjakubnanista is this a hard dependency for interop launch?", "2025-05-08T21:33:03Z", "2025-05-08T21:33:03Z", "teddyknox", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qus2I", "I_kwDODjvEJM60ih3-", "Or \u2013 a hard dependency for supporting multiple supervisors in kurtosis-devnet..?", "2025-05-08T21:33:42Z", "2025-05-08T21:33:42Z", "teddyknox", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qwNuC", "I_kwDODjvEJM60ih3-", "Yes indeed, prerequisite for multiple supervisors unfortunately ", "2025-05-09T00:28:25Z", "2025-05-09T00:28:25Z", "janjakubnanista", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q2xuX", "I_kwDODjvEJM60DSVo", "**WontFix** because: \n1. The standard validator is _primarily_ intended to check that a system meets the standard config.\n2. It is not a priority to make it highly generic with many possible overrides (we want only a small number of overrides). \n3. So we are satisfied that: \n  a. There is a `superchainConfig` state variable which is defined on the StandardValidator\n  b. That value is checked on both the SystemConfig and AnchorStateRegistry.", "2025-05-09T13:12:15Z", "2025-05-09T13:12:15Z", "maurelian", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6oAmU4", "I_kwDODjvEJM6zHYLQ", "Will partner with Seb/George ", "2025-04-21T15:14:32Z", "2025-04-21T15:14:32Z", "yoonchung8822", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6oP5q7", "I_kwDODjvEJM6zHYLQ", "@teddyknox made progress here and implemented it already, in #15517 \ud83d\udc4f ", "2025-04-22T23:59:07Z", "2025-04-22T23:59:07Z", "protolambda", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6pXRdV", "I_kwDODjvEJM6zHYLQ", "spec is merged!", "2025-04-30T09:58:41Z", "2025-04-30T09:58:41Z", "emhane", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qHoIx", "I_kwDODjvEJM6zHYLQ", "fixed in https://github.com/ethereum-optimism/optimism/pull/15517", "2025-05-06T10:51:42Z", "2025-05-06T10:51:42Z", "emhane", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q2X_O", "I_kwDODjvEJM6zBQgZ", "Fixed by https://github.com/ethereum-optimism/optimism/pull/15516\n", "2025-05-09T12:34:32Z", "2025-05-09T12:34:32Z", "protolambda", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6peDJk", "I_kwDODjvEJM6zA33B", "One of the biggest blockers to removing custom logic from Deploy.s.sol is [these game type setting functions](https://github.com/ethereum-optimism/optimism/blob/38aea59a85c754281cfe44a9b75eadcc51e1da0d/packages/contracts-bedrock/scripts/deploy/Deploy.s.sol#L378-L382). \n\nFor `setCannonFaultGameImplementation()` we were able to convert the function to a [wrapper around `opcm.addGameType()`](https://github.com/ethereum-optimism/optimism/blob/ae6bb61c4e0e54fdddde8df6af7a50c11f284c52/packages/contracts-bedrock/scripts/deploy/Deploy.s.sol#L566). However this only works for the Cannon game type which is already supported by `addGameType()`. So we need to find an alternative for removing custom logic from `setAlphabetFaultGameImplementation()`, `setSuperFaultGameImplementation()`, `setSuperPermissionedGameImplementation()` and `setFastFaultGameImplementation()`\n\nI have a couple options in mind that we could consider: \n1. Update `addGameType()` so that it can deploy all of these game types (the OPCM already has [blueprints for the super games](https://github.com/ethereum-optimism/optimism/blob/ae6bb61c4e0e54fdddde8df6af7a50c11f284c52/packages/contracts-bedrock/src/L1/OPContractsManager.sol#L1638-L1641),  for `FastFaultGame` and `AlphabetFaultGame`, I think we would allow passing in optional blueprint addresses to deploy the new game). \n2. move the various `setXGameImplementation()` function out of Deploy.s.sol, and into the relevant test suite. \n\nAlso open to other suggestions.", "2025-04-30T20:39:33Z", "2025-05-01T17:50:32Z", "maurelian", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qtV55", "I_kwDODjvEJM6zA33B", "@AmadiMichael can you please close whichever of these tasks you've completed?", "2025-05-08T19:03:10Z", "2025-05-08T19:03:10Z", "maurelian", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q3MCn", "I_kwDODjvEJM6y2RYv", "What's the state of this? Doesn't op-deployer have verification support already?\n", "2025-05-09T13:39:27Z", "2025-05-09T13:39:27Z", "protolambda", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6ot404", "I_kwDODjvEJM6ypODv", "There is an `admin_stopBatcher` API already\n\nhttps://github.com/ethereum-optimism/optimism/blob/83b1ffc37f24d17791403fde67518f89e6b55f0d/op-batcher/rpc/api.go#L43\n\n you can see the behaviour in this test:\n\n\nhttps://github.com/ethereum-optimism/optimism/blob/0f9897b0686f8b883cdf0d232c6696447a5c185c/op-e2e/system/da/startstop_test.go#L23\n", "2025-04-25T14:30:46Z", "2025-04-25T14:30:46Z", "geoknee", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q19Ua", "I_kwDODjvEJM6ypODv", "Per comment of george, there is an API to disable the batcher already.\n", "2025-05-09T11:58:19Z", "2025-05-09T11:58:19Z", "protolambda", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6p8x9e", "I_kwDODjvEJM6ypIAU", "Closing in favor of https://github.com/ethereum-optimism/optimism/issues/15178", "2025-05-05T14:58:11Z", "2025-05-05T14:58:11Z", "axelKingsley", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6nhfEm", "I_kwDODjvEJM6yDoe7", "Network upgrade transactions are useful for upgrading predeploys at L2 hardforks. The protocol deterministically includes deposit transactions that deploy contracts and upgrade the `Proxy` contracts at the predeploy addresses.\n\nAn example spec can be found here: https://github.com/ethereum-optimism/specs/blob/main/specs/protocol/isthmus/derivation.md#network-upgrade-automation-transactions\n\nYou can see example implementations here\n- https://github.com/ethereum-optimism/optimism/blob/develop/op-node/rollup/derive/isthmus_upgrade_transactions.go\n- https://github.com/op-rs/kona/blob/main/crates/protocol/hardforks/src/isthmus.rs\n\nThere will be some additional changes to the bytecode with regards to https://github.com/ethereum-optimism/design-docs/issues/262, but likely only for the `SuperchainETHBridge` contract. This means that the deployment bytecode for that particular contract isn't yet finalized, so it could be skipped or we could update the spec/implementations when it is completed", "2025-04-16T19:36:39Z", "2025-04-16T19:36:39Z", "tynes", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6oAni_", "I_kwDODjvEJM6yDoe7", "We're tracking [here](https://github.com/op-rs/kona/issues/1475)", "2025-04-21T15:16:05Z", "2025-04-21T15:16:05Z", "clabby", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6pXRl_", "I_kwDODjvEJM6yDoe7", "unblocking since spec is now merged ", "2025-04-30T09:58:54Z", "2025-04-30T09:58:54Z", "emhane", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qHoe5", "I_kwDODjvEJM6yDoe7", "impl issues closed", "2025-05-06T10:52:16Z", "2025-05-06T10:52:16Z", "emhane", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q8ETd", "I_kwDODjvEJM6x8t5T", "@axelKingsley reopening this one as it has some todos still in the code. ", "2025-05-09T21:02:17Z", "2025-05-09T21:02:17Z", "ajsutton", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q9_r5", "I_kwDODjvEJM6x8t5T", "Wrong one. Should have been #13337 sorry. ", "2025-05-10T05:02:21Z", "2025-05-10T05:02:21Z", "ajsutton", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qiZFD", "I_kwDODjvEJM6xCA9B", "Going to close this as won't fix.  The loaded and parsed dependency set is stored in `OracleConfigSource` so \"reloading\" the dependency set is really just calling a getter. And the code is simpler with it loaded where it is.", "2025-05-08T01:26:52Z", "2025-05-08T01:26:52Z", "ajsutton", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qC3_m", "I_kwDODjvEJM6xB9iq", "Note: Retrieving the _block_ is the expensive operation, the receipts are cheap after that.  We should be calculating the block timestamp via the rollup config (block times are fixed) prior to loading the block and using that for validation.", "2025-05-06T01:05:50Z", "2025-05-06T01:05:50Z", "ajsutton", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qDSKJ", "I_kwDODjvEJM6xB9iq", "Confirmed we check the block is within the expiry window before even calling Contains so this is a pretty minor optimisation but does make things slightly clearer since its weird to check block properties inside the receipt loop.", "2025-05-06T01:40:35Z", "2025-05-06T01:40:35Z", "ajsutton", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qC2Ku", "I_kwDODjvEJM6w_i8N", "Param has been removed.", "2025-05-06T00:58:57Z", "2025-05-06T00:58:57Z", "ajsutton", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6jBrDi", "I_kwDODjvEJM6tqG9D", "Hey, Same stuck here\n\n```log\nINFO [03-19|01:36:15.575] initializing pipeline                    stage=init\nINFO [03-19|01:36:16.089] deploying superchain                     stage=deploy-superchain\nApplication failed: error in pipeline stage apply: error deploying superchain: failed to run handler: failed to deploy superchain: failed to load DeploySuperchain script: could not load script artifact: failed to open artifact \"DeploySuperchain.s.sol/DeploySuperchain.json\": open DeploySuperchain.s.sol/DeploySuperchain.json: not a directory\n```\n\nare u also not able to load DeploySuperchain script artifact ?\n", "2025-03-19T01:55:42Z", "2025-03-19T01:56:23Z", "jayendramadaram", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6jFxCN", "I_kwDODjvEJM6tqG9D", "Hi @jayendramadaram I move to this discussion https://github.com/ethereum-optimism/developers/discussions/768 ", "2025-03-19T10:55:45Z", "2025-03-19T10:55:45Z", "chiphamskymavis", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6mdXPR", "I_kwDODjvEJM6tTlK-", "Is this still an issue?\n", "2025-04-10T13:02:40Z", "2025-04-10T13:02:40Z", "protolambda", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q1-Ep", "I_kwDODjvEJM6tTlK-", "This was fixed by the stack of fix PRs by CW: https://github.com/ethereum-optimism/optimism/pull/15568\n", "2025-05-09T11:59:42Z", "2025-05-09T11:59:42Z", "protolambda", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6miiCx", "I_kwDODjvEJM6soaxC", "@Inphi does https://github.com/ethereum-optimism/optimism/pull/14622 closes this issue or we need some extra work to call it done?", "2025-04-10T14:55:45Z", "2025-04-10T14:55:45Z", "BlocksOnAChain", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6mj_JX", "I_kwDODjvEJM6soaxC", "We need to spend a little bit of time, about 1 day, assessing the program before we can call it done. ", "2025-04-10T16:30:58Z", "2025-04-10T16:30:58Z", "Inphi", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q5443", "I_kwDODjvEJM6soaxC", "I spent a couple of days looking into the following hazards that might shatter FPs.\n- deposits-only block replacement\n- consolidation rounds\n- fetching receipts\n- canonical chain walkback\n- degenerate message dependencies\n\nAll of these areas, except for the canonical chain walkback, aren't expensive inside a fault proof. At least compared to the cost of block derivation. This is based on microbenchmarks targeting the worst case behavior in each area.\nFor example, opening a block to decode its messages is inexpensive to execute. And we take care to memoize such operations so that multiple messages references use a cached result.\n\nThe remaining hazard that I'm not certain on is the canonical chain walkback. We can resolve this uncertainty with a microbenchmark. Which can be done as part of https://github.com/ethereum-optimism/optimism/issues/15285 on eth-sepolia with 2 months of EIP-2935 history. Given that all other areas in consolidation are inexpensive, it should suffice to stress test the expiry window using this microbenchmark.", "2025-05-09T16:59:30Z", "2025-05-09T16:59:59Z", "Inphi", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6geW54", "I_kwDODjvEJM6r82qc", "I am having similar issues, stuck on `failed to get blob sidecars for L1BlockRef`.\n\nI have tried node-0.11.1 and node-0.11.7 \n\nI am using a publicnode snapshot trying to sync the Base L2 Sepolia chain. \n\nI have tried `Erigon v3.0.0-alpha5`, `Erigon v3.0.0-beta2`, both Full-mode using Caplin integrated. \n\nI also tried using `go-ethereum-1.15.3` with prysm `beacon-chain-v5.3.0-linux-amd64`\n\n\n\ndocker log:\n\n```\n(when pointed at the synced L1 Sepolia Geth node):\n\n{\n  \"t\": \"2025-03-01T16:36:52+0000\",\n  \"lvl\": \"warn\",\n  \"msg\": \"Deriver system is resetting\",\n  \"err\": \"derivation failed: reset: failed to fetch blobs: failed to get blob sidecars for L1BlockRef 0x303d4ff1bcb1440f42a29fd67f159d3a21a34b6287eea4060e8691204953c77f:7792402: failed to fetch blob sidecars for slot 7072842 block 0x303d4ff1bcb1440f42a29fd67f159d3a21a34b6287eea4060e8691204953c77f:7792402: failed request with status 404: {\\\\\"message\\\\\":\\\\\"Block not found: block not found: no block roots at slot 7072842\\\\\",\\\\\"code\\\\\":404}: not found\"\n}\n\n\n\n(when pointed at the synced L1 Sepolia Erigon node):\n\n{\n  \"t\": \"2025-03-01T16:42:00+0000\",\n  \"lvl\": \"warn\",\n  \"msg\": \"Engine temporary error\",\n  \"err\": \"derivation failed: temp: failed to fetch blobs: failed to get blob sidecars for L1BlockRef 0x303d4ff1bcb1440f42a29fd67f159d3a21a34b6287eea4060e8691204953c77f:7792402: failed to fetch blob sidecars for slot 7072842 block 0x303d4ff1bcb1440f42a29fd67f159d3a21a34b6287eea4060e8691204953c77f:7792402: #returned blobs(0) != #requested blobs(5)\"\n}\n```\n\n", "2025-03-01T16:43:09Z", "2025-03-01T16:45:31Z", "bradj00", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q_Y87", "I_kwDODjvEJM6r82qc", "i have similar issue with erigon3\n`engine temporary error\" err=\"derivation failed: temp: failed to fetch blobs: failed to get blob sidecars for L1BlockRef  0x4892be8764a440cc2d88c0dbce3b0a1d4b831ac102caaa7c36cce045f26bb768:22443308: failed to fetch blob sidecars for slot  11661498 block 0x4892be8764a440cc2d88c0dbce3b0a1d4b831ac102caaa7c36cce045f26bb768:22443308:  #returned blobs(0) != #requested blobs(4)\"`\n\nbypass this issue by adding opnode with  --l1.beacon.fetch-all-sidecars and erigon3 with --caplin.blobs-immediate-backfill\n\nhere are my confs\nbooterigon.sh\n```sh\n#!/bin/sh\n/mnt/nvme1/git-erigon-new/build/bin/erigon  --txpool.disable --datadir=/mnt/nvme1/erigon3 --webseed=https://erigon3-v3-snapshots-mainnet.erigon.network --http.addr=0.0.0.0 --http.port 8546 --authrpc.port 8552 --http.api eth,debug,net,trace,web3,erigon --beacon.api.addr=0.0.0.0 --beacon.api.port=5555 --beacon.api.cors.allow-origins=* --beacon.api=beacon,builder,config,debug,node,validator,lighthouse  --nat=\"stun\" --prune.mode=archive --log.dir.verbosity=debug --torrent.verbosity=4  --maxpeers=30 --torrent.download.rate=3mb --torrent.upload.rate=200k --torrent.download.slots=3  --downloader.verify  --caplin.blobs-immediate-backfill\n```\nbootop.sh\n```sh\n#!/bin/sh\n\nexport OP_NODE_L1_ETH_RPC=http://[fca0::2]:8546\nexport OP_NODE_L1_BEACON=http://[fca0::2]:5555\n\n#export OP_NODE_L1_ETH_RPC=https://ethereum-rpc.publicnode.com\n#export OP_NODE_L1_BEACON=https://ethereum-beacon-api.publicnode.com\nexport OP_NODE_L1_RPC_KIND=erigon\n\nexport OP_NODE_NETWORK=unichain-mainnet\nexport OP_NODE_L2_ENGINE_AUTH=/mnt/nvme1/unichain/shared/jwt.hex\nexport OP_NODE_L2_ENGINE_RPC=ws://0.0.0.0:8551\nexport OP_NODE_LOG_LEVEL=info\nexport OP_NODE_LOG_FORMAT=logfmt\nexport OP_NODE_METRICS_ADDR=0.0.0.0\nexport OP_NODE_METRICS_ENABLED=true\nexport OP_NODE_METRICS_PORT=7300\nexport OP_NODE_P2P_LISTEN_IP=0.0.0.0\nexport OP_NODE_P2P_LISTEN_TCP_PORT=9222\nexport OP_NODE_P2P_LISTEN_UDP_PORT=9222\nexport OP_NODE_RPC_ADDR=0.0.0.0\nexport OP_NODE_RPC_PORT=9545\nexport OP_NODE_VERIFIER_L1_CONFS=4\nexport OP_NODE_P2P_DISCOVERY_PATH=/mnt/nvme1/unichain/data/opnode_discovery_db\nexport OP_NODE_P2P_PEERSTORE_PATH=/mnt/nvme1/unichain/data/opnode_peerstore_db\nexport OP_NODE_P2P_PRIV_PATH=/mnt/nvme1/unichain/data/opnode_p2p_priv.txt\nexport OP_NODE_SYNCMODE=execution-layer\nexport GETH_SYNCMODE=full\n\n#export OP_NODE_P2P_PEERS_HI=40\n#export OP_NODE_P2P_PEERS_LO=15\nexport OP_NODE_L1_RPC_MAX_BATCH_SIZE=50\n\n/mnt/nvme1/git/optimism/op-node/bin/op-node --l2.enginekind=geth --l1.trustrpc --l1.max-concurrency 1 --l1.rpc-rate-limit 100 --l1.beacon.fetch-all-sidecars\n```\nbootgeth.sh\n\n```sh\n#!/bin/sh\nexport GETH_OP_NETWORK=unichain-mainnet\nexport GETH_ROLLUP_SEQUENCERHTTP=https://mainnet-sequencer.unichain.org\nexport GETH_LOG_FORMAT=logfmt\nexport GETH_VERBOSITY=3\nexport GETH_DATADIR=/mnt/nvme1/unichain/data\nexport GETH_HTTP=true\nexport GETH_HTTP_ADDR=0.0.0.0\nexport GETH_HTTP_VHOSTS=\"*\"\nexport GETH_HTTP_CORSDOMAIN=\"*\"\nexport GETH_HTTP_API=web3,debug,eth,txpool,net,admin,rpc\nexport GETH_WS=true\nexport GETH_WS_ADDR=0.0.0.0\nexport GETH_WS_API=web3,debug,eth,txpool,net,admin,rpc\nexport GETH_AUTHRPC_JWTSECRET=/mnt/nvme1/unichain/shared/jwt.hex\nexport GETH_AUTHRPC_PORT=8551\nexport GETH_AUTHRPC_VHOSTS=\"*\"\nexport GETH_AUTHRPC_ADDR=0.0.0.0\nexport GETH_METRICS=true\nexport GETH_METRICS_ADDR=0.0.0.0\nexport GETH_ROLLUP_DISABLETXPOOLGOSSIP=true\nexport GETH_TXPOOL_NOLOCALS=true\n/mnt/nvme1/git/op-geth/build/bin/geth --syncmode=full --gcmode archive\n```\n\n\n", "2025-05-10T10:58:55Z", "2025-05-10T10:58:55Z", "21paradox", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6hBwJV", "I_kwDODjvEJM6onLQJ", "@protolambda ok to assume that this work started and to put it \"in progress\" status?", "2025-03-05T17:20:24Z", "2025-03-05T17:20:24Z", "BlocksOnAChain", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q2Vod", "I_kwDODjvEJM6onLQJ", "Not all tasks are in scope of RC-Beta. Removing the tracker from the milestone to keep things tidy.\n", "2025-05-09T12:30:26Z", "2025-05-09T12:30:26Z", "protolambda", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6b8vPV", "I_kwDODjvEJM6nVYZ8", "@BlocksOnAChain I don't think this is part of the Stable Devnet Milestone. While we do need to support this feature, I don't think we are targeting this for our end-of-January code freeze.\n\nIf you strongly disagree, let's sync on it, but otherwise let's find a better milestone target for this.", "2025-01-27T17:07:33Z", "2025-01-27T17:07:33Z", "axelKingsley", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6b9rKK", "I_kwDODjvEJM6nVYZ8", "@axelKingsley I removed it, I was creating a bunch of issues and moved this one to mentioned milestone. Thanks for flagging. ", "2025-01-27T18:53:13Z", "2025-01-27T23:02:55Z", "BlocksOnAChain", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qDKTq", "I_kwDODjvEJM6nVYZ8", "Closing this as the migration process is implemented now via `opcm.migrate` and we have a script to create or verify the super root anchor state required. ", "2025-05-06T01:30:10Z", "2025-05-06T01:30:10Z", "ajsutton", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qFYiw", "I_kwDODjvEJM6nVYZ8", "Great stuff, thanks @ajsutton ", "2025-05-06T07:27:29Z", "2025-05-06T07:27:29Z", "BlocksOnAChain", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qq3I1", "I_kwDODjvEJM6nFMa3", "@pauldowman @ajsutton is this spike something that we still need to do?\n", "2025-05-08T14:59:04Z", "2025-05-08T14:59:04Z", "BlocksOnAChain", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6qwbg0", "I_kwDODjvEJM6nFMa3", "We will need to do this spike as part of kicking of a project to support adding new chains.  We don't need to block on it for launching interop IMO.", "2025-05-09T01:12:51Z", "2025-05-09T01:12:51Z", "ajsutton", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6OruUH", "I_kwDODjvEJM6Td8fT", "This was closed automatically because [the description](https://github.com/ethereum-optimism/design-docs/pull/106#issue-2562878016) says `Fixes ...`", "2024-10-04T14:18:54Z", "2024-10-04T14:18:54Z", "maurelian", "2025-08-31 04:55:30"]
["IC_kwDODjvEJM6q3YFP", "I_kwDODjvEJM6Td8fT", "Can this be closed?\n", "2025-05-09T13:53:24Z", "2025-05-09T13:53:24Z", "protolambda", "2025-08-31 04:55:30"]
["IC_kwDOL-xLQ86sZAag", "I_kwDOL-xLQ863MJ_7", "Done in https://github.com/ethereum-optimism/optimism/pull/15993", "2025-05-19T20:52:18Z", "2025-05-19T20:52:18Z", "scharissis", "2025-08-31 04:56:36"]
["IC_kwDOL-xLQ86syqsq", "I_kwDOL-xLQ86271G-", "Following a conversation with @scharissis, closing this as undesireable given that we are aligned in preferring these testing configuration files remain agnostic to the test env they target.", "2025-05-21T19:11:20Z", "2025-05-21T19:11:20Z", "teddyknox", "2025-08-31 04:56:36"]
["IC_kwDOL-xLQ86sGiYS", "I_kwDOL-xLQ86y5Prh", "Implemented in [#15979](https://github.com/ethereum-optimism/optimism/pull/15979)", "2025-05-16T18:17:24Z", "2025-05-16T18:17:32Z", "serpixel", "2025-08-31 04:56:36"]
["IC_kwDOKIwiaM6Vq49g", "I_kwDOKIwiaM6Veq9d", "This is done, and it's live", "2024-12-02T09:48:03Z", "2024-12-02T09:48:03Z", "krofax", "2025-08-31 04:56:38"]
["IC_kwDOH2Qg5s6sXjr6", "I_kwDOH2Qg5s6vCQBk", "Possibly related to this report.\nWe found that prestateTracer settings may not be respected in the request.\n\nFor example:\n```\ncurl https://docs-demo.optimism.quiknode.pro/ -X POST -H \"Content-Type: application/json\" --data '{  \"jsonrpc\": \"2.0\",  \"method\": \"debug_traceBlockByNumber\", \"params\": [\"0x6453982\", {\"tracer\": \"prestateTracer\",  \"tracerConfig\": {\"diffMode\": true}}],  \"id\": 1}' | jq '.result[0].result.\"0xb8ff877ed78ba520ece21b1de7843a8a57ca47cb\"'\n```\nreturns the same as \n```\ncurl https://docs-demo.optimism.quiknode.pro/ -X POST -H \"Content-Type: application/json\" --data '{  \"jsonrpc\": \"2.0\",  \"method\": \"debug_traceBlockByNumber\", \"params\": [\"0x6453982\", {\"tracer\": \"prestateTracer\",  \"tracerConfig\": {\"diffMode\": false}}],  \"id\": 1}' | jq '.result[0].result.\"0xb8ff877ed78ba520ece21b1de7843a8a57ca47cb\"'\n```\nboth return:\n```\n{\n  \"balance\": \"0x2dadf385519ae3a5a\",\n  \"nonce\": 885174,\n  \"code\": \"0x\",\n  \"storage\": {}\n}\n```\n\nThis does not align with the results from `eth_getBalance`\n```\ncurl https://docs-demo.optimism.quiknode.pro/ \\          \n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  --data '{\"method\":\"eth_getBalance\",\"params\":[\"0xb8ff877ed78ba520ece21b1de7843a8a57ca47cb\", \"0x6453982\"],\"id\":1,\"jsonrpc\":\"2.0\"}'\n\n{\"jsonrpc\":\"2.0\",\"id\":1,\"result\":\"0x2dadf3836abd4915a\"}\n```\n\nThis is on \n```\nGeth/v1.101503.4-stable-2b9abb39/linux-amd64/go1.24.2\n```\n\n", "2025-05-19T18:05:52Z", "2025-05-19T18:05:52Z", "jwelch-qn", "2025-08-31 04:56:39"]
["IC_kwDOMMiGhs6sIUv_", "I_kwDOMMiGhs61-fvH", "We don't natively support setting state from the previous state, we have an issue in our backlog to add better native support for state loading and state dumps: https://github.com/ethereum-optimism/supersim/issues/108. \n\nHowever, since supersim chains use anvil under the hood, as a workaround you should be able to call `anvil_dumpState` on each chain prior to shutdown and then `anvil_loadState` on each chain after startup in order to set the previous state on each chain. More info on those rpcs can be found here https://book.getfoundry.sh/reference/anvil/", "2025-05-17T00:00:05Z", "2025-05-17T00:01:26Z", "tremarkley", "2025-08-31 04:57:03"]
["IC_kwDOMMiGhs6sZNUH", "I_kwDOMMiGhs61-fvH", "Thanks for the workaround. ", "2025-05-19T21:21:05Z", "2025-05-19T21:21:05Z", "bengivre", "2025-08-31 04:57:03"]
["IC_kwDOJ_r-bs6sWvJi", "I_kwDOJ_r-bs63P_V5", "> * be able to fallback on a default \"standard config\" if one has not been defined yet for this superchain (most likely it never will, we should just compare to the mainnet standard config).\n\nAlthough it is worth noting that we wouldn't expect the chain to be \"compliant\" with respect to mainnet for things like certain `roles` which tend to be superchain specific. Configuration parameters could be compared, though. ", "2025-05-19T16:37:20Z", "2025-05-19T16:37:20Z", "geoknee", "2025-08-31 04:57:17"]
["IC_kwDOJ_r-bs6sZJ_6", "I_kwDOJ_r-bs63P_V5", "Closing since it was descoped from the current interop <> superchain-registry workstream (cc: @mslipper )", "2025-05-19T21:13:24Z", "2025-05-19T21:13:24Z", "bitwiseguy", "2025-08-31 04:57:17"]
["IC_kwDODjvEJM6tIgUZ", "I_kwDODjvEJM635dsW", "Which chain is this syncing? And via a manual configuration file or network parameter? The simple-optimism-node repo may be out of date.\n", "2025-05-23T14:48:39Z", "2025-05-23T14:48:39Z", "protolambda", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tKKXF", "I_kwDODjvEJM635dsW", "> Which chain is this syncing?\n\nop-mainnet\n\n```\n###############################################################################\n#                                \u2193 REQUIRED \u2193                                 #\n###############################################################################\n\n# Network to run the node on (\"op-mainnet\" or \"op-sepolia\")\nNETWORK_NAME=op-mainnet\n\n# Type of node to run (\"full\" or \"archive\"), note that \"archive\" is 10x bigger\nNODE_TYPE=full\n\n###############################################################################\n#                            \u2193 REQUIRED (BEDROCK) \u2193                           #\n###############################################################################\n\n# L1 node that the op-node (Bedrock) will get chain data from\nOP_NODE__RPC_ENDPOINT=...\n\n# L1 beacon endpoint, you can setup your own or use Quicknode\nOP_NODE__L1_BEACON=...\n\n# Type of RPC that op-node is connected to, see README\nOP_NODE__RPC_TYPE=basic\n\n# Reference L2 node to run healthcheck against\nHEALTHCHECK__REFERENCE_RPC_PROVIDER=\n\n###############################################################################\n#                            \u2193 OPTIONAL (BEDROCK) \u2193                           #\n###############################################################################\n\n# Optional provider to serve legacy RPC requests, see README\nOP_GETH__HISTORICAL_RPC=https://mainnet.optimism.io\n\n# Set to \"full\" to force op-geth to use --syncmode=full\nOP_GETH__SYNCMODE=\n\n###############################################################################\n#                                \u2193 OPTIONAL \u2193                                 #\n###############################################################################\n\n# Feel free to customize your image tag if you want, uses \"latest\" by default\n# See here for all available images: https://hub.docker.com/u/ethereumoptimism\nIMAGE_TAG__L2GETH=\nIMAGE_TAG__DTL=\nIMAGE_TAG__HEALTCHECK=\nIMAGE_TAG__PROMETHEUS=\nIMAGE_TAG__GRAFANA=\nIMAGE_TAG__INFLUXDB=\nIMAGE_TAG__OP_GETH=\nIMAGE_TAG__OP_NODE=\n\n# Exposed server ports (must be unique)\n# See docker-compose.yml for default values\nPORT__L2GETH_HTTP=\nPORT__L2GETH_WS=\nPORT__DTL=\nPORT__HEALTHCHECK_METRICS=\nPORT__PROMETHEUS=\nPORT__GRAFANA=\nPORT__INFLUXDB=\nPORT__TORRENT_UI=\nPORT__TORRENT=\nPORT__OP_GETH_HTTP=\nPORT__OP_GETH_WS=\nPORT__OP_GETH_P2P=\nPORT__OP_NODE_P2P=\nPORT__OP_NODE_HTTP=\n```\n\nNote: I have also tried with a Quicknode paid plan RPC\n\n> And via a manual configuration file or network parameter?\n\nVia network parameter\n\nhttps://github.com/Chomtana/simple-optimism-node/blob/b9b8098908aed7a433a03e5b6b2ed711cfae9a29/scripts/start-op-geth.sh#L17\n\nhttps://github.com/Chomtana/simple-optimism-node/blob/b9b8098908aed7a433a03e5b6b2ed711cfae9a29/scripts/start-op-node.sh#L18", "2025-05-23T16:56:29Z", "2025-05-23T16:56:29Z", "Chomtana", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tKYf1", "I_kwDODjvEJM635dsW", "However, I have just tried syncing from scratch again on the latest version, and it worked this time. \n\nSo, this happens randomly...\n\nMaybe it connected to a bad peer from a malicious chain operator that impersonates the OP mainnet and don't ban the bad peer.\n\n```\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:03+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm4UvxerdbDXmVrCDjDe6EwxBrbPjd4pfyHwJuw22mtw4H\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:03+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmGdi199VyuT8TfJkY8B36bwt9k9waiufAB3xrgUx6RL5g\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:03+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmTh7pXVvdAhkaQpynoEi4ai1kUD6zd9hsRYTsL8JCSdXG\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:03+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmC7Stf2EqVG8Cz2KMHbwJ437EfbnYK5DL56jS4gL6hBZf\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:03+0000 lvl=info msg=\"connected to peer\" peer=16Uiu2HAmC7Stf2EqVG8Cz2KMHbwJ437EfbnYK5DL56jS4gL6hBZf addr=/ip4/15.235.226.147/tcp/9222\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:03+0000 lvl=info msg=\"Starting P2P sync client event loop\" peer=16Uiu2HAmC7Stf2EqVG8Cz2KMHbwJ437EfbnYK5DL56jS4gL6hBZf\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:05+0000 lvl=info msg=\"Optimistically inserting unsafe L2 execution payload to drive EL sync\" id=0x8753b2b50c5defac1112ab1a81c71a2b993468a3944657964e7c437b1dc9240f:136210624\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:05+0000 lvl=info msg=\"Inserted new L2 unsafe block (synchronous)\" hash=0x8753b2b50c5defac1112ab1a81c71a2b993468a3944657964e7c437b1dc9240f number=136210624 newpayload_time=4.860ms fcu2_time=1.401ms total_time=6.263ms mgas=6.47052 mgasps=1033.1318063265057\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:05+0000 lvl=info msg=\"Sync progress\" reason=\"new chain head block\" l2_finalized=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_pending_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_unsafe=0x8753b2b50c5defac1112ab1a81c71a2b993468a3944657964e7c437b1dc9240f:136210624 l2_backup_unsafe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_time=1748020025\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:07+0000 lvl=info msg=\"Optimistically inserting unsafe L2 execution payload to drive EL sync\" id=0x65dfbd4640eb6fda9da238e59946347268f9bf0d282e123ca82ca1a433a97fe3:136210625\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:07+0000 lvl=info msg=\"Inserted new L2 unsafe block (synchronous)\" hash=0x65dfbd4640eb6fda9da238e59946347268f9bf0d282e123ca82ca1a433a97fe3 number=136210625 newpayload_time=2.437ms fcu2_time=739.945\u00b5s total_time=3.178ms mgas=25.119798 mgasps=7903.159716213878\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:07+0000 lvl=info msg=\"Sync progress\" reason=\"new chain head block\" l2_finalized=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_pending_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_unsafe=0x65dfbd4640eb6fda9da238e59946347268f9bf0d282e123ca82ca1a433a97fe3:136210625 l2_backup_unsafe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_time=1748020027\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:08+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmTh7pXVvdAhkaQpynoEi4ai1kUD6zd9hsRYTsL8JCSdXG\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:08+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmUdjvRmJM1qfKa2QHzMgTEqXtuvyFUYBeCPJZ6MoSL4ic\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:08+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm4UvxerdbDXmVrCDjDe6EwxBrbPjd4pfyHwJuw22mtw4H\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:08+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmGdi199VyuT8TfJkY8B36bwt9k9waiufAB3xrgUx6RL5g\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:09+0000 lvl=info msg=\"Optimistically inserting unsafe L2 execution payload to drive EL sync\" id=0x9e2a3381120eace4df2be723c6027df838d8114f9183f74cbfcf1b4bd0e87555:136210626\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:09+0000 lvl=info msg=\"Inserted new L2 unsafe block (synchronous)\" hash=0x9e2a3381120eace4df2be723c6027df838d8114f9183f74cbfcf1b4bd0e87555 number=136210626 newpayload_time=4.578ms fcu2_time=3.392ms total_time=7.972ms mgas=22.156035 mgasps=2779.128845583361\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:09+0000 lvl=info msg=\"Sync progress\" reason=\"new chain head block\" l2_finalized=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_pending_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_unsafe=0x9e2a3381120eace4df2be723c6027df838d8114f9183f74cbfcf1b4bd0e87555:136210626 l2_backup_unsafe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_time=1748020029\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:09+0000 lvl=info msg=\"disconnected from peer\" peer=16Uiu2HAm91XtNQsAjRc7AmaddevxrpATY3bVhtikNgiuviVNWiwq addr=/ip4/61.111.3.58/tcp/20040\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:11+0000 lvl=info msg=\"Optimistically inserting unsafe L2 execution payload to drive EL sync\" id=0xa2b31b90fe743579f2c693b583a696158626bac47da195f51cd7b8800fbd490f:136210627\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:11+0000 lvl=info msg=\"Inserted new L2 unsafe block (synchronous)\" hash=0xa2b31b90fe743579f2c693b583a696158626bac47da195f51cd7b8800fbd490f number=136210627 newpayload_time=4.540ms fcu2_time=827.048\u00b5s total_time=5.368ms mgas=12.964511 mgasps=2415.007888888682\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:11+0000 lvl=info msg=\"Sync progress\" reason=\"new chain head block\" l2_finalized=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_pending_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_unsafe=0xa2b31b90fe743579f2c693b583a696158626bac47da195f51cd7b8800fbd490f:136210627 l2_backup_unsafe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_time=1748020031\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmGdi199VyuT8TfJkY8B36bwt9k9waiufAB3xrgUx6RL5g\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmUdjvRmJM1qfKa2QHzMgTEqXtuvyFUYBeCPJZ6MoSL4ic\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm91XtNQsAjRc7AmaddevxrpATY3bVhtikNgiuviVNWiwq\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAmTh7pXVvdAhkaQpynoEi4ai1kUD6zd9hsRYTsL8JCSdXG\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"attempting connection\" peer=16Uiu2HAm4UvxerdbDXmVrCDjDe6EwxBrbPjd4pfyHwJuw22mtw4H\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"Optimistically inserting unsafe L2 execution payload to drive EL sync\" id=0x6a0507b49e915e18fd4bb1f509eba1e345367071cf1b5ee51bee31e951edc1e1:136210628\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"Inserted new L2 unsafe block (synchronous)\" hash=0x6a0507b49e915e18fd4bb1f509eba1e345367071cf1b5ee51bee31e951edc1e1 number=136210628 newpayload_time=2.756ms fcu2_time=3.263ms total_time=6.020ms mgas=12.913584 mgasps=2144.98143054639\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"Sync progress\" reason=\"new chain head block\" l2_finalized=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_pending_safe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_unsafe=0x6a0507b49e915e18fd4bb1f509eba1e345367071cf1b5ee51bee31e951edc1e1:136210628 l2_backup_unsafe=0x0000000000000000000000000000000000000000000000000000000000000000:0 l2_time=1748020033\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:13+0000 lvl=info msg=\"disconnected from peer\" peer=16Uiu2HAmDcKsuKVNz5Z3P4PrR2MHNuhaBkNzRzioxg4DBos4Vhz8 addr=/ip4/193.122.156.47/tcp/9222\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:14+0000 lvl=info msg=\"connected to peer\" peer=16Uiu2HAm91XtNQsAjRc7AmaddevxrpATY3bVhtikNgiuviVNWiwq addr=/ip4/61.111.3.58/tcp/20040\nsimple-optimism-node-op-node-1  | t=2025-05-23T17:07:14+0000 lvl=info msg=\"Starting P2P sync client event loop\" peer=16Uiu2HAm91XtNQsAjRc7AmaddevxrpATY3bVhtikNgiuviVNWiwq\n```", "2025-05-23T17:11:07Z", "2025-05-23T17:11:07Z", "Chomtana", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tKdpb", "I_kwDODjvEJM635dsW", "Just look into the log and saw that it pull an old commit from `pectra-upgrade-temp` branch instead when running `git pull`. So, it's caused from pulling old commit. My bad :(\n\n<img width=\"576\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/33d19d3f-ec96-40de-a1bd-91f275160a95\" />", "2025-05-23T17:20:55Z", "2025-05-23T17:20:55Z", "Chomtana", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6s8KbF", "I_kwDODjvEJM63rXy8", "This comment describes the impacts the current dispute game architecture has on devrel/EVM Safety. These stem from two main causes\n- This issue\n- The dispute games not being MCP (Multi-Chain Prep) compatible (i.e. no immutables)\n\nWe walk through a typical protocol lifecycle to demonstrate the impacts:\n- A new chain launches straight to permissioned, due to the cyclical dependency we currently have. They don't exist yet so aren't in the superchain registry (SR), so I believe we pass a prestate that won't actually work for them\n- They get added to the SR, and now a hard fork comes. What do we do here? There is no \"official\" guidance about whether we need to deploy a new permissioned dispute game (~4M gas) with a new absolute prestate, even though there's no reason to play a game on permissioned chains. This often happens automatically through OPCM, so in addition to process confusion we also increase gas cost + state diff validation overhead unnecessarily here\n- Now they upgrade to permissionless. We call a method on OPCM which costs 4M+ gas to deploy the new game implementation. Expensive, but no real issue\n- Next time there is a hard fork, it costs 8M+ gas to ugprade this chain (4M for each game). This means we can only upgrade 3 chains at once. OP governance contains ~10 chains, meaning there are 4 playbooks (5 once you include Base) to create, sequence, sign, and execute for every hard fork or contracts update. This adds a lot of operational overhead for various stakeholders (playbook creators, playbook reviewers, governance, signers, auditors who review playbooks, bug bounty hunters who review our calldata). Interop doesn't help here much (yet) since the initial interop set is only 2 chains, so it reduces the number of dispute games to deploy from 20 to 18 (since the interop set shares a dispute game)\n\nIn summary:\n- Permissioned dispute games having prestates results in lack of process clarity for devrel / new chains / upgrades of permissioned chains\n- Deploying dispute games is too expensive which results in significant release process overhead. This can be cut down in two ways: making then MCP-compatible, and simplifying permissioned dispute games", "2025-05-22T14:30:01Z", "2025-05-22T14:31:34Z", "mds1", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6s-nK_", "I_kwDODjvEJM63rXy8", "The assumption of a chain using a PermissionedDisputeGame is the the chain operator is the sole proposer, they are proposing valid output roots, and are not going to challenge their own claims. I think there is a scenario where this can happen, but out of an operational mistake. \n\nSimplifying the SuperPermissionedDisputeGame makes sense to me and I'm in support of the reasoning @mds1 had outlined above. Removing the prestates from the permissioned game makes this cleaner. Net new chains wouldn't be using an invalid prestate at genesis, permissioned chain upgrades would be easier to reason about, and they only need to care about the prestate when they're moving to permissionless.", "2025-05-22T17:57:03Z", "2025-05-22T17:57:03Z", "sbvegan", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tAlgs", "I_kwDODjvEJM63rXy8", "Thanks for the detail.  I'm definitely in favour of this simplification, but just to drill down on these impacts a bit, it sounds like most of them are because of the lack of MCP compatibility (all the gas costs for example) which is being fixed separately and doesn't require changing the permissioned game implementations.\n\n> The assumption of a chain using a PermissionedDisputeGame is the the chain operator is the sole proposer, they are proposing valid output roots, and are not going to challenge their own claims. I think there is a scenario where this can happen, but out of an operational mistake.\n\nIt is possible they counter themselves, unless they do so 72 times (bearing in mind the challenger role should be a multisig so that's a _lot_ of signing) the game can still be made to resolve correctly. Simplifying the permissioned game would actually make this slightly worse - if you incorrectly challenge a proposal once it would be permanently invalid.\n\n> Permissioned dispute games having prestates results in lack of process clarity for devrel / new chains / upgrades of permissioned chains\n\nWe could also address this by just asserting that the permissioned game always has a fixed prestate and not ever expect it to have the correct chain config. ie: we can avoid these impacts via a policy change which we could adopt immediately and not have to wait on developer resources being available, gov cycles etc. We may need to update superchain-ops templates (and maybe OPCM for some cases?) for updating the prestates for hard forks so it doesn't bother with the permissioned game.\n\nAnd again for clarity, I'm definitely keen to do the simplification but that isn't going to be possible to ship prior to interop so will be months away at the earliest even ignoring other possible priorities.", "2025-05-22T21:27:12Z", "2025-05-22T21:27:12Z", "ajsutton", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6ssQmD", "I_kwDODjvEJM63dyAG", "Assigning @geoknee for now so we don't forget about it.", "2025-05-21T09:36:49Z", "2025-05-21T09:36:49Z", "sebastianst", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tNhzF", "I_kwDODjvEJM63dCDs", "Given that there is no way to deploy a post migration network at genesis, this is required for the upcoming devnet", "2025-05-23T23:15:12Z", "2025-05-23T23:15:12Z", "tynes", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tKFdx", "I_kwDODjvEJM63TMdq", "I mixed up work streams, closing this in favor of different work stream sub tasks", "2025-05-23T16:49:20Z", "2025-05-23T16:49:20Z", "sbvegan", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tKFhy", "I_kwDODjvEJM63THji", "I mixed up work streams, closing this in favor of different work stream sub tasks", "2025-05-23T16:49:24Z", "2025-05-23T16:49:24Z", "sbvegan", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tKFmb", "I_kwDODjvEJM63TFnA", "I mixed up work streams, closing this in favor of different work stream sub tasks", "2025-05-23T16:49:28Z", "2025-05-23T16:49:28Z", "sbvegan", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tM31p", "I_kwDODjvEJM63SafF", "Closing to track in a different location", "2025-05-23T21:12:24Z", "2025-05-23T21:12:24Z", "sbvegan", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6s6R63", "I_kwDODjvEJM63Ou36", "PoC done https://github.com/ethereum-optimism/optimism/pull/16002", "2025-05-22T11:55:59Z", "2025-05-22T11:55:59Z", "pcw109550", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6sYBie", "I_kwDODjvEJM62y8Sw", "New runbook lives in \nhttps://www.notion.so/Internal-Devnet-Rehearsals-1eff153ee16280569b55edb1b2f8c992?pvs=24", "2025-05-19T18:49:53Z", "2025-05-19T18:49:53Z", "stevennevins", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6sY-zU", "I_kwDODjvEJM62y8Sw", "The new runbook - https://www.notion.so/oplabs/Interop-Upgrade-Runbook-V2-1f2f153ee16280e8b76cd2359e509a7c", "2025-05-19T20:48:53Z", "2025-05-19T20:48:53Z", "Inphi", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6r9Fo2", "I_kwDODjvEJM62pAN2", "This didn't seem to work. The cross safe head is still stuck.\n\nWill close this issue in a while as it was decided to not spend more time on this.", "2025-05-15T19:57:52Z", "2025-05-15T19:57:52Z", "yashvardhan-kukreja", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6szuCl", "I_kwDODjvEJM62oXYm", "This is done, the network was thoroughly tested and looks to be a good candidate for performing acceptance-tests against.", "2025-05-21T21:09:38Z", "2025-05-21T21:09:38Z", "yashvardhan-kukreja", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tGwt2", "I_kwDODjvEJM62mytC", "See comment in PR. Platforms was working on related things, and we should not slow down every single test cleanup with refunding. There are other better approaches than `t.Cleanup`.\n\n", "2025-05-23T12:15:20Z", "2025-05-23T12:15:20Z", "protolambda", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tG9Ge", "I_kwDODjvEJM62mytC", "@protolambda The comment on the PR sounds good. If we rely on the known mnemonic, we'll need to aware of some tests that don't use it: https://github.com/ethereum-optimism/optimism/blob/080f00892e6b5564a057899b4e3fd56846ec93ef/op-acceptance-tests/tests/interop/happy/interop_happy_tx_test.go#L46\nhttps://github.com/ethereum-optimism/optimism/blob/080f00892e6b5564a057899b4e3fd56846ec93ef/op-acceptance-tests/tests/interop/reorgs/invalid_exec_msgs_test.go#L64", "2025-05-23T12:35:53Z", "2025-05-23T12:35:53Z", "joshklop", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rrKpH", "I_kwDODjvEJM62aeQy", "No op-geth updates are required for depsets. See [this notion comment](https://www.notion.so/oplabs/Superchain-Registry-Integration-1eff153ee16280e8a0b7dd6e7ed4213d?pvs=4#1f2f153ee1628072b127fc30935412e0)", "2025-05-14T13:18:28Z", "2025-05-14T13:18:28Z", "bitwiseguy", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rsCF8", "I_kwDODjvEJM62aeQy", "Reopening because monorepo components import scr chain configs via op-geth", "2025-05-14T14:23:34Z", "2025-05-14T14:23:34Z", "bitwiseguy", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6sTysG", "I_kwDODjvEJM62aMHG", "Given our current path is to have managed mode work pre-Interop, I think we don't really need this. It should be sufficient to read from the op-node logs that the Interop timestamp is set. If the timestamp is set, it is automatically starting in managed mode and will require an op-supervisor instance running. So I suggest to close this @jelias2 @protolambda ", "2025-05-19T12:41:45Z", "2025-05-19T12:41:45Z", "sebastianst", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6sJao3", "I_kwDODjvEJM62Zobn", "> A Flashblocks failure mode we need to watch out for is the case where the rollup boost instance of every sequencer fails simultaneously.\n> \n> To be able to respond to this scenario in an automated way, we'll want to run at least one backup sequencer that is not configured with rollup boost (just default geth block building). This way, we can fall back to it if all rollup boost-enabled sequencers fail.\n> \n> However if we do this, we should modify conductor's leader election logic such that it prioritizes / prefers rollup boost-enabled sequencers - this will ensure that conductor only selects the non-rollup boost-enabled sequencer as a last resort in case all other sequencers are unhealthy.\n> \n> **Acceptance Criteria**\n> \n> * [ ]  Short design doc outlining the specific approach[ ]  Align on conductor topology - in particular, how many of what sequencer types to run in Labs infra and Sunnyside infra[ ]  Implement required changes in conductor[ ]  Write op-acceptor tests that verify this failover logic\n\n1. ", "2025-05-17T06:47:41Z", "2025-05-17T06:47:41Z", "KATHI160", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6siuT6", "I_kwDODjvEJM62Zobn", "Per @zhwrd, we wouldn't want to modify conductor itself as proposed, as that would require customizations on the RAFT protocol itself. Leader election policy should likely be implemented in op-conductor-mon instead.", "2025-05-20T15:08:59Z", "2025-05-20T15:08:59Z", "alfonso-op", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6szo2i", "I_kwDODjvEJM62Zobn", "After a discussion with the Unichain team, it was realised that the failure of rollup-boosts in all the sequencers is analogous to the failure of all the EL clients (op-geth) in all the sequencers from the eyes of the respective CL clients (op-nodes), which is currently not accounted for in op-conductor's leader election.\n\nMoreover, considering the non-trivial engineering effort needed to bake this capability within op-conductor, we finalized that this failure mode can be treated as a non-blocker for the Flashblocks support stream.\n\n@alfonso-op @zhwrd my personal suggestion to still counter this problem to a certain extent is by having a P3-P5 (non-blocking) task of adding this capability in op-conductor-mon if that would be trivial enough.\n\nTherefore, whenever it's implemented, any chain can opt-in to this additional failure mode by installing op-conductor-mon on their infra. Thoughts?\n", "2025-05-21T20:59:14Z", "2025-05-21T20:59:14Z", "yashvardhan-kukreja", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6tHEN7", "I_kwDODjvEJM62Zobn", "Hey @yashvardhan-kukreja, thanks for confirming with Uni. I\u2019m supportive of this decision. I do think there\u2019s an elevated risk with rollup-boost as a new and relatively un-battle-tested component, so I don't think it's fully analogous to all sequencer op-geth clients failing, but I also think it's an acceptable risk to not have this capability in place to start.\n\nGiven this, let's leave as a P2 (rather than P3-P5, as P2 is already considered non-blocking for the Interop Workstreams).", "2025-05-23T12:46:47Z", "2025-05-23T12:46:47Z", "alfonso-op", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6re7ii", "I_kwDODjvEJM62QzFE", "Original [Document](https://www.notion.so/oplabs/Unichain-Interop-Devnet-Plan-1e6f153ee16280fbb56afd591cb232bf?pvs=4) to be broken down into tickets\n", "2025-05-13T15:38:49Z", "2025-05-13T15:41:29Z", "jelias2", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6sYxor", "I_kwDODjvEJM62OigU", "I think the network upgrade transactions will break here", "2025-05-19T20:22:16Z", "2025-05-19T20:22:16Z", "tynes", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6q8mLw", "I_kwDODjvEJM61-WC5", "Wrote up the benchmark program here - https://github.com/ethereum-optimism/optimism/compare/inphi/canon-benchmark?expand=1.\n\nThe host supports its execution using these options:\n```\n$program \\\n    --canon-oracle-benchmark=true \\\n    --canon-oracle-benchmark.chain-config=$CHAIN_CONFIG \\\n    --canon-oracle-benchmark.chain-id=$CHAIN_ID \\\n    --canon-oracle-benchmark.head=$HEAD \\\n    --canon-oracle-benchmark.query-hash=$QUERY_HASH \\\n    --canon-oracle-benchmark.query-number=$QUERY_NUMBER \\\n    --canon-oracle-benchmark.url=$ETH_URL \\\n    --datadir $DATADIR \\\n    --log.format terminal \\\n    --log.level debug\n```\n\nI don't have access to an L1 RPC that supports `debug_dbGet`. But here are the results of running the benchmark on op-sepolia inside cannon.\n| # blocks | distance | vm steps | emulation time (m4 max) | \n| --- | --- | --- | --- |\n| 5 * 8192 | ~1 day | 350,000,000 | < 1 min |\n| 10 * 8192 | ~2 days | 400,000,000 | 1 min  |\n| 20 * 8192 | ~4 days | 650,000,000 | 1.5 mins |\n| 50 * 8192 | ~9 days | 1,100,000,000 | 2.5 mins |\n| 100 * 8192 | ~19 days | 2,000,000,000 | 5 mins |\n\nNote that Isthmus activated on Apr 17. So we're bounded by the last 3 weeks for the benchmark.\n\nThis simple benchmark illustrates how expensive the fault proof could be for a cold cache query of the canonical block oracle at various block heights.\n\nNext thing worth doing is another benchmark that uses different access patterns to query the canonical block oracle.", "2025-05-09T22:51:56Z", "2025-05-09T22:51:56Z", "Inphi", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6syOtU", "I_kwDODjvEJM61-WC5", "I've tweaked the microbenchmark use three different access patterns for canonical block oracle queries.\n- `sequential` - queries the oldest N blocks in the range. Ordered by their block number.\n- `reverse sequential` - queries the oldest N blocks in the range. Reverse ordered by their block number.\n- `random` - queries N random blocks in the range\n\nFor this test, the block range is defined by the last 9 days of history. And in all cases, `N`,  the total number of queries, is chosen to be 2000. Which is slightly more than the maximum number of executing messages in a single block.\n\naccess pattern | block depth | distance | vm steps | emulation time (m4 max)\n-- | -- | -- | -- | ---\nsequential | 100 * 8192 | ~19 days | 33,250,000,000 | 21 mins\nreversed sequential | 100 * 8192 | ~19 days | 28,700,000,000 | 17 mins\nrandom | 100 * 8192 | ~19 days | 35,250,000,000 (avg) | 22 mins\n\nThe worst access pattern appears to be when the random. There doesn't seem to be a worse way to increase the number of steps.", "2025-05-21T18:21:34Z", "2025-05-21T18:21:34Z", "Inphi", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6s9tME", "I_kwDODjvEJM615msE", "Closing this to simplify; plans have been changing with the removal of StandardMode and the faster deprecation of legacy mode.\n", "2025-05-22T16:26:58Z", "2025-05-22T16:26:58Z", "protolambda", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rc20r", "I_kwDODjvEJM61sTup", "@protolambda @jelias2 should this really be a blocker of the interop beta rc? I think we can run another rehearsal without this being implemented yet.", "2025-05-13T13:11:58Z", "2025-05-13T13:11:58Z", "sebastianst", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rdbtE", "I_kwDODjvEJM61sTup", "Downgraded to P2 as it's not critical for RC Beta. We should bump priority again when we're working towards prod readiness.", "2025-05-13T13:52:18Z", "2025-05-13T13:52:18Z", "sebastianst", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rdgDp", "I_kwDODjvEJM61sTup", "@sebastianst are you sure? This is functionality that @tynes and @axelKingsley identified as important for the audit.\n", "2025-05-13T13:57:12Z", "2025-05-13T13:57:12Z", "protolambda", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rfCDB", "I_kwDODjvEJM61sTup", "Changed it back to a P1 for now", "2025-05-13T15:47:29Z", "2025-05-13T15:47:29Z", "protolambda", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rfSKs", "I_kwDODjvEJM61sTup", "@protolambda if we want to have all known edge-case handling implemented with the \"Interop RC Beta\" milestone, then yes I agree, we must take care of this. My understanding was that we can already start rehearsing upgrades etc without this being fixed yet, which is why I thought it's only necessary for prod milestones. But maybe I'm mistaken, and we should instead create a milestone before \"Interop RC Beta\" like \"Interop Rehearsal Readiness\".", "2025-05-13T15:57:17Z", "2025-05-13T15:57:17Z", "sebastianst", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6riwjN", "I_kwDODjvEJM61sTup", "> My understanding was that we can already start rehearsing upgrades etc without this being fixed yet, which is why I thought it's only necessary for prod milestones\n\nI think this is correct and would advocate for bumping this back to P2.", "2025-05-13T21:39:23Z", "2025-05-13T21:39:23Z", "teddyknox", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rixjk", "I_kwDODjvEJM61sTup", "Reverting back to P2", "2025-05-13T21:42:02Z", "2025-05-13T21:42:02Z", "protolambda", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6sYx6c", "I_kwDODjvEJM61sTup", "@teddyknox Reopening this one as there's [still a todo in the code](https://app.circleci.com/pipelines/github/ethereum-optimism/optimism/90512/workflows/e5c234a8-c5c0-4f62-aa2d-4efaac91cc52/jobs/3541479):\n```\nethereum-optimism/optimism #15769        | Interop: test and handle sequencing-window expiry                 | op-acceptance-tests/tests/interop/seqwindow/expiry_test.go:18\n```", "2025-05-19T20:22:52Z", "2025-05-19T20:22:52Z", "ajsutton", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6pvJTP", "I_kwDODjvEJM60-1gu", "OPCM already allows adding games with an arbitray mips (and thus an arbitrary oracle), so afaict that already supports the fast and alphabet games. \n\nSo the only case where we'd need to update `addGameType()` is if we need support for the `SuperXDisputeGame` variants, and I'm still unclear if it's safe to activate those without going through opcm.migrate().\n\nAlternatively we could update `addGameType` to allow passing in any arbitrary blueprint from which to deploy a dispute game. Although that is a bit more complex of a change, it is doable.", "2025-05-02T17:09:40Z", "2025-05-02T17:09:40Z", "maurelian", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6p10FA", "I_kwDODjvEJM60-1gu", "The initial activation of super dispute games should be done via `opcm.migrate()` (for chains that are starting prior to interop), but `addGameType` needs to support them so that forks after interop can use them to upgrade the prestate or for chains that are switching from permissioned to permissionless (basically all the reasons we use addGameType today).", "2025-05-04T21:42:38Z", "2025-05-04T21:42:38Z", "ajsutton", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6qR0sA", "I_kwDODjvEJM60-1gu", "`addGameType` now supports super games. I think we've worked out that `addGameType` already supports alphabet and fast games by just setting the inputs appropriately.  Is there anything left to do here?", "2025-05-07T01:58:28Z", "2025-05-07T01:58:28Z", "ajsutton", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6sVfjQ", "I_kwDODjvEJM60-1gu", "> `addGameType` now supports super games. I think we've worked out that `addGameType` already supports alphabet and fast games by just setting the inputs appropriately. Is there anything left to do here?\n\nThis suffices. Closing.", "2025-05-19T14:47:08Z", "2025-05-19T14:47:08Z", "Inphi", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6qC3Qa", "I_kwDODjvEJM60tR99", "The plan is to ensure that the `CrossL2Inbox` contract is not deployed prior to the interop hard fork activating so there cannot be any executing messages prior to then.\nhttps://github.com/ethereum-optimism/optimism/pull/15637 adds an action test to confirm the consolidation step works even if a call is made to the CrossL2Inbox proxy (which is uninitialised since interop isn't active yet).\n\nWe need to understand how op-supervisor is handling the interop activation (https://github.com/ethereum-optimism/optimism/pull/15614) before we can be sure that op-program and op-supervisor behaviour is matching. There's on red flag I'm yet to understand which could be an issue: https://github.com/ethereum-optimism/optimism/pull/15614#discussion_r2074387483", "2025-05-06T01:03:17Z", "2025-05-06T01:03:17Z", "ajsutton", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6sZAS8", "I_kwDODjvEJM60tR99", "> The plan is to ensure that the `CrossL2Inbox` contract is not deployed prior to the interop hard fork activating so there cannot be any executing messages prior to then. [#15637](https://github.com/ethereum-optimism/optimism/pull/15637) adds an action test to confirm the consolidation step works even if a call is made to the CrossL2Inbox proxy (which is uninitialised since interop isn't active yet).\n\nClosing for this reason. Not necessary to skip explicitly in the program.", "2025-05-19T20:52:02Z", "2025-05-19T20:52:02Z", "Inphi", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6psqwz", "I_kwDODjvEJM6zdoaT", "After forcing a bad exec msg with the op-test-sequencer (modifying the exec msg identifier to point to an unknown chain), the supervisor appears to just throw an error and not reorg the executing side.\n\n```\nERROR[05-02|14:05:38.049] Failed to process block                  service=op-supervisor id=Supervisor-dev chain=901 block=4208ed..431737:21 err=\"invalid log 0 from block 0x4208ed7121af95b8a79dea5a0ff237ef8356e3009119e3629173fe2d38431737:21: failed to translate chain ID 1024 to chain index: unknown chain\"\nERROR[05-02|14:05:38.049] Failed to process new block              service=op-supervisor id=Supervisor-dev chain=901 err=\"failed to process block 21: invalid log 0 from block 0x4208ed7121af95b8a79dea5a0ff237ef8356e3009119e3629173fe2d38431737:21: failed to translate chain ID 1024 to chain index: unknown chain\"\n```\n\nI continue to investigate what the expected behaviour is in this scenario.", "2025-05-02T12:09:14Z", "2025-05-02T12:09:14Z", "nonsense", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6rhhbw", "I_kwDODjvEJM6zdoaT", "Related: https://github.com/ethereum-optimism/optimism/pull/15662", "2025-05-13T19:22:36Z", "2025-05-13T19:22:36Z", "teddyknox", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6oH5Li", "I_kwDODjvEJM6xo4OC", "We had to revert #15314 so re-opening to track reimplementing the fix in a way which doesn't break p2p. ", "2025-04-22T08:57:32Z", "2025-04-22T08:57:32Z", "geoknee", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6ojREF", "I_kwDODjvEJM6xo4OC", "I've spent some time trying to reapply this change without any success (using kurtosis to test changes). Some observations: \n\n* Adding more topics to the map tends to break p2p (the unsafe head will sync for a brief period of time, and then stall and be pegged to the safe head thereafter). \n* the number of peers seems to be stable\n* Replacing V1 with V4 results in something that still works.\n* Having an empty map results in something that still works.\n* Debugging this behaviour is very difficult, there isn't much in the way of logging or metrics to help\n* There are other parts of the p2p codebase which are stuck on a single, out of date block topic, example: \n\nhttps://github.com/ethereum-optimism/optimism/blob/0eb6883374caa80b658f9eb7899d5727daae8dbc/op-node/p2p/peer_scorer.go#L64\n\n* We have a skipped test which may be useful to reactivate and run for longer (it might catch the bug)\nhttps://github.com/ethereum-optimism/optimism/blob/6d9d43cb6f2721c9638be9fe11d261c0602beb54/op-e2e/system/p2p/gossip_test.go#L107\n\nBy adding more topics to the map, we do introduce more of a chance for a peer to get a bad (low or negative) score. This might be because nodes aren't doing anything on old block topics, so get penalised on that topic if it is added to the scoring map. \n\nI actually think the way forward may be to remove per-topic scoring altogether. The network will move from one topic to another serially as hardforks activate, and we don't have any dynamic code to leave a mesh or unsubscribe from a topic when that happens. So it doesn't make a lot of sense to carry on scoring peers per topic right now in my opinion. \n\n\n", "2025-04-24T14:18:22Z", "2025-04-24T14:20:56Z", "geoknee", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6otsTD", "I_kwDODjvEJM6xo4OC", "This RPC method could be a useful way to debug this issue https://docs.optimism.io/operators/node-operators/json-rpc#opp2p_peerstats \n\nRunning this on a kurtosis devnet with 3 replicas (of which one is a sequencer), and the topic scoring map fully populated I get this kind of response from each node:\n\n```\n{\"connected\":2,\"table\":101,\"blocksTopic\":2,\"blocksTopicV2\":2,\"blocksTopicV3\":2,\"blocksTopicV4\":2,\"banned\":0,\"known\":3}\n```\nsuggesting that it is not peers being _banned_ which is the problem (still seeing an overall gossip failure). ", "2025-04-25T14:12:08Z", "2025-04-25T17:04:41Z", "geoknee", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6pQQ5E", "I_kwDODjvEJM6xo4OC", "> By adding more topics to the map, we do introduce more of a chance for a peer to get a bad (low or negative) score. This might be because nodes aren't doing anything on old block topics, so get penalised on that topic if it is added to the scoring map.\n> \n> I actually think the way forward may be to remove per-topic scoring altogether. The network will move from one topic to another serially as hardforks activate, and we don't have any dynamic code to leave a mesh or unsubscribe from a topic when that happens. So it doesn't make a lot of sense to carry on scoring peers per topic right now in my opinion.\n\nGetting more confident about this conclusion now. It would explain why having more block topics is harmful to p2p gossip (more topics scored where there is no traffic, penalising peers). \n\nIt does suggest that the correct way to do this is either:\n1. Remove per-topic peer scoring entirely (set the weight to zero). Keep global peer scoring, though. This would be easy to implement.\n2. Have dynamic behaviour where nodes enter and exit different topics as hardfork activations happen. This would be a bigger lift and I'm not sure it is worth it. ", "2025-04-29T17:05:51Z", "2025-04-29T17:05:51Z", "geoknee", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6ejWfO", "I_kwDODjvEJM6qGc9Z", "Good find! `SuperchainERC20` defines an interface + gives minting rights to a particular bridge. There are many possible implementations of `SuperchainERC20`, with the one in the monorepo being one a simple one. It is impossible to have all possible features that app devs may want (ie rebasing, fee on transfer, single chain ownable, multi chain ownable). I recommend making your own implementation that has the specific features that you would like.\n\nWe may consider to merge this into the implementation here, but its not a priority right now.", "2025-02-14T19:04:12Z", "2025-02-14T19:04:12Z", "tynes", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6gp_w6", "I_kwDODjvEJM6phzhb", "The [recursive hazard detection](https://github.com/ethereum-optimism/optimism/pull/14453) partially fixes this. Hazard checks still need to be able to identify cross-dependencies that have become invalid. Without this capability, cross-safety updates stalls with `future data` errors.\nThe following test illustrates the issue - https://github.com/ethereum-optimism/optimism/compare/inphi/intra-block-stall?expand=1#diff-4c89624409707e2f2c4be938eae79a2f0abc5b28a462acc867c4ebe4496e5245R443. The op-supervisor needs to be able to distinguish between cross-chain dependencies that were removed during block replacement versus those that never existed in the first place.", "2025-03-03T19:36:22Z", "2025-03-03T19:36:22Z", "Inphi", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6jKfan", "I_kwDODjvEJM6phzhb", "This PR updates the test from @Inphi to make it work. The issue was due to action test mechanics and not an issue with the supervisor logic: https://github.com/ethereum-optimism/optimism/pull/14949\n\nThe underlying issue described is still a real issue for multi-node though, and is outlined in https://github.com/ethereum-optimism/optimism/issues/14936.", "2025-03-19T17:10:08Z", "2025-03-19T17:17:04Z", "unknown", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6jhk6a", "I_kwDODjvEJM6phzhb", "Confirmed that the [out-of-range](https://github.com/ethereum-optimism/optimism/pull/14973) fix in the supervisor [passes all cascade FP cases](https://github.com/ethereum-optimism/optimism/pull/14979).", "2025-03-21T14:10:29Z", "2025-03-21T14:10:29Z", "Inphi", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6qSNLj", "I_kwDODjvEJM6nFL4a", "This depends on how the DisputeGameFactoryProxy addresses are actually recorded in the superchain-registry as part of https://github.com/ethereum-optimism/optimism/issues/13917 Most likely they'll stay chain specific so we won't need to change how we load it, but should validate that they are all the same given challenger can only monitor one factory.", "2025-05-07T03:20:39Z", "2025-05-07T03:20:39Z", "ajsutton", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6s8bIa", "I_kwDODjvEJM6nFL4a", "Closing as done.", "2025-05-22T14:49:27Z", "2025-05-22T14:49:27Z", "pauldowman", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6abDSn", "I_kwDODjvEJM6mLUA0", "Regarding metrics, that could be a good team exercise of identifying metric locations. It promotes an understanding of the service behavior. At first glance, \n\n- every event can support a counter metric at least, with more metrics specific to the nature of the event.\n- API calls over the wire probably already have metrics for free, which just need to be unified with events in Grafana\n- Database specific metrics? \ud83e\udd14\n", "2025-01-14T18:21:06Z", "2025-01-14T18:21:06Z", "axelKingsley", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6abapy", "I_kwDODjvEJM6mLUA0", "Copy-pasting from discord for context:\n\nThe op-node has generic events-systems metrics utils which we can reuse\n\nAnd I think it may be useful to just create a kind of like a status package like we have with op-node. And then just wait for all kinds of events, and use the event contents to update the metrics. That avoids having to install metrics into every individual module.\n", "2025-01-14T18:55:59Z", "2025-01-14T18:55:59Z", "protolambda", "2025-08-31 04:57:37"]
["IC_kwDODjvEJM6g5edI", "I_kwDODjvEJM6lK5G9", "The todo checker is flagging a TODOs that reference this issue - reopening to stop it failing:\n\n```\nethereum-optimism/optimism #13595        | interop: Walkback on reset failure                                | op-supervisor/supervisor/backend/syncnode/node_test.go:63\n```\nThe todo checker was broken for a fair while so these slipped through.  Could you please remove the TODOs if no longer relevant or update the issue number they reference if there's something else tracking that work now.", "2025-03-05T00:50:46Z", "2025-03-05T00:50:46Z", "ajsutton", "2025-08-31 04:57:37"]
["IC_kwDOL-xLQ86sweUf", "I_kwDOL-xLQ863ntT7", "Includes https://github.com/ethereum-optimism/k8s/pull/6663 and https://github.com/ethereum-optimism/optimism/pull/16050", "2025-05-21T15:31:02Z", "2025-05-21T15:31:02Z", "serpixel", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86slmGE", "I_kwDOL-xLQ863deQ3", "Latest errors:\n\n```\nTest Run Results (359.5s):\nTotal: 8, Passed: 1, Failed: 5, Skipped: 2\n\nGate: holocene (359.5s)\n\u251c\u2500\u2500 Status: fail\n\u251c\u2500\u2500 Tests: 1 passed, 5 failed, 2 skipped\n\u251c\u2500\u2500 Test: github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/fjord (206.4s) [status=pass]\n\u2502       \u2502       \u251c\u2500\u2500 Test: TestCheckFjordScript (0.0s) [status=skip]\n\u2502       \u2502       \u2514\u2500\u2500 Error: === RUN   TestCheckFjordScript\n    systest.go:185: precondition not met: failed to create system from URL \"/etc/op-acceptor/devnet-env.json\": failed to load devnet from URL: error fetching devnet data: error parsing JSON: json: cannot unmarshal object into Go struct field L2Cha\nin.l2.services of type []*descriptors.Service\n--- SKIP: TestCheckFjordScript (0.00s)\nPASS\nok      github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/fjord   0.029s\n\n\u2502       \u2502       \u2514\u2500\u2500 Test: TestFees (0.0s) [status=skip]\n\u2502       \u2502       \u2514\u2500\u2500 Error: === RUN   TestFees\n    systest.go:185: precondition not met: failed to create system from URL \"/etc/op-acceptor/devnet-env.json\": failed to load devnet from URL: error fetching devnet data: error parsing JSON: json: cannot unmarshal object into Go struct field L2Cha\nin.l2.services of type []*descriptors.Service\n--- SKIP: TestFees (0.00s)\nPASS\nok      github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/fjord   0.030s\n\n\u251c\u2500\u2500 Test: github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/base (153.1s) [status=fail]\n\u2502       \u2514\u2500\u2500 Error: TestCLAdvance: panic: invalid artifacts path: failed to stat path: stat /data/optimism/packages/contracts-bedrock/forge-artifacts: no such file or directory\n\ngoroutine 1 [running]:\ngithub.com/ethereum-optimism/optimism/op-e2e/config.initAllocType({0xc00005c044, 0xe}, {0x28e1b16, 0x6})\n        /data/optimism/op-e2e/config/init.go:239 +0x656\ngithub.com/ethereum-optimism/optimism/op-e2e/config.init.0()\n        /data/optimism/op-e2e/config/init.go:192 +0x185\nFAIL    github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/base    0.078s\n\nstderr: go: downloading github.com/lmittmann/w3 v0.19.5\n\nTestChainFork: panic: invalid artifacts path: failed to stat path: stat /data/optimism/packages/contracts-bedrock/forge-artifacts: no such file or directory\n```", "2025-05-20T19:15:22Z", "2025-05-20T19:15:22Z", "scharissis", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86tYKm9", "I_kwDOL-xLQ863deQ3", "Numerous improvements have been made here. The crashloops no longer occur.\n\n* https://github.com/ethereum-optimism/infra/pull/353\n* https://github.com/ethereum-optimism/infra/pull/312\n* https://github.com/ethereum-optimism/infra/pull/339\n* https://github.com/ethereum-optimism/k8s/pull/6678\n* https://github.com/ethereum-optimism/k8s/pull/6662\n* https://github.com/ethereum-optimism/k8s/pull/6660\n* https://github.com/ethereum-optimism/k8s/pull/6659\n* https://github.com/ethereum-optimism/k8s/pull/6644\n* https://github.com/ethereum-optimism/k8s/pull/6676\n* https://github.com/ethereum-optimism/k8s/pull/6675\n* https://github.com/ethereum-optimism/k8s/pull/6674\n* https://github.com/ethereum-optimism/k8s/pull/6672\n* https://github.com/ethereum-optimism/k8s/pull/6612", "2025-05-26T07:16:07Z", "2025-05-26T07:16:07Z", "scharissis", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86pQkF7", "I_kwDOL-xLQ86y5QXY", "Hi, is this issue still open.", "2025-04-29T17:34:36Z", "2025-04-29T17:34:36Z", "Ayoseun", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86pUQuS", "I_kwDOL-xLQ86y5QXY", "Hi @Ayoseun . It is still open, although a bit tricky for external contributors to implement.", "2025-04-30T02:40:44Z", "2025-04-30T02:40:44Z", "scharissis", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86pVU5L", "I_kwDOL-xLQ86y5QXY", "Oh, how though\r\n\r\nOn Wed, 30 Apr 2025 at 3:41\u202fAM, Stefano Charissis ***@***.***>\r\nwrote:\r\n\r\n> *scharissis* left a comment (ethereum-optimism/infra#286)\r\n> <https://github.com/ethereum-optimism/infra/issues/286#issuecomment-2840660882>\r\n>\r\n> Hi @Ayoseun <https://github.com/Ayoseun> . It is still open, although a\r\n> bit tricky for external contributors to implement.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/ethereum-optimism/infra/issues/286#issuecomment-2840660882>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ALP2YPNHPCLUERSCDJZNGCL24AZ4FAVCNFSM6AAAAAB3J255L6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDQNBQGY3DAOBYGI>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n", "2025-04-30T06:30:14Z", "2025-04-30T06:30:14Z", "Ayoseun", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86tq4-u", "I_kwDOL-xLQ86y5QXY", "Implemented on [#16129](https://github.com/ethereum-optimism/optimism/pull/16129)", "2025-05-27T19:15:38Z", "2025-05-27T19:15:38Z", "serpixel", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86oMD7b", "I_kwDOL-xLQ86wroFu", "Hi, here are some insights I can share which can help steering the implementation in the right direction.\n\n- **Definition of divergence**\n    - When two nodes (geth/reth) respond with different stateRoot-hex-strings at the same block height, they're divergent.\n- **Regarding my work in the past**\n    - We always wanted to ensure that whenever a reth-based sequencer operates as a leader and diverges from a geth-based non-leader sequencer, we should transfer the leadership to the geth-based sequencer. \n    - Here's the runbook relating to it - [ref](https://www.notion.so/oplabs/Runbook-op-reth-diverges-from-op-geth-151f153ee162806eb8f1c323233043eb)\n- replica-healthcheck is a very lightweight component which already is capable of tracking such divergence. \n    - It runs on a per-node (per-geth or per-reth), basically, tracking if that node is diverging (or stalling) or not.\n    - It has two inputs:\n        - target RPC: the EL URL of the component whose divergence needs to be tracked\n        - reference RPC: the EL URL which is considered to be the source of truth of block height and state roots.\n    - replica-healthcheck simply queries these two RPCs (with `eth_getBlockByNumber` method) to see if there's any certain block height where state root reported by target RPC and reference RPC differs. \n    - It reports the results as Prometheus metrics which we visualize [here](https://optimistic.grafana.net/d/n-MDITAVz/replica-healthcheck?orgId=1&from=now-12h&to=now&timezone=browser&var-network=arena-z-sepolia-prod&var-instance=$__all&var-target=$__all)\n\n\n## My ideas regarding this ticket\n\nThis devnet acceptance test can do one of two things:\n1. Just minimally do what replica-healthcheck does:\n    - Take two inputs (resembling replica-healthcheck)\n        - target RPC would be the URL of the reth-node which it wants to check for divergence (Can be `http://op-reth:8545` if the test would run literally within the same namespace as that reth-node)\n        - reference RPC would be the URL of any geth-based node/sequencer in the same devnet (considering we want to see if a reth-based node diverges from geth-based or not)\n    - Use the `eth_getBlockByNumber latest` RPC to last matching block heights between target and reference RPC:\n        - so if target RPC (the reth node)'s tip is at block height 42 and reference RPC's tip is at 69, the last matching block height is 42\n    - Check at this last matching block height (42) if the `stateRoot` is same or not for target RPC and reference RPC\n        - If it's same, there's no divergence.\n        - If it's different, there's divergence.\n            - Optionally, the point/block of divergence can be simply found through binary searching between block number 0 and 42 i.e. finding the last block number where the state roots were same for both the target and reference RPC.\n2. Re-use replica-healthcheck:\n    - Configure replica-healthcheck per every reth-based node in the devnet where:\n        - target RPC would be `http://op-reth:8545` (because we deploy replica-healthcheck in the same namespace as the reth-node)\n        - reference RPC would be the URL of any geth-based node/sequencer in the same devnet.\n    - The acceptance test then just checks for `healthcheck_height_difference` and `healthcheck_last_matching_state_root_height` to exactly if and where the divergence occurred between any reth and geth based node.\n\nI would prefer the former approach (i.e. lightweight re-implementation of what replica-healthcheck does) because:\n- It's easier to script/automate from a testability standpoint.\n- IIUC, there's already a set of acceptance tests in Go. So this re-implementation can be appended to that existing suite of tests easily.\n- Operationally there's no effort to deploy anything new like replica-healthcheck per reth-based node.\n- You can have a modified implementation if need be unlike re-using replica-healthcheck whose source code we aren't maintaining anymore.\n\nBut again, I have no experience with setting up and running acceptance tests so feel free to let me know if I am missing something or if you'd like to seek answers to any other questions.\n\nCheers!", "2025-04-22T15:39:35Z", "2025-04-22T15:39:35Z", "yashvardhan-kukreja", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86pHcxa", "I_kwDOL-xLQ86wroFu", "https://github.com/ethereum-optimism/optimism/pull/15585", "2025-04-29T02:51:21Z", "2025-04-29T02:51:21Z", "scharissis", "2025-08-31 04:58:43"]
["IC_kwDOL-xLQ86tYIeU", "I_kwDOL-xLQ86qR-DM", "Done in https://github.com/ethereum-optimism/infra/pull/353", "2025-05-26T07:09:49Z", "2025-05-26T07:09:49Z", "scharissis", "2025-08-31 04:58:43"]
["IC_kwDODjvEJM6uQJAo", "I_kwDODjvEJM65AsKL", "fix should be also applied to netchef [here](https://github.com/ethereum-optimism/infrastructure-services/blob/main/netchef/pkg/devnet_env/devnet_env.go#L207)", "2025-05-30T20:57:35Z", "2025-05-30T20:57:35Z", "zhwrd", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6uOKgL", "I_kwDODjvEJM65AB8w", "Closed by https://github.com/ethereum-optimism/superchain-registry/pull/1041", "2025-05-30T17:24:08Z", "2025-05-30T17:24:08Z", "bitwiseguy", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6tyI95", "I_kwDODjvEJM64k1wt", "Set as P1 since it's not blocking, for for consistency reasons must eventually happen.", "2025-05-28T09:23:21Z", "2025-05-28T09:23:21Z", "sebastianst", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6tfpGg", "I_kwDODjvEJM64TwHJ", "Ah nevermind it's from the v1.6.0 tag.", "2025-05-26T23:20:13Z", "2025-05-26T23:20:13Z", "DaniPopes", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6ti4Q5", "I_kwDODjvEJM63-9Cq", "This check-for-pr step [no longer exists](https://github.com/ethereum-optimism/optimism/pull/16020), so we can close this ticket.\nFor the PR above a rebase from optimism@develop will sort things out.", "2025-05-27T08:15:12Z", "2025-05-27T08:15:12Z", "scharissis", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6s_ooY", "I_kwDODjvEJM630bhk", "`chainCode` is a smart upgrade, thanks for cutting this. \n\nJust thinking through it a bit: I think the current \"ChainNotFound\" code will still need to be present. Elsewise, an Executing Message could be included in a chain which is invalid *but uses the same 31 bits* as a valid one. Once it's indexed in the smaller format, it will be indistinguishable from an Executing Message pointed at a valid chain. Same for if a chain would get a Code that happens to match the 31 bytes of a chain that doesn't have a Code assigned.\n\nThe entire Executing Message would need to be valid on the valid chain for any outcomes to change, but just the potential for the Supervisor to misunderstand one chain for another is something we should avoid.", "2025-05-22T19:37:29Z", "2025-05-22T19:37:29Z", "axelKingsley", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6thUb1", "I_kwDODjvEJM630bhk", "I changed my mind. The DB growth is manageable, it only affects executing messages, not initiating messages. And it creates meaningful simplicity in the config system and checking of messaging.\n\nSo no more chain-index type anymore, and no shortcuts to have the alias.\n\nMaking these changes was painful (+1200 -1600 diff for all config changes combined), but the results are worthwhile (simplicity, less code, better encapsulated invariants)\n", "2025-05-27T05:52:40Z", "2025-05-27T05:52:40Z", "protolambda", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6s4_nU", "I_kwDODjvEJM63dgdN", "From what I can tell we do expect depsets to be included in `state.json` when the intent demands it. However, the behaviour is changing with this PR https://github.com/ethereum-optimism/optimism/pull/16072. \n\nThe behavior will be : \n\n> if we deploy multiple chains together with op-deployer, it will generate a dependency set which is the subset of those chains which schedule the interop fork", "2025-05-22T09:58:42Z", "2025-05-22T09:58:42Z", "geoknee", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6s8Nov", "I_kwDODjvEJM63dgdN", "If we wanted to be really sure about closing this issue, we could run through a deployment with the latest op-deployer and confirm the state.json has the expected fields populated. ", "2025-05-22T14:33:32Z", "2025-05-22T14:33:32Z", "geoknee", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6tAgzP", "I_kwDODjvEJM63dgdN", "@geoknee Is it a problem if there is a dependency set generated for all deployments (even if there's only a single chain and/or interop fork isn't actually scheduled)? Upcoming versions of op-node will require a dep set be provided so I think we'll want op-deployer to just always create one so it's available. If only a single chain is deployed that would be a dependency set of 1 which would be the most common case.\n\nI don't expect that will cause issues but please yell if it does.", "2025-05-22T21:16:49Z", "2025-05-22T21:16:49Z", "ajsutton", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6tFdwZ", "I_kwDODjvEJM63dgdN", "I don't think that's a problem it actually sounds like the cleanest approach. I think the end to end data flow is something like this: \n\nop-deployer => superchain-registry => op-geth => op-node/op-supervisor/op-program\n\n\nIn my opinion we don't want intermediate links in this pipeline to do anything clever. So we can just have the registry mirror exactly what op-deployer spits out and then it is as transparent as possible. ", "2025-05-23T09:47:59Z", "2025-05-23T09:47:59Z", "geoknee", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6t3ReF", "I_kwDODjvEJM63dgdN", "Completed by https://github.com/ethereum-optimism/optimism/pull/16072 and https://github.com/ethereum-optimism/optimism/pull/16089", "2025-05-28T16:31:09Z", "2025-05-28T16:31:09Z", "bitwiseguy", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6sVQgS", "I_kwDODjvEJM62o_ve", "@joshklop in terms of what we measure, I recommend we also include the signals from our [Release Readiness Checklist](https://pm.optimism.io/acceptance-testing/release-checklist.html). The Platforms team is currently working on writing automated acceptance tests for several items on this checklist (see [this issue](https://github.com/ethereum-optimism/infra/issues/190)) that we should run against a devnet under load so we can gain confidence in general system availability (supporting the [High Availability workstream](https://www.notion.so/oplabs/High-Availability-1eff153ee1628055b360e3af73de81f9?pvs=4)).\n\ncc @scharissis who is leading acceptance testing efforts and cc @teddyknox as Load Testing DRI.", "2025-05-19T14:29:58Z", "2025-05-19T14:39:48Z", "alfonso-op", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6sgxz0", "I_kwDODjvEJM62o_ve", "Measuring the release readiness metrics qualitatively as part of the release readiness process, using @scharissis's eventual grafana dashboard, while running tests against kurtosis/devnets, makes sense to me. Is this what you had in mind? Under current time constraints I don't think we can stomach increasing the scope of the tests to include collecting/interpreting metrics quantitatively.\n\n@scharissis and I chatted a bit yesterday and I agreed that a new dashboard tracking these metrics would be useful for this purpose.", "2025-05-20T12:46:04Z", "2025-05-20T12:47:12Z", "teddyknox", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6tYH9b", "I_kwDODjvEJM62o_ve", "You can find our work-in-progress grafana dashboard here: https://optimistic.grafana.net/goto/lSy9hNfHR?orgId=1", "2025-05-26T07:07:27Z", "2025-05-26T07:07:27Z", "scharissis", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6ru2jt", "I_kwDODjvEJM62muMe", "do we really need a separate devnet?\nEspecially if the tests are going to live in op-acceptance-tests/tests/interop we should probably use a unified devnet configuration to run them all.\n\nIt *seems* to me that we could just bump the participants[].count to 2 in interop.yaml and call it a day.\nUnless we also have test cases that require only 1 node?", "2025-05-14T18:40:36Z", "2025-05-14T18:40:36Z", "sigma", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6rwWtY", "I_kwDODjvEJM62muMe", "Let me give that a try.", "2025-05-14T21:07:10Z", "2025-05-14T21:07:10Z", "teddyknox", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6sfsSw", "I_kwDODjvEJM62muMe", "To add more context, we need a local kt devnet which is general enough for all sync tests. So mirror `MultiSupervisorInteropSystemIDs` : https://github.com/ethereum-optimism/optimism/blob/7962d43f57e6a939d901b47533a451bd73d1a3fe/op-devstack/sysgo/system.go#L220\n\nWe need additional supervisor, and all L2s have sequencer and verifier pairs.\n\nWe may update https://github.com/ethereum-optimism/optimism/blob/develop/kurtosis-devnet/interop.yaml for this. \n\nWhen we update this, the default interop kt devnet will take more time to spawn. To overcome, my suggestion was to maintain two interop yamls(or description), which contains the minimal interop set, and the redundant yaml which is a superset of minimal interop set, containing verifier EL|CLs per L2s and an additional supervisor.\n", "2025-05-20T11:25:52Z", "2025-05-20T11:25:52Z", "pcw109550", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6uMklo", "I_kwDODjvEJM62muMe", "@janjakubnanista is working on this now IIUC, @janjakubnanista can you comment?", "2025-05-30T14:19:32Z", "2025-05-30T14:19:32Z", "teddyknox", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6uMGSI", "I_kwDODjvEJM62a7mw", "fixed as of #16186", "2025-05-30T13:29:58Z", "2025-05-30T13:29:58Z", "sigma", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6re1WC", "I_kwDODjvEJM62axfy", "https://github.com/ethereum-optimism/infra/pull/336 provides faucet feature to op-acceptor\n", "2025-05-13T15:30:41Z", "2025-05-16T19:25:16Z", "sigma", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6rsbo-", "I_kwDODjvEJM62O358", "> add must DSL for starting/stopping of op-batcher, in order to not have to specify error checking, and retries\n\nWe need to distinguish that we actually shut down the entire batch process, or calling the stop API. Currently op node and supervisor supports start/stop functionality via control plane, and these are all implemented for completely shutting down. Much more natural to completely shut down, because calling the stop API and considering it as a shutdown state is somewhat artificial.\n\n> add DSL for wait for reorg on L2 chain, based on block hash and number\n\nSimilar implementation is currently done at https://github.com/ethereum-optimism/optimism/pull/15908, refer to L2CL DSL's Rewind function.\n\n> add DSL for safe-chain progression for a given L2 chain (i.e. comparing supervisor safe ref, and L2 CL safe ref)\n\nDSL added at https://github.com/ethereum-optimism/optimism/pull/15878 does not compare each components' sync state, but at least check for head progression. Refer to L2CL DSL's Advance function.\n\n> merge TestInteropHappyTx and TestInitExecMsg - they are essentially doing the same thing - we could merge if we upgrade TestInitExecMsg to wait until cross-safe progresses and confirms that blocks containing init/exec msgs haven't been reorged out.\n\nTracked at https://github.com/ethereum-optimism/optimism/issues/15917.\n", "2025-05-14T14:53:39Z", "2025-05-14T14:53:39Z", "pcw109550", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6siCfG", "I_kwDODjvEJM62O358", "Addressed all, but 1 and 6 in https://github.com/ethereum-optimism/optimism/pull/16025 . `Funder` DSL is still not relying on invocation ID, so there is currently no way to create random EOAs, so I am skipping this one for the initial PR.", "2025-05-20T14:17:00Z", "2025-05-20T14:18:04Z", "nonsense", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6siQ8z", "I_kwDODjvEJM62O358", "@pcw109550 \n\n> Similar implementation is currently done at https://github.com/ethereum-optimism/optimism/pull/15908, refer to L2CL DSL's Rewind function.\n\nI added a new method `ReorgTriggered`, as I want to be able to compare against a specific BlockID. Let me know if you think this method should be under L2CL or L2EL.\n\n> DSL added at https://github.com/ethereum-optimism/optimism/pull/15878 does not compare each components' sync state, but at least check for head progression. Refer to L2CL DSL's Advance function.\n\nI added `ReachedRef` to L2CL, similar to `Reached`, but the target is `eth.BlockID` and not on block number, as we want to make sure that the specific block hasn't been reorged, when we reach the given block number.", "2025-05-20T14:34:44Z", "2025-05-20T14:34:44Z", "nonsense", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6t1Tht", "I_kwDODjvEJM60_jVM", "note: the CI check should be portable to other repos, as we should use the same check to verify that other consumers of the kurtosis deployments are still functional", "2025-05-28T13:48:24Z", "2025-05-28T13:48:24Z", "sigma", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6uBPU-", "I_kwDODjvEJM6z3fZa", "superseded by https://github.com/ethereum-optimism/optimism/issues/15771", "2025-05-29T14:20:17Z", "2025-05-29T14:20:17Z", "emhane", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6oIMOa", "I_kwDODjvEJM6zcHrk", "Blocked by https://github.com/ethereum-optimism/optimism/issues/15151", "2025-04-22T09:25:23Z", "2025-04-22T09:25:23Z", "pcw109550", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6seKPG", "I_kwDODjvEJM6zcHrk", "This seems to be covered already by @pcw109550 here: https://github.com/ethereum-optimism/optimism/blob/d474182026cb0a56874c1c2658849f7a1951b55d/op-acceptance-tests/tests/interop/interop_txplan_test.go#L391\nGiven the existing coverage, this maybe can be closed, unless me/CW missed something. And definitely does not seem like a P1.\n", "2025-05-20T09:11:57Z", "2025-05-20T09:11:57Z", "protolambda", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6lflsR", "I_kwDODjvEJM6xB-yR", "This is a supervisor land issue. I'll inform the client team.", "2025-04-03T17:47:43Z", "2025-04-03T17:47:43Z", "Inphi", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6lfm1e", "I_kwDODjvEJM6xB-yR", "cc @axelKingsley @tyler-smith", "2025-04-03T17:49:47Z", "2025-04-03T17:49:47Z", "Inphi", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6lf3pa", "I_kwDODjvEJM6xAzP_", "cc @axelKingsley @tyler-smith . It'll be good for the fault proof if we can check message timestamps earlier before we have to reach out to the db.", "2025-04-03T18:22:04Z", "2025-04-03T18:22:04Z", "Inphi", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6sn7SW", "I_kwDODjvEJM6v5Uly", "Has a TODO still in the code.", "2025-05-21T00:45:55Z", "2025-05-21T00:45:55Z", "ajsutton", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6qqeR_", "I_kwDODjvEJM6uVtO8", "@yashvardhan-kukreja can we now close this issue? Or do you want to keep this open until we've done additional validation on the RC Betanet next week?", "2025-05-08T14:25:00Z", "2025-05-08T14:25:00Z", "alfonso-op", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6qr-NM", "I_kwDODjvEJM6uVtO8", "Happy to close this now. Tested against all the possible error types coming from supervisor.\n- Proxyd is explicitly aware of and respecting the interop error spec.", "2025-05-08T16:34:16Z", "2025-05-08T16:35:04Z", "yashvardhan-kukreja", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6q1cQP", "I_kwDODjvEJM6uVtO8", "This should not be closed yet.\n\nIs this reproducible somewhere in an integration test?\n\nHave you reviewed this with @axelKingsley as part of the load testing?\n\nThere were several interop FMAs, which you can find in the design-docs repo, calling out validation risks with proxyd, which I don't believe where addressed fully.\n\nIn proxyd `validateInteropSendRpcRequest` the access-list length of an incoming transaction can currently be tens of thousands of entries (one bytes32 each, easily fits many in a single tx), which in the current implementation all get passed on to the supervisor-RPC without any bound check or rate-limit, which can likely overwhelm RPC traffic and hang up the supervisor with unreasonable amounts of verification work.\n\nAnd we should be careful that even with some rate-limits, the one user shouldn't be denying interop access to another user by consuming the same global rate-limit fully. Ideally the size of the interop workload applies to the sender / traffic-source specific limiting somehow.\n\n", "2025-05-09T11:06:24Z", "2025-05-09T11:06:24Z", "protolambda", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6q6yCa", "I_kwDODjvEJM6uVtO8", "Ohh I was under the impression of this issue being scoped to proxyd only validating the incoming interop-oriented messages.\n\nWe had discussions in the past regarding interop-related rate-limiting but that was left with the idea of addressing that as a step 2. after the basic implementation of validation was in-place.\n\nI can start off immediately with the other asks you mentioned.\n\nThank you", "2025-05-09T18:18:52Z", "2025-05-09T18:18:52Z", "yashvardhan-kukreja", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6rXaSD", "I_kwDODjvEJM6uVtO8", "Here are a few CRRs in its favor:\n- [sendRawTransaction request params size limit](https://github.com/ethereum-optimism/infra/pull/337)\n- [interop access list size limit](https://github.com/ethereum-optimism/infra/pull/338)\n- [client-side deduplication of interop access lists' inbox entries](https://github.com/ethereum-optimism/infra/pull/340)\n- [client-side prevalidation of the parseability of interop access-list](https://github.com/ethereum-optimism/infra/pull/341)\n- [interop message validation signer-based sender rate limit](https://github.com/ethereum-optimism/infra/pull/343)", "2025-05-13T04:49:04Z", "2025-05-13T04:49:15Z", "yashvardhan-kukreja", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6uLPlN", "I_kwDODjvEJM6uVtO8", "I believe this work has been completed. Closing the issue now.", "2025-05-30T11:51:30Z", "2025-05-30T11:51:30Z", "protolambda", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6oAgjp", "I_kwDODjvEJM6tkCE6", "Aiming to get final reviews by today Apr 21st.", "2025-04-21T15:07:40Z", "2025-04-21T15:07:40Z", "yoonchung8822", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6oNM3H", "I_kwDODjvEJM6tkCE6", "The current status is working through one last issue with tests. Validating that it's a test issue and not a real issue.", "2025-04-22T17:39:04Z", "2025-04-22T17:39:04Z", "unknown", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6pFiiS", "I_kwDODjvEJM6tkCE6", "@axelKingsley Reopening as there are still TODOs in the code.\n```\nRepository & Issue                       | Title                                                             | Location                                \n-----------------------------------------+-------------------------------------------------------------------+---------------------------------------------------\nethereum-optimism/optimism #14800        | op-supervisor: introduce rewind-lock                              | op-supervisor/supervisor/backend/backend.go:530   \nethereum-optimism/optimism #14800        | op-supervisor: introduce rewind-lock                              | op-supervisor/supervisor/backend/backend.go:561   \n```", "2025-04-28T21:46:07Z", "2025-04-28T21:46:07Z", "ajsutton", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6pgSbw", "I_kwDODjvEJM6tkCE6", "@tyler-smith Reopening as there are still TODOs in the code (same ones as last time I think):\n```\n-----------------------------------------+-------------------------------------------------------------------+---------------------------------------------------\nethereum-optimism/optimism #14800        | op-supervisor: introduce rewind-lock                              | op-supervisor/supervisor/backend/backend.go:530   \nethereum-optimism/optimism #14800        | op-supervisor: introduce rewind-lock                              | op-supervisor/supervisor/backend/backend.go:561   \n```", "2025-05-01T00:44:43Z", "2025-05-01T00:44:43Z", "ajsutton", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6pxLAF", "I_kwDODjvEJM6tkCE6", "There is one final TODO for this task: Making the CheckAccessList RPC endpoint use a a read handle. ", "2025-05-02T22:52:32Z", "2025-05-02T22:52:32Z", "unknown", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6tzxpz", "I_kwDODjvEJM6tkCE6", "Working on changing the read-handle system, to make it more usable, and fix the remaining TODOs.", "2025-05-28T11:56:04Z", "2025-05-28T11:56:04Z", "protolambda", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6uBRgt", "I_kwDODjvEJM6sUXUU", "superseded by https://github.com/ethereum-optimism/optimism/issues/15771", "2025-05-29T14:23:28Z", "2025-05-29T14:23:39Z", "emhane", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6pnED_", "I_kwDODjvEJM6nFNBA", "Is this the same as https://github.com/ethereum-optimism/superchain-registry/issues/898 ?\n", "2025-05-01T19:35:13Z", "2025-05-01T19:35:13Z", "pauldowman", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6qSL7Z", "I_kwDODjvEJM6nFNBA", "This is different to https://github.com/ethereum-optimism/superchain-registry/issues/898 - this is about how the DisputeGameFactoryProxy address is recorded and 898 is about recording the dependency set config.\n\nGiven the superchain registry has been moving more towards chains being more stand alone it's fairly likely that the DisputeGameFactoryProxy address will still just be recorded per chain. The registry should still get validation that the dependency set config it has matches up with which chains are sharing factories (and ETH lockboxes).", "2025-05-07T03:16:03Z", "2025-05-07T03:16:03Z", "ajsutton", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6ttap-", "I_kwDODjvEJM6nFNBA", "Closing this due to already being complete", "2025-05-27T22:50:32Z", "2025-05-27T22:50:32Z", "tynes", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6pxLaC", "I_kwDODjvEJM6l9sMx", "A number of review items have been addressed, including changing to event-based activation checking.\n\nThe remaining work is:\n- Use finalized blocks for activation instead of unsafe blocks. \n- Complete e2e action tests to show correctness. ", "2025-05-02T22:54:34Z", "2025-05-02T22:54:34Z", "unknown", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6p9gxk", "I_kwDODjvEJM6l9sMx", "We merged a version of this to facilitate the beta net test, however it should really have much more automated tests added and some more review. ", "2025-05-05T15:59:59Z", "2025-05-05T15:59:59Z", "unknown", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6qr7Ys", "I_kwDODjvEJM6l9sMx", "We've found a number of issues and they are resolved some. The remaining is to fix \"pre activation\" mode so the supervisor works correctly before interop is active, along with acceptance tests that show the correct behavior. ", "2025-05-08T16:29:13Z", "2025-05-08T16:29:13Z", "unknown", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6qr9LZ", "I_kwDODjvEJM6l9sMx", "Acceptance Test Checks:\n\n- Before activation\n    - Blocks produced\n    - Blocks become safe\n    - Blocks can be reset if they're not finalized\n    - Blocks can't be reset if they are finalized\n- During activation\n    - Correct anchor block is loaded\n    - DBs are all initialized with correct anchor block\n    - Pre-activation heads in memory are dropped\n    - Should not experience resets; should transition smoothly\n- After activation\n    - Blocks produced\n    - Blocks become safe\n    - Blocks become cross-valid if no exec msgs\n    - Blocks become cross-valid if only valid exec msgs\n    - Blocks do not become cross-valid if any invalid exec msgs\n    - Blocks can be reset if they're not finalized\n    - Blocks can't be reset if they are finalized\n    - New blocks update DBs correctly\n    - New messages are in the DBs", "2025-05-08T16:32:16Z", "2025-05-08T16:32:16Z", "unknown", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6rUv-R", "I_kwDODjvEJM6l9sMx", "Update from calls earlier today:\n- We merge the op-supervisor PR that handles upgrades, but with known issues (anchor block may reorg out, safety isn't correct).\n- We follow up the anchor block work in https://github.com/ethereum-optimism/optimism/issues/15774\n", "2025-05-12T22:28:57Z", "2025-05-12T22:28:57Z", "protolambda", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6kdgM4", "I_kwDODjvEJM6iyWtz", "We talked about this recently, but to update on the ticket -- this item won't block Alpha closure (and won't be done by Alpha deadline), and should be worked on between Alpha and Beta.", "2025-03-27T19:29:01Z", "2025-03-27T19:29:01Z", "axelKingsley", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6nUQZz", "I_kwDODjvEJM6iyWtz", "@protolambda , @teddyknox and I spoke yesterday to give context on this work. Sounds like upgrade based testing is higher priority for now.", "2025-04-15T18:24:38Z", "2025-04-15T18:24:38Z", "axelKingsley", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6q14W4", "I_kwDODjvEJM6iyWtz", "Superseded by https://github.com/ethereum-optimism/optimism/issues/15812\n", "2025-05-09T11:49:39Z", "2025-05-09T11:49:39Z", "protolambda", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6q-A9O", "I_kwDODjvEJM6iyWtz", "Reopened because there are still todos in the code (I'm guessing they should be updated to the new issue but I'm not at my laptop). ", "2025-05-10T05:03:12Z", "2025-05-10T05:03:12Z", "ajsutton", "2025-08-31 04:59:37"]
["IC_kwDODjvEJM6uLN82", "I_kwDODjvEJM6iyWtz", "Cannot find any todo references left, closing now", "2025-05-30T11:48:14Z", "2025-05-30T11:48:14Z", "protolambda", "2025-08-31 04:59:37"]
["IC_kwDOL-xLQ86vKbD6", "I_kwDOL-xLQ865w97O", "Done in https://github.com/ethereum-optimism/infra/pull/388", "2025-06-04T06:24:24Z", "2025-06-04T06:24:24Z", "scharissis", "2025-08-31 05:00:44"]
["IC_kwDOL-xLQ86vKZ58", "I_kwDOL-xLQ865w6C-", "Done in:\n[Pull Request #383](https://github.com/ethereum-optimism/infra/pull/383)\n[Pull Request #386](https://github.com/ethereum-optimism/infra/pull/386)", "2025-06-04T06:22:46Z", "2025-06-04T06:22:46Z", "scharissis", "2025-08-31 05:00:44"]
["IC_kwDOL-xLQ86vKOqM", "I_kwDOL-xLQ865wxHH", "Done in https://github.com/ethereum-optimism/infra/pull/336", "2025-06-04T06:06:52Z", "2025-06-04T06:06:52Z", "scharissis", "2025-08-31 05:00:44"]
["IC_kwDOL-xLQ86vKdNU", "I_kwDOL-xLQ863dwgq", "Done in #388 ", "2025-06-04T06:27:58Z", "2025-06-04T06:27:58Z", "scharissis", "2025-08-31 05:00:44"]
["IC_kwDOLB-lzc6vEPzd", "I_kwDOLB-lzc6zn7WW", "would like to work on this!", "2025-06-03T20:40:45Z", "2025-06-03T20:40:45Z", "rose2221", "2025-08-31 05:00:46"]
["IC_kwDOH2Qg5s6wE-HI", "I_kwDOH2Qg5s6YA8Eu", "Hello, @Eoous  is there a solution?", "2025-06-08T13:56:28Z", "2025-06-08T13:56:28Z", "0xWilliamWang", "2025-08-31 05:00:47"]
["IC_kwDOJ_r-bs6uwrqw", "I_kwDOJ_r-bs65AHRA", "Closing since this is a duplicate of https://github.com/ethereum-optimism/optimism/issues/15875", "2025-06-02T18:39:18Z", "2025-06-02T18:39:18Z", "bitwiseguy", "2025-08-31 05:01:02"]
["IC_kwDOJ_r-bs6u-pzj", "I_kwDOJ_r-bs65AD2j", "Completed by https://github.com/ethereum-optimism/superchain-registry/pull/1043", "2025-06-03T14:41:32Z", "2025-06-03T14:41:32Z", "bitwiseguy", "2025-08-31 05:01:02"]
["IC_kwDODjvEJM6vmup1", "I_kwDODjvEJM66Hq2P", "Preliminary fix: https://github.com/ethereum-optimism/optimism/pull/16307", "2025-06-05T20:45:55Z", "2025-06-05T20:45:55Z", "axelKingsley", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6vM9w2", "I_kwDODjvEJM65xH7X", "@scharissis consideration - do we want to have a special \"staging\" gate? Behavior of this gate could be that these tests get run but they don't block merges once https://github.com/ethereum-optimism/optimism/issues/16270 is implemented.\n\nThis can ensure that all tests are in a gate but that there is a path towards promoting them to be blocking checks in CI.", "2025-06-04T10:04:57Z", "2025-06-04T10:04:57Z", "alfonso-op", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6usBOv", "I_kwDODjvEJM648dUf", "+ `TestL2CLAheadOfSupervisor` also used multinode support, so we need to fix this also.", "2025-06-02T13:37:19Z", "2025-06-02T13:37:19Z", "pcw109550", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6vudT4", "I_kwDODjvEJM648dUf", "@teddyknox reassigning my self since this is already done \ud83d\ude4f ", "2025-06-06T05:59:57Z", "2025-06-06T05:59:57Z", "pcw109550", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uRBri", "I_kwDODjvEJM642uQm", "We can drop support completely for now and re-introduce it in a future release", "2025-05-30T23:13:03Z", "2025-05-30T23:13:03Z", "tynes", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uJlU5", "I_kwDODjvEJM640nIo", "@ajsutton provided a first analysis at https://github.com/ethereum-optimism/optimism/pull/16008#pullrequestreview-2879961007", "2025-05-30T09:17:04Z", "2025-05-30T09:17:04Z", "sebastianst", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uJ0Pg", "I_kwDODjvEJM640nIo", "I see two options:\n1. Try to get the L2 derived -> L1 source mapping from the old op-node SafeDB, as @ajsutton is suggesting. I don't understand the challenger and the SafeDB API well enough to see how this just works. From what I can tell, the safe db only provides the query `SafeHeadAtL1Block(ctx context.Context, l1BlockNum uint64) (*eth.SafeHeadResponse, error)` which translates an L1 block number to the latest L2 safe head ID and the L1 block ID for this number. This feels the other way around than the mapping that is needed to construct the super roots, L2 derived -> L1 source. I can see how the required mapping could be derived from the safe db API but it doesn't seem straight forward. But it's probably just my very limited understanding of how Interop proofs work.\n2. We actually start indexing the safe db in the supervisor just at the first random derived block it receives, even if it's pre-Interop. Then it could serve the super roots the same way it does now. This means we'd just remove any special meaning of the _first block_ in the safe db. We would still only start indexing logs in the events db after the Interop activation block. ", "2025-05-30T09:38:50Z", "2025-05-30T09:38:50Z", "sebastianst", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uJ4s-", "I_kwDODjvEJM640nIo", "You're right the index in op-node is the other way around. We could binary search and make it work or we could tweak the request since the challenger has both the L1 head and the L2 block so it can ask either way around. Would be a bit interesting to create a sensible response format but I'm sure we could work something out. \n\nBut I like option 2 a lot more if it's doable. It would mean you need to have run the supervisor from whatever anchor state we pick but that fits well with us getting people to run it well ahead of interop anyway. And my gut feel is that approach will fit better with adding chains later where we likely do need to index all the chains prior to them merging (not that adding chains is a priority now that we need to worry about the details). ", "2025-05-30T09:45:56Z", "2025-05-30T09:45:56Z", "ajsutton", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uJ9j0", "I_kwDODjvEJM640nIo", "Option 2 means that we could also handle pre-Interop resets for the block range that's already covered by the supervisor's safe db. But we'd need to take a careful look at the current supervisor because there are a few places where the implicit assumption may be made that the first safe db block is the interop activation block.", "2025-05-30T09:51:08Z", "2025-05-30T09:51:08Z", "sebastianst", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uj4T0", "I_kwDODjvEJM640nIo", "Updated the title to make it clear this will need to be fixed in op-supervisor. Not placing blame or anything but given plans to \"decouple\" protocol and proofs work we need to be really clear that the current situation can't be fixed on the fault proof side - it just doesn't have the information available to correctly play games between opcm.migrate and interop activating.\n\nI would advocate that we fix this before moving forward with devnets since the current code isn't a viable way to go through the migration process.", "2025-06-02T03:59:07Z", "2025-06-02T03:59:07Z", "ajsutton", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6unXBN", "I_kwDODjvEJM640nIo", "@ajsutton thanks, makes sense. I'm implementing option 2 right now.", "2025-06-02T09:03:06Z", "2025-06-02T09:03:06Z", "sebastianst", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6vmg2b", "I_kwDODjvEJM640nIo", "Another option is to handle pre-interop super roots is to have the op-challenger manually construct the root. Until interop activates, there is no risk of unresolved cross-dependencies. So the op-challenger should be able to manually construct the data it needs directly from rollup nodes (or via the supervisor as a proxy) without relying on the supervisor's db.\nThe logic to do this will be very similar to what we do for output roots. Except done across all chains.", "2025-06-05T20:32:14Z", "2025-06-05T20:32:51Z", "Inphi", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6vr70f", "I_kwDODjvEJM640nIo", "Fixing this in the challenger would then require the challenger to be given every rollup node endpoint and it would have to aggregate the sync status and super roots using its own custom code. Just combining the op-node output pre-interop is definitely a viable option, but I'd say it would make far more sense to have that code in supervisor since it already has all the op-node endpoints and the code for creating super roots by combining their output roots.", "2025-06-06T01:12:00Z", "2025-06-06T01:12:00Z", "ajsutton", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6v1TbZ", "I_kwDODjvEJM640nIo", "It also allows us to remove the op-node's safe db for managed nodes.", "2025-06-06T17:42:28Z", "2025-06-06T17:42:28Z", "sebastianst", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6s1lVh", "I_kwDODjvEJM63hv2J", "Getting it addressed here - https://www.notion.so/oplabs/Flashblocks-Release-Readiness-Checklist-1faf153ee16280ac80d8cda0162f2392?pvs=4", "2025-05-22T02:33:22Z", "2025-05-22T02:33:22Z", "yashvardhan-kukreja", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6tvBVg", "I_kwDODjvEJM63hv2J", "Tracking it here - https://www.notion.so/oplabs/Flashblocks-Release-Readiness-Checklist-1faf153ee16280ac80d8cda0162f2392", "2025-05-28T03:30:31Z", "2025-05-28T03:30:31Z", "yashvardhan-kukreja", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6tV4sZ", "I_kwDODjvEJM62zGFy", "Remaining work is to add this to the devnet runbook, but proofs has been descoped from the current devnets.", "2025-05-26T00:23:06Z", "2025-05-26T00:23:06Z", "ajsutton", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6vG7Gl", "I_kwDODjvEJM62zGFy", "Closing this - I've added instructions to the runbook.", "2025-06-04T00:19:01Z", "2025-06-04T00:19:01Z", "ajsutton", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6u-rBR", "I_kwDODjvEJM62nk4L", "Closing since all child issues are now complete", "2025-06-03T14:42:16Z", "2025-06-03T14:42:16Z", "bitwiseguy", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uuNyL", "I_kwDODjvEJM62lniJ", "I did this.", "2025-06-02T15:51:22Z", "2025-06-02T15:51:22Z", "maurelian", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uuNqQ", "I_kwDODjvEJM62lm6b", "I did this.", "2025-06-02T15:51:15Z", "2025-06-02T15:51:15Z", "maurelian", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6rjfMQ", "I_kwDODjvEJM62dk8k", "For op-program we want to avoid parsing the depset multiple times as it increases runtime a fair bit.  So just loading the depset for one chain and depending on it being consistent for all chains makes sense (especially since we will have tests in superchain-registry to ensure the configs are consistent).\n\nGiven the loading code is generally shared I suspect op-supervisor would just do the same and load a single depset config.", "2025-05-13T23:54:18Z", "2025-05-13T23:54:18Z", "ajsutton", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6tzTCp", "I_kwDODjvEJM62dk8k", "@Inphi we should probably add development effort estimates to this issues, since I saw that most of the issues in the interop burndown list have them now.", "2025-05-28T11:06:34Z", "2025-05-28T11:06:34Z", "BlocksOnAChain", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6t3Kqb", "I_kwDODjvEJM62dk8k", "@bitwiseguy It seems this ticket is unblocked with the new depset APIs. Are there any changes to the depset or its schema that are still needed?", "2025-05-28T16:20:48Z", "2025-05-28T16:20:48Z", "Inphi", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uB_r_", "I_kwDODjvEJM62dk8k", "@Inphi - as far as I know the depset schema in op-geth (the one returned by `superchain.GetDepset(chainId)` should now be stable", "2025-05-29T15:28:32Z", "2025-05-29T15:28:32Z", "bitwiseguy", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uNnac", "I_kwDODjvEJM62ZtZ-", "This work is blocked on the SCR supporting multiple historical versions of op-deployer. That's so we can continue importing chains from op-deployer artifacts as the shape of those artifacts changes: for example importing `interop-rc-alpha` and `eris` devnets is not possible using a single version of op-deployer. \n\nThe work on that is being partially tracked here https://github.com/ethereum-optimism/optimism/issues/15934 but we should also get a new issue up to cover overall support for the feature. ", "2025-05-30T16:12:00Z", "2025-05-30T16:12:31Z", "geoknee", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uycYN", "I_kwDODjvEJM61npN6", "This is done, with #16220 and https://github.com/ethereum-optimism/optimism/pull/16071", "2025-06-02T20:55:08Z", "2025-06-02T20:55:08Z", "protolambda", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uvfKM", "I_kwDODjvEJM61nhWJ", "@protolambda can we close this given that we have descoped running multiple nodes running the same chain with a single supervisor?", "2025-06-02T17:22:34Z", "2025-06-02T17:22:34Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6svEGp", "I_kwDODjvEJM61nYka", "@protolambda we now want to run nodes in managed mode pre-Interop. So I think we can close this issue? We want to _require_ nodes to have a supervisor already running as soon as they have an Interop timestamp set.", "2025-05-21T13:41:38Z", "2025-05-21T13:41:38Z", "sebastianst", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6svP-9", "I_kwDODjvEJM61nYka", "We need some minor changes in op-node, I was going to implement those now that the resetTracker/sequence window stuff is out of the way. Managed mode currently does not turn on when the interop fork time is not set, we need to change that. \n", "2025-05-21T13:56:36Z", "2025-05-21T13:56:36Z", "protolambda", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6qm2TP", "I_kwDODjvEJM602i7W", "This spans around both upgrade and test-checklist scope. Who will own this? @teddyknox @sebastianst ", "2025-05-08T09:05:06Z", "2025-05-08T09:05:06Z", "protolambda", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6rRGBS", "I_kwDODjvEJM602i7W", "Not sure what \"test-checklist\" scope is, but I've created an \"Upgrade testing checklist\" ticket to track this issue and others related to the upgrade behaviors.", "2025-05-12T17:27:38Z", "2025-05-12T17:27:38Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6sxbWa", "I_kwDODjvEJM602i7W", "WIP test here: https://github.com/ethereum-optimism/optimism/pull/15723", "2025-05-21T17:00:08Z", "2025-05-21T17:00:08Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uvXoT", "I_kwDODjvEJM602i7W", "Closed with https://github.com/ethereum-optimism/optimism/issues/15647, remaining task is captured by successor ticket https://github.com/ethereum-optimism/optimism/issues/16239", "2025-06-02T17:13:19Z", "2025-06-02T17:13:19Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6nFLvO", "I_kwDODjvEJM6ybzqI", "The key thing with `compareGames` was to flag what changed - not really whether it was right or wrong.  When doing upgrades you generally know what should be different and so anything else changing is a red flag.\n\nThe same kind of thing can be built into OPCM or superchain ops task by comparing the game implementation prior to the upgrade with the one after it. Most of the values that could change are defined as constants in the standard charter so they can be verified easily, but things like ASR and DWETH are really about whether they were expected to have changed or not (apart from checking they use the right version/implementation etc). The ASR in particular needs off-chain input to be fully validated as you need to check if the anchor state it holds is actually a canonical block.", "2025-04-14T22:07:25Z", "2025-04-14T22:07:25Z", "ajsutton", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6viYHb", "I_kwDODjvEJM6ybzqI", "I am going to close this issue as we have other workstreams which cover the need to validate the dispute games. Namely: \n1. StandardValidator work validates the configuration of these games\n2. VerifyOPCM.s.sol validates the bytecode of games deployed by the OPCM.", "2025-06-05T15:39:32Z", "2025-06-05T15:39:32Z", "maurelian", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6rOOD3", "I_kwDODjvEJM6x8bTL", "By \"deposit-only\" block do you mean upgrade block?", "2025-05-12T13:42:14Z", "2025-05-12T13:42:14Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6rO_NE", "I_kwDODjvEJM6x8bTL", "@teddyknox if there are no user tx in the block, the block only contains deposit tx.", "2025-05-12T14:35:20Z", "2025-05-12T14:35:20Z", "pcw109550", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6re_-O", "I_kwDODjvEJM6x8bTL", "I understand that, was more asking why this is a case we need to test explicitly.", "2025-05-13T15:44:28Z", "2025-05-13T15:44:28Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6rhZeQ", "I_kwDODjvEJM6x8bTL", "@pcw109550 or @Inphi has the last bullet in this ticket's TODO list been implemented as an action test already?", "2025-05-13T19:09:52Z", "2025-05-13T19:09:52Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6rsO8B", "I_kwDODjvEJM6x8bTL", "@teddyknox For the interop action test written by me at https://github.com/ethereum-optimism/optimism/blob/develop/op-e2e/actions/interop/interop_txplan_test.go, the test covers https://github.com/ethereum-optimism/optimism/blob/3a8c970d12e1f1fb867d6d89e1f6938ddffe6143/op-e2e/actions/interop/interop_txplan_test.go#L218\n\nSo not exactly the sequence that proto mentioned. Think its worth it to also cover the exact path by proto, and it seems it can be covered using both sysgo and sysext, as well as action test.", "2025-05-14T14:39:05Z", "2025-05-14T14:56:49Z", "pcw109550", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6quYRV", "I_kwDODjvEJM6x8Wqn", "> missing op-node for a chain: warn about lacking source, keep other chains synced\n\nShould exec txs originating from the chain lacking source be gracefully excluded?", "2025-05-08T20:51:48Z", "2025-05-08T20:51:48Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6quYoq", "I_kwDODjvEJM6x8Wqn", "> 2 managed op-nodes per chain: avoid duplicate sync work\n\nWhat duplicate sync work specifically?", "2025-05-08T20:52:36Z", "2025-05-08T20:52:36Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6quY6H", "I_kwDODjvEJM6x8Wqn", "> op-node managed by two op-supervisors: need detection, automatic disconnect\n\nThe second supervisor should disconnect, right?", "2025-05-08T20:53:17Z", "2025-05-08T20:53:17Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6utQoY", "I_kwDODjvEJM6x8Wqn", "Multi-node support has been removed from the release, demoting this to P3.", "2025-06-02T14:49:11Z", "2025-06-02T14:49:11Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uwFYj", "I_kwDODjvEJM6x8Wqn", "Closing as a duplicate of platforms interop acceptance testing.", "2025-06-02T17:59:21Z", "2025-06-02T17:59:21Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6utVn0", "I_kwDODjvEJM6x8II3", "Closing in favor of https://github.com/orgs/ethereum-optimism/projects/128/views/16?pane=issue&itemId=110729525&issue=ethereum-optimism%7Coptimism%7C15941", "2025-06-02T14:53:24Z", "2025-06-02T14:53:24Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6uvjtm", "I_kwDODjvEJM6onM1H", "@nonsense can we close this one?", "2025-06-02T17:29:34Z", "2025-06-02T17:29:34Z", "teddyknox", "2025-08-31 05:01:22"]
["IC_kwDODjvEJM6q2VFz", "I_kwDODjvEJM6onMaA", "Bumping to testnet milestone. Reorg testing should continue, there were some findings already, but it's not realistic to block the RC Beta next week on this functionality.\n", "2025-05-09T12:29:33Z", "2025-05-09T12:29:33Z", "protolambda", "2025-08-31 05:01:22"]
["IC_kwDOFpg0Ns6xGqsf", "I_kwDOFpg0Ns63Gq0E", "Hi @brianfryer thank you for pointing this out, I'll try to get a fix out asap.", "2025-06-13T18:48:12Z", "2025-06-13T18:48:12Z", "fainashalts", "2025-08-31 05:02:36"]
["IC_kwDOFpg0Ns6xHzK_", "I_kwDOFpg0Ns63Gq0E", "#1161 was just merged and should address this! ", "2025-06-13T20:34:03Z", "2025-06-13T20:34:03Z", "fainashalts", "2025-08-31 05:02:36"]
["IC_kwDOKSJyfM6wsa_y", "I_kwDOKSJyfM67Br_z", "PR for Gas Tank contract https://github.com/defi-wonderland/optimism/pull/414", "2025-06-11T22:19:36Z", "2025-06-11T22:19:36Z", "tremarkley", "2025-08-31 05:02:36"]
["IC_kwDOL-xLQ86slQhn", "I_kwDOL-xLQ862KHH2", "Worth waiting for the wrappers for interacting with contracts currently under development", "2025-05-20T18:36:09Z", "2025-05-20T18:36:37Z", "serpixel", "2025-08-31 05:02:37"]
["IC_kwDOL-xLQ86s2lsH", "I_kwDOL-xLQ862KHH2", "To add to the above, I believe @pcw109550 is working on the DSL improvements to contract-management and we expect them to land soon.", "2025-05-22T05:47:02Z", "2025-05-22T05:47:02Z", "scharissis", "2025-08-31 05:02:37"]
["IC_kwDOL-xLQ86u34Fn", "I_kwDOL-xLQ862KHH2", "- We have a draft implementation [here](https://github.com/ethereum-optimism/optimism/pull/16249)\n- Currently blocked due to missing DSL for calldata structs (tracked [here](https://github.com/ethereum-optimism/optimism/issues/16256))", "2025-06-03T07:27:10Z", "2025-06-10T09:12:02Z", "serpixel", "2025-08-31 05:02:37"]
["IC_kwDOL-xLQ86wOcEi", "I_kwDOL-xLQ862KHH2", "Implemented (sysgo) on [#16333](https://github.com/ethereum-optimism/optimism/pull/16333).\nNeeds two followup tickets:\n- Add support for kt (sysext)\n- Only execute a withdrawal, do not prove the withdrawal (devnet)", "2025-06-09T18:17:07Z", "2025-06-09T18:17:07Z", "serpixel", "2025-08-31 05:02:37"]
["IC_kwDOKIwiaM6wNRSs", "I_kwDOKIwiaM654CxO", "@Nicca42 This is a great callout, i'll have a draft by tomorrow, and send it over to you.", "2025-06-09T16:18:23Z", "2025-06-09T16:18:23Z", "krofax", "2025-08-31 05:02:38"]
["IC_kwDOKIwiaM6wOIP8", "I_kwDOKIwiaM654CxO", "@bradleycamacho, @krofax, and I just chatted with this. We think this would good content for a home page, so we're explore it further and perhaps even take a wider lens for this.", "2025-06-09T17:45:48Z", "2025-06-09T17:45:48Z", "sbvegan", "2025-08-31 05:02:38"]
["IC_kwDOKIwiaM6wQatu", "I_kwDOKIwiaM63Lhyg", "The L2 Output Oracle is deprecated and the testnet tutorial is unfortunately extremely out of date, please see the call out at the top of the page \n\n![Image](https://github.com/user-attachments/assets/0b3f36e6-a8e9-4d4e-a0a4-16b0cdee559a)\n\nThank you for creating this issue, we have these tasks tracked in our internal backlog, so I'm going to close this issue", "2025-06-09T21:36:13Z", "2025-06-09T21:36:13Z", "sbvegan", "2025-08-31 05:02:38"]
["IC_kwDOKIwiaM6bQMQE", "I_kwDOKIwiaM6lNXlG", "@brucexu-eth Thanks for catching this, we will take a look at the issues you started and update the tutorial.", "2025-01-21T13:18:03Z", "2025-01-21T13:18:03Z", "krofax", "2025-08-31 05:02:38"]
["IC_kwDOH2Qg5s6vvheS", "I_kwDOH2Qg5s65wYa7", "Any update?", "2025-06-06T08:20:34Z", "2025-06-06T08:20:34Z", "mukul3097", "2025-08-31 05:02:38"]
["IC_kwDOH2Qg5s6wKZW3", "I_kwDOH2Qg5s65wYa7", "Hey, could someone please look into this?", "2025-06-09T11:27:43Z", "2025-06-09T11:27:43Z", "ntrzr", "2025-08-31 05:02:38"]
["IC_kwDOH2Qg5s6w4cww", "I_kwDOH2Qg5s65wYa7", "Any update?", "2025-06-12T16:59:13Z", "2025-06-12T16:59:13Z", "mukul3097", "2025-08-31 05:02:38"]
["IC_kwDOL-xLQ86sIOF2", "I_kwDOL-xLQ8627WNb", "Just calling it out here, the \"pending\" field is not exactly new and just the availability of an RPC response against \"pending\" param wouldn't be enough because good ol' non-flashblocks EL clients today also give a successful response against \"pending\" param.\n\nFor example: ` cast rpc --rpc-url=https://mainnet.optimism.io/ eth_getBlockByNumber pending false | jq -r .`\n\nWe would need a more distinctive expectation from the RPC response which would help validating the specific correctness \"pending\" eth_getBlockByNumber response from flashblocks ELs.\n\n----\n\nI also tried running the eth_getBlockByNumber against a flashblocks aware endpoint and it doesn't seem to return a new block number every 200-300ms. Instead it returns a new block number every ~2s.\n\nThe only thing indicative of flashblocks behaviour is the websocket stream of flashblocks over a websocket connection with the rbuilder.\n\nI believe that can be a good acceptance test here.\n1. Identifying the leader's rbuilder service.\n2. Establishing a websocket connection with it.\n3. Expecting a stream of 6-9 flashblocks per 2s. (considering the 250ms flashblock time)", "2025-05-16T23:24:12Z", "2025-05-16T23:25:43Z", "yashvardhan-kukreja", "2025-08-31 06:27:31"]
["IC_kwDOL-xLQ86sN-Br", "I_kwDOL-xLQ8627WNb", "Thanks Yash.\n\nThat's very interesting. I am surprised that `eth_getBlockByNumber` doesn't return a new block at least twice a second.\n\nIf so, we should reappropriate this ticket as you suggest above.\nI wonder if that'll require adding a WS client to devstack.", "2025-05-19T01:01:19Z", "2025-05-19T01:01:19Z", "scharissis", "2025-08-31 06:27:31"]
["IC_kwDOL-xLQ86sWA4x", "I_kwDOL-xLQ8627WNb", "We will probably need to add the flashblock websocket stream to be read by the testing for this ticket. \n\nI think a good test would be send a transaction and then confirm that a flashblock has been produced with that tx by reading the websocket stream. \n\nAlso could try using the eth_getBalance on the pending block as well. \n\nI think the tough think will be that most of these tests will need to execute <2s as well. Since flashblocks are quite quick\n\n", "2025-05-19T15:29:17Z", "2025-05-19T15:29:17Z", "jelias2", "2025-08-31 06:27:31"]
["IC_kwDOL-xLQ86sWDXb", "I_kwDOL-xLQ8627WNb", "Yep, you're right Jacob. Although I did ask the Base folks around the Flashblocks data's availability through the means of an RPC endpoint, I doubt there would be something. So hacked together a test which reads the stream over a 5s and accepts some ~22 streams to be parsed well. \n\nJust that now, I am working to porting it to devstack", "2025-05-19T15:32:34Z", "2025-05-19T15:32:46Z", "yashvardhan-kukreja", "2025-08-31 06:27:31"]
["IC_kwDOL-xLQ86zJtby", "I_kwDOL-xLQ8627WNb", "Completed here by the following PRs:\n- Transfer test - https://github.com/ethereum-optimism/optimism/pull/16515\n- Streaming test - https://github.com/ethereum-optimism/optimism/pull/16515\n\nAd-hoc PRs backing this work:\n- https://github.com/ethereum-optimism/optimism/pull/16348\n- https://github.com/ethereum-optimism/optimism/pull/16512", "2025-06-25T17:55:15Z", "2025-06-25T17:55:15Z", "yashvardhan-kukreja", "2025-08-31 06:27:31"]
["IC_kwDOKSJyfM6sHIpk", "I_kwDOKSJyfM62-K_9", "Need to confirm if we can test against rc beta to inform prioritization", "2025-05-16T19:47:16Z", "2025-05-16T19:47:16Z", "zainbacchus", "2025-08-31 06:27:32"]
["IC_kwDOKSJyfM6sHQoG", "I_kwDOKSJyfM62-K_9", "Confirmed can test against RC Beta, we'll want to get kinks ironed out before Sepolia so need to priortize sometime between Beta and Sepolia", "2025-05-16T20:05:50Z", "2025-05-16T20:05:50Z", "zainbacchus", "2025-08-31 06:27:32"]
["IC_kwDOKSJyfM6zL3sE", "I_kwDOKSJyfM62-K_9", "@zainbacchus since all the devnets got renamed, do you know if the upcoming one is the one to test with? Can we already test against an endpoint? ", "2025-06-25T21:40:21Z", "2025-06-25T21:40:21Z", "fainashalts", "2025-08-31 06:27:32"]
["IC_kwDOKIwiaM6iuSds", "I_kwDOKIwiaM6uUI1y", "Hey Simple, to address a couple of your points:\n\n> Status of the Optimism SDK: My research indicates that the package was last updated four months ago. Has the team decided to continue maintaining the existing features of the Optimism SDK, or will support for them be discontinued?\n\nSupport for them will be discontinued in favor of Viem.\n\n> Current support in Viem: Based on my findings, Viem has limited support for ERC-20 bridging. However, it is still possible for users to complete ERC-20 token bridging using Viem alone.\n\nI believe OP Labs team has added these bindings to https://github.com/ethereum-optimism/ecosystem/tree/main/packages/viem. I'll ask the team internally and relay your feedback. Thanks!", "2025-03-17T15:55:50Z", "2025-03-17T15:55:50Z", "sbvegan", "2025-08-31 06:27:32"]
["IC_kwDOKIwiaM6jjV-G", "I_kwDOKIwiaM6uUI1y", "Hey @opfocus! Thanks for bringing this up.\nGood news - we've just addressed this exact issue in[ PR #1470](https://github.com/ethereum-optimism/docs/pull/1470/), which provides a complete implementation for token bridging using viem. The PR adds working code examples for both L1\u2192L2 deposits and L2\u2192L1 withdrawals using the `@eth-optimism/viem` package that @sbvegan mentioned.\nThe implementation demonstrates:\n\n- How to use depositERC20 for sending tokens to L2\n- How to use withdrawOptimismERC20 for withdrawing tokens back to L1\n- Proper error handling and balance checking\n\nThis should fill the gap in the documentation while we transition away from the SDK. The code is intentionally kept simple and beginner-friendly while still being production-ready in a way.\nFeel free to take a look at the PR and let us know if it addresses your concerns!", "2025-03-21T17:01:44Z", "2025-03-21T17:01:44Z", "krofax", "2025-08-31 06:27:32"]
["IC_kwDOKIwiaM6y4m39", "I_kwDOKIwiaM6uUI1y", "@opfocus We have closed out on this, i'm closing this ticket.", "2025-06-24T16:45:50Z", "2025-06-24T16:45:50Z", "krofax", "2025-08-31 06:27:32"]
["IC_kwDODjvEJM60CqzA", "I_kwDODjvEJM6-HU-o", "This was fixed in a later release candidate [here](https://github.com/ethereum-optimism/optimism/commit/b1c9cd150bb18139c189eb7aa2960ca6aebd1976) and can be closed", "2025-06-30T20:32:33Z", "2025-06-30T20:32:33Z", "Melvillian", "2025-08-31 06:27:35"]
["IC_kwDOLB-lzc6xllce", "I_kwDOLB-lzc67wzyZ", "cc @dhyaniarun1993 @itschaindev @sadiq1971", "2025-06-17T08:25:21Z", "2025-06-17T08:25:21Z", "emhane", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6zqLKz", "I_kwDODjvEJM69xhV_", "https://github.com/cosmos/go-bip39 is a fork of github.com/tyler-smith/go-bip39, but its latest version is `v1.0.0` whereas one from op uses `v1.1.0`\n\nFrom README,\n```\nThis is a fork of github.com/tyler-smith/go-bip39 from right after the fixes from bartekn for MnemonicToByteArray were merged (commit hash: 52158e4697b87de16ed390e1bdaf813e581008fa).\n\nThe tyler-smith repo is undergoing significant refactoring at present that we may not want (eg. some vars becoming private).\n```", "2025-06-27T19:34:08Z", "2025-06-27T19:34:08Z", "seungjulee", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6zJzwu", "I_kwDODjvEJM67-LDK", "Completed by the following PRs:\n\nPhase 1\n- https://github.com/ethereum-optimism/optimism-charts/pull/106\n- https://github.com/ethereum-optimism/optimism-charts/pull/107\n\nPhase 2\n- https://github.com/ethereum-optimism/devnets/pull/96\n- https://github.com/ethereum-optimism/k8s/pull/6829\n\nPhase 3\n- https://github.com/ethereum-optimism/optimism-charts/pull/115\n- https://github.com/ethereum-optimism/k8s/pull/6872\n- https://github.com/ethereum-optimism/k8s/pull/6888\n\nPhase 4\n- https://github.com/ethereum-optimism/optimism-charts/pull/116\n- https://github.com/ethereum-optimism/k8s/pull/6882", "2025-06-25T18:05:57Z", "2025-06-25T18:05:57Z", "yashvardhan-kukreja", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6xWNrU", "I_kwDODjvEJM67r6hh", "### Why SLAs/SLOs Make Sense\n1. Clear expectations; engineering teams know what to expect from CI performance\n2. Accountability; Platforms team has measurable targets to optimize against\n3. Objective metrics; we can objectively discuss CI health with concrete metrics", "2025-06-16T07:24:18Z", "2025-06-16T07:25:27Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6xWeRr", "I_kwDODjvEJM67r6hh", "# CI Pipeline SLAs & Accountability Framework\n\n## Service Level Agreements\n\n### **Availability & Performance**\n- **Job Success Rate**: 95% (excluding legitimate test failures)\n- **PR Feedback Time**: 80% of PR pipelines complete within 1 hour\n\n### **Developer Experience**\n- **Flaky Test Identification**: Flaky tests are identified within a day of flaking\n- **Flaky Test Remediation**: Flaky tests are corrected within two weeks of first flake\n- **Clear Failure Attribution**: Failures clearly indicate whether the issue is with the code or infrastructure\n- **PR Merge Time**: Time from PR open to merge is is within 7 days (168 hours)\n- **Merge queue failure rate + merge queue time**: Failure rate is <5%, Queue Time is <1h\n\n### **Future Metrics**\nThese are metrics that would be good to include, but we aren't/can't measure them at this time.\n- **CI System Uptime**: 99%\n\n\n## Accountability Framework\n\n### **Bad Test Resolution**\n\n| **Severity** | **Impact** | **Protocol Team** | **Platforms Team** |\n|:-------------|:-----------|:------------------|:-------------------|\n| **P0** | `develop` blocked | Fix/disable in 2hrs | Can disable in 4hrs |\n| **P1** | >50% PRs failing | Fix in 8hrs | Implement retries in 4hrs |\n| **P2** | <50% intermittent | Fix in 2 days | Monitor & diagnose in 1 day |\n\n### **Responsibility Matrix**\n- **Flaky test logic** \u2192 Protocol owns, Platforms provides diagnostics\n- **Infrastructure issues** \u2192 Platforms owns and fixes\n- **External dependencies** \u2192 Joint responsibility, Platforms implements fallbacks\n\n## How They Work Together\n\n> **SLAs define the targets** \u2192 **Accountability ensures they're met**\n\n### **Key Metrics**\n- **MTTD**: Mean Time to Detection (<2 hours)\n- **MTTM**: Mean Time to Mitigation (<8 hours)  \n- **MTTR**: Mean Time to Resolution (<1 business day for P0, <14 business days otherwise)\n", "2025-06-16T07:49:08Z", "2025-06-24T01:21:06Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6yuNjm", "I_kwDODjvEJM67r6hh", "Seeking feedback on the above. Especially if there's any irrelevant metrics that we can remove.\nWe wish to close this out before the end of the week.", "2025-06-24T01:21:46Z", "2025-06-24T01:21:46Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6yvUK3", "I_kwDODjvEJM67r6hh", "As a developer I'm not sure these SLAs really matter to me. I've never had SLAs for CI before and never had reason to ask for or care about them. They don't hurt either so if they're useful for platforms by all means carry on. Just know that we developers are going to complain pretty much whenever there's an issue regardless of whether it's within the SLA or not. We're fun like that. :) \n\nIn terms of the actual numbers though:\n*  5% expected failure rate feels very high. I'd expect that to translate to multiple failures each day (spread across different developers so not as bad as it sounds).\n* An hour is an extraordinarily long time for CI to complete - especially for only 80% of PRs. Pre-merge checks should be under 10 minutes as a base level standard (which we've generally failed to meet). Post-merge can be longer and for some types of tests reducing runtime either isn't an objective or isn't feasible (e.g. sync tests or long running performance tests).\n* The idea of disabling checks that are breaking seems wrong. If they're new checks then yes we should roll them back, but otherwise it is generally a very bad idea to disable checks and can lead to multiple issues slipping through which make it very hard to get back to things passing.\n* PR merge time is mostly dependent on when people provide reviews and how much rework/discussion is required. I'm not sure it's a meaningful CI metric.", "2025-06-24T04:18:18Z", "2025-06-24T04:18:18Z", "ajsutton", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6y29KJ", "I_kwDODjvEJM67r6hh", "Seconding @ajsutton's comment. Right now the issue afflicting CI is flaky tests, and we can end the game of whack-a-mole if we either **(1)** develop a strong process for rapidly fixing flaky tests in the first place (represented by this proposal), or **(2)** get a little smarter about how we allow new tests into the repo.\n\nOption **(2)** seems smarter to me. What if we do a diff of which test cases have been introduced to the repo in a given PR, and for any new test cases we run many instances of those tests in parallel to proactively detect flaky tests and reject their admittance to the repo?", "2025-06-24T14:33:24Z", "2025-06-24T14:33:24Z", "teddyknox", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6y3t5f", "I_kwDODjvEJM67r6hh", "I love that you're looking at ways to keep the infrastructure great with metrics. I like the flaky test identification, and clear failure attribution. And I agree with @ajsutton's comment about PR merge time, it isn't really an infra SLA, it's dependent on the PR authors and reviewers. Also, flaky test remediation isn't necessarily a platforms issue.\n\nAnd I agree that it's dangerous to disable tests. Shouldn't it actually be _reverting the PR that broke the tests_?", "2025-06-24T15:23:35Z", "2025-06-24T15:23:35Z", "pauldowman", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6zBGPf", "I_kwDODjvEJM67r6hh", "Thank you very much for the feedback all!\n\nI like the idea of \"getting smarter about how we allow new tests into the repo\". We've previously discussed potentially introducing a staging gate, for example.\n\nI'll have a think about it, produce a lightweight proposal with some ideas and send it around for some feedback! (probably after the Pause, realistically)", "2025-06-25T05:42:41Z", "2025-06-25T05:42:41Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6zBNVe", "I_kwDODjvEJM67r6hh", "Will not implement CI SLAs for now; closing.", "2025-06-25T05:58:06Z", "2025-06-25T05:58:06Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6wrKlu", "I_kwDODjvEJM66_Tuz", "> All mentions of standard mode should be replaced with indexing mode\n\n> Rename managed mode to indexing mode #16385\n\nSpeaking of confusion; small mix up there\n", "2025-06-11T20:44:14Z", "2025-06-11T20:44:14Z", "protolambda", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6yjFE0", "I_kwDODjvEJM6602pY", "As part of this, I will do this in 2 sets of PRs:\n\n- Remove checks that already exist in ChainAssertions.sol from the other Deploy*.s.sol scripts and their invocations will simply just use chainAssertions.methodName(...)\n- Move the checks in ChainAssertions.sol to the StandardValidator. \n      - If the codesize limit gets exceeded as is very likely, move some checks to a child contract that gets deployed by the validator at construction time and gets invoked in the validate function.", "2025-06-23T08:50:18Z", "2025-06-23T08:50:18Z", "AmadiMichael", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6zKquV", "I_kwDODjvEJM66TE8n", "Can I close this or is there anything else needed beyond what #16532 does?", "2025-06-25T19:34:35Z", "2025-06-25T19:34:35Z", "pauldowman", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6zaHnm", "I_kwDODjvEJM66TE8n", "I've updated the runbook to also sanity check inputs to the script. With that we can close this.", "2025-06-26T20:48:35Z", "2025-06-26T20:48:35Z", "Inphi", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6znDOb", "I_kwDODjvEJM66RqbA", "Current status after https://github.com/ethereum-optimism/optimism/pull/16557:\n- we detect correct local unsafe head, checking the l1 origin at the supervisor side, before sending `interop_reset` request to the op-node.\n- we do not check l1 origin validity for other safety levels. Tracked at https://github.com/ethereum-optimism/optimism/issues/16315", "2025-06-27T14:55:47Z", "2025-06-27T14:55:47Z", "pcw109550", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6vy8PJ", "I_kwDODjvEJM66QYRB", "It's worth noting that pre-interop, part of this functionality was handled at `sync.FindL2Heads`, which does check that the L1 origin of the target is canonical.", "2025-06-06T14:02:21Z", "2025-06-06T14:02:29Z", "nonsense", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6vzkzS", "I_kwDODjvEJM66QYRB", "@axelKingsley made the valid comment that during the reset, we should be able to rely on the DB to only hold valid L1 blocks. So maybe a full traversal at the end shouldn't be necessary. But at least, at the end of a reset, we should check the target's L1 origin validity and if it's off, perform another reset with that target as the upper bound. This would catch cases where a reorg happens during the reset bisection and we can safe some back and forths between the node and the supervisor.", "2025-06-06T14:59:30Z", "2025-06-06T14:59:30Z", "sebastianst", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6wKEVl", "I_kwDODjvEJM66QYRB", "The more I think about it I agree, that we shouldn't be performing walkbacks, which just adds complexity.\n\n_All_ targets provided by the supervisor should be valid targets. And if any aren't valid, we shouldn't be duplicating validity checks, binary searches and walkbacks on the op-node, but just request another reset from the supervisor. Because by then, the supervisor should also have realized that the L1 chain changed. This would reduce a lot of complexity.", "2025-06-09T10:49:21Z", "2025-06-09T10:49:21Z", "sebastianst", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6znCAB", "I_kwDODjvEJM66QYRB", "By https://github.com/ethereum-optimism/optimism/pull/16557, we now have a L1 origin verifier method at supervisor side, at \nhttps://github.com/ethereum-optimism/optimism/blob/2c29c81c5917e6b7abf524c7ca23e82ada14b6ba/op-supervisor/supervisor/backend/syncnode/reset_tracker.go#L132-L133\n\nThis is currently used to detect the validity of L1 origin for local unsafe, but not other safety level heads.\n\nThis method and logic implementation for unsafe head should be extended to safe head to satisfy the first comment of this issue:\n> After bisecting to a common safe block, we may want to check the L1 origin of the found reset target and if it's not canonical, walk back the safe chain until we find a valid L1 origin\n", "2025-06-27T14:53:56Z", "2025-06-27T14:53:56Z", "pcw109550", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6wGlUs", "I_kwDODjvEJM65xFnA", "Most of the flakiness has nothing to do with acceptance testing - it's things like CircleCI machine issues, networking issues, infra issues.", "2025-06-09T03:09:36Z", "2025-06-09T03:09:36Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6wGptb", "I_kwDODjvEJM65xFnA", "TOP10 flakiest acceptance tests (by #flakes):\n\n1. TestSequencingWindowExpiry (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/seqwindow) [104 flakes]\n2. TestL2CLAheadOfSupervisor (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/sync/multisupervisor_interop) [97 flakes]\n3. TestUnsafeChainUnknownToL2CL (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/sync/multisupervisor_interop) [29 flakes]\n4. TestReorgInvalidExecMsgs/invalid_chain_id (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/reorgs) [21 flakes]\n5. TestReorgInvalidExecMsgs/invalid_block_number (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/reorgs) [19 flakes]\n6. TestUnsafeChainUnknownToL2CL (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/sync/redundant_interop) [19 flakes]\n7. TestL2CLSyncP2P (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/sync/multisupervisor_interop) [15 flakes]\n8. TestReorgUnsafeHead (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/reorgs) [14 flakes]\n9. TestReorgInvalidExecMsgs/invalid_log_index (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/reorgs) [10 flakes]\n10. TestLoad (github.com/ethereum-optimism/optimism/op-acceptance-tests/tests/interop/loadtest) [9 flakes]\n", "2025-06-09T03:26:51Z", "2025-06-09T03:26:51Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6w4aq9", "I_kwDODjvEJM65xFnA", "Related: https://github.com/ethereum-optimism/optimism/issues/16378", "2025-06-12T16:55:47Z", "2025-06-12T16:55:47Z", "nonsense", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6w-hvA", "I_kwDODjvEJM65xFnA", "Related:\n- Issue #16404 \n- PR: #16411 ", "2025-06-13T06:10:45Z", "2025-06-13T06:10:45Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6yvd3d", "I_kwDODjvEJM65xFnA", "We have:\n* Released a test flake report generator\n* Iterated on the flake report generator, and also incorporating Protocol feedback\n* Publicised the report\n* Improved test flakiness\n\nWe'll continue to monitor flakyness and I'll report on it periodically to ensure it has visibility.", "2025-06-24T04:44:09Z", "2025-06-24T04:44:09Z", "scharissis", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6tIQhW", "I_kwDODjvEJM637-bq", "I think GoReleaser is a great opportunity. But __*please*__ let's not add a 3rd (4th? 5th?) way of building services before we remove old ways.\n\n#### Current bloat\n\nCurrently we have:\n- docker bake file\n- makefiles, calling the justfiles, and not passing through all data\n- justfiles that then re-invent everything that docker-bake does (git version discovery and build recipes)\n- global make recipe to call the docker bake\n- other external bespoke ways of invoking the shared service dockerfile while bypassing all of the above\n\nI liked the docker-bake file because it's a native docker format, supports all the configuration and is clear with how variables are sourced. But in practice it's not used well.\n\nI think the jutfile situation is bad, because it duplicated configuration, and makes every service build slightly differently for no reason.\n\nIf we add a go-releaser config, let's remove all justfile/makefile/bake references. There should be 1 way to prepare docker images and release builds. Not so many as we have now.\n\n#### op-deployer config (as-is) is not a good example\n\nAlso, the op-deployer makefile/justfile setup still has inconsistencies with git versioning, and a configuration entry-point from the op-chain-ops dir where it used to be located, that is half-broken, like here: https://github.com/ethereum-optimism/optimism/blob/bfb9de2081a8fef8ae998025e1a8ca0510bd5c45/op-chain-ops/justfile#L8\n\nop-deployer also introduces a separate `Dockerfile`, which is nice in theory, but the Docker build caching and maintainability is only getting worse the more we split it. The `ops/docker/op-stack-go/Dockerfile` should unify it, and support the proper Go module download and build caching. (does GoReleaser handle docker-layer-caching and Go build caching between different sub-modules?)\n\nGoReleaser is good. Taking previous rushed build systems as example for GoReleaser is not. \n\nAnd duplicating go-releaser complexity for N services without removing debt here first is even worse. If we do what op-deployer does we end up with N different configs and inconsistent docker base images and build systems etc. to maintain. We need to approach GoReleaser from a monorepo-first perspective.\n", "2025-05-23T14:31:34Z", "2025-05-23T14:31:34Z", "protolambda", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6tI83L", "I_kwDODjvEJM637-bq", "I was primarily looking for a way to automate the generation of release notes, because currently we have to manually go over 100s of commits manually, of which usually 98% are irrelevant to the component that's being released. I saw that op-deployer uses GoReleaser to generate its release notes automatically. It also comes with a whole build+release pipeline, which wasn't clear to me at first. So I just recorded my experience with it here. I'm not pushing to use GoReleaser. I'm actually happy if we just use it for auto-generation of release notes as a first step. We can fix the issues you mentioned later when we find time to streamline the release process.", "2025-05-23T15:24:22Z", "2025-05-23T15:24:22Z", "sebastianst", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6xoW_7", "I_kwDODjvEJM637-bq", "Documenting a lighter-weight solution for changelog generation:\n\n```\ngit cliff --include-path \"op-node/**/*\" --include-path \"op-service/**/*\" --tag-pattern \"op-node/*\" -- op-node/v1.13.3..HEAD\n```\n\nhttps://git-cliff.org/\nWith this template https://github.com/orhun/git-cliff/blob/main/examples/github.toml (only `git.filter_unconventional = false`).", "2025-06-17T12:15:59Z", "2025-06-17T12:16:12Z", "geoknee", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6yU65Y", "I_kwDODjvEJM637-bq", "Whatever solution we use for generating release notes or changelogs, it would be awesome to commit it to the codebase -- either in the monorepo directly or into `op-workbench`. The dream would be a one-click solution that makes it easy to ship high quality / accurate changelogs in a consistent format. A lot of our release pipeline is fairly well defined already, so it shouldn't be hard to get the last piece set up in a similarly nice way. ", "2025-06-20T14:25:09Z", "2025-06-20T14:25:09Z", "geoknee", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6zRyHO", "I_kwDODjvEJM637-bq", "If we wired up e.g. `./op-workbench release-notes op-batcher` to automagically execute:\n```\ngit cliff --include-path \"op-batcher/**/*\" --include-path \"op-service/**/*\" --config-url=https://gist.githubusercontent.com/geoknee/48d0187c591b968fb21b419bc7efb80e/raw/cd402351fd859f9fcd9bdbe97c622f3f25ecb21f/git-cliff.toml --tag-pattern=\"op-batcher/v1.14.0-rc.2\" -- op-batcher/v1.13.2..op-batcher/v1.14.0-rc.2\n```\nand even push that up to github via the API. Would be pretty great. ", "2025-06-26T09:15:11Z", "2025-06-26T09:15:11Z", "geoknee", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6sFino", "I_kwDODjvEJM628eYO", "Would it be helpful to have an option where its possible to read an arbitrary `addresses.json` from disk? Would this be useful? Trying to understand if its an absolute must to update the registry dep in `superchain-ops` to be able to use an ops task", "2025-05-16T16:04:53Z", "2025-05-16T16:04:53Z", "tynes", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6sNpyR", "I_kwDODjvEJM628eYO", "Related: I raised a draft PR that allows task developers to define a local, custom `addresses.json` file: https://github.com/ethereum-optimism/superchain-ops/pull/1070\n\n", "2025-05-18T23:02:20Z", "2025-06-12T12:51:30Z", "blmalone", "2025-08-31 06:27:47"]
["IC_kwDODjvEJM6sHOto", "I_kwDODjvEJM62yb9q", "Kurtosis also can't time travel so it's not going to be useful here either.  We could add time travel to the sys go backend easily though.", "2025-05-16T20:02:05Z", "2025-05-16T20:02:05Z", "ajsutton", "2025-08-31 06:27:47"]
["IC_kwDOMMiGhs6zLRsL", "I_kwDOMMiGhs69HPiT", "great idea!", "2025-06-25T20:38:25Z", "2025-06-25T20:38:25Z", "fainashalts", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6y5ccn", "I_kwDOMMiGhs68r2rY", "@hexshire when this is set, is it expected that the `block.basefee` never changes for each block? I am testing this with vanilla anvil, and it seems like setting the `--base-fee` flag sets the initial block to the specified base fee, but as new blocks are mined the base fee fluctuates, will this work for what you are trying to accomplish?", "2025-06-24T17:57:58Z", "2025-06-24T17:58:52Z", "tremarkley", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6y5gHp", "I_kwDOMMiGhs68r2rY", "Ok, I think I misunderstood the `--base-fee` functionality. It will be better to adapt the tests to query the `bassFee` for the tests. Should I close the issue?", "2025-06-24T18:03:50Z", "2025-06-24T18:03:50Z", "hexshire", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6y5ne_", "I_kwDOMMiGhs68r2rY", "closing this issue for now, but feel free to reopen if you find that setting the flag would be useful", "2025-06-24T18:15:58Z", "2025-06-24T18:15:58Z", "tremarkley", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6xtePY", "I_kwDOMMiGhs67LtsY", "supersim is a wrapper around [anvil](https://getfoundry.sh/anvil/overview#anvil) and is much lighter to run for local development.", "2025-06-17T18:51:10Z", "2025-06-17T18:51:24Z", "tremarkley", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6yG-vf", "I_kwDOMMiGhs67LtsY", "> supersim is a wrapper around [anvil](https://getfoundry.sh/anvil/overview#anvil) and is much lighter to run for local development.\n\nWhere is supersim mainly used?", "2025-06-19T13:48:26Z", "2025-06-19T13:48:38Z", "zhiqiangxu", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6zLTU1", "I_kwDOMMiGhs67LtsY", "hi @zhiqiangxu supersim is a local tool for testing multichain/interop applications. its anvil, with the op-stack already deployed and the ability to test upcoming features such as interop. several apps that plan to launch when interop is live on mainnet started their development with supersim. happy to answer any other questions you might have!", "2025-06-25T20:40:49Z", "2025-06-25T20:40:49Z", "fainashalts", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6zNP6G", "I_kwDOMMiGhs67LtsY", "Thanks for the reply!\n\nSo supersim is mainly for testing the dapps instead of the chains(which op stack is using kurtosis), right?", "2025-06-26T00:18:54Z", "2025-06-26T00:18:54Z", "zhiqiangxu", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6vCs1_", "I_kwDOMMiGhs65qkcj", "The first thing I tried was to check if the proxyAdmin owner was one of the anvil's address to upgrade the predeploys. Maybe that could be a fix, being able to upgrade the contracts.", "2025-06-03T18:46:23Z", "2025-06-03T18:46:23Z", "hexshire", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6vEW4p", "I_kwDOMMiGhs65qkcj", "I will be starting work on integrating the relayer with the gas tank late this week. To start I am planning to add a forge script to etch the bytecode for the updated `L2toL2CDM` and to also deploy the `GasTank` contract. An example of a script where we do something similar can be found [here](https://github.com/ethereum-optimism/supersim/blob/220adad19da543d31b96a5542c91166d73a4d178/contracts/script/DeployL2PeripheryContracts.s.sol#L47)", "2025-06-03T20:51:11Z", "2025-06-03T20:51:11Z", "tremarkley", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6zLW2v", "I_kwDOMMiGhs65qkcj", "@tremarkley can this issue be closed? #396 seems to address it and is merged.", "2025-06-25T20:46:26Z", "2025-06-25T20:46:26Z", "fainashalts", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6zMXRB", "I_kwDOMMiGhs65qkcj", "closing this since supersim now supports modifying the L2toL2CDM predeploy. if more predeploys are needed in the future, we should open individual issues for them.", "2025-06-25T21:58:52Z", "2025-06-25T21:58:52Z", "tremarkley", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6zLxGF", "I_kwDOMMiGhs6gqK2a", "Closing this as its been resolved.", "2025-06-25T21:31:57Z", "2025-06-25T21:31:57Z", "fainashalts", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6WV2h2", "I_kwDOMMiGhs6RyGEV", "@fainashalts can we have the doc permission if we would like to contribute this task", "2024-12-06T07:24:57Z", "2024-12-06T07:24:57Z", "GrapeBaBa", "2025-08-31 06:27:56"]
["IC_kwDOMMiGhs6XZvIX", "I_kwDOMMiGhs6RyGEV", "Hi @GrapeBaBa this issue isn't the best for external contribution as its more of a research task, but if you are familiar with ERC-7683 you are welcome to write an example app that uses Supersim with it! We are always open to ecosystem contributions and examples. If the example is robust, we can add it to the /examples folder within Supersim for others to find (caveat: must be ok with open sourcing for us to include it there). ", "2024-12-12T22:06:11Z", "2024-12-12T22:06:11Z", "fainashalts", "2025-08-31 06:27:56"]
["IC_kwDOKIsnqM6zYwWF", "I_kwDOKIsnqM6ziV0O", "Closing, implemented here: https://github.com/ethereum-optimism/superchain-ops/pull/1070 ", "2025-06-26T18:59:51Z", "2025-06-26T18:59:51Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYilD", "I_kwDOKIsnqM6xGAtI", "We've been implementing these checks in the `_validate` function of relevant OPCM templates. ", "2025-06-26T18:52:25Z", "2025-06-26T18:52:25Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYttu", "I_kwDOKIsnqM6wQodD", "This has been fixed: https://github.com/ethereum-optimism/superchain-ops/pull/822/files", "2025-06-26T18:57:45Z", "2025-06-26T18:57:45Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6kChIL", "I_kwDOKIsnqM6vMNvV", "Hello @maurelian, I would love to work on this, can I be assigned this task?", "2025-03-25T18:06:48Z", "2025-03-25T18:06:48Z", "TheOnma", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6kNzPo", "I_kwDOKIsnqM6vMNvV", "Hello @maurelian,\nI've identified that the noisy terminal output is primarily due to the call to the `print` function within the `simulateRun` function. I propose removing (or commenting out) the `print` call in `simulateRun` to reduce unnecessary logs during simulation, which should allow users to focus on the critical signing information.\n\nDoes this change meet the expectations for resolving the issue, or would you prefer a more selective reduction of the output instead of removing it entirely? I\u2019m happy to adjust the implementation based on your feedback.\n\nBelow is the relevant code snippet on where I believe the issue stems from:\n\n```solidity\n        Action[] memory actions = build();\n        VmSafe.AccountAccess[] memory accountAccesses = simulate(signatures, actions);\n        validate(accountAccesses, actions);\n        print(actions, accountAccesses, optionalChildMultisig, true); // This is what might be unnecessarily log the output to the console\n```", "2025-03-26T16:49:00Z", "2025-03-26T16:49:00Z", "Cass402", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zTFyP", "I_kwDOKIsnqM6vMNvV", "may I work on this", "2025-06-26T11:17:35Z", "2025-06-26T11:17:35Z", "nsancheznez", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zU-ux", "I_kwDOKIsnqM6vMNvV", "Hi team,\n\nI'd like to apply to work on this improvement. I have experience with Solidity and improving the clarity of terminal output to help users focus on the most relevant information.\n\nI'm confident I can help make this flow cleaner and more user-friendly without affecting the contract\u2019s functionality. Happy to jump in if selected.\n\nThanks for the opportunity.\nBest regards,\nJose Valperga", "2025-06-26T14:04:47Z", "2025-06-26T14:04:47Z", "JoseValperga", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zxAe6", "I_kwDOKIsnqM6vMNvV", "Hi I'm an student from the ETH Kipu from Costa Rica and I would like to work on these issue to gain hands-on experience with solidity.", "2025-06-28T18:17:44Z", "2025-06-28T18:17:44Z", "Mau1401", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6jZfa8", "I_kwDOKIsnqM6u0feh", "We should also try to include the domain + message hashes in the markdown here that can be copied so we don't have to constantly make sure they're up to date.  ", "2025-03-20T18:44:17Z", "2025-03-20T18:44:17Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYQWa", "I_kwDOKIsnqM6u0feh", "Closing as this has been implemented. ", "2025-06-26T18:37:48Z", "2025-06-26T18:37:48Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYURf", "I_kwDOKIsnqM6unhpY", "Closing this issue request, `config.toml` files have proven to be clear and the UX is working. ", "2025-06-26T18:44:10Z", "2025-06-26T18:44:10Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6ifwSS", "I_kwDOKIsnqM6uGK2L", "If you want to be sure superchain-registry is up to date, why not just have the build process checkout the latest version instead of using a submodule? It does mean that builds aren't reproducible because superchain-registry updates are pulled in even without a change to this git repo but that may be the right trade off for superchain-ops given we want to compare them to latest data before they execute and drop them from CI once they are executed anyway.", "2025-03-15T05:22:04Z", "2025-03-15T05:22:04Z", "ajsutton", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYWcG", "I_kwDOKIsnqM6uGK2L", "Closing as implemented here: https://github.com/ethereum-optimism/superchain-ops/pull/1024 ", "2025-06-26T18:46:02Z", "2025-06-26T18:46:02Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6i-PUT", "I_kwDOKIsnqM6sAdgI", "Closed by: https://github.com/ethereum-optimism/superchain-ops/pull/734 ", "2025-03-18T17:54:20Z", "2025-03-18T17:54:20Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6ggOLp", "I_kwDOKIsnqM6rx-Xs", "Working on retrieving the domain and message hashes from a simulation in #704.", "2025-03-02T16:18:51Z", "2025-03-02T16:18:51Z", "alcueca", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6hkf8H", "I_kwDOKIsnqM6rx-Xs", "This code below (subject to change) goes into the circle ci config:\n```\n  # Simulate any non-terminal tasks that will eventually be executed.\n  simulate_non_teminal_tasks:\n    circleci_ip_ranges: true\n    docker:\n      - image: <<pipeline.parameters.default_docker_image>>\n    environment:\n      FOUNDRY_PROFILE: ci\n    steps:\n      - utils/checkout-with-mise\n      - run:\n          name: simulate non terminal tasks\n          command: |\n            (cd src/improvements && just simulate-non-terminal-tasks)\n      - notify-failures-on-develop:\n\t\t\tmentions: \"@security-team\u201d\n```\n\n\nThis code below (subject to improvements and change) would go into `src/improvements/justfile`. \n```\nsimulate-non-terminal-tasks:\n    #!/usr/bin/env bash\n    set -euo pipefail\n    root_dir=$(git rev-parse --show-toplevel)\n    forge build\n\n    # Later this networks list should be dynamically generated from the src/improvements/tasks directory.\n    networks=(\"eth\" \"sep\")\n    for network in ${networks[@]}; do\n        if [ \"$network\" != \"src/improvements/tasks/example\" ]; then # skip example tasks\n            for task in ${root_dir}/src/improvements/tasks/${network}/*; do\n                ${root_dir}/src/improvements/script/simulate-task.sh $task\n            done\n        fi\n        echo \"Done simulating non-terminal tasks for network: $network\"\n    done\n```\n", "2025-03-10T13:58:00Z", "2025-03-10T13:58:00Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6htMry", "I_kwDOKIsnqM6rx-Xs", "Did part of the work on #736, still pending:\n\n- Decide on a format for including hashes in VALIDATIONS.md (toml?)\n- Extend scripts to all safes, not just foundation\n- Exclude examples directory\n- Add just simulate-non-terminal-tasks to CI with tenderly context (for the Tenderly Access Token)", "2025-03-11T07:33:27Z", "2025-03-11T09:31:11Z", "alcueca", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6joTT6", "I_kwDOKIsnqM6rx-Xs", "Closed #736, but now #783 has all the scripts necessary. A different PR will get them on CI.", "2025-03-22T12:40:55Z", "2025-03-22T12:40:55Z", "alcueca", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYPf5", "I_kwDOKIsnqM6rx-Xs", "Closing this as it's implemented as part of stacked simulations now. ", "2025-06-26T18:36:20Z", "2025-06-26T18:36:20Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6f5bcs", "I_kwDOKIsnqM6rC8XD", "This might mean adding another step _after_ task setup so we have access to the task registry. ", "2025-02-25T16:47:35Z", "2025-02-25T16:47:35Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYvrF", "I_kwDOKIsnqM6rC8XD", "Closing this, we can perform necessary prevalidations in any tasks _templateSetup function.", "2025-06-26T18:59:02Z", "2025-06-26T18:59:02Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYbX7", "I_kwDOKIsnqM6rClmy", "Closing. We've got many examples for various OPCM versions in `test/tasks/example/eth`.", "2025-06-26T18:48:45Z", "2025-06-26T18:48:45Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYR54", "I_kwDOKIsnqM6rCQ9g", "Closing with this PR: https://github.com/ethereum-optimism/superchain-ops/pull/1067 ", "2025-06-26T18:40:15Z", "2025-06-26T18:40:15Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6f5doY", "I_kwDOKIsnqM6q1YN6", "Working on this in: https://github.com/ethereum-optimism/superchain-ops/pull/640 ", "2025-02-25T16:50:19Z", "2025-02-25T16:50:19Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6i-jNd", "I_kwDOKIsnqM6q1YN6", "Keeping this issue open. I think we can use this issue to more broadly check if the api, state variable etc in this contract can be simplified for readability and maintainability. ", "2025-03-18T18:17:04Z", "2025-03-18T18:17:04Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYSt9", "I_kwDOKIsnqM6q1YN6", "Closing this issue as we're undertaking a larger refactor project that tries to support an arbitrary amount of nesting. ", "2025-06-26T18:41:40Z", "2025-06-26T18:41:40Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6f8SEJ", "I_kwDOKIsnqM6qF_uW", "We should change `_buildStarted` back to a boolean as part of this: https://github.com/ethereum-optimism/superchain-ops/pull/640#discussion_r1965805909", "2025-02-25T21:53:25Z", "2025-02-25T21:53:25Z", "mds1", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYZ3I", "I_kwDOKIsnqM6qF_uW", "Closing as `getBuildStarted` is implemented and stdStorage is being used.", "2025-06-26T18:47:55Z", "2025-06-26T18:47:55Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6eaBDk", "I_kwDOKIsnqM6p_hM9", "For example, this file: https://github.com/ethereum-optimism/superchain-ops/blob/main/test/tasks/mock/example/fetch-tasks.sh#L27 ", "2025-02-13T20:15:17Z", "2025-02-13T20:15:17Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYe8B", "I_kwDOKIsnqM6p_hM9", "Completed.", "2025-06-26T18:50:37Z", "2025-06-26T18:50:37Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6dOlXM", "I_kwDOKIsnqM6o6Ct1", "I would probably create a general ownership transfer template where the inputs are a the chain, contract name, and the new owner address, where the new owner address is a named string like \"Foundation Upgrades Safe\" which the template maps to the correct address ", "2025-02-05T19:29:05Z", "2025-02-05T19:29:05Z", "mds1", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6ejGHN", "I_kwDOKIsnqM6o6Ct1", "Make sense, we've got this implemented here: https://github.com/ethereum-optimism/superchain-ops/pull/595 ", "2025-02-14T18:24:59Z", "2025-02-14T18:24:59Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6fTqQl", "I_kwDOKIsnqM6o6Ct1", "Doing deputy pause module first, this can come after ", "2025-02-20T21:31:54Z", "2025-02-20T21:31:54Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6fuTym", "I_kwDOKIsnqM6o6Ct1", "Where can we find the `ProtocolVersions` contract in the superchain ops or superchain registry repo?\n\nSearching this address [0x8062AbC286f5e7D9428a0Ccb9AbD71e50d93b935](https://etherscan.io/address/0x8062AbC286f5e7D9428a0Ccb9AbD71e50d93b935#readProxyContract) in the addresses.json file in superchain-registry shows no results.\n\nLooking in the AddressRegistry contract in superchain ops doesn't show anywhere an address named or close to `ProtocolVersions` is stored.\n\nShould this be another address that is added to the hardcoded `addresses.toml` file?\n\nOr, should we modify AddressRegistry contract to fetch this onchain from the OPCM?", "2025-02-24T21:42:20Z", "2025-02-24T21:42:20Z", "ElliotFriedman", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6fuVRZ", "I_kwDOKIsnqM6o6Ct1", "The most generic version of the `TransferOwnerTemplate` would accept a toml configuration file with values for both the new owner and contract whose ownership is being transferred.", "2025-02-24T21:45:31Z", "2025-02-24T21:45:31Z", "ElliotFriedman", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYciz", "I_kwDOKIsnqM6o6Ct1", "Closing this as we have templates for transferring L1 and L2 owners implemented.", "2025-06-26T18:49:25Z", "2025-06-26T18:49:25Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6dNIYr", "I_kwDOKIsnqM6o4sVi", "We should come back to the docs and update all the path references to ensure they're right at some point when paths change.", "2025-02-05T16:45:33Z", "2025-02-05T16:45:33Z", "ElliotFriedman", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6jMbly", "I_kwDOKIsnqM6o4sVi", "We should also add automated checking of links in markdown docs. I think there is something similar in the specs repo already.", "2025-03-19T19:55:38Z", "2025-03-19T19:55:38Z", "maurelian", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6kLnZx", "I_kwDOKIsnqM6o4sVi", "Logic like this: https://github.com/ethereum-optimism/superchain-ops/blob/09441b7eeebfad53697d9d2a839a47b2f91a7d13/src/improvements/script/fetch-tasks.sh#L34 should be updated to us the mainnet directory and not the example directory. ", "2025-03-26T14:00:22Z", "2025-03-26T14:00:22Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYfQu", "I_kwDOKIsnqM6o4sVi", "Completed.", "2025-06-26T18:50:49Z", "2025-06-26T18:50:49Z", "blmalone", "2025-08-31 06:28:01"]
["IC_kwDOKIsnqM6zYf2Z", "I_kwDOKIsnqM6o0BQc", "Comepleted.", "2025-06-26T18:51:13Z", "2025-06-26T18:51:13Z", "blmalone", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6zYRSi", "I_kwDOKIsnqM6oNNBC", "Loom videos and been created and various training calls have taken place. ", "2025-06-26T18:39:28Z", "2025-06-26T18:39:28Z", "blmalone", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6Z2MbN", "I_kwDOKIsnqM6lUHjU", "I suggest we change the semantics to `true` and `false` so that it's the same as foundry forge script parsing of bool env vars.", "2025-01-09T19:45:35Z", "2025-01-09T19:45:35Z", "sebastianst", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6zYSPS", "I_kwDOKIsnqM6lUHjU", "Addressed with this PR: https://github.com/ethereum-optimism/superchain-ops/pull/803 ", "2025-06-26T18:40:50Z", "2025-06-26T18:40:50Z", "blmalone", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6YZ3Lf", "I_kwDOKIsnqM6ZdYW8", "## Stacked Tasks\n\nI have some ideas how to improve this.\n\n* We remove all prefixes like `base-`, `ink-` etc from tasks.\n* We use the _lexicographic_ ordering of task folder names as the canonical order in which they are executed\n  * This also allows us to slide in a task in between existing once, without having to renumber all following.\n* Instead of manually adding individual tasks to `CI`, we have one smart CI task that\n  * for each tasks folder(`eth`, `sep`), it creates an lexicographically ordered list of all tasks\n  * it iterates from the start over all tasks and finds the first non-`EXECUTED` (could just parse from the README as a start)\n  * all future tasks are executed one after another as a stack\n  * this works by requiring tasks to adhere to the convention that they either have a `SignFromJson.s.sol` or `NestedSignFromJson.s.sol` file, and the affected Safes could be parsed from the `.env` file\n* the `just sign` and `just simulate` steps would reuse the same logic to determine Safe accounts and nonces\n\n## Safe Config\n\nWe could also improve how an individual task specifies some inputs like the Safes it uses. This could be more robust than how we currently define them in `.env` files. E.g. in each tasks main dir (`eth` etc), we could have a config like `safes.toml` that defines a list of safes with aliases, e.g.\n```toml\n[safes.l1poa]\ndesc = \"Superchain L1 ProxyAdminOwner\"\naddress = \"0x5a0Aae59D09fccBdDb6C6CcEB07B7279367C3d2A\"\nowners = [\"fus\", \"scs\"]\n\n[safes.fus]\ndesc = \"Foundation Upgrade Safe\"\naddress=\"0x847B5c174615B1B7fDF770882256e2D3E95b9D92\"\n\n[safes.scs]\ndesc = \"Security Council Safe\"\naddress=\"0xc2819DC788505Aac350142A7A707BF9D03E3Bd03\"\n```\n\nThen in each individual task folder, we could have a `task.toml` file that defines some metadata about the task, e.g.\n```toml\ntype = \"nested\"\nexecuter = \"l1poa\"\n```\nand then the smart per-L1 stacked task executor can use this metadata to calculate determine nonces etc.", "2024-12-20T12:30:52Z", "2024-12-20T12:34:01Z", "sebastianst", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6aYQ5Y", "I_kwDOKIsnqM6ZdYW8", "Closed: https://github.com/ethereum-optimism/superchain-ops/issues/447 as it looks like a duplicate of this task.\n", "2025-01-14T14:22:06Z", "2025-01-14T14:22:06Z", "blmalone", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6eoRFG", "I_kwDOKIsnqM6ZdYW8", "I have finished a preversion that would need to be slighty adapted for the new templating system -> https://github.com/ethereum-optimism/superchain-ops/pull/611", "2025-02-16T10:07:37Z", "2025-02-16T10:07:37Z", "Ethnical", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6imC1w", "I_kwDOKIsnqM6ZdYW8", "Great suggestions in this issue.\n@Ethnical, I haven\u2019t had a chance to fully review your PR yet.\n\nI wanted to put some thoughts down on how I might see this working with the new system (I like Seb\u2019s suggestion about lexicographical ordering of task names).\n\nI was thinking the config.toml file of each new task could have a new TOML table [dependsOn]. Under this, we would include a key called task with the name of the task that must be simulated first (if it hasn\u2019t been executed yet). This would be recursive if that task also depends on another, then that will be simulated before all of them.\n\nThis approach allows for more granularity and clearer descriptions of our \u201cstacked\u201d tasks. It\u2019s also more robust against task developers incorrectly naming a task.\n\ne.g.\n\n![Image](https://github.com/user-attachments/assets/e5eca379-528c-44a5-a207-e70104161f32)", "2025-03-17T02:34:26Z", "2025-03-24T16:04:58Z", "blmalone", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6iqSA_", "I_kwDOKIsnqM6ZdYW8", "Oh interesting. \nJust we need to take in consideration simple usage also for tasks creator to make sure they can still simulated the stacked sequence in the CI easy and add the task easy. ", "2025-03-17T10:31:14Z", "2025-03-17T10:31:14Z", "Ethnical", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6zYTWy", "I_kwDOKIsnqM6ZdYW8", "Closing, implemented in this PR: https://github.com/ethereum-optimism/superchain-ops/pull/819 ", "2025-06-26T18:42:50Z", "2025-06-26T18:42:50Z", "blmalone", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6VtWVR", "I_kwDOKIsnqM6Yuc9k", "Hey, I'm wondering if I could do that, but I'm not sure, if I'm understanding this task correclty.\n\nAm I right in thinking that the job is to modify ./circleci/config.yml file by adding a new job to the workflow?\n\nThe job involves listing all non-terminal tasks, and running a simulation for each, as defined in the README.md file of the task? Is that correct?", "2024-12-02T14:28:35Z", "2024-12-02T14:28:35Z", "bordolot", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6V9eZh", "I_kwDOKIsnqM6Yuc9k", "Hey! Yes that's close. So you'll see the `script/utils/check-task-statuses.sh` already loops through all tasks to check their status. We can modify that file (and rename it accordingly, to also simulate). That file is already executed in CI in the `check_task_statuses`, so we'd modify that existing workflow based on the file rename.\n\nSomething like this diff is how I'd approach it. Let me know if you have any more questions, and if you want to go ahead implementing this! Would love to assign the issue to you if you're interested :)\n\n```diff\ndiff --git a/script/utils/check-task-statuses.sh b/script/utils/check-task-statuses.sh\nindex 89c77c5..4494383 100644\n--- a/script/utils/check-task-statuses.sh\n+++ b/script/utils/check-task-statuses.sh\n@@ -4,6 +4,9 @@ set -euo pipefail\n VALID_STATUSES=(\"DRAFT, NOT READY TO SIGN\" \"CONTINGENCY TASK, SIGN AS NEEDED\" \"READY TO SIGN\" \"SIGNED\" \"EXECUTED\" \"CANCELLED\")\n errors=() # We collect all errors then print them at the end.\n \n+# Array holding paths of tasks ready to simulate.\n+ready_to_simulate=()\n+\n # Function to check status and hyperlinks for a single file.\n check_status_and_hyperlinks() {\n   local file_path=$1\n@@ -40,6 +43,11 @@ check_status_and_hyperlinks() {\n       errors+=(\"Error: Status is EXECUTED but no link to transaction found in $file_path\")\n     fi\n   fi\n+\n+  # If a file is not in a terminal state, add it to the ready_to_simulate array.\n+  if [[ \"$status_line\" != *\"SIGNED\"* && \"$status_line\" != *\"EXECUTED\"* && \"$status_line\" != *\"CANCELLED\"* ]]; then\n+    ready_to_simulate+=(\"$file_path\")\n+  fi\n }\n \n # Find README.md files for all tasks and process them.\n@@ -57,3 +65,16 @@ if [[ ${#errors[@]} -gt 0 ]]; then\n else\n   echo \"\u2705 All task statuses are valid\"\n fi\n+\n+# Execute all tasks that are ready to simulate.\n+if [[ ${#ready_to_simulate[@]} -gt 0 ]]; then\n+  echo \"Simulating tasks:\"\n+  for path in \"${ready_to_simulate[@]}\"; do\n+    echo \"  Simulating $path\"\n+    # Check if the task is nested or single. We can usually do this by looking\n+    # at the name of the Solidity file in this directory. If it's SignFromJson.s.sol\n+    # it's single, and if it's NestedSignFromJson.s.sol it's nested.\n+    # For nested tasks we should sign as both the foundation and the council.\n+    # TODO implement the above.\n+  done\n+fi\n```", "2024-12-04T01:11:22Z", "2024-12-04T01:12:08Z", "mds1", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6WFx0G", "I_kwDOKIsnqM6Yuc9k", "Thanks for the answer! ;)\n\"assign the issue to you if you're interested\"? Yeah, I\u2019d like to take it on. Whether I\u2019ll be able to finish it, well, that\u2019s another question XD.\n\nI've already listed all non-terminal tasks. I've made some changes, such as:\n1. Dividing the VALID_STATUSES variable into NON_TERMINAL_STATUSES and TERMINAL_STATUSES\n2. Defining FOLDER_WITH_NO_TASKS - I suppose this is necessary because, without this filter, the list would include folders like:\n./tasks/sep/fp-recovery/005-set-game-implementation/templates/README.md\n./tasks/templates/00-default/README.md\n./tasks/templates/00-protocol-versions/README.md\nAm I correct?There is no need to do simulations for these?\n\nLet\u2019s say I have all the non-terminal tasks, and I want to simulate them. Should there be a universal simulation pattern: one for nested tasks and another for single tasks?\n\nCurrently, we have the following (these folders were provided by my listing function):\nIn the folders:\n./tasks/eth/fp-recovery/001-blacklist-game/README.md\n./tasks/eth/fp-recovery/002-fallback-permissioned-game/README.md\n./tasks/eth/fp-recovery/003-enable-permissionless-game/README.md\n./tasks/eth/fp-recovery/004-recover-bonds/README.md\n./tasks/sep/fp-recovery/001-blacklist-game/README.md\n./tasks/sep/fp-recovery/002-fallback-permissioned-game/README.md\n./tasks/sep/fp-recovery/003-enable-permissionless-game/README.md\n./tasks/sep/fp-recovery/004-recover-bonds/README.md\n./tasks/sep/fp-recovery/005-set-game-implementation/README.md\nShould the simulation for these be prepared based on the \"Preparing the Operation\" section in each respective README.md file? There are no clear guidelines in these files.\n\nIn the folder:\n./tasks/sep/001-op-extended-pause/README.md\nThe README.md provides a guideline to use the local justfile and call just simulate.\n\nIn the folder:\n./tasks/sep/base-003-fp-granite-prestate/README.md\nThe task simulation is described in ./SINGLE.md.\n\nIn the folder:\n./tasks/sep/013-fp-granite-prestate/README.md\nThe task simulation is described in ./NESTED.md.\n\nAm I correct in thinking that ./SINGLE.md and ./NESTED.md contain direct, universal guidelines for single and nested tasks, respectively? Should all subsequent tasks be simulated based on one of these two files?\n\nIf so, what should we do with tasks in the following folders: './tasks/eth/fp-recovery/...' ,'./tasks/sep/fp-recovery/...' and './tasks/sep/001-op-extended-pause'?\n\n\nCurrent changes I've made:\n```diff\ndiff --git a/script/utils/check-task-statuses.sh b/script/utils/check-task-statuses.sh\nindex 89c77c5..8c09e67 100644\n--- a/script/utils/check-task-statuses.sh\n+++ b/script/utils/check-task-statuses.sh\n@@ -1,9 +1,19 @@\n #!/bin/bash\n set -euo pipefail\n \n-VALID_STATUSES=(\"DRAFT, NOT READY TO SIGN\" \"CONTINGENCY TASK, SIGN AS NEEDED\" \"READY TO SIGN\" \"SIGNED\" \"EXECUTED\" \"CANCELLED\")\n+NON_TERMINAL_STATUSES=(\"DRAFT, NOT READY TO SIGN\" \"CONTINGENCY TASK, SIGN AS NEEDED\" \"READY TO SIGN\")\n+TERMINAL_STATUSES=(\"SIGNED\" \"EXECUTED\" \"CANCELLED\")\n+VALID_STATUSES=( \"${NON_TERMINAL_STATUSES[@]}\" \"${TERMINAL_STATUSES[@]}\" )\n errors=() # We collect all errors then print them at the end.\n \n+# Name of a file to exclude from searching for non-terminal tasks.\n+FOLDER_WITH_NO_TASKS=\"templates\"\n+# Name of a file in a task directiory that specifies that the task is a nested safe task.\n+IF_THIS_ITS_NESTED=\"NestedSignFromJson.s.sol\"\n+\n+single_tasks_to_simulate=()\n+nested_tasks_to_simulate=()\n+\n # Function to check status and hyperlinks for a single file.\n check_status_and_hyperlinks() {\n   local file_path=$1\n@@ -42,6 +52,28 @@ check_status_and_hyperlinks() {\n   fi\n }\n \n+search_non_terminal_tasks(){\n+  local directory\n+  for file in $filtered_files; do\n+    # Ensure it's a regular file.\n+    if [[ -f \"$file\" ]]; then\n+      # Read file content and search for any status in the NON_TERMINAL_STATUSES array.\n+      for status in \"${NON_TERMINAL_STATUSES[@]}\"; do\n+        if grep -q \"$status\" \"$file\"; then\n+          directory=$(dirname \"$file\")\n+          # Specify if a task is safe or nested.\n+          if [[ -f \"$directory/$IF_THIS_ITS_NESTED\" ]]; then\n+            nested_tasks_to_simulate+=(\"$file\")\n+          else\n+            single_tasks_to_simulate+=(\"$file\")\n+          fi\n+          break\n+        fi\n+      done\n+    fi\n+  done\n+}\n+\n # Find README.md files for all tasks and process them.\n files=$(find ./tasks -type f -path './tasks/*/*/README.md')\n for file in $files; do\n@@ -57,3 +89,15 @@ if [[ ${#errors[@]} -gt 0 ]]; then\n else\n   echo \"\u2705 All task statuses are valid\"\n fi\n+\n+# Excludes tasks defined in folder FOLDER_WITH_NO_TASKS\n+filtered_files=$(echo \"$files\" | grep -v \"/${FOLDER_WITH_NO_TASKS}/\")\n+search_non_terminal_tasks\n+echo \"Simulating single tasks...\"\n+for value in \"${single_tasks_to_simulate[@]}\"; do\n+  echo \"$value\"\n+done\n+echo \"Simulating nested tasks...\"\n+for value in \"${nested_tasks_to_simulate[@]}\"; do\n+  echo \"$value\"\n+done\n```", "2024-12-04T17:29:35Z", "2024-12-04T17:29:35Z", "bordolot", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6WIcrF", "I_kwDOKIsnqM6Yuc9k", "Thank you!\n\n> Am I correct in thinking that ./SINGLE.md and ./NESTED.md contain direct, universal guidelines for single and nested tasks, respectively? Should all subsequent tasks be simulated based on one of these two files?\n\nThis is correct, for those it should always be something like\n```sh\n# single.md\nSIMULATE_WITHOUT_LEDGER=1 just \\\n   --dotenv-path $(pwd)/.env \\\n   --justfile ../../../single.just \\\n   simulate\n\n# nested.md\n# for nested, first simulate as council\nSIMULATE_WITHOUT_LEDGER=1 just \\\n   --dotenv-path $(pwd)/.env \\\n   --justfile ../../../nested.just \\\n   simulate \\    \n   council 0\n\n# then simulate as foundation\nSIMULATE_WITHOUT_LEDGER=1 just \\\n   --dotenv-path $(pwd)/.env \\\n   --justfile ../../../nested.just \\\n   simulate \\    \n   foundation 0\n```\n\nDo you mind opening a draft PR and we can continue discussions there? That would make it easier to stay organized :) \n\nFor `./tasks/sep/001-op-extended-pause/README.md`, `./tasks/sep/base-003-fp-granite-prestate/README.md`, and `./tasks/sep/013-fp-granite-prestate/README.md`\u2014these actually were already executed. I will go back and look for the transaction hashes to correct these in a separate PR", "2024-12-04T23:56:40Z", "2024-12-04T23:57:18Z", "mds1", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6WZ76e", "I_kwDOKIsnqM6Yuc9k", "A draft PR is ready.", "2024-12-06T14:33:42Z", "2024-12-06T14:33:42Z", "bordolot", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6Xwq4a", "I_kwDOKIsnqM6Yuc9k", "Comment from @maurelian:\n\n> Do we have plans to run them sequentially in the same forked EVM state to avoid nonce issues?\n\nThis is a good idea, since some tasks are dependent on previous tasks, and something we should think about how to best support in a follow up PR", "2024-12-16T16:41:13Z", "2024-12-16T16:41:13Z", "mds1", "2025-08-31 06:28:02"]
["IC_kwDOKIsnqM6zYQI_", "I_kwDOKIsnqM6Yuc9k", "Closing as this is implemented with stacked simulations.", "2025-06-26T18:37:24Z", "2025-06-26T18:37:24Z", "blmalone", "2025-08-31 06:28:02"]
["IC_kwDOH2Qg5s6kEr6v", "I_kwDOH2Qg5s6pDCyH", "I think that we want to have a process that checks on an interval all of the transactions in the mempool based on their access list", "2025-03-25T22:30:36Z", "2025-06-03T17:19:56Z", "tynes", "2025-08-31 06:28:02"]
["IC_kwDOH2Qg5s6mm_QO", "I_kwDOH2Qg5s6pDCyH", "Moving this to the backlog because we will want to measure on a live network to know what we should do", "2025-04-10T21:47:50Z", "2025-06-03T17:19:51Z", "tynes", "2025-08-31 06:28:02"]
["IC_kwDOH2Qg5s6tWIUH", "I_kwDOH2Qg5s64KcB3", "Also, for every major backports op-geth would be broken or become useless with bugs not checked thoroughly. Instead of relying on people's complains with issues being appeared, I would like to suggest having some good QA by spawning different nodes to sync from scratch before suggesting people to upgrade to buggy ones. Don't think it is a good idea to release mission-critical softwares with such optimistic minds.", "2025-05-26T01:25:29Z", "2025-05-26T01:26:29Z", "cpuchainorg", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s6uFUbH", "I_kwDOH2Qg5s64KcB3", "[v1.101511.0](https://github.com/ethereum-optimism/op-geth/releases/tag/v1.101511.0) constantly eating 50% of AMD 5950X, that's 16 cpus\n\nUpdate: after switching back to v1.101503.4 there's new line in the logs:\n```INFO [05-29|21:37:49.196] Upgrading chain index                    type=bloombits percentage=23```\nCould be those CPU spike a result of some kind db upgrade in new version, but it wasn't stated anywhere and how long it will take", "2025-05-29T21:28:25Z", "2025-05-29T21:44:49Z", "hekich", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s6uT21J", "I_kwDOH2Qg5s64KcB3", "@hekich Yes this is an obvious bug I've also downgraded the version but regardless of the upgrading index log I think the old node runs fine now", "2025-05-31T06:09:01Z", "2025-05-31T06:09:01Z", "cpuchainorg", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s6zWsmD", "I_kwDOH2Qg5s64KcB3", "AFAIK, as [v1.101511.0](https://github.com/ethereum-optimism/op-geth/releases/tag/v1.101511.0) backported go-ethereum v1.15.11, in go-ethereum it rebuilds the log indexing with a new struct. Therefore, I believe the high CPU usage is only temporary. Once the process of rebuilding all the logs is completed, the CPU usage should return to normal levels.", "2025-06-26T16:30:22Z", "2025-06-26T16:30:22Z", "jsvisa", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s6zgr03", "I_kwDOH2Qg5s64KcB3", "@jsvisa I am also running ETH mainnet node for go-ethereum with latest version and unlike op-geth it doesn't use 50% of the CPU usage, which means something is broken while backporting", "2025-06-27T05:09:00Z", "2025-06-27T05:09:00Z", "cpuchainorg", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s6zngq1", "I_kwDOH2Qg5s64KcB3", "> [@jsvisa](https://github.com/jsvisa) I am also running ETH mainnet node for go-ethereum with latest version and unlike op-geth it doesn't use 50% of the CPU usage, which means something is broken while backporting\n\nThanks for the reply, could you please make a pprof of the op-geth to see which parts consumes the CPU, or could you please share the start command line args to run op-geth, so I can try to run op-geth to reproduce it ", "2025-06-27T15:34:57Z", "2025-06-27T15:34:57Z", "jsvisa", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s6zuH6d", "I_kwDOH2Qg5s64KcB3", "I gave it another try, upgraded to [v1.101511.0](https://github.com/ethereum-optimism/op-geth/releases/tag/v1.101511.0), and left it working for some time. It looks like after the index upgrade, it stopped eating CPU and is back to normal behavior. I can't say how much time it took, but it was less than 24h for sure", "2025-06-28T12:08:33Z", "2025-06-28T12:08:53Z", "hekich", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s6zuJ1O", "I_kwDOH2Qg5s64KcB3", "@jsvisa Could you share me command line arguments to see pprof?\n\n@hekich The problem is this persists even after index upgrade is finished and thus have nothing to do with it, I speculate that it is just some goroutine broken and doing infinite loops.", "2025-06-28T12:25:45Z", "2025-06-28T12:25:45Z", "cpuchainorg", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s6zuMEb", "I_kwDOH2Qg5s64KcB3", "> [@jsvisa](https://github.com/jsvisa) Could you share me command line arguments to see pprof?\n\n\nfirst you need to start op-geth with metrics enabled, eg:\n\n```bash\nop-geth --metrics --metrics.addr=0.0.0.0 --metrics.port=6060\n```\n\nAnd then you can use the below command to capture 60s op-geth cpu performance:\n\n```bash\ncurl \"http://127.0.0.1:6060/debug/pprof/profile?seconds=60\" > cpu.pprof\n```\n\nAnd open it with `go tool` as below:\n\n```bash\n go tool pprof -http 0.0.0.0:8818  ./cpu.pprof\n```\n\nOr you can directly paste it as file here, then I'll take a look at it\n", "2025-06-28T12:47:21Z", "2025-06-28T12:47:21Z", "jsvisa", "2025-08-31 06:29:20"]
["IC_kwDOH2Qg5s615dr5", "I_kwDOH2Qg5s64KcB3", "> > [@jsvisa](https://github.com/jsvisa) Could you share me command line arguments to see pprof?\n> \n> first you need to start op-geth with metrics enabled, eg:\n> \n> op-geth --metrics --metrics.addr=0.0.0.0 --metrics.port=6060\n> And then you can use the below command to capture 60s op-geth cpu performance:\n> \n> curl \"http://127.0.0.1:6060/debug/pprof/profile?seconds=60\" > cpu.pprof\n> And open it with `go tool` as below:\n> \n>  go tool pprof -http 0.0.0.0:8818  ./cpu.pprof\n> Or you can directly paste it as file here, then I'll take a look at it\n\nI have the same performance issue.\n\nThe pprof metrics need to be enabled with the following params:\n`--pprof --pprof.addr=0.0.0.0 --pprof.port=6060`\n\nI have attached the profile.pb file:\n[profile.pb.gz](https://github.com/user-attachments/files/21138568/profile.pb.gz)\n\nEDIT: looks like the performance issue was due to the index update.\nIt took about 24 hours on an Orange Pi 5 Plus 32Gb.", "2025-07-09T08:47:54Z", "2025-07-10T08:11:09Z", "PierrickGT", "2025-08-31 06:29:20"]
["IC_kwDOMMiGhs6jkajb", "I_kwDOMMiGhs6vMPV-", "Great idea! \ud83d\udc4d Especially useful if someone wants to test out an app that works in and outside of the dependency set.", "2025-03-21T19:07:13Z", "2025-03-21T19:07:13Z", "fainashalts", "2025-08-31 06:29:35"]
["IC_kwDOMMiGhs6xh4uO", "I_kwDOMMiGhs6vMPV-", "@tremarkley do you mind providing an example flag and the effect you'd expect from it? Thanks!", "2025-06-16T23:17:05Z", "2025-06-16T23:17:05Z", "its-everdred", "2025-08-31 06:29:35"]
["IC_kwDOMMiGhs6xh9gV", "I_kwDOMMiGhs6vMPV-", "I think you could potentially just have a flag that is `--dependency-set <chains>` and any chains set in the flag would be in a dependency set together. This is just me thinking off the top of my head, so there may be a more ideal format. You want to have a way to specify what the dependency set is for a set of supersim chains. Here is the spec for dependency set: https://specs.optimism.io/interop/dependency-set.html and you can grep the monorepo to see how the dependency set is managed, this might be a good place to start: https://github.com/ethereum-optimism/optimism/blob/develop/op-program/client/tasks/derive.go#L40", "2025-06-16T23:31:06Z", "2025-06-16T23:31:45Z", "tremarkley", "2025-08-31 06:29:35"]
["IC_kwDODjvEJM61jXbz", "I_kwDODjvEJM6_UGiw", "Associated PR - https://github.com/ethereum-optimism/optimism/pull/16584", "2025-07-07T17:01:22Z", "2025-07-07T17:01:22Z", "yashvardhan-kukreja", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM61o2A1", "I_kwDODjvEJM6-Ss4K", "These are precompile addresses, it is normal to put 1 wei in them due to obscure rules in how empty accounts work. Having 1 wei means its not an empty account", "2025-07-08T05:12:16Z", "2025-07-08T05:12:16Z", "tynes", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6zmnTS", "I_kwDODjvEJM69siDx", "Speculation: so far this looks like `ChainsDB` or `GlobalSyncExec` deadlock, but can't say for sure, before seeing a full panic stacktrace, similar to: https://optimistic.grafana.net/a/grafana-lokiexplore-app/explore/cluster/oplabs-dev-infra-primary/logs?patterns=%5B%5D&from=2025-06-27T06:22:35.000Z&to=2025-06-27T06:22:38.000Z&var-lineFormat=&var-ds=grafanacloud-logs&var-filters=cluster%7C%3D%7Coplabs-dev-infra-primary&var-filters=namespace%7C%3D%7Crehearsal-1-bn-op-supervisor-sequencer-2&var-fields=&var-levels=&var-metadata=&var-jsonFields=&var-patterns=&var-lineFilterV2=&var-lineFilters=&displayedFields=%5B%5D&urlColumns=%5B%22Time%22,%22Line%22%5D&visualizationType=%22logs%22&var-fieldBy=$__all&var-labelBy=&timezone=browser&var-all-fields=&prettifyLogMessage=false&sortOrder=%22Descending%22&wrapLogMessage=false", "2025-06-27T14:19:21Z", "2025-06-27T14:20:00Z", "nonsense", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM62Dvb5", "I_kwDODjvEJM686g_x", "@axelKingsley salute! made a pr, pls check it out when you'll have a free time", "2025-07-09T23:31:32Z", "2025-07-09T23:31:32Z", "DeVikingMark", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM61jByS", "I_kwDODjvEJM68gn1T", "Have we considered doing something similar to how we do `op-e2e` or `sysgo` where we run each of the services in the same process? Would this work?", "2025-07-07T16:34:20Z", "2025-07-07T16:34:20Z", "tynes", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM61_rXT", "I_kwDODjvEJM68gn1T", "Closing this as we will no longer be prioritizing this", "2025-07-09T17:00:54Z", "2025-07-09T17:00:54Z", "tynes", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6yWWG0", "I_kwDODjvEJM68gjE0", "Is there any reason this is being considered other to avoid not running a supervisor? I have a vague sense that the upgrade path to merging these 1-depset chains with an existing interop cluster would be more complicated if they're not already being driven by a supervisor.", "2025-06-20T16:29:30Z", "2025-06-20T16:29:30Z", "Inphi", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6ym9rO", "I_kwDODjvEJM68gjE0", "@Inphi they can start running a supervisor already weeks before the actual merge into a depset would occur. This is just to prevent churn for chain ops to require running a supervisor during the next few months if they only have a trivial depset. By the time we have implemented interop merging, we should have come up with a better solution, probably a better overall op-supervisor+op-node implementation.", "2025-06-23T13:43:33Z", "2025-06-23T13:43:33Z", "sebastianst", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6wm4eb", "I_kwDODjvEJM66-Z_f", "Note that even if we measure the flakiness of single tests using \n```\ngo test -v -count=1337 -run ^TestName$\n```\nThis may not match the CI because some tests share the same environment, initialized by\n```go\nfunc TestMain(m *testing.M) {\n...\n```\nSo for example sync tests located at multisupervisor_interop may interfere each other, boosting flakiness.", "2025-06-11T14:07:10Z", "2025-06-11T14:07:10Z", "pcw109550", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6wm5-o", "I_kwDODjvEJM66-Z_f", "Good point @pcw109550, we should measure flakiness by package rather than by test.", "2025-06-11T14:08:25Z", "2025-06-11T14:08:25Z", "teddyknox", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6wn6bJ", "I_kwDODjvEJM66-Z_f", "`TestL2CLAheadOfSupervisor` passes with `-count=5`\n\nThe `reorg` package passes with `-count=5`.\n\nWill continue to review the tests, but hopefully we also catch some useful logs from CircleCI.\n\nIn any case I think the same test with `-count=5` is also useful indicator, as it will reuse the environment across runs, although I agree per-package runs increase interference.\n\n---\n\nPackage `op-acceptance-tests/tests/interop/sync/multisupervisor_interop` seems to be flaky when all tests are run within the package, so I will be looking into it (`TestUnsafeChainUnknownToL2CL` and `TestL2CLAheadOfSupervisor`)", "2025-06-11T15:22:38Z", "2025-06-11T16:08:02Z", "nonsense", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6w-ryK", "I_kwDODjvEJM66-Z_f", "Also see the new \"Flakiness Report\", FYI\nhttps://github.com/ethereum-optimism/optimism/pull/16411\n\nNote that we also include the \"Job Name\" here, which tells us which backend was used and may give us more clues as to where the flakyness arises.\n\n<img width=\"2540\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f8f887c7-0f9e-42b3-b68c-b28fec6387c4\" />", "2025-06-13T06:30:11Z", "2025-06-13T06:32:26Z", "scharissis", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM619aSc", "I_kwDODjvEJM66-Z_f", "Closing this as we improved tests considerable. We should continuously monitor CI and improve it as well as tests, and make sure we don't introduce too many flaky tests.", "2025-07-09T13:54:15Z", "2025-07-09T13:54:15Z", "nonsense", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6waZLS", "I_kwDODjvEJM66l_3a", "`ChainIndexingContinueEvent` do not stack up. They are in *response* to a previous stacking up of `ChainProcess` events.\n\nSee: https://github.com/ethereum-optimism/optimism/blob/7928e3c054c940638a9769756bbede773c2da2fc/op-supervisor/supervisor/backend/processors/chain_processor.go#L110\n\nIf a `ChainProcess` event comes in but the indexer is already running, it simply updates the target. This is a *fix* to previous behavior where any `ChainProcess` event would kick off a totally new stack of indexing events.\n\nMeanwhile, `ChainIndexingContinueEvents` are emitted *only* by the indexing actor. So there is an equal 1:1 emit:consume of indexing events. There should only be *one* in the event system per chain.\n\nFull description of the Indexing Continue Event here: https://github.com/ethereum-optimism/optimism/pull/16130\n\nWhen I put that together, I did run an A/B test and I found that this eliminated the queue-depth issue, but obviously if you're seeing it here there are still *more* issues in the event system.", "2025-06-10T15:25:29Z", "2025-06-10T15:29:02Z", "axelKingsley", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6xqMwB", "I_kwDODjvEJM66l_3a", "I am working on this, and reviewing the codepaths. While at the moment I agree with @sebastianst that we could potentially get rid of the `ChainIndexingContinueEvents` and refactor the code, I am inclined to also trust @axelKingsley that these events do not stack up, and it is very possible that some other event type is stacking up and triggering the too many events error.\n\nTherefore as a first step I think it makes sense adding a counter per event type, so that if this happens again, we have more data to debug: https://github.com/ethereum-optimism/optimism/pull/16471", "2025-06-17T14:33:38Z", "2025-06-17T14:33:38Z", "nonsense", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6xqNrg", "I_kwDODjvEJM66l_3a", "@sebastianst can you elaborate on how you decided that `ChainIndexingContinueEvent` is the culprit here?", "2025-06-17T14:34:40Z", "2025-06-17T14:34:40Z", "nonsense", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6xqxVj", "I_kwDODjvEJM66l_3a", "> [@sebastianst](https://github.com/sebastianst) can you elaborate on how you decided that `ChainIndexingContinueEvent` is the culprit here?\n\nI was just looking at the code and metrics and saw that this is emitted internally in a loop. But @axelKingsley is probably right and this event is 1:1 emitted/consumed. So it's probably the `\"cross-unsafe-update\"` events that are stacking up.", "2025-06-17T15:14:10Z", "2025-06-17T15:14:10Z", "sebastianst", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6yEP_b", "I_kwDODjvEJM66l_3a", "I've added a PR that adds a gauge for all in-flight events, i.e. enqueued within the `GlobalSyncExec`, so that we have an idea which event is creating the issues if this happens again - https://github.com/ethereum-optimism/optimism/pull/16473\n\nI've also added a gauge for the total in-flight events directly from within the `GlobalSyncExec`, and we could potentially setup an alert for it, if it crosses a threshold (for example 1k or 5k, given that at 10k we panic).", "2025-06-19T09:48:05Z", "2025-06-19T09:48:05Z", "nonsense", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6tp2jz", "I_kwDODjvEJM64dV_X", "The underlying issue is that apparently we can't reach out the service through Kurtosis' reverse proxy. All the other services work but we will get 404 for the L1 EL.\nThis can also be verified by:\n- Spin up a local devnet (e.g. interop)\n- Get the Host from `optimism/kurtosis-devnet/tests/<name>-devnet.json` => L1 -> Nodes -> Services -> El -> RPC -> reverse_proxy_header -> Host\n- Run the following command\n```sh\ncurl -X POST -H \"Content-Type: application/json\" --data '{\"jsonrpc\":\"2.0\",\"method\":\"\neth_blockNumber\",\"params\":[],\"id\":1}' -v -H \"Host: <Host>\" http://127.0.0.1:9730\n```", "2025-05-27T17:48:53Z", "2025-05-27T17:49:14Z", "serpixel", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6trHZM", "I_kwDODjvEJM64dV_X", "Reopening it so that we can spend some time investigating this issue. (Currently we have applied a workaround)", "2025-05-27T19:29:04Z", "2025-05-27T19:29:04Z", "serpixel", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6tsHtk", "I_kwDODjvEJM64dV_X", "Reopening because there are still two TODOs in the code:\n```\nRepository & Issue                       | Title                                                             | Location                                \n-----------------------------------------+-------------------------------------------------------------------+---------------------------------------------------\nethereum-optimism/optimism #16127        | op-acceptor: L1 Network is not accessible when running under d... | op-devstack/sysext/l1.go:30                       \nethereum-optimism/optimism #16127        | op-acceptor: L1 Network is not accessible when running under d... | op-devstack/sysext/l1.go:33                       \n```", "2025-05-27T20:57:19Z", "2025-05-27T20:57:19Z", "ajsutton", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6vIz4W", "I_kwDODjvEJM62o8Z-", "Blocked on - https://github.com/ethereum-optimism/optimism/pull/15681", "2025-06-04T03:47:57Z", "2025-06-04T03:47:57Z", "yashvardhan-kukreja", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6zJvzh", "I_kwDODjvEJM62o8Z-", "Associated PRs:\n- https://github.com/ethereum-optimism/infrastructure-services/pull/471\n- https://github.com/ethereum-optimism/infrastructure-services/pull/474", "2025-06-25T17:59:19Z", "2025-06-26T01:25:58Z", "yashvardhan-kukreja", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6xiR18", "I_kwDODjvEJM61nwo5", "The execution client isn't in a very good position to perform this validation. As far as I can tell it doesn't actually have the dependency set at the moment so we'd have to make that a required option just for this check. And it doesn't necessarily have the world state available at startup - it would have to perform this check after snap sync and then crash if the contract was unexpectedly available.\n\nThe genesis generation process is simpler than I think we originally assumed when writing this ticket as well. op-deployer will automatically decide if the CrossL2Inbox should be included in genesis based on whether there are multiple chains (which also decides if the lockbox is shared which is equally security critical). And if interop is not active at genesis, no interop contracts are included at all. So if you use an op-deployer that's capable of deploying CrossL2Inbox, it will only be deployed if there are multiple chains. If you use an earlier op-deployer without that condition, it won't support any contract release that includes CrossL2Inbox anyway. So we don't have a foot gun where the \"wrong\" genesis is selected - the only genesis that is generated is the right one.\n\nSo I'd argue that we solved this in the tooling and shouldn't then add the complexity to op-geth/op-reth to require specifying the dependency set just so this can be double checked.", "2025-06-17T00:30:08Z", "2025-06-17T00:30:08Z", "ajsutton", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM62GhvV", "I_kwDODjvEJM61nwo5", "I'm going to be opinionated and close this one. As mentioned above, the execution client isn't in a good position to perform this check and I think we mitigated the risk of generating genesis incorrectly in the generation tool to significantly reduce that risk.", "2025-07-10T03:03:24Z", "2025-07-10T03:03:24Z", "ajsutton", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM6ot9rL", "I_kwDODjvEJM6yBRJE", "Is the sequencer's EL client configured correctly according to these instructions? https://docs.optimism.io/operators/chain-operators/configuration/batcher#batcher-sequencer-throttling", "2025-04-25T14:38:28Z", "2025-04-25T14:38:28Z", "geoknee", "2025-08-31 06:30:02"]
["IC_kwDODjvEJM619KNl", "I_kwDODjvEJM6yBRJE", "Closing due to inactivity @abhi8960git please reopen if there is still a problem. ", "2025-07-09T13:37:06Z", "2025-07-09T13:37:06Z", "geoknee", "2025-08-31 06:30:02"]
["IC_kwDOL-xLQ8636HgE", "I_kwDOL-xLQ869HWXn", "Implemented in [#16744](https://github.com/ethereum-optimism/optimism/pull/16744)", "2025-07-17T21:04:09Z", "2025-07-17T21:04:09Z", "serpixel", "2025-08-31 06:31:06"]
["IC_kwDOL-xLQ863EFyY", "I_kwDOL-xLQ869HVxR", "Ported [here](https://github.com/ethereum-optimism/optimism/pull/16695)", "2025-07-14T23:12:46Z", "2025-07-14T23:12:46Z", "serpixel", "2025-08-31 06:31:06"]
["IC_kwDOL-xLQ863SQi-", "I_kwDOL-xLQ869HU_F", "Implemented in [#16594](https://github.com/ethereum-optimism/optimism/pull/16594)", "2025-07-15T18:48:25Z", "2025-07-15T18:48:25Z", "serpixel", "2025-08-31 06:31:06"]
["IC_kwDOL-xLQ863Yb6e", "I_kwDOL-xLQ869HU_F", "Duplicate of https://github.com/ethereum-optimism/infra/issues/367 ?\nI think we can close the other, yes?", "2025-07-16T03:51:53Z", "2025-07-16T03:51:53Z", "scharissis", "2025-08-31 06:31:06"]
["IC_kwDOL-xLQ86y5Z5R", "I_kwDOL-xLQ869HUbN", "Implemented in [#16547](https://github.com/ethereum-optimism/optimism/pull/16547)", "2025-06-24T17:53:45Z", "2025-06-24T17:53:45Z", "serpixel", "2025-08-31 06:31:06"]
["IC_kwDOL-xLQ86r1tdl", "I_kwDOL-xLQ862Kf9n", "Duplicate of https://github.com/ethereum-optimism/infra/issues/284", "2025-05-15T08:19:30Z", "2025-05-15T08:19:30Z", "scharissis", "2025-08-31 06:31:06"]
["IC_kwDOL-xLQ864G9M0", "I_kwDOL-xLQ860dEu0", "Will be handled by Protocol team.", "2025-07-18T09:44:40Z", "2025-07-18T09:44:40Z", "scharissis", "2025-08-31 06:31:06"]
["IC_kwDOH2Qg5s63NlSR", "I_kwDOH2Qg5s69Ua98", "I have been trying to dig deeper into this issue, and I cannot figure out how this bug might be possible.\n\nReproduction: I have been running the test in question from @pcw109550 's branch on repeat, with a filter to show the mempool contents on a separte screen. I'm running a count of 4, and requesting to run in parallel (though I think it's still serial). In my experience, running many tests in parallel is a good way to reproduce race conditions.\n\nHowever, I have not had a single instance of the test include an invalid transaction in the mempool.\n\nAnalysis: I traced through `geth`, and at no point did I find any possible branching logic which could add to the mempool in any other way from the public `Add` function. The flow looks like:\n- sendRawTransaction\n- SubmitTransaction\n- backend.SendTransaction\n- internal backend sendTransaction\n- pool.Add\n- subpool.Add\n- addTransactionsLocked\n\nPrior to a recent change, the mempool performed ingress filtering at the subpool.Add level, but now it is at the addTransactionsLocked level. In either case, any transaction submitted would have gone through the filter, unavoidably. There are a couple notable branches, but those turned out not to matter either:\n- The backend forwards Tx to the sequencer, but this is through sendRawTx anyway\n- Some transactions can be added to the Local Tracker, but this feeds transactions back through pool.Add anyway\n\nI am running the test on a loop in the background while I do other work, but am at a dead-end. @pcw109550 do you have any tips fro reproduction?", "2025-07-15T14:15:33Z", "2025-07-15T14:15:33Z", "axelKingsley", "2025-08-31 06:31:08"]
["IC_kwDOH2Qg5s63Nw5K", "I_kwDOH2Qg5s69Ua98", "@axelKingsley I experienced this behavior at the op-acceptance-tests CI at https://github.com/ethereum-optimism/optimism/pull/16569.\nIf you see the CI logs at https://app.circleci.com/pipelines/github/ethereum-optimism/optimism?branch=pcw109550%2Facceptance-test-interop-mempool-evict-experiment, you can see that I attempted to run the `acceptance-tests-pr` pipeline 9 times. 8 of them were green, and the last trial gave me the error case where the invalid tx was conditionally included. \n\nThere are some artifacts at https://app.circleci.com/pipelines/github/ethereum-optimism/optimism/94456/workflows/5e423a63-cea0-47cd-b777-51b7002daab6/jobs/3669140/artifacts when the flakiness occurred.\n\nhttps://output.circle-artifacts.com/output/job/60e6f41e-fc0d-4d0a-adf9-02b46f876dc4/artifacts/0/op-acceptance-tests/logs/testrun-c79fd1f4-1d6c-444c-aafa-cb4991bb6b3d/passed/interop_message_TestExecMessageInvalidAttributes.log\nthis is the most interesting one since at least two invalid txs were inserted to the mempool, but one got evicted, the other got stuck.", "2025-07-15T14:24:47Z", "2025-07-15T14:26:07Z", "pcw109550", "2025-08-31 06:31:08"]
["IC_kwDOKIsnqM62-fWu", "I_kwDOKIsnqM69Uuo6", "Closed by https://github.com/ethereum-optimism/superchain-ops/pull/1119", "2025-07-14T14:31:46Z", "2025-07-14T14:31:46Z", "mds1", "2025-08-31 06:31:16"]
["IC_kwDOLB-lzc621kG1", "I_kwDOLB-lzc6z2azT", "can i take this?", "2025-07-14T01:29:40Z", "2025-07-14T01:29:40Z", "JoshdfG", "2025-08-31 06:31:25"]
["IC_kwDODjvEJM64TxzP", "I_kwDODjvEJM7BbF8t", "We already run golangci-lint as part of CI. ", "2025-07-19T09:00:37Z", "2025-07-19T09:00:37Z", "ajsutton", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM64T6pz", "I_kwDODjvEJM7BbF8t", "Yes, saw it - [here](https://github.com/ethereum-optimism/optimism/blob/db8f6f78a8a60a2a822768ddd85cd06a16450543/Makefile#L23C1-L31C20)\nI propose to have the .golangci-lint.yaml file in the repository,\nAnd have more good quality linters like - staticcheck, revive etc.", "2025-07-19T09:59:37Z", "2025-07-19T09:59:37Z", "Saurabh2402", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM64IVW4", "I_kwDODjvEJM7BS2zr", "This issue stems from https://github.com/ethereum-optimism/optimism/pull/16732, which the op-geth sequencer at interop kurtosis devnet used by acceptance tests did not set the proper flag, causing test failure. We may focus to make the kurtosisnet yaml description match the production(adding proxyd, verifier nodes etc).", "2025-07-18T11:40:27Z", "2025-07-18T11:40:27Z", "pcw109550", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM64IPVX", "I_kwDODjvEJM7BBjZX", "https://github.com/ethereum-optimism/optimism/pull/16735 will solve this.", "2025-07-18T11:31:10Z", "2025-07-18T11:31:10Z", "pcw109550", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63RuYQ", "I_kwDODjvEJM7Ac9_t", "Considering the additional amount of initial effort, our designs and discussions scoping one namespace per node as well as needing to tweak our metrics and alerts even with dedicated namespacing, it was realised that it's better to stick with including flashblocks-components in the same namespace as other components of the sequencer.\n\nTo tackle the monitoring and alerting gaps, it would be preferable to rather add additional labels to alerts to group on as well as to the flashblocks-realted components.", "2025-07-15T18:19:54Z", "2025-07-15T18:19:54Z", "yashvardhan-kukreja", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63Re7z", "I_kwDODjvEJM7Ac7YN", "It was decided to skip nvme-node for now considering the flashblocks generation they had been depicting.\n\nThat being said, more granular metrics will be evaluated later to bolster this decision further.\n\nFor now, the more immediate issue to handle is the issue with the slow transaction inclusion of flashblocks-enabled nodes, which doesn't get much positively impacted with nvme-node enablement as well.", "2025-07-15T18:12:40Z", "2025-07-15T18:12:40Z", "yashvardhan-kukreja", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63AP9S", "I_kwDODjvEJM7AQGgM", "Thanks @chuwt good observation. We would welcome a PR to fix this if you have the time!", "2025-07-14T17:00:08Z", "2025-07-14T17:00:08Z", "geoknee", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63FYnr", "I_kwDODjvEJM7AQGgM", "Sure.", "2025-07-15T01:58:07Z", "2025-07-15T01:58:07Z", "chuwt", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM627BVB", "I_kwDODjvEJM6_wRuw", "By using the op-test-sequencer, we may force include invalid exec msgs. See `TestReorgInvalidExecMsgs` acceptance test at \nhttps://github.com/ethereum-optimism/optimism/blob/822366687bb77ecaf78ce97ce19768aa5accffb0/op-acceptance-tests/tests/interop/reorgs/invalid_exec_msgs_test.go#L48\n\nAfter this we may check the \n1. invalid msg included block is reorged\n2. (Implicit) Check new L2 blocks does not contain invalid interop tx\n3. (Explicit) Check mempool and make sure the invalid msg tx is not present\n\nThis PR https://github.com/ethereum-optimism/optimism/pull/16569 may be referred to inspect the mempool.\n\nIf we decide not to use the op-test-sequencer, and make the situation where interop msg was valid, but after some dependency updates it became invalid, this would be more tricky to write the test.", "2025-07-14T10:45:09Z", "2025-07-14T10:46:22Z", "pcw109550", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63CKyH", "I_kwDODjvEJM6_t_mA", "I discussed this with the VerifyOPCM auditors, and they felt that the VerifyOPCM script provided redundancy, and was complimentary to simply rerunning DeployImplementations.s.sol. \n\nThis way if either script is modified significantly, the other acts as a check to ensure it is not broken.\n\nA nice approach would be to re-run DeployImplementations within VerifyOPCM alongside the other checks it makes.", "2025-07-14T20:01:29Z", "2025-07-14T20:01:29Z", "maurelian", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6xnYnD", "I_kwDODjvEJM66iZC9", "Same here, also on op-node v1.13.3\n\n```\ngoroutine 1357345 [select, 3406 minutes]:                                                                                                                                                                                                                                                 11:12:30 [1527/1825]\ngithub.com/libp2p/go-yamux/v4.(*Stream).Read(0xc00f4f9a40, {0xc0254d7e74, 0x1, 0x1})                                                                                                                                                                                                                          \n        /go/pkg/mod/github.com/libp2p/go-yamux/v4@v4.0.1/stream.go:111 +0x1a5                                                                                                                                                                                                                                 \ngithub.com/libp2p/go-libp2p/p2p/muxer/yamux.(*stream).Read(0x48b4ac?, {0xc0254d7e74?, 0x100c0028cae58?, 0xc0028cae60?})                                                                                                                                                                                       \n        /go/pkg/mod/github.com/libp2p/go-libp2p@v0.36.2/p2p/muxer/yamux/stream.go:17 +0x18                                                                                                                                                                                                                    \ngithub.com/libp2p/go-libp2p/p2p/net/swarm.(*Stream).Read(0xc02498fa80, {0xc0254d7e74?, 0x1000000001c?, 0xc0028caf10?})                                                                                                                                                                                        \n        /go/pkg/mod/github.com/libp2p/go-libp2p@v0.36.2/p2p/net/swarm/swarm_stream.go:58 +0x2d                                                         \ngithub.com/multiformats/go-multistream.(*lazyClientConn[...]).Read(0xc000100008?, {0xc0254d7e74?, 0x1?, 0x1?})                                         \n        /go/pkg/mod/github.com/multiformats/go-multistream@v0.5.0/lazyClient.go:68 +0x98                                                               \ngithub.com/libp2p/go-libp2p/p2p/host/basic.(*streamWrapper).Read(0x222e700?, {0xc0254d7e74?, 0x0?, 0x0?})                                              \n        /go/pkg/mod/github.com/libp2p/go-libp2p@v0.36.2/p2p/host/basic/basic_host.go:1108 +0x22                                                        \ngithub.com/libp2p/go-libp2p-pubsub.(*PubSub).handlePeerDead(0xc00131d8c8, {0x225a8b0, 0xc02395c500})                                                   \n        /go/pkg/mod/github.com/libp2p/go-libp2p-pubsub@v0.12.0/comm.go:150 +0x73                                                                       \ncreated by github.com/libp2p/go-libp2p-pubsub.(*PubSub).handleNewPeer in goroutine 1357295\n\ngoroutine 943461 [IO wait]:                                                                                                                                                                                                                                                               11:12:30 [1585/1825]\ninternal/poll.runtime_pollWait(0x7bc6cccc8ad8, 0x72)                                                                                                                                                                                                                                                          \n        /usr/local/go/src/runtime/netpoll.go:351 +0x85                                                                                                                                                                                                                                                        \ninternal/poll.(*pollDesc).wait(0xc0183a3880?, 0xc019096000?, 0x0)                                                                                                                                                                                                                                             \n        /usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27                                                                                                                                                                                                                                           \ninternal/poll.(*pollDesc).waitRead(...)   \n```", "2025-06-17T10:52:35Z", "2025-06-17T10:52:51Z", "Creamers158", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6y6CsK", "I_kwDODjvEJM66iZC9", "Same here, also on op-node v1.13.3, haven't tried v1.13.4 yet", "2025-06-24T19:01:08Z", "2025-06-24T19:01:08Z", "rrrengineer", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6sXNBl", "I_kwDODjvEJM62qF51", "We need to know the op-proposer EOA so we can correctly assert it's submitting super roots.", "2025-05-19T17:24:29Z", "2025-05-19T17:24:29Z", "Inphi", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6sXPpk", "I_kwDODjvEJM62qF51", "The op-proposer submits proposals at a fixed interval. This means the op-acceptance tests may wait a while before it observes this.\nWe should consider either adding an admin interface to op-proposer so that a human/kurtosis can manually trigger proposals - thereby reducing wait time. Or, inspect the chain history for a proposal.", "2025-05-19T17:28:39Z", "2025-05-19T17:28:39Z", "Inphi", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6sZFRn", "I_kwDODjvEJM62qF51", "> We need to know the op-proposer EOA so we can correctly assert it's submitting super roots.\n\nSimply assume whatever proposes is our op-proposer. It's unlikely to be anything else in a freshly minted public devnet.", "2025-05-19T21:02:31Z", "2025-05-19T21:02:31Z", "Inphi", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6nNee9", "I_kwDODjvEJM6ylVVj", "Can you try changing L1 RPC to Alchemy and see if issue still persist?", "2025-04-15T14:16:03Z", "2025-04-15T14:16:03Z", "Chomtana", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63gB3P", "I_kwDODjvEJM6ylVVj", "I assume this was resolved but please reopen if still an issue. ", "2025-07-16T13:28:27Z", "2025-07-16T13:28:27Z", "geoknee", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6n_kyo", "I_kwDODjvEJM6xh3u9", "Which milesstone should this be? ", "2025-04-21T13:56:57Z", "2025-04-21T13:56:57Z", "pauldowman", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6oEZrP", "I_kwDODjvEJM6xh3u9", "I'm not really sure what the milestone definitions actually are. Definitely before we ship... It doesn't block launching the beta net though but don't know if that's meant to be a final RC or not.", "2025-04-21T23:50:58Z", "2025-04-21T23:50:58Z", "ajsutton", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6q57nX", "I_kwDODjvEJM6xh3u9", "I'm pretty confident that the main concern with the expiry window is the canonical chain walk back. The deposits-only block execution dominates all other areas in the fault proof. And now that Pectra has been activated on eth-sepolia for a couple months, we can develop a microbenchmark program that utilizes the fast canonical oracle.", "2025-05-09T17:05:12Z", "2025-05-09T17:05:12Z", "Inphi", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6rLzbN", "I_kwDODjvEJM6xh3u9", "Given the expiry window has been reduced to 7 days, we've already done a microbenchmark and don't expect issues here.  I'd still like to run an actual worst case scenario test on actual interop enabled chains though to be sure there aren't any surprises when the whole thing hangs together.", "2025-05-12T10:35:48Z", "2025-05-12T10:35:48Z", "ajsutton", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM629b25", "I_kwDODjvEJM6xh3u9", "Just for my own understanding, what is the \"consolidation step\" referenced in this issue?", "2025-07-14T13:07:39Z", "2025-07-14T13:07:39Z", "mds1", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM62-1R5", "I_kwDODjvEJM6xh3u9", "> what is the \"consolidation step\" referenced in this issue?\n\n@mds1 it's the final step in the fault proof program where it checks if all the executing messages were valid, after running the derivation for each chain in the dependency set. More details here: https://specs.optimism.io/interop/fault-proof.html#consolidation", "2025-07-14T14:58:42Z", "2025-07-14T14:58:42Z", "pauldowman", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63r0l3", "I_kwDODjvEJM6jvwBR", "Hi there - we are no longer updating the NPM packages.", "2025-07-16T23:17:42Z", "2025-07-16T23:17:42Z", "mslipper", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6P1-hI", "I_kwDODjvEJM6aL_BU", "The timeout stops state update correctly.", "2024-10-15T08:55:45Z", "2024-10-15T08:55:45Z", "threewebcode", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6P9gry", "I_kwDODjvEJM6aL_BU", "Thank you for filing this bug report, this information is very helpful.\r\n\r\nThe `nextActionOK` handling in the sequencer should be improved, I will look into what can be done there.\r\n\r\nThe leadership transfer is more difficult: with async-gossip, when the payload has been published already, it becomes the canonical block required to continue sequencing. If other replicas don't pick up on this block via p2p before getting the leadership transfer, then a re-attempt of leadership-transfer may be useful, but ultimately it can flake. If the op-conductor is able to insert this committed payload content, then it can recover from the missing-block case. Even then, the replica does need to make it a canonical block. If it's timing out / not doing a final forkchoice update, then something in the block-processing itself is wrong or is hitting a performance issue, and that then needs further investigation before we can fix it.\r\n", "2024-10-15T22:17:08Z", "2024-10-15T22:17:08Z", "protolambda", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6P-3Fg", "I_kwDODjvEJM6aL_BU", "> Thank you for filing this bug report, this information is very helpful.\r\n> \r\n> The `nextActionOK` handling in the sequencer should be improved, I will look into what can be done there.\r\n> \r\n> The leadership transfer is more difficult: with async-gossip, when the payload has been published already, it becomes the canonical block required to continue sequencing. If other replicas don't pick up on this block via p2p before getting the leadership transfer, then a re-attempt of leadership-transfer may be useful, but ultimately it can flake. If the op-conductor is able to insert this committed payload content, then it can recover from the missing-block case. Even then, the replica does need to make it a canonical block. If it's timing out / not doing a final forkchoice update, then something in the block-processing itself is wrong or is hitting a performance issue, and that then needs further investigation before we can fix it.\r\n\r\nRegarding leadership transfer:\r\n\r\n1. conductor did indeed [try to insert this committed payload content into op-node](https://github.com/ethereum-optimism/optimism/blob/develop/op-conductor/conductor/service.go#L766) if it does not have it already (only for the latest block)\r\n2. currently the issue here is not the timeout, it has already received p2p payload and tried to insert it (timed out), it is that when op-conductor tries to insert the payload again, because the payload is already inside the [payloadsQueue](https://github.com/ethereum-optimism/optimism/blob/develop/op-node/rollup/clsync/clsync.go#L175), it will [return immediately](https://github.com/ethereum-optimism/optimism/blob/develop/op-node/rollup/clsync/clsync.go#L175-L178) without calling `ForkChoiceUpdateRequest` later. => a immediate thought is to allow conductor to bypass the check (keep other behaviors the same), but not sure if that is the most elegant solution", "2024-10-16T02:55:28Z", "2024-10-16T02:59:12Z", "0x00101010", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6Q3j8H", "I_kwDODjvEJM6aL_BU", "@protolambda Hey proto, curious if you got time to take a look at this?", "2024-10-22T23:12:08Z", "2024-10-22T23:12:08Z", "0x00101010", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6OYxaO", "I_kwDODjvEJM6YqWW4", "Hey @sebastianst, your analysis is correct. I also reported this about two weeks ago btw :grimacing: \r\nSee the issue description for an easy way to reproduce it as well\r\nhttps://github.com/ethereum-optimism/optimism/issues/12041", "2024-10-02T14:48:34Z", "2024-10-02T14:48:34Z", "bearpebble", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6OaGzf", "I_kwDODjvEJM6YqWW4", "Hey @bearpebble sorry your report got missed! I've attempted a simple fix with https://github.com/ethereum-optimism/optimism/pull/12258 and created an [op-node docker image with tag `v1.9.4-dev.0`](https://us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:v1.9.4-dev.0). Feel free to also chime into the discussion on [Discord](https://discord.com/channels/1244729134312198194/1268238544493744210/1291086210957180999).", "2024-10-02T17:21:42Z", "2024-10-02T17:21:51Z", "sebastianst", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6Oi6OU", "I_kwDODjvEJM6YqWW4", "to add, we've been experiencing sequencer halt also when l1 becomes unavailable for more than a sequencer drift time emitting a `L1TemporaryErrorEvent`\r\n\r\nnot sure if being masked by an enginetemporaryerror. will try to reproduce using the latest build by @sebastianst ", "2024-10-03T14:06:16Z", "2024-10-03T14:06:36Z", "emilianobonassi", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63r6OA", "I_kwDODjvEJM6YqWW4", "Closing this for now. Please reopen if this issue still persists.", "2025-07-16T23:22:31Z", "2025-07-16T23:22:31Z", "mslipper", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6GD9ug", "I_kwDODjvEJM6QwLpt", "Thanks @ImTei for tracking the issue on this edge case we found! \r\n\r\nI do agree this might not be handled automatically in first instance. \r\n\r\nI think could be a good opportunity to spin-up a playbook/runbook section in the docs to describe this and other scenarios that might realize, providing best practices (at conduit we have internally). \r\n\r\nAnother example, blob congestion => switch to calldata, make a blob transaction type to cancel the pending one (see LZ airdrop).", "2024-07-25T01:26:46Z", "2024-07-25T01:27:45Z", "emilianobonassi", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6IkQxF", "I_kwDODjvEJM6QwLpt", "> Another example, blob congestion => switch to calldata, make a blob transaction type to cancel the pending one (see LZ airdrop).\r\n\r\n@emilianobonassi This is already done automatically by the batcher since https://github.com/ethereum-optimism/optimism/pull/10941", "2024-08-15T12:48:59Z", "2024-08-15T12:48:59Z", "sebastianst", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6ZniNs", "I_kwDODjvEJM6QwLpt", "Relevant work (not yet in develop) which can help address this issue #13550 .", "2025-01-08T10:05:43Z", "2025-01-08T10:05:43Z", "geoknee", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM6kZJah", "I_kwDODjvEJM6QwLpt", "`incident mode` has been implemented as of #13550 (called \"recover mode\"). The sync status and batcher changes could also be a good idea, but then again some manual intervention is reasonable to recover from a sequencing window expiry (which should be pretty rare). \n\n@ImTei should we close this, or do you want to keep it in the backlog for the batcher changes?", "2025-03-27T13:32:27Z", "2025-03-27T13:32:27Z", "geoknee", "2025-08-31 06:31:40"]
["IC_kwDODjvEJM63r4zD", "I_kwDODjvEJM6QwLpt", "Closing this for now. Please reopen if it's still relevant.", "2025-07-16T23:21:04Z", "2025-07-16T23:21:04Z", "mslipper", "2025-08-31 06:31:40"]
["IC_kwDOKIwiaM6344t-", "I_kwDOKIwiaM7BJskC", "@fainashalts This [pr](https://github.com/ethereum-optimism/docs/pull/1693) addresses this.\nWaiting for reviews and approval. cc @bradleycamacho ", "2025-07-17T19:06:12Z", "2025-07-17T19:06:12Z", "krofax", "2025-08-31 06:32:54"]
["IC_kwDOKIwiaM6345u0", "I_kwDOKIwiaM7BJskC", "@krofax awesome thank you! Feel free to close this once that is merged. \ud83d\udc4d ", "2025-07-17T19:07:07Z", "2025-07-17T19:07:07Z", "fainashalts", "2025-08-31 06:32:54"]
["IC_kwDOKIwiaM635ChS", "I_kwDOKIwiaM7BJskC", "Approved!", "2025-07-17T19:15:06Z", "2025-07-17T19:15:06Z", "bradleycamacho", "2025-08-31 06:32:54"]
["IC_kwDOLB-lzc6iLiAH", "I_kwDOLB-lzc6suPCg", "Not going into Isthmus", "2025-03-13T11:29:52Z", "2025-03-13T11:29:52Z", "sebastianst", "2025-08-31 06:32:56"]
["IC_kwDODjvEJM65XXFg", "I_kwDODjvEJM7Ac1EG", "Closed by  https://github.com/ethereum-optimism/k8s/pull/7006", "2025-07-23T19:41:53Z", "2025-07-23T19:41:53Z", "yashvardhan-kukreja", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM61-x-w", "I_kwDODjvEJM6_td38", "@maurelian just cc'ing you for visibility so you can follow this", "2025-07-09T15:43:15Z", "2025-07-09T15:43:15Z", "mds1", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM649kAV", "I_kwDODjvEJM6_td38", "Updated references from other runbooks to point to V3, renamed the three different versions of this runbook to indicate which releases they apply to (v1 = pre-OPCM; v2 = U14 & U15; v3 = U16+), and each one links to the others.\n", "2025-07-22T14:49:12Z", "2025-07-22T14:49:12Z", "pauldowman", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6zHwlU", "I_kwDODjvEJM69TbCR", "We have existing benchmarks for the channel compressors here https://github.com/ethereum-optimism/optimism/blob/3d88b1695a0bb0e38674f9c9e976fa3cf8e435ba/op-node/benchmarks/batchbuilding_test.go#L167 but we need to ensure:\n* they run locally\n* they run on CI in a meaningful way\n* they are up to date with Brotli compression", "2025-06-25T15:12:22Z", "2025-06-25T15:12:46Z", "geoknee", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6zIjl_", "I_kwDODjvEJM69TbCR", "Running locally, and with some local fixes, I am already seeing evidence of the memory load we are seeing in the batcher via the benchmarks:\n\n```\n> go test -benchmem -run=^$ -bench ^BenchmarkAllBatchesChannelOut$ github.com/ethereum-optimism/optimism/op-node/benchmarks -v\nBenchmarkAllBatchesChannelOut/BatchType=Span,_txPerBatch=100,_BatchCount=100,_Compressor=ShadowCompressor-brotli-10-100000000000\nBenchmarkAllBatchesChannelOut/BatchType=Span,_txPerBatch=100,_BatchCount=100,_Compressor=ShadowCompressor-brotli-10-100000000000-14                   12         89781448 ns/op 190224683 B/op   1701754 allocs/op\n```\n\n190MB per operation for a channel with 100 blocks of 100 txs each.", "2025-06-25T16:15:18Z", "2025-06-25T16:15:18Z", "geoknee", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6zMphl", "I_kwDODjvEJM69TbCR", "@trianglesphere has also raised performance issues with this code, perhaps related to memory pressure", "2025-06-25T22:35:53Z", "2025-06-25T22:35:53Z", "tynes", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM64lZ3y", "I_kwDODjvEJM6839Vx", "Closed in favor of #16687 ", "2025-07-21T13:33:15Z", "2025-07-21T13:33:15Z", "yashvardhan-kukreja", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6ykQOI", "I_kwDODjvEJM68zhET", "We could run a memory profiler.", "2025-06-23T10:23:25Z", "2025-06-23T10:23:25Z", "sebastianst", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6zEghf", "I_kwDODjvEJM68zhET", "I had some success running `pprof` and it suggests to me that the compressor is where the majority of the memory usage lies when we get a backlog of channels in the batcher. \n\nRecipe:\n1. Run `just simple-devnet`, and when it starts up stop the batcher (easy to do from Docker GUI)\n2. Customize the  batcher should have these lines commented out:\nhttps://github.com/ethereum-optimism/optimism/blob/6540f767739eaa502ef31c5f08726d8dc85c8448/op-batcher/batcher/driver.go#L443-L444 (this simulates l1 congestion / a safe head stall from the point of view of memory management in the batcher)\n3.  Run the custom batcher on the host machine using the public ports reported by kurtosis for various components.\nand then From `op-batcher` directory \n```\ngo run cmd/main.go \\\n--l1-eth-rpc=http://127.0.0.1:55760 \\\n--l2-eth-rpc=http://127.0.0.1:55884 \\\n--rollup-rpc=http://127.0.0.1:55887 \\\n--pprof.enabled=true \\\n--private-key=<get this from kurtosis output> \\\n--data-availability-type=blobs \\\n--log.level=debug \\\n--target-num-frames=6 \\\n--compression-algo=brotli-10 \\\n--throttle-threshold=0 \\\n--compressor=shadow\n```\n4. Run Marius's [tx fuzzer ](https://github.com/MariusVanDerWijden/tx-fuzz)to fill up the L2 blocks and work the compressor hard (note slot time and rpc endpoints):\n```\ngo run cmd/livefuzzer/main.go spam --rpc http://127.0.0.1:55884 -sk=0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80 --slot-time=2\n```\n5. Let things run for a few minutes, confirm that blocks are coming in with lots of transactions by inspecting the batcher logs\n6. Run pprof like so\n```\ngo tool pprof localhost:6060/debug/pprof/heap\n```\n7. Dump the output into https://pprof.me/ and select \"memory in use bytes\"", "2025-06-25T10:41:48Z", "2025-06-25T10:41:48Z", "geoknee", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6zEk89", "I_kwDODjvEJM68zhET", "<img width=\"1854\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9b82b00b-be49-45ff-9ef1-32692fda023e\" />\n\nYou can see that the publishing loop dominates, and this is almost entirely made up of memory in use due to allocations by writing to the compressor. The compressor is complex, it is not surprising that it consumes a lot more memory than just the input and output data.\n\nThere are two optimizations that come to mind:\n\n1. The shadow compressor could be made to use half the memory by not using two compressors under the hood. I have an incoming PR for that.\n2. We may not actually need to retain the compressor after the channel is submitted. We can throw that away more eagerly, since we cache all the blocks we need for any requeuing events (like a DA switch or lack of expected progress). We could go even further and do away with the channel queue, this might even be a nice simplification. We probably don't need to keep a channel once we submitted it.", "2025-06-25T10:47:08Z", "2025-06-25T10:47:08Z", "geoknee", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6zGoZq", "I_kwDODjvEJM68zhET", "Idea from @sebastianst : we can also use a single compressor and pass it around / share it between channel to save even more memory. ", "2025-06-25T13:50:57Z", "2025-06-25T13:50:57Z", "geoknee", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6x5LiZ", "I_kwDODjvEJM68LzRI", "Blocked on #16468 ", "2025-06-18T14:56:45Z", "2025-06-18T14:56:45Z", "yashvardhan-kukreja", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM64lXj7", "I_kwDODjvEJM68LzRI", "Closed in favor of the creation of the milestone", "2025-07-21T13:30:28Z", "2025-07-21T13:30:28Z", "yashvardhan-kukreja", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM62-fkv", "I_kwDODjvEJM670Scm", "@protolambda is there any specific reason to use the query parameters like `/synctest?head=100&safe=95&finalized=90`, not the post request? If there are no specific reasons, I want to propose the below session management flow.\n\n1. Sync tester digests yaml config, and can be used for multiple chains, same design with faucet.\n   - the config will include {chain id, engine api endpoint, engine jwt, standard rpc endpoint}\n3. User initializes API endpoint via `/chain/{chain_id}` with `sync_init` RPC. Request object will be `unsafe, safe, finalized` and will return the session id `{session_id}`.    - session id will be generated using `uuid`.  \n3. Sync tester registers new endpoint `/{session_id}`, and all CL request to EL(`eth_*`, `engine_*`) will be proxyed statefully.\n4. Provide the new sync tester provided endpoint to the CL and do the test.\n\n\n", "2025-07-14T14:32:02Z", "2025-07-14T14:34:42Z", "pcw109550", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM62-2zP", "I_kwDODjvEJM670Scm", "@pcw109550 good idea to handle sessions more uniquely that way.\n\nWith http-middleware you can pre-process any request, and attach things to the ctx, and handle any URL parsing and such.\n\nBut yes, since there might not be cookies or similar, a UUID in the URL as session identifier makes sense.\n\nI think it would still be nice to not create a `sync_init` RPC though, since that means we have to follow additional special steps, before connecting a node to the sync-tester. If it's just based on the URL then no special step is required.\n\nMaybe we can rely on the caller to use a unique UUID in their request, and just use the http mux handler to match the UUID part of the path, read it as path-value from the request, look up from a global session map, and attach the session to the context? That way the RPC handler and everything is static, but from the ctx on every RPC call you'll be able to retrieve the session info that the http-middleware prepared.\n", "2025-07-14T15:00:40Z", "2025-07-14T15:00:40Z", "protolambda", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6zm_IT", "I_kwDODjvEJM66QS0p", "Partially implemented at https://github.com/ethereum-optimism/optimism/pull/16557, for\n>  It would be more consistent if the supervisor was be responsible for providing the right reset targets for all labels.\n\nbut it lacks\n- need to handle pre- and post-Interop differently because the supervisor doesn't have the logs DB pre-Interop", "2025-06-27T14:49:45Z", "2025-06-27T14:49:51Z", "pcw109550", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM64hFTT", "I_kwDODjvEJM66QS0p", "@sebastianst I think we covered everything actionable in this issue and can close it now. A lot of the work was done at https://github.com/ethereum-optimism/optimism/pull/16557 and the PRs linked here.\n\nDo you think there is anything else to be done here, considering that we are about to refactor some of these components in the near future?", "2025-07-21T08:14:15Z", "2025-07-21T08:14:15Z", "nonsense", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM653GHy", "I_kwDODjvEJM66QS0p", "Closing for now given recent project changes. Let's re-open or create a new issue if necessary.", "2025-07-25T14:44:31Z", "2025-07-25T14:44:31Z", "nonsense", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6utpv3", "I_kwDODjvEJM630f1m", "@protolambda I think we can close this one, assuming that the \"full scope\" section was just for context", "2025-06-02T15:12:37Z", "2025-06-02T15:12:37Z", "sebastianst", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6uDqgM", "I_kwDODjvEJM63Z052", "Most are cleaned up in https://github.com/ethereum-optimism/optimism/pull/16008 but a few less critical ones remain", "2025-05-29T18:14:16Z", "2025-05-29T18:14:16Z", "sebastianst", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6rhdgY", "I_kwDODjvEJM62ajwf", "for reference: https://docs.safe.global/core-api/safe-contracts-deployment", "2025-05-13T19:16:34Z", "2025-05-13T19:16:34Z", "sigma", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6sELQU", "I_kwDODjvEJM62ajwf", "The motivation is ensuring parity between our kt-devnet setup and our production setup, correct?", "2025-05-16T13:46:07Z", "2025-05-16T13:46:07Z", "teddyknox", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6sEn01", "I_kwDODjvEJM62ajwf", "> The motivation is ensuring parity between our kt-devnet setup and our production setup, correct?\n\nyes, that's my understanding", "2025-05-16T14:31:04Z", "2025-05-16T14:31:04Z", "sigma", "2025-08-31 06:33:19"]
["IC_kwDODjvEJM6rjHy0", "I_kwDODjvEJM62SUAM", "Bumping this to P1, since it relates closely with the pre-interop activation behavior, and I would like these two functions to coherently fit together.\n", "2025-05-13T22:34:42Z", "2025-05-13T22:34:42Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6s9r13", "I_kwDODjvEJM62SUAM", "The scope of this issue has been changing. The child issue of removing StandardMode is done and was what primarily blocked the rehearsal/audit work, together with the separately ticketed configuration + supervisor upgrade related issues.\nI'm removing this issue from the burndown; doing the full scope of this will take longer.", "2025-05-22T16:24:46Z", "2025-05-22T16:24:46Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6qvnUU", "I_kwDODjvEJM61uccU", "Critically important that op-supervisor still applies the full interop rules for any L2 block >= activation timestamp, including replacing any invalid executing messages that happen to get in *and* not-replacing any valid executing messages that might get in.  It's ok for the sequencer to refuse to include executing messages for a while, but if they somehow slip through into safe blocks they *must* be seen as valid.  Otherwise it will cause a chain split between the fault proof program and op-supervisor.  The fault proof programs have no concept of when blocks became safe/finalized, it just sees the whatever block the bisection process found was agreed by both parties as the unsafe, safe and finalized head and it can only derive safe blocks since it only reads L1 data.\n\nFlagging this because it seems like if op-supervisor doesn't start indexing from the activation block then it wouldn't be able to correctly determine the validity of executing messages that refer to those blocks.  Also important to remember that finalisation is not part of consensus so blocks can finalise based on different triggers on different nodes.", "2025-05-08T22:48:20Z", "2025-05-08T22:48:20Z", "ajsutton", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6sUq-R", "I_kwDODjvEJM61uccU", "Moving this from P0 to P1: it is still technically possible to try a rehearsal without this functionality, although it is still important for safety and full RC Beta scope.", "2025-05-19T13:46:39Z", "2025-05-19T13:46:39Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6urjfN", "I_kwDODjvEJM61uccU", "The solution for https://github.com/ethereum-optimism/optimism/issues/16166 will start populating the derived databases pre-Interop, so there's no special notion of the activation block any more. The logs DB will only be populated starting at the activation block. In both cases, we still need support for the first block in the DB to be rewindable. It will be a random pre-Interop block for the safe DBs and the activation block for the logs DB.\n\nKeeping this ticket open to track support for this, in case it won't already be handled fully when fixing #16166. At least, further testing for these cases will be added within the scope of resolving this ticket.", "2025-06-02T13:12:48Z", "2025-06-02T13:12:48Z", "sebastianst", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6ot6Hu", "I_kwDODjvEJM6ypLpZ", "The batcher already has a flag to enable it to switch behaviors here:\n\nhttps://github.com/ethereum-optimism/optimism/blob/c46bb168e221c2b961a63b0107cbf6a95ffc5348/op-batcher/flags/flags.go#L183-L188\n\nCurrently it defaults to load unsafe blocks above the cross safe head (i.e. it uses the `SafeL2` sync status field). It can be switched to the more eager behaviour (using the `LocalSafeL2` field) using the flag. \n\nI think recently there was a feeling that we _would_ want to do the more eager behaviour. That is reflected in this comment:\n\nhttps://github.com/ethereum-optimism/optimism/blob/fcc5a36e3135e2f0b5fc0160a362c0666cee07c6/op-batcher/batcher/sync_actions.go#L67-L71\n\n@axelKingsley LMK if you think this comment is incorrect and we can remove it. ", "2025-04-25T14:32:58Z", "2025-05-10T08:38:28Z", "geoknee", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6q7bsO", "I_kwDODjvEJM6ypLpZ", "@geoknee thank you for this information! But I'm not sure if this is what we are looking for. This ticket is about the scope of unsafe blocks that get batched and posted by the batcher, so IDK how `SafeL2` comes into play.\n\nCurrently the Batcher collects up *unsafe* data and posts that data to the L1. Post-interop, the batcher collects up *local-unsafe* data for publishing.\n\nThe change here would be that the batcher should only batch up to the limit of what is *cross-unsafe*. Because if the batcher posts beyond this limit, it is possible that it includes an invalid interop message. Limiting the scope to just the *cross-unsafe* allows for the Sequencer to attempt to fix the local-unsafe chain without invalid messages being published.\n\nThere are risks associated that I will document in the ticket description though, we should not do this until we have one of a couple spec/design changes complete.", "2025-05-09T19:38:11Z", "2025-05-09T19:38:11Z", "axelKingsley", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6rJpL9", "I_kwDODjvEJM6ypLpZ", "@axelKingsley I see, sorry got my wires crossed there. \n\nIt would be good to think about how this behaviour ought to be controlled, e.g. by command line flags (or not). If a chain is not running interop yet, would it break things to have the batcher wait for _cross-unsafe_, or will _cross-unsafe_ just track _local-unsafe_ in that case? If the latter, we should be able to just change the behaviour for all chains and don't need to make it configurable (right)?\n\nWe do need to be sure though, since we already known of some edge cases where the sync status reported by op-node apparently violates the rules about cross safety -- namely that the local (un)safe >= cross (un)safe https://github.com/ethereum-optimism/optimism/issues/14576.", "2025-05-12T08:30:16Z", "2025-05-12T08:30:16Z", "geoknee", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6rthY0", "I_kwDODjvEJM6ypLpZ", "> [@axelKingsley](https://github.com/axelKingsley) I see, sorry got my wires crossed there.\n> \n> It would be good to think about how this behaviour ought to be controlled, e.g. by command line flags (or not). If a chain is not running interop yet, would it break things to have the batcher wait for _cross-unsafe_, or will _cross-unsafe_ just track _local-unsafe_ in that case? If the latter, we should be able to just change the behaviour for all chains and don't need to make it configurable (right)?\n> \n> We do need to be sure though, since we already known of some edge cases where the sync status reported by op-node apparently violates the rules about cross safety -- namely that the local (un)safe >= cross (un)safe [#14576](https://github.com/ethereum-optimism/optimism/issues/14576).\n\nI think I can answer my own question here:\n\nhttps://github.com/ethereum-optimism/optimism/blob/3a8c970d12e1f1fb867d6d89e1f6938ddffe6143/op-service/eth/sync_status.go#L36-L38\n\nSo we shouldn't need to make this configurable. But let's proceed with some caution given previous issues.", "2025-05-14T16:23:24Z", "2025-05-14T16:23:24Z", "geoknee", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6rt8R0", "I_kwDODjvEJM6ypLpZ", "I re-read your initial piece @axelKingsley and I'm not super clear on why we want this behaviour to be controlled by admin API or command line flag. \n\nIf `CrossUnsafe < LocalUnsafe` and the unsafe chain is _invalid_ then I agree we will get a safe head stall, because we will have a cross unsafe head stall. In that scenario, why would we want to flip the batcher to start loading unsafe (but not cross unsafe) blocks? Wouldn't that cause it to batch an invalid chain and make things worse?\n\nSeems to me that the mitigation of this scenario can happen entirely outside of the batcher. The sequencer causes the unsafe reorg and signals to the batcher by the cross unsafe head starting to move again. Perhaps the rest of the network learns about the unsafe reorg quickly, or only after the batcher posts the reorged chain, but again this is not a batcher concern. The batcher wouldn't know there was a reorg, because it is focussed on the cross unsafe head only, which just stalled for a time and then moved again. ", "2025-05-14T17:05:11Z", "2025-05-14T17:05:11Z", "geoknee", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6se-MN", "I_kwDODjvEJM6ypLpZ", "@axelKingsley @protolambda Talked about this briefly with @geoknee. Ideally we want to batch only cross-unsafe data, but we also don't want to stall batching so we don't risk safe head stalls. So what the batcher could do is wait for some configurable time x, say 1 min, for local unsafe blocks to become cross unsafe, and only then start batching the local unsafe blocks. This would allow, in the majority of cases, to batch cross-safe blocks but still fall back to local safe only as to not risk safe head stalls. We'd only batch local safe blocks if the local supervisor/node setup didn't make expected progress on a chain in the dependency set.", "2025-05-20T10:15:37Z", "2025-05-20T10:15:37Z", "sebastianst", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6sxZQF", "I_kwDODjvEJM6ypLpZ", "Closing the loop here since I think I understand the problem now. When I said above\n\n>  The sequencer causes the unsafe reorg and signals to the batcher by the cross unsafe head starting to move again\n\nI hadn't realised that this is actually not possible with the current protocol. The only way to issue a reorg is via the safe chain, i.e. with the batcher's help. So we really would be in a deadlock if we were waiting for a reorg that never comes. ", "2025-05-21T16:56:26Z", "2025-05-21T16:56:26Z", "geoknee", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6q1gyf", "I_kwDODjvEJM6yeNsV", "Is this still relevant? Does this block RC Beta? I think it is not needed?\n\nAlso, I do not recommend using `SuperSystem` or `interopgen`, we are looking to remove those. And we've op-devstack now, with op-deployer integration and better option lifecycle stages to configure changes like this.\n", "2025-05-09T11:12:31Z", "2025-05-09T11:12:31Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6regC5", "I_kwDODjvEJM6yeNsV", "We will need to do something whether it winds up being a small change in the existing op-e2e to get the test running or just do this as part of a migration of all the FP tests to dev stack.  But since we have the blob preimage test working with the pre-interop fault proofs which is testing the same code path this doesn't need to block RC beta.  Moved it to the testnet milestone so it doesn't get lost but we can revisit it then. Really it just needs to be done before we can remove the pre-interop FP support so it likely doesn't need to block shipping at all (but should be done before we consider the project done).", "2025-05-13T15:04:01Z", "2025-05-13T15:04:01Z", "ajsutton", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6skCoW", "I_kwDODjvEJM6yeNsV", "@ajsutton will you own this line of work?\n I saw that we moved the issue  into \"to-do\", but we are also saying it's not betanet critical?", "2025-05-20T16:32:01Z", "2025-05-20T16:32:09Z", "BlocksOnAChain", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6smYjE", "I_kwDODjvEJM6yeNsV", "@BlocksOnAChain I was not currently owning it.  Not sure why it moved to todo. Personally I'm comfortable that blob preimage has sufficient test coverage via the existing pre-interop tests and that the interop changes don't change that part of the code.\n\nSo I think we need to do this before we can remove the pre-interop fault dispute code but it doesn't need to block interop shipping.", "2025-05-20T20:24:32Z", "2025-05-20T20:24:32Z", "ajsutton", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM65CusF", "I_kwDODjvEJM6yeNsV", "Looks like it got closed incorrectly due to automation. Reopening.", "2025-07-22T19:15:58Z", "2025-07-22T19:15:58Z", "Inphi", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6l4Srq", "I_kwDODjvEJM6xLGlt", "This issue bothers me a lot, every PR I'm working around this. Let's try and fix it this cycle.", "2025-04-07T11:27:47Z", "2025-04-07T11:27:47Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6lTWcU", "I_kwDODjvEJM6w2vrh", "Open Decisions\n\n- Do we monitor both executing messages and ether balances or just executing messages? Given that the executing messages checks cover ether balances\n- Do we monitor the superchain set or do we need monitoring for chains that upgrade to interop that are outside of the superchain set since they can send messages to themselves?", "2025-04-02T17:57:38Z", "2025-04-02T17:57:38Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6pDQR7", "I_kwDODjvEJM6w2vrh", "My opinions on these open Decisions:\n\n> Do we monitor both executing messages and ether balances or just executing messages? Given that the executing messages checks cover ether balances\n\nWe would monitor both, because the quantity of executing messages can be in excess of the ether balance movement when interop messages are being used for other applications.\n\n> Do we monitor the superchain set or do we need monitoring for chains that upgrade to interop that are outside of the superchain set since they can send messages to themselves?\n\n we should monitor for any interop messages on mono-chains, though I'm not sure what to expect with them. There will probably be some usage just from cross-compatible apps deployed on multiple chains, but their utility would be limited so we should potentially expect very minimal traffic.\n", "2025-04-28T18:55:17Z", "2025-04-28T18:55:17Z", "axelKingsley", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6pDQr_", "I_kwDODjvEJM6w2vrh", "I also already have this task:\n\nhttps://github.com/ethereum-optimism/optimism/issues/15418\n\nWhich intersects with this somewhat. I think the definition of the runbooks will show off some of what we need to monitor in concrete terms.", "2025-04-28T18:56:04Z", "2025-04-28T18:56:04Z", "axelKingsley", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM65yFJ0", "I_kwDODjvEJM6wiTvt", "I think it'd be good to also expose `set-log-level API`, so that we can change the log-level of our services during runtime, without having to restart a process -- this will be especially useful for devnets, if we encounter an issue, and need to change the log-level from `INFO` to `TRACE` for example.", "2025-07-25T08:37:20Z", "2025-07-25T08:37:20Z", "nonsense", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM650Mwo", "I_kwDODjvEJM6wiTvt", "This issue is old and can be closed. The right way to adjust log level is based on context data, inside of the log handler method that pre-filters logs.\nThe handler layer that filters can be accessed via op-service via a log-mod unwrap traversal of the handler stack. This is already supported by devstack, to adjust log levels per service.", "2025-07-25T11:37:35Z", "2025-07-25T11:37:35Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM65YJPD", "I_kwDODjvEJM6wRaa8", "@smartcontracts I hit the following errors while trying to work on issue #15115.\n\n- GitHub Actions is disabled for this repository. To learn how to enable GitHub Actions, head to the [GitHub Docs](https://gh.io/copilot-coding-agent-enable-actions).\n\n\nAfter resolving these issues, you can ask me to try again by unassigning and then reassigning the issue to me again.\n\n<!-- copilot-coding-agent-error: issue-rules-error -->", "2025-07-23T20:43:48Z", "2025-07-23T20:43:48Z", "copilot-swe-agent", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6kQFZP", "I_kwDODjvEJM6v2h5K", "My suggestion would be to remove the block tag argument, and have the endpoint simply return the full dependency set config - pretty much just the content of depset.json that is given to op-supervisor. That format already includes support for the dependency set changing over time (adding chains at least) since it includes the activation time.", "2025-03-26T20:30:33Z", "2025-03-26T20:30:33Z", "ajsutton", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6oA05O", "I_kwDODjvEJM6v2h5K", "@protolambda - to review", "2025-04-21T15:27:40Z", "2025-04-21T15:27:40Z", "yoonchung8822", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6oBGiJ", "I_kwDODjvEJM6v2h5K", "I thought there's something like this in the op-supervisor already, but there's not. We can quite easily add it, if we agree on the format.\nIs proofs team blocked by this? Do we have any new ideas about this?\n\nAlso, the platforms ticket lacks a lot of information. Can we help unblock platforms in any way?\n", "2025-04-21T15:46:52Z", "2025-04-21T15:46:52Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6oBIF0", "I_kwDODjvEJM6v2h5K", "cross referencing the discord thread related with this API addition here: https://discord.com/channels/1244729134312198194/1354418948308992061/1354504255863783697. AFAIK this RPC implementation is not a blocker for anyone, but its nice to double check ", "2025-04-21T15:49:28Z", "2025-04-21T15:49:28Z", "pcw109550", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6oAoJ5", "I_kwDODjvEJM6vtWEb", "Sync up with Platform team, see where they are with their proxy and supervisor work. @mslipper ", "2025-04-21T15:16:47Z", "2025-04-21T15:16:47Z", "yoonchung8822", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6oAr2n", "I_kwDODjvEJM6vtWEb", "Corresponding Platforms proxyd ticket:\nhttps://github.com/ethereum-optimism/platforms-team/issues/729", "2025-04-21T15:21:22Z", "2025-04-21T15:21:22Z", "lewejahlil", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6oBsZ3", "I_kwDODjvEJM6vtWEb", "deleted the duplicate platforms proxyd ticket and moved https://github.com/ethereum-optimism/optimism/issues/14903 under our tracker", "2025-04-21T16:57:08Z", "2025-04-21T16:57:08Z", "zhwrd", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6kOm2T", "I_kwDODjvEJM6vAPmX", "Updates are being posted in https://github.com/flashbots/op-rbuilder/issues/25 by @sozinm", "2025-03-26T17:45:07Z", "2025-03-26T17:45:07Z", "K-Ho", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6khsvL", "I_kwDODjvEJM6vAPmX", "Need these 2 prs for interop support without builder slowdown:\nhttps://github.com/paradigmxyz/reth/pull/15316\nhttps://github.com/paradigmxyz/reth/pull/15313\n", "2025-03-28T06:01:02Z", "2025-03-28T06:01:02Z", "SozinM", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6okKRD", "I_kwDODjvEJM6ucH3I", "is this complete now @refcell ", "2025-04-24T15:28:32Z", "2025-04-24T15:28:32Z", "lewejahlil", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6iC-CW", "I_kwDODjvEJM6tBn-r", "Would the following be a satisfying solution?\n\nDefine top-level devnet test suites that each do the following:\n1. Spin up a network based on a specified config\n2. Check that the network satisfies certain requirements\n3. Run the devnet-sdk tests (as defined in gates in the op-acceptor yaml manifest) against this network\n\nThis design encodes the view that top-level configuration should define which devnet configurations are being tested, rather than the tests themselves.", "2025-03-12T17:59:57Z", "2025-03-12T17:59:57Z", "teddyknox", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6iLhdr", "I_kwDODjvEJM6suOdM", "Decided not to go into Isthmus", "2025-03-13T11:29:08Z", "2025-03-13T11:29:08Z", "sebastianst", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6o_v_d", "I_kwDODjvEJM6sGjYe", "This ticket should also cover making bindings to the new API available to the devstack front end, so that we can perform reorg testing with the test sequencer. \n", "2025-04-28T13:40:03Z", "2025-04-28T13:40:03Z", "geoknee", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6gGaOZ", "I_kwDODjvEJM6r0SRl", "Very cool idea! In the meantime you could also just hot-swap a service by changing the image and cmd like you mention.\nBit clunky to do right now, but once kurtosis has implemented https://github.com/kurtosis-tech/kurtosis/issues/2628 (Tedi is working on it atm) then you could just swap a service for its debug equivalent, with something like\n```\nkurtosis service update enclave_name service_name --image debug_mode_image --command <delve prefix...> -- <normal_command>\n```\n\nDagger for eg has this interactive debugging feature that drops you into a shell for debugging a container that is failing during CI: https://docs.dagger.io/features/debugging/\nWonder if kurtosis could offer something similar. It would have to understand that the image is a golang binary for eg, and then could potentially offer something like\n```\nkurtosis service update ... --remote-debug\n```\nwhich would add the delve prefix command (or the corresponding debugger for other languages)", "2025-02-26T19:46:48Z", "2025-02-26T19:46:48Z", "samlaf", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6gGgtB", "I_kwDODjvEJM6r0SRl", "Two things here as well:\n\n- An even better version of `https://github.com/kurtosis-tech/kurtosis/issues/2628` would be a watch mode on kurtosis `eg. `kurtosis run -w` where you tell kurtosis what docker images to track and if it detects changes to the docker image(eg. a new docker build), it will automatically restart the service in the enclave with that docker image. actually not too big of a lift to do if there's interest - it's essentially an automated version of @samlaf suggestion\n\n- regarding remote debugging - @leoporoli worked on adding remote debugging in kurtosis, leo do you have any docs on this you can share? All I could find was a loom debug video you created: https://www.loom.com/share/55a5f2b160fb42fb88170dd11f3a0906?sid=b6ade011-a873-420c-aa66-cd28086e313d", "2025-02-26T19:59:08Z", "2025-02-26T19:59:08Z", "tedim52", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6gGkN0", "I_kwDODjvEJM6r0SRl", "That's intriguing, it would definitely be helpful to debug an existing  deployment.\n\nI'm wondering about the next steps though. Presumably, some binaries will then have to be built and swapped-in. At this point, having to package them into containers seems like an annoyance.\n\nMaybe we could complement that with an ability to run \"proxy\" containers within the enclave that would then be bridged from the local dev environment so the process(es) of interest actually runs directly within your IDE", "2025-02-26T20:05:59Z", "2025-02-26T20:05:59Z", "sigma", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6gGpms", "I_kwDODjvEJM6r0SRl", "Oh I also like the proxy container idea. If there's some way to get all ports of the host process mapped to within the kurtosis cluster, then you can run and rebuild all kinds of dev builds without restarting containers.\n\nMost of our containers would probably just work with JSON-RPC proxies, only op-node / op-geth would require more raw proxies to bridge the P2P functionality.\n\nSpeaking of JSON-RPC proxies, I still need to complete my json-rpc side-project some time: https://github.com/protolambda/switcheroo  (The idea was to create a HTTP/WS server that you can mux/proxy any JSON-RPC with, and mess with the latency, error rate, etc.). A single endpoint like that, with HTTP routes for every possible JSON-RPC in the kurtosis cluster, would be pretty powerful to quickly interact with the kurtosis instances.\n\n", "2025-02-26T20:16:49Z", "2025-02-26T20:17:06Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6gd85o", "I_kwDODjvEJM6r0SRl", "Hey @tedim52, hey guys, yeap the instructions for enabling the remote debug with Goland in Kurtosis [are here](https://github.com/kurtosis-tech/kurtosis?tab=readme-ov-file#run-debug-instructions-for-golang-code-so-far).", "2025-03-01T13:35:14Z", "2025-03-01T13:35:14Z", "leoporoli", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6oO_yQ", "I_kwDODjvEJM6r0SRl", "Just wanted to drop back in here and say `kurtosis service update` is released on `1.7.1` if thats useful for this @protolambda (cc. @samlaf )", "2025-04-22T21:16:52Z", "2025-04-22T21:17:22Z", "tedim52", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6moRGf", "I_kwDODjvEJM6rCxKs", "Going to move this out of the betanet scope and label as an optimization", "2025-04-11T01:08:43Z", "2025-04-11T01:08:43Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6e_GAi", "I_kwDODjvEJM6qj6sP", "It would be nice if this was a configurable parameter that defaults to a sane value slightly larger than `max(expiry_window, finalization_period)`", "2025-02-19T02:07:34Z", "2025-02-19T02:07:34Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6mm9aq", "I_kwDODjvEJM6qj6sP", "We should deprioritize this as a must have for the betanet release. We will definitely want this as a future optimization", "2025-04-10T21:42:46Z", "2025-04-10T21:42:46Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6mnBkI", "I_kwDODjvEJM6prHcN", "Going to move this out of the betanet release. If we need to improve sync speeds then we can profile and figure out where the best optimizations are", "2025-04-10T21:54:04Z", "2025-04-10T21:54:04Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6mnAgs", "I_kwDODjvEJM6pDVIi", "This does not need to be a blocker for the betanet release, this is an optimization on top of the core functional software", "2025-04-10T21:51:27Z", "2025-04-10T21:51:27Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6kb9Gj", "I_kwDODjvEJM6pDKfS", "WIP PR here: https://github.com/ethereum-optimism/optimism/pull/15043", "2025-03-27T17:01:39Z", "2025-03-27T17:01:39Z", "unknown", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6q15Cl", "I_kwDODjvEJM6pDKfS", "Bumping this to testnet milestone. It's nice to have, but not an RC Beta blocker\n", "2025-05-09T11:50:56Z", "2025-05-09T11:50:56Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6q13Vw", "I_kwDODjvEJM6onZWq", "Bumping this from Beta to Testnet. It's test-scope only, and at least somewhat functional, so it's not sufficient priority to block RC Beta on. We should clean this up for long term maintainability and useful de-duplication to other testing work.\n", "2025-05-09T11:47:46Z", "2025-05-09T11:47:46Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6oJbJk", "I_kwDODjvEJM6onNeB", "Useful for production block-building, but not for testing or Interop RC Beta.\nRemoving the interop label for now.\n", "2025-04-22T11:34:44Z", "2025-04-22T11:34:44Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6muRqx", "I_kwDODjvEJM6onDqc", "I think the MVP solution for this is to define a standard error code/message that is returned from ingress that means that the transaction failed its access list validation and is going to be dropped. This catches the case in which a transaction declares an initiating message that does not exist and this is the only case that we care about filtering on. If an executing message is not declared in the access list, then the onchain transaction will revert. Tooling already exists for understanding reverting transactions (look at receipt status, or confirm that the executing message event exists)", "2025-04-11T14:55:09Z", "2025-04-11T14:55:09Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6lVqYI", "I_kwDODjvEJM6nVaIX", "This decision was made that the Proofs FMA would not be a blocker to scheduling the end 2 end audit or Betanet release.", "2025-04-02T23:03:33Z", "2025-04-02T23:03:33Z", "op-aaron", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6abJ-e", "I_kwDODjvEJM6mKYKI", "The concurrent tx setting is (I think?) called \"pending txs\" https://docs.optimism.io/builders/chain-operators/configuration/batcher#max-pending-tx . \n", "2025-01-14T18:28:23Z", "2025-01-14T18:28:23Z", "geoknee", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6XPIKs", "I_kwDODjvEJM6cGQMK", "op-reth tracking issue https://github.com/paradigmxyz/reth/issues/13145", "2024-12-11T22:23:12Z", "2024-12-11T22:23:12Z", "emhane", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6ZLy1B", "I_kwDODjvEJM6cGQMK", "~Hey \ud83d\udc4b  what do we think of including the consensus nonce change (as an alternative to the deposit queue)? I've created a tracking issue here: https://github.com/ethereum-optimism/optimism/issues/13543, hoping to have it code complete early next week.~\n\nMoved to Jovian", "2025-01-04T02:09:39Z", "2025-01-12T00:09:06Z", "mdehoog", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6Zrno0", "I_kwDODjvEJM6cGQMK", "could you please strike through withdrawals root list to reflect the decision made in yesterdays meeting to exclude it from isthmus. same with operator fee @smartcontracts ", "2025-01-08T18:29:08Z", "2025-01-08T18:29:08Z", "emhane", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6hqVTB", "I_kwDODjvEJM6cGQMK", "Regarding the Isthmus upgrade, do you already have a list of major changes planned? I\u2019d be interested in testing specific areas or providing feedback on certain functionalities. Let me know if there are particular areas where help is needed.", "2025-03-11T01:17:15Z", "2025-03-11T01:17:15Z", "bentrnr21", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6j9-WR", "I_kwDODjvEJM6cGQMK", "> General tracking for features proposed to be included in the Isthmus upgrade.\n> \n> ## Pre-requisite\n> * [ ]  [[Tracker] OPCM Upgrades (Upgrade 13)\u00a0#13069](https://github.com/ethereum-optimism/optimism/issues/13069)\n> \n> ## Isthmus Candidates\n> ### Smart contract work\n> * * [ ]  [EVM Engineering: Update all `0.8.15` code to `0.8.25`\u00a0#11527](https://github.com/ethereum-optimism/optimism/issues/11527)\n>     \n>     * [ ]  [Implementation](https://github.com/ethereum-optimism/optimism/pull/12356)* [x]  [PR: make Cannon libraries version agnostic](https://github.com/ethereum-optimism/optimism/pull/13175)[ ]  [L2 Genesis Simplification\u00a0#11558](https://github.com/ethereum-optimism/optimism/issues/11558)* [ ]  [EVM Engineering: Standardize a custom error convention\u00a0#12478](https://github.com/ethereum-optimism/optimism/issues/12478)\n>     \n>     * [ ]  [Design Doc](https://github.com/ethereum-optimism/design-docs/pull/143)* [ ]  Implementation* [ ]  [EVM Engineering: stylistic cleanup\u00a0#13058](https://github.com/ethereum-optimism/optimism/issues/13058)\n>     \n>     * [x]  [PR: import statement cleanup](https://github.com/ethereum-optimism/optimism/pull/13056)* [x]  [PR: move interfaces to new top-level folder](https://github.com/ethereum-optimism/optimism/pull/13114)* [x]  [[Tracker] EVM Engineering: solidity code coverage\u00a0#11093](https://github.com/ethereum-optimism/optimism/issues/11093)\n>     \n>     * [x]  [PR: fix stack too deep error in PermissionedDisputeGame](https://github.com/ethereum-optimism/optimism/pull/13141)* [x]  [PR: fix stack too deep error in MIPS](https://github.com/ethereum-optimism/optimism/pull/13137)* [x]  [PR: fix stack too deep in LibKeccak](https://github.com/ethereum-optimism/optimism/pull/13136)[ ]   [[Tracker] Proofs: Incident Response Improvements (Upgrade 13)\u00a0#12172](https://github.com/ethereum-optimism/optimism/issues/12172)[ ]  Include Custom Gas Token code\n> \n> ### Other features\n> * * [x]  [[Tracker] L2 withdrawals-root in block header (Isthmus)\u00a0#12044](https://github.com/ethereum-optimism/optimism/issues/12044)\n>     \n>     * [x]  [PR op-geth: Isthmus: Tests and misc updates for L2 withdrawals root](https://github.com/ethereum-optimism/op-geth/pull/399)* [x]  [PR op-geth: Isthmus: withdrawals root in block header](https://github.com/ethereum-optimism/op-geth/pull/451)* [x]  [PR: Config: Add support for Isthmus](https://github.com/ethereum-optimism/optimism/pull/12847)* [x]  [PR: Updates for withdrawals root in block header](https://github.com/ethereum-optimism/optimism/pull/13962)* [x]  [PR: op-program use withdrawalRoot from header if Isthmus](https://github.com/ethereum-optimism/optimism/pull/14180)* [ ]  [PR: Action Tests](https://github.com/ethereum-optimism/optimism/pull/12925)* [x]  [Operator Fee](https://github.com/ethereum-optimism/specs/pull/382)\n>     \n>     * [ ]  [Design Doc](https://github.com/ethereum-optimism/design-docs/pull/81/files)[ ]  [Header Accumulator](https://github.com/ethereum-optimism/specs/pull/259)[x]  [EIP-7702: Set EOA account code\u00a0specs#511](https://github.com/ethereum-optimism/specs/issues/511)\n> \n> ## Probably NGMI this time\n> * [ ]  [[Tracker] OP Stack: Support Pectra (Isthmus)\u00a0#12435](https://github.com/ethereum-optimism/optimism/issues/12435)[ ]  [Deposit Queue](https://github.com/ethereum-optimism/specs/pull/82)[ ]  [Decouple block times between L1 and L2 (or L2 and L3)](https://github.com/ethereum-optimism/specs/issues/416)\n> \n> NB - the above is all subject to further discussion and agreement; nothing is set in stone just yet. But we are no longer taking further submissions for Isthmus (unless they are extraordinarily important).\n\n", "2025-03-25T11:20:43Z", "2025-03-25T11:20:43Z", "Dustin4444", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6Q0J5k", "I_kwDODjvEJM6aLHVe", "The l1 origin update assertion on single increments got disproven by the `testCrossLayerUser` tests in `op-e2e/actions/helpers/user_test.go`. They panicked at the assertion. So even after Holocene, we sometimes see origin jumps larger than 1 during updates in the batch queue.", "2024-10-22T15:25:22Z", "2024-10-22T15:25:22Z", "sebastianst", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6PmGRK", "I_kwDODjvEJM6Z6evF", "Also just to note: we should make sure that Pectra activates on L1 _first_, not just that the spec is finalized because core devs don't really ever finalise a spec and there have been last minute changes and cancellations. We can still get the implementation ready and start governance but should make sure that:\r\n1. we retain the ability to abort activating Pectra on OP Stack if there are last minute changes to it on L1 (ie note this in the gov post)\r\n2. we don't upgrade the fault proofs absolute prestate (ie op-program) prior to Pectra activating. Partly because it includes the scheduled upgrade of the L2 but also partly to ensure that any new precompiles that are accelerated in the fork are actually present on L1 prior.\r\n\r\nIf there are new accelerated precompiles we need to deploy a new `PreimageOracle` that is used with the `FaultDisputeGame` contracts for the upgrade. No changes are required, it just needs to have a fresh state to ensure that preimages added from before the Pectra upgrade on L1 (where the precompiles just return 0) can't be used by games after Pectra updates (where it returns the actual response).", "2024-10-13T21:00:11Z", "2024-10-13T21:00:11Z", "ajsutton", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6P2I3h", "I_kwDODjvEJM6Z6evF", "Pectra is split into several releases with some unfinalized ones", "2024-10-15T09:12:27Z", "2024-10-31T05:40:07Z", "threewebcode", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6UyPkb", "I_kwDODjvEJM6Z6evF", "Separate from L2 Pectra Support, is there any additional work required to make sure that chains in the Superchain don't break when L1 activates Pectra? Thinking back to when some things broke back when Shanghai activated on L1.", "2024-11-24T19:42:53Z", "2024-11-24T19:42:53Z", "K-Ho", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6Wb8kF", "I_kwDODjvEJM6Z6evF", "With protocol team organization I think we can create time to implement Pectra support in Go soon. I am looking forward to test-running 7702!\nSee https://github.com/ethereum-optimism/pm/issues/11", "2024-12-06T18:25:08Z", "2024-12-06T18:25:08Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6X6mAf", "I_kwDODjvEJM6Z6evF", "Overview of Pectra changes and necessary OP-Stack integration changes: https://gist.github.com/protolambda/59b534d804c23dc7b3592ad1c38048d4", "2024-12-17T15:14:01Z", "2024-12-17T15:14:01Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6alRfZ", "I_kwDODjvEJM6Z6evF", "@emhane and myself have written up tickets in the specs repo for all the EIPs to be included as part of Pectra on L2.\nWe've based a lot of this off of proto's gist: https://gist.github.com/protolambda/59b534d804c23dc7b3592ad1c38048d4\nPlease feel free to comment and leave suggestions!\n\nHere is a tracking issue in specs for all the issues: https://github.com/ethereum-optimism/specs/issues/499\n\n--- Update ---\n\nhttps://notes.ethereum.org/@ethpandaops/pectra-devnet-5 is the up to date list of EIPs to be included in Pectra.", "2025-01-15T16:58:57Z", "2025-01-15T17:50:32Z", "refcell", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6cfzy7", "I_kwDODjvEJM6Z6evF", "@K-Ho and I would like to propose the following timeline for rolling out Pectra L2 support[*]:\n\n- 2/10: Pectra L2 is code complete\n- 2/17: Launch [Badger](https://github.com/ethereum-optimism/devnets/issues/9) alphanet\n- 2/24: Launch Balrog betanet\n- 2/27: Gov cycle starts\n- 3/3: Testnet release is cut\n- 3/10: Testnet activates and mainnet release is cut\n- 3/19: Gov cycle concludes\n- 3/19 - 3/26: Window for node operators to revert if proposal is rejected\n- 3/26: Mainnet activates\n\nThis is an acceleration of our standard rollout timeline, to ensure that Pectra L2 support lands in Cycle 34 and goes to mainnet in early April.\n\nNote that this requires doing a Sepolia testnet activation midway through the gov cycle (e.g., we bring the gov proposal before the testnet is ready, and we upgrade the testnet before voting is over). @K-Ho is working on a P/PS doc to make sure that we have buy-in on this approach.\n\n[*] Note also that Pectra L2 support is the work required so that OP Chains can benefit from Pectra. It is not the \"Pectra defense\" release that ensures that OP Chains keep working post-Pectra. ", "2025-01-30T21:00:14Z", "2025-01-30T21:02:38Z", "tessr", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6clM_i", "I_kwDODjvEJM6Z6evF", "@tessr checkout https://github.com/ethereum-optimism/pm/issues/23, here all the implementation issues are linked. you can see how many are still open. all the go implementations aren't tracked with individual issues, so it's a bit of a blackbox, and not as easy as for rust to say how far away go side is from being code complete for that reason. as you can see the only eip which is specd + implemented rn is 7251.", "2025-01-31T11:36:37Z", "2025-01-31T11:36:37Z", "emhane", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6s7X_p", "I_kwDODjvEJM6Z6evF", "@sebastianst ok to close this one now?", "2025-05-22T13:32:18Z", "2025-05-22T13:32:18Z", "BlocksOnAChain", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6XyF6d", "I_kwDODjvEJM6VrkU3", "This work would now be better described as \"Sync Node Pools\" per chain, where the supervisor can leverage multiple Managed Nodes to sync the data.", "2024-12-16T19:32:20Z", "2024-12-16T19:32:20Z", "axelKingsley", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6mk5EC", "I_kwDODjvEJM6VrkU3", "The work here is building out an RPC client that has multiple HTTP endpoints in it?", "2025-04-10T17:50:29Z", "2025-04-10T17:50:29Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6c_3vs", "I_kwDODjvEJM6MirOl", "Bumping this to testnet. Stable-devnet scope was limited to stability and reorg support. We should work towards this in the next milestone with priority.", "2025-02-04T13:39:39Z", "2025-02-04T13:39:39Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6f7Ajl", "I_kwDODjvEJM6MirOl", "Until StandardMode for read-only replica is a default thing this issue has little effect on the stack. Instead we can consider to make op-supervisor attach to nodes if the nodes report they are not managed already. That may provide some redundancy. But until there is clarity and capacity on that work I am going to bump this to the next milestone.\n", "2025-02-25T19:23:07Z", "2025-02-25T19:23:07Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6nAnVq", "I_kwDODjvEJM6MirOl", "Here's a document I wrote some time back to resolve confusion over the various \"modes\" we've talked about.\n\nhttps://www.notion.so/oplabs/Supervisor-Modes-Disambiguation-1bcf153ee1628084aeb0e52d5e1b9fc6\n", "2025-04-14T14:42:32Z", "2025-04-14T14:42:32Z", "axelKingsley", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6JjmNc", "I_kwDODjvEJM6MirR9", "We have too many conflicting op-supervisor tracking issues. The MVP issue https://github.com/ethereum-optimism/optimism/issues/10856 captures a good summary. Closing this.", "2024-08-23T21:05:36Z", "2024-08-23T21:05:36Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6K37uA", "I_kwDODjvEJM6MirXy", "Closing this issue to reduce cognitive overhead of large-scope issues. The remaining work, for devnet 3 (TBD), is now tracked in https://github.com/ethereum-optimism/optimism/issues/11744", "2024-09-04T20:32:54Z", "2024-09-04T20:32:54Z", "protolambda", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM59cQVT", "I_kwDODjvEJM6IbxEP", "Do you have an estimate on how much of a performance boost this would enable? \r\n\r\nIf i understand correctly, the solution involves adding a config file to the monorepo and then a config file to the host and references the config file in the monorepo? I wish there was a way around that second file", "2024-05-10T12:53:30Z", "2024-05-10T12:53:30Z", "tynes", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6Huy-p", "I_kwDODjvEJM6IbxEP", "> Do you have an estimate on how much of a performance boost this would enable?\r\n> \r\n> If i understand correctly, the solution involves adding a config file to the monorepo and then a config file to the host and references the config file in the monorepo? I wish there was a way around that second file\r\n\r\nI have re-scoped this issue with the latest (2.46) capabilities and also an entirely different approach that leverages using `--reference` and bare repo for storing an incremental cache for submodules\r\n\r\n\r\n", "2024-08-09T05:43:15Z", "2024-08-09T05:43:15Z", "sambacha", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6dAOEW", "I_kwDODjvEJM6IYeXe", "@trianglesphere ok to close this now, it's looking stale for over a year?", "2025-02-04T14:09:04Z", "2025-02-04T14:09:04Z", "BlocksOnAChain", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6BuAkh", "I_kwDODjvEJM6Mq_nl", "Candidate to be a public fix-it issue.", "2024-05-13T14:19:43Z", "2024-05-13T14:19:43Z", "alfonso-op", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6YpKTq", "I_kwDODjvEJM6Mq_nl", "Is this issue fixed? Having same problem with the sequencer drift. I updated the sequencer drift to higher value and now the l1Origin is stucked at l1Block: 14098814. Any possible solution? Thanks!\n\n<img width=\"933\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7ceb4a0a-2eaf-4f7c-84eb-e81dc3d6f9a5\" />\n", "2024-12-24T09:58:09Z", "2024-12-24T09:58:09Z", "dev-jb", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6cCJ7e", "I_kwDODjvEJM6Mq_nl", "Please unassign.", "2025-01-28T04:22:19Z", "2025-01-28T04:22:19Z", "anacrolix", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6dBMeM", "I_kwDODjvEJM6Mq_u2", "@sebastianst ok to close it, since it is stale for over 10 months now?", "2025-02-04T15:35:39Z", "2025-02-04T15:35:39Z", "BlocksOnAChain", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6dBXrR", "I_kwDODjvEJM6Mq_u2", "This may already have been fixed with some recent batcher changes. @geoknee is that the case?", "2025-02-04T15:52:10Z", "2025-02-04T15:52:10Z", "sebastianst", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6dNU2Q", "I_kwDODjvEJM6Mq_u2", "No I don't think this has been fixed. But I am also unsure how high priority this is right now. So I would be comfortable closing it as \"not planned\".\n\n\nI would also be happy to spend half a day working on it, wdyt @sebastianst?", "2025-02-05T17:03:43Z", "2025-02-05T17:03:43Z", "geoknee", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6dO63X", "I_kwDODjvEJM6Mq_u2", "Hmm, it's a real issue and maybe worth keeping it open so that if others are searching for this, they know at least that it's known. But no priority to work on this now.", "2025-02-05T20:15:51Z", "2025-02-05T20:16:05Z", "sebastianst", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6BuA8-", "I_kwDODjvEJM6Mq_-6", "Status Update:\r\n* We did split out our CL bootnodes from our EL bootnodes\r\n* We're not going to switch from enodes -> ENR\r\n* We did switch to V5 on the EL\r\n* We need to add a `opel` key with a chainID/forkID value to the EL V5.\r\n\r\n\r\nslack link: https://oplabs-pbc.slack.com/archives/C01HLRH0LKZ/p1713371903852319", "2024-04-18T15:57:28Z", "2024-04-18T15:57:28Z", "trianglesphere", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6ExRLB", "I_kwDODjvEJM6Mq_-6", "Relevant spec entry is [here](https://specs.optimism.io/protocol/exec-engine.html?highlight=opel#p2p-modifications).", "2024-07-14T22:49:37Z", "2024-07-14T22:49:37Z", "sebastianst", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6BuBu5", "I_kwDODjvEJM6MrArQ", "@axelKingsley is this still something we plan to do or we can close this one?", "2024-03-29T16:33:23Z", "2024-03-29T16:33:23Z", "BlocksOnAChain", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6BuBvC", "I_kwDODjvEJM6MrArQ", "Yes, this is architectural/tech-debt. No real expiration to the need.", "2024-05-10T19:15:32Z", "2024-05-10T19:15:32Z", "axelKingsley", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6BuB2u", "I_kwDODjvEJM6MrAyb", "We can probably ask an external contrib to do this as a good first issue since it's trivial.", "2024-02-19T17:42:10Z", "2024-02-19T17:42:10Z", "mslipper", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6BuCFB", "I_kwDODjvEJM6MrBAb", "The large preimage verification process would benefit from being able to request (non-contiguous) batches of blocks in a single call which this would help with.", "2024-01-30T20:19:08Z", "2024-01-30T20:19:08Z", "ajsutton", "2025-08-31 06:33:20"]
["IC_kwDODjvEJM6dBKxJ", "I_kwDODjvEJM6MrBAb", "@axelKingsley is this still something we plan to work on, since it's stale issue from over a year ago?", "2025-02-04T15:33:17Z", "2025-02-04T15:33:17Z", "BlocksOnAChain", "2025-08-31 06:33:20"]
["IC_kwDOKIwiaM65HG1V", "I_kwDOKIwiaM7CAIgh", "---\ntitle: \"Your Title Here\"\ntags: [\"tag1\", \"tag2\"]\ndescription: \"A short description of the content.\"\n---", "2025-07-23T04:29:20Z", "2025-07-23T04:29:20Z", "killers10", "2025-08-31 06:34:44"]
["IC_kwDOKIwiaM6fb9GD", "I_kwDOKIwiaM6q-FSo", "Hi @Leoforever123! Thanks for the feedback; I'm gonna tag @krofax who's more familiar with these tutorials.\n\nWe'll take a look at getting this updated if needed and get back to you when fixed. Sorry you've had issues with out docs!", "2025-02-21T15:50:06Z", "2025-02-21T15:50:06Z", "bradleycamacho", "2025-08-31 06:34:44"]
["IC_kwDOKIwiaM6h9v37", "I_kwDOKIwiaM6q-FSo", "Hi @bradleycamacho any update on above query as i am also stuck on this issue.", "2025-03-12T10:01:49Z", "2025-03-12T10:01:49Z", "ahmedshehzadBiM", "2025-08-31 06:34:44"]
["IC_kwDOKIwiaM6h91Jl", "I_kwDOKIwiaM6q-FSo", "@ahmedshehzadBiM thanks for nudging.\nI\u2019m going to prioritize looking into this today, and update the instructions.", "2025-03-12T10:08:56Z", "2025-03-12T10:08:56Z", "krofax", "2025-08-31 06:34:44"]
["IC_kwDOKIwiaM6jI8sK", "I_kwDOKIwiaM6q-FSo", "> [@ahmedshehzadBiM](https://github.com/ahmedshehzadBiM) thanks for nudging. I\u2019m going to prioritize looking into this today, and update the instructions.\n\nWe are eager for waiting the update from the docs too. ", "2025-03-19T15:16:45Z", "2025-03-19T15:16:45Z", "huyngopt1994", "2025-08-31 06:34:44"]
["IC_kwDOKIwiaM6jMT8p", "I_kwDOKIwiaM6q-FSo", "Hi @krofax thank you. Looking forward to it. Further there are many other things as well in the tutorial doc that need your attention to be streamlined. It would be helpful.\n", "2025-03-19T19:44:16Z", "2025-03-20T05:32:53Z", "ahmedshehzadBiM", "2025-08-31 06:34:44"]
["IC_kwDOKIwiaM67Mk8J", "I_kwDOKIwiaM6q-FSo", "@Leoforever123 Apology for the delay, we have long updated this [doc](https://docs.optimism.io/operators/chain-operators/deploy/smart-contracts), feel free to raise this issue again, if you encounter any blocker in the guide.", "2025-07-31T16:40:45Z", "2025-07-31T16:40:45Z", "krofax", "2025-08-31 06:34:44"]
["IC_kwDOL-xLQ86vNAxv", "I_kwDOL-xLQ865xPnC", "@scharissis let's also use this ticket to update the devnet repo [issue template](https://github.com/ethereum-optimism/devnets/blob/main/.github/ISSUE_TEMPLATE/devnet-request.yml) to include a required field outlining the specific funding requirements for that devnet.", "2025-06-04T10:09:08Z", "2025-06-04T10:09:18Z", "alfonso-op", "2025-08-31 08:02:45"]
["IC_kwDOL-xLQ865J-v3", "I_kwDOL-xLQ863qW-V", "Related:\n* https://github.com/ethereum-optimism/optimism/pull/16550\n* https://github.com/ethereum-optimism/infra/pull/428", "2025-07-23T08:07:21Z", "2025-07-23T08:07:21Z", "scharissis", "2025-08-31 08:02:45"]
["IC_kwDOL-xLQ86_c61K", "I_kwDOL-xLQ863qW-V", "Closing since I no longer believe kurtosis is important for network testing vs sysgo.", "2025-08-21T20:48:42Z", "2025-08-21T20:48:42Z", "teddyknox", "2025-08-31 08:02:45"]
["IC_kwDOI7W0xc6gcasH", "I_kwDOI7W0xc6sKBIi", "Adding `SuperchainTokenBridge` so that you can capture who is sending/receiving tokens. Otherwise `tx_from/to` for the perspective of the `L2ToL2CrossDomainMessenger` is just the `SuperchainTokenBridge`", "2025-03-01T01:09:37Z", "2025-03-01T01:09:37Z", "hamdiallam", "2025-08-31 08:02:46"]
["IC_kwDOKIwiaM6-yn2P", "I_kwDOKIwiaM694cYB", "@cshein45 What page are you referring to?", "2025-08-19T14:12:35Z", "2025-08-19T14:12:35Z", "krofax", "2025-08-31 08:02:48"]
["IC_kwDOJ_r-bs6U7xRT", "I_kwDOJ_r-bs6eo8EE", "One area that seems particularly brittle is the parsing of the standard TOML files. Coupled with the approach we take to ignore empty structs in several places, small typos break things easily. See #705.\n\n", "2024-11-25T18:02:51Z", "2024-11-25T18:02:51Z", "geoknee", "2025-08-31 08:02:59"]
["IC_kwDOJ_r-bs6U7ylI", "I_kwDOJ_r-bs6eo8EE", "The direction I would like us to take on this is to:\n* move as much of the complexity as possible into pure functions which live in regular `.go` files (NOT `_test.go` files)\n* this formalizes that code as production code, not test code\n* this makes the code easy to test without putting fake data on chain\n* we add codecov to this repo and start ramping up the coverage of this newly ordained \"production code' by writing tests for it", "2024-11-25T18:04:39Z", "2024-11-25T18:04:39Z", "geoknee", "2025-08-31 08:02:59"]
["IC_kwDOJ_r-bs6XLH2V", "I_kwDOJ_r-bs6eo8EE", "@bitwiseguy to create sub issues for different validation tests we want to add tests for.", "2024-12-11T15:13:17Z", "2024-12-11T15:13:17Z", "alfonso-op", "2025-08-31 08:02:59"]
["IC_kwDOJ_r-bs6_oWOs", "I_kwDOJ_r-bs6eo8EE", "@bitwiseguy when you're back, let's (you, me and Tess) chat about whether there are any validation tests in the list above marked as \"not planned\" that we still want to implement.", "2025-08-22T17:01:27Z", "2025-08-22T17:01:27Z", "alfonso-op", "2025-08-31 08:02:59"]
["IC_kwDOMMiGhs6oX7bD", "I_kwDOMMiGhs6zrmGC", "Also, I think this is causing an issue with how wagmi reads `eth_getTransactionReceipt`.\nAlthough the rpc returns a receipt, and I see it in Chrome's network tab, Wagmi doesn't give me back a receipt", "2025-04-23T16:19:48Z", "2025-04-23T16:19:48Z", "OscBacon", "2025-08-31 08:03:11"]
["IC_kwDOMMiGhs6_LcSK", "I_kwDOMMiGhs6zrmGC", "How can I suggest a fix for this?", "2025-08-20T17:43:20Z", "2025-08-20T17:43:20Z", "OBlackmon3", "2025-08-31 08:03:11"]
["IC_kwDOMMiGhs6_wrcp", "I_kwDOMMiGhs6zrmGC", "[@smartcontracts ", "2025-08-23T17:40:02Z", "2025-08-23T17:40:02Z", "Hazimehhassan", "2025-08-31 08:03:11"]
["IC_kwDOKIsnqM6-E-11", "I_kwDOKIsnqM7GCsYs", "Happy to help here with sync time or async. I'll also make myself available for PR review. ", "2025-08-14T15:58:17Z", "2025-08-14T15:58:17Z", "blmalone", "2025-08-31 08:03:25"]
["IC_kwDODjvEJM6-obGP", "I_kwDODjvEJM7FSb5h", "First one: #17122", "2025-08-18T20:09:41Z", "2025-08-18T20:09:41Z", "joshklop", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6_LIlL", "I_kwDODjvEJM7FSb5h", "Second one: https://github.com/ethereum-optimism/optimism/pull/17165", "2025-08-20T17:21:02Z", "2025-08-20T17:21:02Z", "joshklop", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6_Tqnl", "I_kwDODjvEJM7FSb5h", "https://github.com/ethereum-optimism/optimism/pull/16997, https://github.com/ethereum-optimism/optimism/pull/17161", "2025-08-21T08:50:07Z", "2025-08-21T08:50:07Z", "pcw109550", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM69tqI9", "I_kwDODjvEJM7Apal9", "Reopening to track testcase impl for the eth namespace", "2025-08-13T09:03:49Z", "2025-08-13T09:03:49Z", "pcw109550", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM63Rw7p", "I_kwDODjvEJM7Ac5Xu", "This has been depriortised now in favor of getting a flashhblocks stably running on OP Labs Sepolia infra first.", "2025-07-15T18:21:09Z", "2025-07-15T18:21:09Z", "yashvardhan-kukreja", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM63Rx9P", "I_kwDODjvEJM7Ac5Xu", "Hence, the issue #16685   is a blocker to it", "2025-07-15T18:21:50Z", "2025-07-16T02:05:26Z", "yashvardhan-kukreja", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6-x_A7", "I_kwDODjvEJM7Ac5Xu", "@yashvardhan-kukreja \n\nAll the sequencers (voting and non-voting) for sepolia on Sunnyside are running with Flashblocks, and are connected to the websocket proxy. The failover is not tested directly on sepolia, but has been tested on eris-0 devnet.\n\nHappy to mark this resolved on our side!", "2025-08-19T13:28:38Z", "2025-08-19T13:28:38Z", "parkgunou", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6sXLBM", "I_kwDODjvEJM62qAGf", "We need to figure out how the acceptance tests can identify the op-challenger's EOA. So that we can assert that the invalid proposal was disputed by the op-challenger.", "2025-05-19T17:20:54Z", "2025-05-19T17:21:02Z", "Inphi", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6sZE7Z", "I_kwDODjvEJM62qAGf", "Let's assume whoever interacts with the chain is the op-challenger. It's unlikely to be anything else in a freshly minted public devnet.", "2025-05-19T21:01:43Z", "2025-05-19T21:01:43Z", "Inphi", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6919ie", "I_kwDODjvEJM62qAGf", "There are a few TODOs left that point to this ticket.  Reopening while those get addressed.", "2025-08-13T18:30:56Z", "2025-08-13T18:30:56Z", "mbaxter", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6_dXpV", "I_kwDODjvEJM62qAGf", "The last TODO for this issue was addressed in https://github.com/ethereum-optimism/optimism/pull/16994 - closing as complete.", "2025-08-21T21:34:05Z", "2025-08-21T21:34:05Z", "mbaxter", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6r7t4B", "I_kwDODjvEJM62p-p3", "@Inphi  - Maybe some improvement we can add to the sub-issue level is to add assignees for each and add dates to it, so we know how this Parent issue results in an XL Interop Workstreams?\nI think adding that would tell a full story about who, when and how for the proofs work for RC Betanet.\nCC: @pauldowman for visibility on this idea.", "2025-05-15T17:24:40Z", "2025-05-15T17:24:40Z", "BlocksOnAChain", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6r70hU", "I_kwDODjvEJM62p-p3", "> [@Inphi](https://github.com/Inphi) - Maybe some improvement we can add to the sub-issue level is to add assignees for each and add dates to it, so we know how this Parent issue results in an XL Interop Workstreams? I think adding that would tell a full story about who, when and how for the proofs work for RC Betanet. CC: [@pauldowman](https://github.com/pauldowman) for visibility on this idea.\n\nWe'll figure out assignees next Monday. I'm not confident adding dates at this time though. But we will begin working on this next week.", "2025-05-15T17:36:04Z", "2025-05-15T17:36:04Z", "Inphi", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6sZD-k", "I_kwDODjvEJM62p-p3", "We can begin writing op-challenger and op-proposer acceptance tests by validating using gostack as the backend.", "2025-05-19T20:59:44Z", "2025-05-19T20:59:44Z", "Inphi", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6_dYWe", "I_kwDODjvEJM62p-p3", "All sub-issues are complete - closing.", "2025-08-21T21:35:20Z", "2025-08-21T21:35:20Z", "mbaxter", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6utWBN", "I_kwDODjvEJM6xau5F", "Deprioritizing this following conversations with Protocol pending use-cases for this testing vs using the test-sequencer with sysgo.", "2025-06-02T14:53:47Z", "2025-06-02T14:54:51Z", "teddyknox", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6uyS1E", "I_kwDODjvEJM6xau5F", "@teddyknox Re-opening this as there's a todo in the code:\n```\nRepository & Issue                       | Title                                                             | Location                                \n-----------------------------------------+-------------------------------------------------------------------+---------------------------------------------------\nethereum-optimism/optimism #15265        | op-test-sequencer: add to devnet env and kurtosis                 | op-devstack/sysext/system.go:71                   \n```", "2025-06-02T20:42:52Z", "2025-06-02T20:42:52Z", "ajsutton", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM69O4vV", "I_kwDODjvEJM6xau5F", "this is required to run reorg tests in kona-supervisor devstack env", "2025-08-11T13:16:17Z", "2025-08-11T13:16:17Z", "emhane", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6nrz24", "I_kwDODjvEJM6ts6zp", "Consider using sourcify, which other block explorers are integrated with and pull data from: https://docs.sourcify.dev/docs/how-to-verify/", "2025-04-17T15:12:48Z", "2025-04-17T15:12:48Z", "bitwiseguy", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6_byRO", "I_kwDODjvEJM6ts6zp", "Once we finish implementing forge binary support (instead of custom golang evm implementation) we should revisit op-deployer contract verification such that it shells out to `forge verify-contract` to verify against etherscan and blockscout. We should be able to get rid of some of our custom verification code and only keep the parts that help pass the correct args to that forge command. \n\nExample code that shows `forge verify-contract` commands for OP contracts [here](https://github.com/celo-org/celo-monorepo/tree/def7ffd6e137e7da1c1f9f3a60f3cd0e2dc46aea/packages/op-tooling/verify).", "2025-08-21T18:54:21Z", "2025-09-29T14:15:02Z", "bitwiseguy", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6MyriR", "I_kwDODjvEJM6Tues5", "Update: \r\n- The state snapshots issue has been addressed [here](https://github.com/ethereum-optimism/optimism/issues/11694).\r\n- There is a write up on memory merkleization [here](https://github.com/ethereum-optimism/optimism/issues/11695)", "2024-09-19T20:15:40Z", "2024-09-19T20:15:40Z", "mbaxter", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6eBtgO", "I_kwDODjvEJM6Tues5", "Memory merklization is being addressed in https://github.com/ethereum-optimism/optimism/pull/14292", "2025-02-11T15:56:56Z", "2025-02-11T15:56:56Z", "pauldowman", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6gaXYO", "I_kwDODjvEJM6Tues5", "Chainsafe is working on this.", "2025-02-28T18:11:41Z", "2025-02-28T18:11:41Z", "BlocksOnAChain", "2025-08-31 08:03:31"]
["IC_kwDODjvEJM6s7mcr", "I_kwDODjvEJM6Tues5", "Related PR for this project, adding for visibility: https://github.com/ethereum-optimism/optimism/pull/16030", "2025-05-22T13:49:34Z", "2025-05-22T13:49:34Z", "BlocksOnAChain", "2025-08-31 08:03:31"]
["IC_kwDOL-xLQ86xtgDF", "I_kwDOL-xLQ865xg5L", "Added the L1 Faucet support [here](https://github.com/ethereum-optimism/optimism/pull/16423)", "2025-06-17T18:53:31Z", "2025-06-17T18:53:31Z", "serpixel", "2025-09-04 02:24:57"]
["IC_kwDOL-xLQ86r9LTp", "I_kwDOL-xLQ862aHq_", "A script for generating a custom absolute prestate is provided here - https://www.notion.so/oplabs/Script-Permissionless-Devnet-Spinup-1f1f153ee16280748c5ee22cc845584a", "2025-05-15T20:08:45Z", "2025-05-15T20:08:45Z", "Inphi", "2025-09-04 02:24:57"]
["IC_kwDOL-xLQ87BsO_t", "I_kwDOL-xLQ862aHq_", "Can we close this?", "2025-09-03T14:53:16Z", "2025-09-03T14:53:16Z", "pauldowman", "2025-09-04 02:24:57"]
["IC_kwDOH2Qg5s7BnPRA", "I_kwDOH2Qg5s7C6nj1", "Specs https://specs.optimism.io/protocol/jovian/overview.html", "2025-09-03T08:49:23Z", "2025-09-03T08:49:23Z", "geoknee", "2025-09-04 02:25:01"]
["IC_kwDOH2Qg5s6St09q", "I_kwDOH2Qg5s6cPmZJ", "I get this too. Have you find a way to solve ?", "2024-11-07T07:16:26Z", "2024-11-07T07:16:26Z", "feynman-x", "2025-09-04 02:25:01"]
["IC_kwDOH2Qg5s6St3g9", "I_kwDOH2Qg5s6cPmZJ", "I got this too, then i did the step 'build/bin/geth init --datadir=datadir genesis.json' again, it worked fine.", "2024-11-07T07:22:57Z", "2024-11-07T07:22:57Z", "lei335", "2025-09-04 02:25:01"]
["IC_kwDOH2Qg5s6TGQYe", "I_kwDOH2Qg5s6cPmZJ", "> I get this too. Have you find a way to solve ?\r\n\r\njust continue with the setup, the head block error is not important", "2024-11-11T11:00:36Z", "2024-11-11T11:00:36Z", "SyedMuhamadYasir", "2025-09-04 02:25:01"]
["IC_kwDOH2Qg5s6e5G09", "I_kwDOH2Qg5s6cPmZJ", "Hi, @SyedMuhamadYasir \nHow to check the geth is fully synced?\nI tried on the documentation, but it shows eth.blockNumber is 0 consistently\n", "2025-02-18T13:47:41Z", "2025-02-18T13:47:41Z", "0x4k-helios", "2025-09-04 02:25:01"]
["IC_kwDOH2Qg5s6hSjml", "I_kwDOH2Qg5s6cPmZJ", "I'm having this same issue , i get this error and carried on but geth is always at Block 0 and when i connect and propose a block from Beacon i get errors that the parent is nil , which seems linked I'm using the latest builds , has anyoen setup a PoS Testnet recently and isn't seen these issues ?", "2025-03-07T09:47:07Z", "2025-03-07T09:47:07Z", "mikelear", "2025-09-04 02:25:01"]
["IC_kwDOH2Qg5s7BzRhl", "I_kwDOH2Qg5s6cPmZJ", "I got this too", "2025-09-04T02:05:06Z", "2025-09-04T02:05:06Z", "cliff0412", "2025-09-04 02:25:01"]
["IC_kwDODjvEJM7BjRO4", "I_kwDODjvEJM7JUN70", "Why is this a problem - it's setting the values to what was specified in the config that it was told to set the config from.  It would be really useful to get the actual steps you're going through that leads to this code's behaviour causing a problem.", "2025-09-03T00:38:06Z", "2025-09-03T00:38:06Z", "ajsutton", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BrR92", "I_kwDODjvEJM7JUN70", "Hi @ajsutton, that's a fair question. I happen to work on a fork of optimism, and in maintaining that fork I found that in updating one dependency (`github.com/libp2p/go-libp2p-pubsub v0.12.0 => v0.14.2`) I was suddenly seeing some tests panic.\n\nIn particular, [TestDiscovery](https://github.com/ethereum-optimism/optimism/blob/develop/op-node/p2p/host_test.go#L265) (this is skipped in your develop branch, but isn't in mine) and at least one other I can't recall. The message is as follows:\n\n```\npanic: runtime error: slice bounds out of range [4:0]\n\ngoroutine 198 [running]:\ngithub.com/libp2p/go-libp2p-pubsub.(*GossipSubRouter).heartbeat(0xc000576fc8)\n\t/home/jwood/go/pkg/mod/github.com/libp2p/go-libp2p-pubsub@v0.14.2/gossipsub.go:1538 +0x265c\ngithub.com/libp2p/go-libp2p-pubsub.(*PubSub).processLoop(0xc000577208, {0x2161b88, 0x343aae0})\n\t/home/jwood/go/pkg/mod/github.com/libp2p/go-libp2p-pubsub@v0.14.2/pubsub.go:861 +0xc45\ncreated by github.com/libp2p/go-libp2p-pubsub.NewPubSub in goroutine 52\n\t/home/jwood/go/pkg/mod/github.com/libp2p/go-libp2p-pubsub@v0.14.2/pubsub.go:543 +0xea5\nFAIL\tgithub.com/ethereum-optimism/optimism/op-node/p2p\t0.137s\nFAIL\n```\n\nI initially thought that this was a bug in `github.com/libp2p/go-libp2p-pubsub` since generally code should never see a panic like this. I filed an [issue](https://github.com/libp2p/go-libp2p-pubsub/issues/634) there, but the reality is that the code there makes a few assumptions about the config that if met would make this perfectly safe - one of which is that `Dscore` must be less than `Dhi`. Thanks to MarcoPolo over there, I found that in this test `Dhi=0` and `Dscore=4`\n\nThat led me to the test setup [here](https://github.com/ethereum-optimism/optimism/blob/develop/op-node/p2p/host_test.go#L288) where the config is created directly in the test and `MeshDHi` is unset (thus defaults to 0). I reasoned that not setting this value was meant to leave it at the default, which [is set to 12](https://github.com/ethereum-optimism/optimism/blob/develop/op-node/p2p/gossip.go#L42). However, as I point out in the description this default is clobbered by the 0 that was (unintentionally) set in the setup of the test.", "2025-09-03T13:50:48Z", "2025-09-03T13:50:48Z", "wood-jp", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7ByINM", "I_kwDODjvEJM7JUN70", "hmm, a config with 0 for those values is actually invalid:\nhttps://github.com/ethereum-optimism/optimism/blob/41c6e19f7c82ca3926927c09677b5464c6cb7851/op-node/p2p/config.go#L190-L221\n\nI feel like if we try to make invalid configs work we're going to wind up in a game of whack-a-mole and have to introduce a bunch of extra conditionals in a lot of places.  What do you think about adding a `DefaultP2PConfig` method that returns a fully filled out, valid P2PConfig with sensible defaults and then these tests can just modify the options they need rather than specifying everything from scratch? That way if we add new options in the future we can just add them in the `DefaultP2PConfig` method and all tests gets updated etc.", "2025-09-03T23:44:26Z", "2025-09-03T23:44:26Z", "ajsutton", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BzMAh", "I_kwDODjvEJM7JUN70", "> hmm, a config with 0 for those values is actually invalid\n\nYes, exactly. That's why the PR I provided assumes that a 0 should actually be the sane default.\n\n> What do you think about adding a DefaultP2PConfig method that returns a fully filled out, valid P2PConfig with sensible defaults and then these tests can just modify the options they need rather than specifying everything from scratch? \n\nThat could also work, but this requires future devs to know this exists and that they should use it vs the easy way of directly creating a Config struct and filling in only what you care about. I've seen this pattern in many other places, I just don't find it idiomatic. The Config struct is rather large through, so this may end up the better solution in the end - especially if it might grow even larger.\n\nThe PR I've provided is how I've chosen to solve the problem on my end. I just felt the need to give back to the source. If you want to go a different route, that's also cool and I'll eventually adopt it I'm sure if for no other reason than to minimize drift.", "2025-09-04T01:56:27Z", "2025-09-04T01:56:27Z", "wood-jp", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BT_dB", "I_kwDODjvEJM7Iv49k", "I'm not quite sure what \"It needs a change to only look at specific game types\" means but it doesn't seem right.  dispute-mon *must* return an error if it encounters a game it does not support. Note that you can only ever create a game if an implementation for that game type has been set - you can't just create arbitrary game types. We will need to update dispute-mon to support cannon-kana game types.", "2025-09-01T20:55:30Z", "2025-09-01T20:55:30Z", "ajsutton", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BhseK", "I_kwDODjvEJM7Iv49k", "Changed to say \"Update to support cannon-kona games.\"", "2025-09-02T21:25:01Z", "2025-09-02T21:25:01Z", "pauldowman", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BeYaO", "I_kwDODjvEJM7Iv3WF", "Maybe nothing to do, let's confirm before closing.", "2025-09-02T16:09:33Z", "2025-09-02T16:09:33Z", "pauldowman", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BjF7i", "I_kwDODjvEJM7Iv3WF", "It needs to support deploying the new game type.  Same contracts but at least this if block will need to be updated:\nhttps://github.com/ethereum-optimism/optimism/blob/d24cf93fed41b1f4ba912ef8ccf58c250d1cd75c/packages/contracts-bedrock/src/L1/OPContractsManager.sol#L402-L431", "2025-09-03T00:12:04Z", "2025-09-03T00:12:04Z", "ajsutton", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BjNuH", "I_kwDODjvEJM7IvqFh", "I'd go with:\n```\n[[prestates.\"1.0.2\"]]\ntype = \"cannon-kona\"\nhash = \"0x0333bca6b4388310837e65d0e0749aa878a0cd2696933ec4ee39cc509e8a5101\"\n```\n\nThe version number doesn't need to match up with op-program but it won't matter if they ever happen to be the same either (we already have multiple op-program prestates of different types for the same release).", "2025-09-03T00:28:52Z", "2025-09-03T00:28:52Z", "ajsutton", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BsQ58", "I_kwDODjvEJM7Hb9US", "Gathering some more information:\n\n(from @bitwiseguy )\n1. What are the highest value monorepo ci jobs you would like to be able to run locally? I suspect `kurtosis` jobs and `go-tests-short` since they're some of the longest ones but trying to verify that.\n2. Does most of the friction come from the fact that ci automates prerequisite setup steps before running the tests, whereas locally you have to manually piece together the setup before running some tests?\n3. Would you find it valuable to automate validation of circleci config.yml syntax?\n\n(from @teddyknox)\n1. Getting the proofs tests running locally would be useful, since they have a lot of prerequisite dependency steps that make them harder to run. In general there's always the off chance you push some code that breaks proofs, but it's unlikely you'd catch this before pushing to origin with a local test run. go-tests-short runs locally via makefile but is no longer short, I think we have some work to do recategorizing the tests within it. If we wanted to solve this once and for all, we'd have a tool to update go testing manifests automatically by:\n  a. Timing the test run of each go package\n  b. Sorting ascending\n  c. Greedily packing these packages into manifest files for 1m, 5m, 10m, 20m, 1h testing jobs\n  d. Creating those 1m, 5m, 10m, 20m, 1h testing jobs that consume these manifest files\n2. Yes, exactly. This, plus the fact that some build steps are not cached I believe.\n3. This might be helpful for engineers modifying the circleci config in the future but in general that seems like a different problem to be solving than users looking to run tests locally.\n\n(from @bitwiseguy)\n\nInteresting idea about dynamically sorting the go tests by test time. A few questions on that:\n1. By \"go testing manifests\" you mean [these make recipes](https://github.com/ethereum-optimism/optimism/blob/develop/Makefile#L255-L278)?\n2. Perhaps this could work by a ci job querying circleci api for test statistics of last n runs, then sorting the tests into group based on that? Or you want that sorting to happen locally?\n3. Are you thinking test jobs that take longer than a given threshold (>10m?) would run as post-merge instead of pre-merge? Or what's the main benefit you hope to get from sorting them?\n\n(from @teddyknox)\n1. These manifests would be new files that specify lists of go packages, and would pair with new go make recipes that correspond to the test tasks of different durations.\n2. Feels like something we should run locally so that the tests are sorted based on laptop run duration rather than CI run duration. Ideally it would be a script we run locally once in a while which updates the manifests. Then we could commit the updated manifests into VCS to share.\n3. Yes, past a different threshold test jobs should probably run post-merge. The main benefit I see is that a large part of why devs don't run tests locally at the moment is because they take too long, so triaging the tests by duration would allow devs to run the greatest number of tests in the time budget they have (by selecting which test job to run, i.e. which duration). Ideally the jobs would reference each other, so if you run the 1h+ job you are running all tests.", "2025-09-03T14:55:23Z", "2025-09-03T14:55:23Z", "bitwiseguy", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7Bw5Vf", "I_kwDODjvEJM7F4Hds", "Go 1.25.1 is released: https://go.dev/doc/devel/release#go1.25.1", "2025-09-03T21:11:47Z", "2025-09-03T21:11:47Z", "pauldowman", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7ByB2P", "I_kwDODjvEJM7F4Hds", "But it doesn't appear to include the MIPS fix: https://github.com/golang/go/issues?q=milestone%3AGo1.25.1+label%3ACherryPickApproved", "2025-09-03T23:26:22Z", "2025-09-03T23:26:22Z", "ajsutton", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BNLbo", "I_kwDODjvEJM7FrsJx", "```bash\n$ cast decode-error 0x014f6fe57bbbf0d8eba98101aac254652b30209b11c3b47156dcc4f475fa134a7209ae9d      \nGameAlreadyExists(bytes32)\n0x7bbbf0d8eba98101aac254652b30209b11c3b47156dcc4f475fa134a7209ae9d\n```\n\nSeems to be the `GameAlreadyExists` error and I am observing a lot of this as well. Running `op-proposer/v1.10.0`.", "2025-09-01T08:46:13Z", "2025-09-01T08:46:48Z", "eshaan7", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BUBhA", "I_kwDODjvEJM7FrsJx", "This is expected. If the safe head hasn't advanced there is no new valid proposal so proposals will fail with GameAlreadyExists because it is proposing the same output as last time.  Ensure that the batcher is running correctly and consider shortening it's maximum channel duration if you want to propose more often.", "2025-09-01T21:03:28Z", "2025-09-01T21:03:28Z", "ajsutton", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM7BsYlr", "I_kwDODjvEJM7Apbxc", "Reopening since we did not test each HF transition (yet)", "2025-09-03T15:03:43Z", "2025-09-08T11:13:00Z", "pcw109550", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM6zOIfY", "I_kwDODjvEJM65xGGP", "Related:\n\n- https://github.com/ethereum-optimism/infra/pull/411\n- https://github.com/ethereum-optimism/optimism/pull/16549\n- https://github.com/ethereum-optimism/optimism/pull/16550", "2025-06-26T02:24:30Z", "2025-06-27T01:50:34Z", "scharissis", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM6Ttu8j", "I_kwDODjvEJM6df1bM", "@smartcontracts @pauldowman is the current plan for @ControlCplusControlV to take this line of work in Q4 2024?", "2024-11-15T08:39:39Z", "2024-11-15T08:39:39Z", "BlocksOnAChain", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM6T190t", "I_kwDODjvEJM6df1bM", "> @smartcontracts @pauldowman is the current plan for @ControlCplusControlV to take this line of work in Q4 2024?\n\nYes. ", "2024-11-16T04:43:28Z", "2024-11-16T04:43:28Z", "pauldowman", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM6UCdqv", "I_kwDODjvEJM6df1bM", "I'm coordinating with @ControlCplusControlV on this", "2024-11-18T17:27:02Z", "2024-11-18T17:27:02Z", "smartcontracts", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM6cQB1k", "I_kwDODjvEJM6df1bM", "@ControlCplusControlV how are we feeling on this issue, do you think it's realistic to close it + all drippie related tasks until the 7th of Feb and start fresh from next spring on the Interop proofs side?", "2025-01-29T11:59:34Z", "2025-01-29T11:59:34Z", "BlocksOnAChain", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM6cTajZ", "I_kwDODjvEJM6df1bM", "7th of Feb I think is a good goal, we need some time to see everything is working on Sepolia, so that might be an arbitrary delay but I don't think it means much in terms of additional work", "2025-01-29T17:23:36Z", "2025-01-29T17:23:36Z", "ControlCplusControlV", "2025-09-04 02:25:12"]
["IC_kwDODjvEJM6cmqvt", "I_kwDODjvEJM6df1bM", "Ok great, I will keep an eye on this issue to track our progress on closing this project and remaining tasks we have here.", "2025-01-31T13:41:51Z", "2025-01-31T13:41:51Z", "BlocksOnAChain", "2025-09-04 02:25:12"]
["IC_kwDOLyTYAM69H8-0", "I_kwDOLyTYAM7DKZyv", "This is complete, see https://www.notion.so/oplabs/MinBaseFee-Threat-Model-241f153ee162807dbc0ff1f598059a46.", "2025-08-10T23:56:29Z", "2025-08-10T23:56:29Z", "mslipper", "2025-09-04 03:00:45"]
["IC_kwDOLyTYAM69Qph5", "I_kwDOLyTYAM7FUN9l", "Napkin math: blocks get an extra byte for the `minBaseFeeLog2`, so we have\n```\n  (1 byte / 2 seconds) x 365 days \u00d7 24 hours \u00d7 60 minutes \u00d7 60 seconds = 15,768,000 bytes in 1 year.\n``` \n\n16MB per year", "2025-08-11T14:59:50Z", "2025-08-11T14:59:50Z", "geoknee", "2025-09-04 03:00:46"]
["IC_kwDOLyTYAM66zLqC", "I_kwDOLyTYAM7C5zY6", "Hi @sebastianst \nCould you give me chance to work with you?", "2025-07-29T20:33:56Z", "2025-07-29T20:33:56Z", "moonfury-ops", "2025-09-04 03:00:47"]
["IC_kwDONqt9zM68PL7q", "I_kwDONqt9zM7Eb9HY", "SuperCat: app_bd741f81dc7e30a9286ce8e3d4c4a39f\nNFT: app_34e4c5f54bd0e73eedc8cb7f6ca9a584\nClaim Race: app_16c96f4c461235b11db52c6d8376d128\n\nSince multiple mini apps on World, the project is called \"World Mini Apps\". These are all in store.", "2025-08-06T08:15:59Z", "2025-08-06T08:15:59Z", "sexychinese", "2025-09-04 03:01:05"]
["IC_kwDONqt9zM68Zvig", "I_kwDONqt9zM7Eb9HY", "Hi @sexychinese thanks for your issue. We're looking into this now.", "2025-08-06T16:43:59Z", "2025-08-06T16:43:59Z", "ccerv1", "2025-09-04 03:01:05"]
["IC_kwDONqt9zM68aH-_", "I_kwDONqt9zM7Eb9HY", "Issue has been resolved: #73 ", "2025-08-06T17:20:51Z", "2025-08-06T17:20:51Z", "ccerv1", "2025-09-04 03:01:05"]
["IC_kwDONqt9zM67v2af", "I_kwDONqt9zM63ncl2", "Addressed during M5; https://github.com/ethereum-optimism/Retro-Funding/pull/69", "2025-08-04T09:52:09Z", "2025-08-04T09:52:09Z", "ccerv1", "2025-09-04 03:01:05"]
["IC_kwDOLyTYAM7EMVQT", "I_kwDOLyTYAM7KlBq1", "https://miro.com/app/board/uXjVJIlXd1Q=/", "2025-09-15T10:53:58Z", "2025-09-15T10:53:58Z", "geoknee", "2025-10-13 20:32:16"]
["IC_kwDOKSJyfM7D8cdZ", "I_kwDOKSJyfM7JR-2p", "I'm so sorry not to assist you \ud83d\ude14", "2025-09-13T02:49:24Z", "2025-09-13T02:49:24Z", "divine9549", "2025-10-13 20:32:46"]
["IC_kwDOFpg0Ns7EtfhF", "I_kwDOFpg0Ns7L1ZgO", "@superbridgeapp isn't a maintainer for this repository.\n\nBut, my assumption is [0x431151F26c48B4D4BEFc34048a0966Bf960A0c50](https://basescan.org/address/0x431151F26c48B4D4BEFc34048a0966Bf960A0c50) is not an OptimismMintableERC20, that's why the generated entry doesn't include a bridge address.\n\nIf the implementation could be verified we can confirm", "2025-09-16T20:31:27Z", "2025-09-16T20:31:27Z", "AlexBHarley", "2025-10-13 20:33:13"]
["IC_kwDOKIwiaM6hOjzf", "I_kwDOKIwiaM6s7UR1", "Hi @opfocus , thanks for this suggestion! I'll look into it early next week and get it implemented soon once approved!", "2025-03-06T21:10:09Z", "2025-03-06T21:10:09Z", "bradleycamacho", "2025-10-13 20:33:22"]
["IC_kwDOKIwiaM6hnM6D", "I_kwDOKIwiaM6s7UR1", "I encountered an error while executing the following step,\nMaybe` publicClientL1.buildInitiateWithdrawal` should be used here[tested].\nI noticed that there seems to be no introduction to the `buildWithdrawalTransaction` function in the `viem `documentation\n```\nconst withdrawalArgs = await publicClientL2.buildWithdrawalTransaction({\n                                            ^\n\nTypeError: publicClientL2.buildWithdrawalTransaction is not a function\n    at file:///D:/crosss-dom/index.js:20:45\n    at ModuleJob.run (node:internal/modules/esm/module_job:218:25)\n    at async ModuleLoader.import (node:internal/modules/esm/loader:329:24)\n    at async loadESM (node:internal/process/esm_loader:34:7)\n    at async handleMainPromise (node:internal/modules/run_main:113:12)\n```\n", "2025-03-10T18:00:55Z", "2025-03-10T18:00:55Z", "opfocus", "2025-10-13 20:33:22"]
["IC_kwDOKIwiaM6h0DWI", "I_kwDOKIwiaM6s7UR1", "> I encountered an error while executing the following step, Maybe` publicClientL1.buildInitiateWithdrawal` should be used here[tested]. I noticed that there seems to be no introduction to the `buildWithdrawalTransaction` function in the `viem `documentation\n> \n> ```\n> const withdrawalArgs = await publicClientL2.buildWithdrawalTransaction({\n>                                             ^\n> \n> TypeError: publicClientL2.buildWithdrawalTransaction is not a function\n>     at file:///D:/crosss-dom/index.js:20:45\n>     at ModuleJob.run (node:internal/modules/esm/module_job:218:25)\n>     at async ModuleLoader.import (node:internal/modules/esm/loader:329:24)\n>     at async loadESM (node:internal/process/esm_loader:34:7)\n>     at async handleMainPromise (node:internal/modules/run_main:113:12)\n> ```\n\nHello @opfocus , thanks for pointing this, i'll look into this and update accordingly.", "2025-03-11T15:31:04Z", "2025-03-11T15:31:04Z", "krofax", "2025-10-13 20:33:22"]
["IC_kwDOKIwiaM7DpK7A", "I_kwDOKIwiaM6s7UR1", "Updated this tutorial", "2025-09-11T19:25:57Z", "2025-09-11T19:25:57Z", "krofax", "2025-10-13 20:33:22"]
["IC_kwDOL-xLQ87FBiuy", "I_kwDOL-xLQ87LKkym", "Done as part of https://github.com/ethereum-optimism/infra/pull/463", "2025-09-18T05:46:18Z", "2025-09-18T05:46:18Z", "scharissis", "2025-10-13 20:33:29"]
["IC_kwDOL-xLQ87INGVM", "I_kwDOL-xLQ8645VUA", "Done in https://github.com/ethereum-optimism/infra/pull/463", "2025-10-02T03:20:44Z", "2025-10-02T03:20:44Z", "scharissis", "2025-10-13 20:33:29"]
["IC_kwDOL-xLQ87E13He", "I_kwDOL-xLQ87MMRu8", "by #464 ", "2025-09-17T10:59:06Z", "2025-09-17T10:59:06Z", "geoknee", "2025-10-13 20:33:35"]
["IC_kwDOH2Qg5s7CPC5P", "I_kwDOH2Qg5s7Jc0Uk", "The Genesis-State-Spec (it's just the \"alloc\" part of `genesis.json`) is sort of optional and does not have to be stored in geth. From what I can tell it's used primarily when initializing a geth node with a live tracer (a sidecar-like program that does additional processing for the chain, e.g. to index data), but that's not used much.\n\nI think it's best if you mod op-geth for the one-time import of state, by:\n- not storing this alloc data as flat json blob (comment out the `rawdb.WriteGenesisStateSpec` call)\n- still do the other work of importing the state into the DB\n\nAnd additionally, you should include the `stateHash` attribute of the genesis block into the chain-`config` part of the `genesis.json`: this way fresh nodes start with an empty DB but valid genesis-block header, and can snap-sync the state from other bootstrap nodes. Without that `stateHash`, and without a full genesis state, it wouldn't otherwise be able to determine the state-root attribute of the genesis block.\n\nThis is basically how op-mainnet works, by migrating it once, and then doing the DB init in new nodes in a way that can sync from the bootstrapped nodes. We never actually embed the full genesis state alloc into the op-geth binary. (7GB op-geth binary or docker image would be way too big anyway). But thanks to the `stateHash` attribute, the correct genesis block can be loaded, and from there it can snap-sync state of another node.\n\nThere might be some other minor quirks when importing state or starting the node, just let me know, it should be possible to resolve by following the same strategy as with op-mainnet.\n", "2025-09-05T15:14:50Z", "2025-09-05T15:14:50Z", "protolambda", "2025-10-13 20:33:46"]
["IC_kwDOH2Qg5s7ClULU", "I_kwDOH2Qg5s7Jc0Uk", "thanks for ur reply. we will not save the genesis.json", "2025-09-08T04:32:57Z", "2025-09-08T04:32:57Z", "cliff0412", "2025-10-13 20:33:46"]
["IC_kwDOH2Qg5s7DzgWC", "I_kwDOH2Qg5s64KcB3", "Think it has gone lower after finishing log indexing introduced by 1.15.x of Geth.\n\nIf anyone face similar issues just track logs with \"log index\"", "2025-09-12T12:15:48Z", "2025-09-12T12:15:48Z", "cpuchainorg", "2025-10-13 20:33:46"]
["IC_kwDOH2Qg5s7CPwb-", "I_kwDOH2Qg5s7C6yYY", "via https://github.com/ethereum-optimism/op-geth/pull/666", "2025-09-05T16:10:15Z", "2025-09-05T16:10:15Z", "geoknee", "2025-10-13 20:33:50"]
["IC_kwDOH2Qg5s7CPwWl", "I_kwDOH2Qg5s7C6nj1", "Via https://github.com/ethereum-optimism/op-geth/pull/666", "2025-09-05T16:10:07Z", "2025-09-05T16:10:07Z", "geoknee", "2025-10-13 20:33:50"]
["IC_kwDOH2Qg5s7CPwPn", "I_kwDOH2Qg5s7C6eoE", "Via https://github.com/ethereum-optimism/op-geth/pull/666", "2025-09-05T16:09:58Z", "2025-09-05T16:09:58Z", "geoknee", "2025-10-13 20:33:50"]
["IC_kwDOL-xLQ87CF1lZ", "I_kwDOL-xLQ87JptDX", "We currently have l2 ELs exposed in CI for mainnet and sepolia on the following networks\n\nunichain-sepolia-rpc.optimism.io\nunichain-mainnet-rpc.optimism.io\nink-sepolia-rpc.optimism.io\nink-mainnet-rpc.optimism.io\nbase-sepolia-rpc.optimism.io\nbase-mainnet-rpc.optimism.io\n\nWhat is remaining is exposing an L1 beacon API in CI. I will get that done shortly tomorrow or early next latest\n", "2025-09-04T23:22:23Z", "2025-09-05T01:51:09Z", "jelias2", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87CF5m9", "I_kwDOL-xLQ87JptDX", "I've modifed the synctest to be a bit more dynamic and take in envirnoment variables. Please see this draft PR. \n\nIgnore the linting changes. I will fix those. \n\nPlease see the updated circle ci job with parameters [link](https://github.com/ethereum-optimism/optimism/pull/17334/files#diff-78a8a19706dbd2a4425dd72bdab0502ed7a2cef16365ab7030a5a0588927bf47R2627) and updated synctest to read from environment variable [link](https://github.com/ethereum-optimism/optimism/pull/17334/files#diff-b478e9e439ad93bca17ac7a1ba5ef955169b3940f807f6997bafc25820726620R15)\n\nI left a couple of TODOs and the logging needs to be fixed as well. Feel free to make changes off my branch for how you would like to see these implemented.\n\n", "2025-09-04T23:25:26Z", "2025-09-04T23:25:26Z", "jelias2", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87C7yv2", "I_kwDOL-xLQ87JptDX", "Thanks @jelias2 ! I think at the moment we are missing only:\n\n- [ ] L1 ETH Mainnet beacon API in CI (to be used for Base and OPM)\n\nTests are already running daily, we'll refactor them a bit over the next sprint, but overall this is mostly done! :)", "2025-09-09T12:19:09Z", "2025-09-09T13:01:53Z", "nonsense", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87C8T3b", "I_kwDOL-xLQ87JptDX", "L1 Beacon API may be shared for all L2s that use ETH Mainnet L1s, so \n- L1 beacon API in CI for Base Mainnet == L1 beacon API in CI for OP Mainnet", "2025-09-09T12:48:31Z", "2025-09-09T12:48:31Z", "pcw109550", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87C9MoC", "I_kwDOL-xLQ87JptDX", "Mainnet L2 RPCs: https://base-mainnet-rpc.optimism.io, https://ink-mainnet-rpc.optimism.io, https://unichain-mainnet-rpc.optimism.io", "2025-09-09T13:45:52Z", "2025-09-09T13:45:52Z", "jelias2", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87C9RkH", "I_kwDOL-xLQ87JptDX", "Thank you!", "2025-09-09T13:50:45Z", "2025-09-09T13:50:45Z", "nonsense", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ865BKhF", "I_kwDOL-xLQ87B7T3w", "- Sysgo we have\n- Kurtosis we have #401 but the amount of work there is non-trivial so we could leave it out of scope for this ticket\n- Could we maybe have this as a manual CircleCI job?", "2025-07-22T17:49:43Z", "2025-07-22T17:49:43Z", "serpixel", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87C5Gqn", "I_kwDOL-xLQ865xg5L", "Some of these are still failing on Eris; created tickets to follow up on them:\n\n- [TestERC20Bridge](https://github.com/ethereum-optimism/optimism/issues/17385)\n- [TestOperatorFeeDevstack](https://github.com/ethereum-optimism/optimism/issues/17387)", "2025-09-09T09:29:49Z", "2025-09-09T09:29:49Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87DtmZd", "I_kwDOL-xLQ862aHq_", "Checked with @jelias2 - implemented elsewhere/differently; closing.", "2025-09-12T03:23:11Z", "2025-09-12T03:23:11Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ86y4v6C", "I_kwDOL-xLQ862UH4l", "https://github.com/ethereum-optimism/infra/issues/344", "2025-06-24T17:00:01Z", "2025-06-24T17:00:01Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87DsO_b", "I_kwDOL-xLQ86qxm5N", "Done", "2025-09-12T01:03:25Z", "2025-09-12T01:03:25Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87DsQ8E", "I_kwDOL-xLQ86qR-5M", "No longer wanted.", "2025-09-12T01:05:01Z", "2025-09-12T01:05:01Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87DzDhb", "I_kwDOL-xLQ86pEMA3", "closing since isthmus has already shipped", "2025-09-12T11:39:43Z", "2025-09-12T11:39:43Z", "emhane", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87DfcMh", "I_kwDOL-xLQ86n5PHx", "No longer required; closing.\nConfirmed with @ajsutton .", "2025-09-11T10:29:59Z", "2025-09-11T10:29:59Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87DfcH3", "I_kwDOL-xLQ86n5PAT", "No longer required; closing.\nConfirmed with @ajsutton .", "2025-09-11T10:29:54Z", "2025-09-11T10:29:54Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87DfcBC", "I_kwDOL-xLQ86n5O0_", "No longer required; closing.\nConfirmed with @ajsutton .", "2025-09-11T10:29:48Z", "2025-09-11T10:29:48Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87DfbvK", "I_kwDOL-xLQ86n5OX4", "No longer required; closing.\nConfirmed with @ajsutton .", "2025-09-11T10:29:35Z", "2025-09-11T10:29:35Z", "scharissis", "2025-10-13 20:33:57"]
["IC_kwDOL-xLQ87FBhse", "I_kwDOL-xLQ87LKPLV", "Done in https://github.com/ethereum-optimism/infra/pull/463", "2025-09-18T05:44:11Z", "2025-09-18T05:44:11Z", "scharissis", "2025-10-13 20:33:59"]
["IC_kwDOH2Qg5s7Dzjrt", "I_kwDOH2Qg5s7LRJIt", "Full log is not available since Geth is crashed with logs filled with Receipt Correction", "2025-09-12T12:20:18Z", "2025-09-12T12:20:23Z", "cpuchainorg", "2025-10-13 20:34:26"]
["IC_kwDOH2Qg5s7EMtcW", "I_kwDOH2Qg5s7LRJIt", "@cpuchainorg thanks for your bug report! We can confirm that we have experienced this issue as well when snap-syncing an op-mainnet node. We're investigating this and report back here soon.", "2025-09-15T11:22:17Z", "2025-09-15T11:22:17Z", "sebastianst", "2025-10-13 20:34:26"]
["IC_kwDOH2Qg5s7EOD69", "I_kwDOH2Qg5s7LRJIt", "@cpuchainorg please give [this fix](https://github.com/ethereum-optimism/op-geth/pull/680) a try.", "2025-09-15T12:50:43Z", "2025-09-15T12:50:43Z", "sebastianst", "2025-10-13 20:34:26"]
["IC_kwDOH2Qg5s7FqLnf", "I_kwDOH2Qg5s7LRJIt", "Thanks I think the issue is fixed with the latest version patch", "2025-09-21T18:18:49Z", "2025-09-21T18:18:49Z", "cpuchainorg", "2025-10-13 20:34:26"]
["IC_kwDOH2Qg5s7G0rsY", "I_kwDOH2Qg5s7N6fWc", "We may also want to move additional checks into the Engine API checks, like\n- Optimism chains must have a force-included L1 attributes deposit tx\n- Jovian must have 178 bytes long L1 attributes deposit data (this tx data is read during payload building preparation to extract the `daFootprintScalar`)", "2025-09-25T19:46:09Z", "2025-09-25T19:46:09Z", "sebastianst", "2025-10-13 20:34:29"]
["IC_kwDOH2Qg5s7F38WS", "I_kwDOH2Qg5s7NIjFS", "This was fixed upstream, so it should be resolved by the next upstream merge https://github.com/ethereum/go-ethereum/pull/32579. ", "2025-09-22T15:34:49Z", "2025-09-22T15:34:49Z", "geoknee", "2025-10-13 20:34:32"]
["IC_kwDOL-xLQ87CPxS9", "I_kwDOL-xLQ86oj5jr", "Given our shift towards prioritizing other local devnet environments, will close this request.", "2025-09-05T16:11:34Z", "2025-09-05T16:11:34Z", "alfonso-op", "2025-10-13 20:34:36"]
["IC_kwDOLB-lzc7FbGfR", "I_kwDOLB-lzc7MrIoZ", "Note on address collision resistance of the suggestion:\n\n- `CREATE` addresses are computed as: `keccak256(rlp([sender, nonce]))` (the RLP never starts with many zeroes, and certain ranges of bytes, alike to legacy-tx-typing)\n-  `CREATE2` addresses are computed as: `keccak256( 0xff ++ address ++ salt ++ keccak256(init_code))[12:]` (the 0xff separates it from the RLP)\n- upgrade source-hashes are computed as: `keccak256(bytes32(uint256(2)), keccak256(intent))` (and I assume the `sourceHash[12:]` as address here)\n\nSo the start of the keccak preimage always differs (non-zero RLP byte in legacy create case, `0xff` in eth create2 case, `0x000...002` in this update case) and so there are no possible address collisions.\n\n\nIf we want to be extra-safe, we could use the `CREATE2` computation, set the deployer address to a system-address, and the source-hash as salt. Then it fits in the existing address computation framework, and won't collide in the future in unexpected ways, and users are stopped from colliding with it by using an inaccessible system address as create2-deployer.\n", "2025-09-19T13:31:06Z", "2025-09-19T13:34:41Z", "protolambda", "2025-10-13 20:34:58"]
["IC_kwDOLB-lzc7Eq5HF", "I_kwDOLB-lzc7Lx1O6", "Suggestion from @sebastianst  https://github.com/ethereum-optimism/specs/pull/767#discussion_r2352850909", "2025-09-16T16:48:25Z", "2025-09-16T16:48:25Z", "joshklop", "2025-10-13 20:34:58"]
["IC_kwDOLB-lzc7FqWCp", "I_kwDOLB-lzc7M8-Q9", "Not updating", "2025-09-21T19:27:14Z", "2025-09-21T19:27:14Z", "ajit2903", "2025-10-13 20:35:08"]
["IC_kwDOLB-lzc7Ci4Hm", "I_kwDOLB-lzc7KKFIw", "Add", "2025-09-07T17:27:16Z", "2025-09-07T17:27:16Z", "ajit2903", "2025-10-13 20:35:27"]
["IC_kwDODjvEJM7DijIX", "I_kwDODjvEJM7JyMbt", "@smartcontracts Hi, I opened PR that would close this issue", "2025-09-11T13:15:42Z", "2025-09-11T13:15:42Z", "MozirDmitriy", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DZCJY", "I_kwDODjvEJM7JwbRf", "This is was realised to be a non-blocker considering L2 being safe from it. Hence, closing it in that favor.", "2025-09-11T05:54:45Z", "2025-09-11T05:54:45Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7C2o_o", "I_kwDODjvEJM7Js9gP", "Done after the completion of #17318 ", "2025-09-09T06:40:27Z", "2025-09-09T06:40:27Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7C2omM", "I_kwDODjvEJM7Js8cD", "Completed by the following PRs:\n- https://github.com/ethereum-optimism/k8s/pull/7246\n- https://github.com/ethereum-optimism/k8s/pull/7247\n- https://github.com/ethereum-optimism/k8s/pull/7253", "2025-09-09T06:39:55Z", "2025-09-09T06:39:55Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7B87vF", "I_kwDODjvEJM7Js7jj", "Associated PR until now:\n- https://github.com/ethereum-optimism/k8s/pull/7233\n- https://github.com/ethereum-optimism/k8s/pull/7239\n- https://github.com/ethereum-optimism/k8s/pull/7212\n\nRemaining work:\n- Deploy rollup-boost on this sequencer with disabled execution_mode\n- Upon observing the stability of flashblocks, enable the rollup-boost's execution mode.", "2025-09-04T14:20:42Z", "2025-09-04T14:20:42Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DanRa", "I_kwDODjvEJM7Js7jj", "Updates on remaining work:\n- https://github.com/ethereum-optimism/k8s/pull/7307\n- Enabled rollup-boost on sequencers on mainnet as well.", "2025-09-11T06:46:54Z", "2025-09-11T06:46:54Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DNgKR", "I_kwDODjvEJM7IgFGD", "Closing as duplicate of https://github.com/ethereum-optimism/optimism/issues/17260", "2025-09-10T13:56:00Z", "2025-09-10T13:56:00Z", "mbaxter", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7B8qf4", "I_kwDODjvEJM7G8vab", "Associated PRs:\n- https://github.com/ethereum-optimism/specs/pull/703\n- https://github.com/ethereum-optimism/specs/pull/754", "2025-09-04T14:05:43Z", "2025-09-04T14:05:43Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7Dcp2U", "I_kwDODjvEJM7GzVRh", "Done and it's working healthily.\n\nhttps://github.com/ethereum-optimism/k8s/pull/7325", "2025-09-11T08:08:25Z", "2025-09-11T08:08:25Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7CrIeg", "I_kwDODjvEJM7GyIKl", "I think this is quite easy to solve in Grafana. I updated the dashboard to only plot values ` > 0`. \n\n(Also added another axis showing the raw / unnormalized data with a floating axis maximum, makes it easier to see activations). ", "2025-09-08T12:33:53Z", "2025-09-08T12:33:53Z", "geoknee", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7CrI2J", "I_kwDODjvEJM7GyIKl", "<img width=\"1499\" height=\"316\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/72617dec-cd40-44cb-9923-2cddb666bc77\" />", "2025-09-08T12:34:20Z", "2025-09-08T12:34:20Z", "geoknee", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7CshFu", "I_kwDODjvEJM7GyIKl", "oh that's a nice solution, thanks!", "2025-09-08T13:53:51Z", "2025-09-08T13:53:51Z", "sebastianst", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DJFuW", "I_kwDODjvEJM7Gwh0j", "Sat down with @yashvardhan-kukreja yesterday about this. The TLDR is that while the process of building and delivering flashblocks container images to our registry should be improved both for reliability and for security, there is no novel risk to treat here. We currently accept the same risk for all other images, including both first party images produced in CI and 3rd party images. **The best short term mitigation for this risk is to hardcode the image digest of the latest image that was manually built in the kubernetes deployment manifests, meaning an attacker would have to open a PR in `k8s` and seek approval to replace existing images with a malicious image.**\n\n@yashvardhan-kukreja explained that there are 2 flashblocks and 2 coinbase container images that OP engineers have to manually (from their workstations) clone flashblocks github repos, build locally and push to our registries, as these are not built and distributed by flashbots upstream. This manual process provides attack surface in various stages: before the build, during the build, after the build (See  [SLSA](https://slsa.dev/spec/v1.1/threats-overview) taxonomy). For instance:\n- attackers may substitute source that goes in the build before the build happens\n- the `docker` binary on the workstation may be compromised and insert a backdoor at build time\n- a malicious developer may build from a tampered source on purpose\n- a malicious developer may poison an image in the registry previously pushed\n\nWhile this is concerning, this does not worsen our security posture as we already currently do not treat this risk when it comes to all other container images. We can distinguish between:\n1. 1st party images we build in CI\n2. 1st party images we build manually (such as these flashblocks images)\n3. 3rd party images we pull from public registries (traefik, redis...)\n\nOur clusters currently cannot discriminate between these 3 categories. An image built on a workstation is indistinguishable from an image built in CI. There is no trustworthy evidence to make a decision on. The long term solution is provenance based admission control and we plan to work on this in H2. Some build systems can generate cryptographically signed provenance attestations at build time, and attach this evidence to the built images. This attestation for images built in CI captures metadata such as repository of origin, git commit, branch, what CI workflow was executed and on what CI infrastructure (runner, cloud...) the build happened. Signing means that both the provenance and the image cannot be tampered after the build. Only when all our images are built in CI  with this attestation we can then deploy an admission controller that does not allow deployment of images built in circumstances deemed unsafe... For instance we may not allow deployment of images built on workstations in production clusters.\n\nPS: for 3rd party images stored in public registries, there is an additional risk that they become unavailable. This is for instance the case with the https://github.com/ethereum-optimism/platforms-team/issues/1001", "2025-09-10T08:41:44Z", "2025-09-10T09:07:14Z", "falcorocks", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DcpNF", "I_kwDODjvEJM7GweAB", "We already have alerts tracking the healthiness of all the sequencers, which would include the backup sequencers' health tracking as well.", "2025-09-11T08:07:50Z", "2025-09-11T08:07:50Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7Cq8H_", "I_kwDODjvEJM7EhQsD", "My suggestion is that we lean into the templating system to do this overriding automatically. So we would write something like image: {{ $remote_images.op-geth-go-mod }}, and have that resolve to a Docker image at a URL we construct from the githash in go.mod. Since we build op-geth Docker images for every commit we ought to be able to pull an image in this way, as long as there it has finished building. Alternatively, we could just allow the source code to be cloned somewhere temporary (again using the go.mod githash) and a local Docker image built that way (using existing templating syntax).", "2025-09-08T12:21:17Z", "2025-09-08T12:21:17Z", "geoknee", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7Dcqzf", "I_kwDODjvEJM7C_jKk", "Solved here - https://github.com/ethereum-optimism/k8s/pull/7298", "2025-09-11T08:09:22Z", "2025-09-11T08:09:22Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM66zNLU", "I_kwDODjvEJM7CV9tM", "Hi @mslipper \nCould you give me chance to work with you?", "2025-07-29T20:35:26Z", "2025-07-29T20:35:26Z", "moonfury-ops", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7B8iKH", "I_kwDODjvEJM7BpFKN", "Blocked on the availability of websocket connection of conductors.\n\nBut here's the PR which will close this issue - https://github.com/ethereum-optimism/k8s/pull/7240", "2025-09-04T13:58:59Z", "2025-09-04T13:58:59Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DaqSZ", "I_kwDODjvEJM7BpBWH", "Enabled flashblocks on the remaining sequencers\n- Ref - https://github.com/ethereum-optimism/k8s/pull/7307", "2025-09-11T06:48:36Z", "2025-09-11T06:48:36Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7B8mD8", "I_kwDODjvEJM7Bo_z9", "Associated PRs: \n- https://github.com/ethereum-optimism/k8s/pull/7212 \n- https://github.com/ethereum-optimism/k8s/pull/7235\n\nRemaining tasks:\n- Deploy rollup-boost with disabled mode and observe the stability of block production over the weekend.\n- Enable rollup-boost's execution mode on the basis of stability and other observations.", "2025-09-04T14:02:14Z", "2025-09-04T14:02:14Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DaopT", "I_kwDODjvEJM7Bo_z9", "Updates on the remaining items:\n- https://github.com/ethereum-optimism/k8s/pull/7307\n- Enabled this sequencer as well.\n\nStability of flashblock production confirmed.", "2025-09-11T06:47:38Z", "2025-09-11T06:47:38Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DSbH6", "I_kwDODjvEJM7AuE_1", "Won't implement, closing.", "2025-09-10T20:22:17Z", "2025-09-10T20:22:17Z", "alfonso-op", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7C8dYY", "I_kwDODjvEJM7Apbxc", "This completes the first milestone for the sync-tester.", "2025-09-09T12:57:56Z", "2025-09-09T12:57:56Z", "pcw109550", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7C3Sms", "I_kwDODjvEJM67Cx_F", "- Monitoring dashboard completed [here](https://optimistic.grafana.net/d/yal7wr8/flashblocks-central?orgId=1&from=now-7d&to=now&timezone=browser&var-network=sepolia-prod&var-current_leader=prod-sepolia-op-opn-geth-a-sequencer-1&var-current_leader_namespace=op-opn-geth-a-sequencer-1&var-current_flashblocks_mode=%201%20&tab=queries)\n\n- Remaining alerting PRs raised here - https://github.com/ethereum-optimism/k8s/pull/7298", "2025-09-09T07:22:28Z", "2025-09-09T07:22:28Z", "yashvardhan-kukreja", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DSaRU", "I_kwDODjvEJM61mqR-", "Won't implement, closing.", "2025-09-10T20:21:10Z", "2025-09-10T20:21:10Z", "alfonso-op", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DSXXo", "I_kwDODjvEJM6z6LlS", "Won't implement, closing.", "2025-09-10T20:16:16Z", "2025-09-10T20:16:16Z", "alfonso-op", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DSXQW", "I_kwDODjvEJM6zut2m", "Won't implement, closing.", "2025-09-10T20:16:05Z", "2025-09-10T20:16:05Z", "alfonso-op", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DSaAY", "I_kwDODjvEJM6zgh3l", "Won't implement, closing.", "2025-09-10T20:20:44Z", "2025-09-10T20:20:44Z", "alfonso-op", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DSZvT", "I_kwDODjvEJM6v3AZl", "Won't implement, closing.", "2025-09-10T20:20:14Z", "2025-09-10T20:20:14Z", "alfonso-op", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6epLcc", "I_kwDODjvEJM6qPqN_", "Love this! Would suggest numbering the issues so we can refer to them more easily.\n\nRegarding issue 4 (having fileserver and op packages in same enclave), I don't quite understand why that is an issue. Afterall, the op-package itself [deploys the ethereum-package](https://github.com/ethpandaops/optimism-package/blob/1fbe3ea05881de385621bf6cc10647f2d7013154/main.star#L78) in the same enclave already right? So if fileserver is idempotent itself, then it should be fine?", "2025-02-16T20:06:37Z", "2025-02-16T20:06:37Z", "samlaf", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6epQhM", "I_kwDODjvEJM6qPqN_", "So, the situation is a bit different for ethereum-package, as optimism-package \"inherits\" from ethereum-package: for the most part, we have interactions that look like:\n\n```\nethereum_package = import_module(\"github.com/ethpandaops/ethereum-package/main.star\")\n...\n```\n\nSo in the end, the optimism-package is still a single package, that embeds the logic it needs from ethereum-package.\n\nWith fileserver, currently, we really have a completely separate package (optimism-package doesn't even know about it, it only manifests itself as a magical URL in some places).\nI'm not sure exactly why kurtosis invalidates the steps in that situation (I'm guessing this is from an abundance of caution? I agree this feels wrong), but at some level I'm glad it does, cause otherwise modifying contracts would currently not lead to an op-deployer re-run, which would definitely be functionally wrong \ud83e\udd37\ud83c\udffc ", "2025-02-16T21:00:11Z", "2025-02-16T21:00:11Z", "sigma", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6jUvmP", "I_kwDODjvEJM6qPqN_", "Recording a data point, on my machine (M3 Max MBP) it takes at >10 minutes to provision a kt devnet, the _second_ time I do so (already built all the docker images, didn't change anything, enclave already exists from previous run). \n\n\nEDIT: I tried this again, not sure what I changed, and things were a lot faster (~4 minutes) and seeing a lot of\n\n```\nSKIPPED - This instruction has already been run in this enclave\n```\nThis is a great improvement but I'm still not sure that docker images are being cached properly (building images seems to take the majority of the run time now). ", "2025-03-20T11:37:08Z", "2025-03-25T09:50:04Z", "geoknee", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6j2yaH", "I_kwDODjvEJM6qPqN_", "> Recording a data point, on my machine (M3 Max MBP) it takes at >10 minutes to provision a kt devnet, the _second_ time I do so (already built all the docker images, didn't change anything, enclave already exists from previous run).\n> \n> EDIT: I tried this again, not sure what I changed, and things were a lot faster (~4 minutes) and seeing a lot og\n> \n> ```\n> SKIPPED - This instruction has already been run in this enclave\n> ```\n> \n> This is a great improvement but I'm still not sure that docker images are being cached properly (building images seems to take the majority of the run time now).\n\nIf you get the chance, can you paste the beginning of the logs? For me, a \"noop\" deployment looks like the following right now\n\n```\n2025/03/24 19:08:44 Building docker image for project: op-node with tag: op-node:interop-devnet\n2025/03/24 19:09:00 Building docker image for project: op-batcher with tag: op-batcher:interop-devnet\n2025/03/24 19:09:04 Building docker image for project: op-challenger with tag: op-challenger:interop-devnet\n2025/03/24 19:09:10 Building docker image for project: op-proposer with tag: op-proposer:interop-devnet\n2025/03/24 19:09:14 Building docker image for project: op-deployer with tag: op-deployer:interop-devnet\n2025/03/24 19:09:16 Building docker image for project: op-supervisor with tag: op-supervisor:interop-devnet\n2025/03/24 19:09:20 Building prestate: /var/folders/g_/6769w4vx2_n5qpnq1vbvx4ph0000gn/T/interop-devnet2872997765/proofs/op-program/cannon\n2025/03/24 19:13:31 prestate-proof-interop.json available at: http://fileserver/proofs/op-program/cannon/0x03682614730beeeff06adbf825a5e654d860a36374ae8229ccf981e8bc02555e.json\n2025/03/24 19:13:31 prestate-interop.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x03682614730beeeff06adbf825a5e654d860a36374ae8229ccf981e8bc02555e.bin.gz\n2025/03/24 19:13:31 prestate-proof-mt64.json available at: http://fileserver/proofs/op-program/cannon/0x0337882ae9555b4a104d4cf7c57a2f498f01101b92e8da72c229286cb4297420.json\n2025/03/24 19:13:31 prestate-mt64.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x0337882ae9555b4a104d4cf7c57a2f498f01101b92e8da72c229286cb4297420.bin.gz\n2025/03/24 19:13:31 prestate-proof.json available at: http://fileserver/proofs/op-program/cannon/0x0377b63af853ee472f9a9e7cb960debf37fe577596fc40eee390de38521340b8.json\n2025/03/24 19:13:31 prestate.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x0377b63af853ee472f9a9e7cb960debf37fe577596fc40eee390de38521340b8.bin.gz\n2025/03/24 19:13:31 Building contracts bundle: /var/folders/g_/6769w4vx2_n5qpnq1vbvx4ph0000gn/T/interop-devnet2872997765/contracts-bundle-interop-devnet.tar.gz\n2025/03/24 19:13:56 l1: contract artifacts available at: http://fileserver/contracts-bundle-interop-devnet.tar.gz\n2025/03/24 19:13:56 l2: contract artifacts available at: http://fileserver/contracts-bundle-interop-devnet.tar.gz\n2025/03/24 19:13:56 No changes to fileserver, skipping deployment\n2025/03/24 19:13:56 Deployment input:\n```\n\nSo, more or less:\n- 30 seconds to build the docker images\n- 4 minutes to build the prestate\n- 30 seconds to build the contracts\n\nI'm mostly looking at the latter 2 right now, although I agree that *everything* should disappear here.\n\n", "2025-03-24T18:25:23Z", "2025-03-24T18:25:23Z", "sigma", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6j3pkW", "I_kwDODjvEJM6qPqN_", "Another data point. It looks like Docker Desktop on Mac at least slows down dramatically after the computer goes to sleep / wakes up.\nI'm experimenting with different VM options to see if I can find something that's not affected.\n\nBut at least on a freshly started Docker Desktop instance, with Docker VMM, I'm getting:\n\n- fresh deployment in 4 minutes\n- incremental \"noop\" deployment in 30 seconds (well, with https://github.com/ethereum-optimism/optimism/pull/14932 but it shouldn't matter *that* much)\n\n```\n2025/03/24 21:04:42 Building docker image for project: op-node with tag: op-node:interop-devnet\n2025/03/24 21:04:45 Building docker image for project: op-batcher with tag: op-batcher:interop-devnet\n2025/03/24 21:04:46 Building docker image for project: op-challenger with tag: op-challenger:interop-devnet\n2025/03/24 21:04:48 Building docker image for project: op-proposer with tag: op-proposer:interop-devnet\n2025/03/24 21:04:49 Building docker image for project: op-deployer with tag: op-deployer:interop-devnet\n2025/03/24 21:04:50 Building docker image for project: op-supervisor with tag: op-supervisor:interop-devnet\n2025/03/24 21:04:51 Building prestate: /var/folders/g_/6769w4vx2_n5qpnq1vbvx4ph0000gn/T/interop-devnet517384906/proofs/op-program/cannon\n2025/03/24 21:04:54 prestate-proof-interop.json available at: http://fileserver/proofs/op-program/cannon/0x03682614730beeeff06adbf825a5e654d860a36374ae8229ccf981e8bc02555e.json\n2025/03/24 21:04:54 prestate-interop.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x03682614730beeeff06adbf825a5e654d860a36374ae8229ccf981e8bc02555e.bin.gz\n2025/03/24 21:04:54 prestate-proof-mt64.json available at: http://fileserver/proofs/op-program/cannon/0x0337882ae9555b4a104d4cf7c57a2f498f01101b92e8da72c229286cb4297420.json\n2025/03/24 21:04:54 prestate-mt64.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x0337882ae9555b4a104d4cf7c57a2f498f01101b92e8da72c229286cb4297420.bin.gz\n2025/03/24 21:04:54 prestate-proof.json available at: http://fileserver/proofs/op-program/cannon/0x0377b63af853ee472f9a9e7cb960debf37fe577596fc40eee390de38521340b8.json\n2025/03/24 21:04:54 prestate.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x0377b63af853ee472f9a9e7cb960debf37fe577596fc40eee390de38521340b8.bin.gz\n2025/03/24 21:04:54 Building contracts bundle\nUsing existing enclave 'interop-devnet'\n\n2025/03/24 21:04:59 Artifact 'contracts-65cd68a820ada40660f339e5034eab700b97093bd4384db5db669569abe44bce' already exists, skipping creation\n2025/03/24 21:04:59 l1: contract artifacts available at: artifact://contracts-65cd68a820ada40660f339e5034eab700b97093bd4384db5db669569abe44bce\n2025/03/24 21:04:59 l2: contract artifacts available at: artifact://contracts-65cd68a820ada40660f339e5034eab700b97093bd4384db5db669569abe44bce\n2025/03/24 21:04:59 No changes to fileserver, skipping deployment\n2025/03/24 21:04:59 Deployment input:\n```", "2025-03-24T20:09:15Z", "2025-03-24T20:09:15Z", "sigma", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6j6Pn1", "I_kwDODjvEJM6qPqN_", "> 4 minutes to build the prestate\n\nFor the record, short lived, local devnets don't need to use the reproducible build process for prestates - just a simple `make cannon-prestates` would build it natively a lot faster.  You could even trim that down the specific prestate type you want with `make cannon-prestate-interop`, `make cannon-prestate-mt64` or `make cannon-prestate`. I think you typically want the interop or mt64 versions now given mt64 is going through the release process currently.\n\nNot sure how to fit that build into the kurtosis build process without slowing it all down again but it should be able to use the cached go build artefacts from op-node/op-geth etc and so compile much faster. Even without having built locally it's under a minute to build for me locally and with no changes can rebuild in under 10s.", "2025-03-25T03:19:37Z", "2025-03-25T03:19:37Z", "ajsutton", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6j-aZb", "I_kwDODjvEJM6qPqN_", "@sigma it's a slightly different issue, but I would love for the `just` command to fail faster if there is (say) a kurtosis API mismatch. Can we somehow check this immediately, rather than after the several minutes of docker builds?\n\nI will report back with logs and runtimes shortly, currently struggling with the upgraded kurtosis version `1.6.0` (I have `1.4.3` installed with `brew` and I can't seem to get things to work with the `mise` managed version yet). \n", "2025-03-25T12:08:36Z", "2025-03-25T12:08:36Z", "geoknee", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6j_NMC", "I_kwDODjvEJM6qPqN_", "> [@sigma](https://github.com/sigma) it's a slightly different issue, but I would love for the `just` command to fail faster if there is (say) a kurtosis API mismatch. Can we somehow check this immediately, rather than after the several minutes of docker builds?\n\nSure. Let's track this one separately in #15022 \n", "2025-03-25T13:24:58Z", "2025-03-25T13:24:58Z", "sigma", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6kLska", "I_kwDODjvEJM6qPqN_", "> I will report back with logs and runtimes shortly, currently struggling with the upgraded kurtosis version `1.6.0` (I have `1.4.3` installed with `brew` and I can't seem to get things to work with the `mise` managed version yet).\n\nI uninstalled `kurtosis` that I had previously with `brew`, and then made sure to activate `mise` properly which fixed the issue above. \n\n`just simple-devnet` took 213s starting from a fresh state, and then 32s when I changed the op-geth log level and ran the command again. \n\n<details>\n<summary>first invocation logs</summary>\n\n```\n\u279c  kurtosis-devnet git:(develop) \u2717 time just simple-devnet\n2025/03/26 13:19:56 Building docker image for project: op-node with tag: op-node:simple-devnet\n2025/03/26 13:20:16 Building docker image for project: op-batcher with tag: op-batcher:simple-devnet\n2025/03/26 13:20:18 Building docker image for project: op-challenger with tag: op-challenger:simple-devnet\n2025/03/26 13:20:26 Building docker image for project: op-proposer with tag: op-proposer:simple-devnet\n2025/03/26 13:20:28 Building docker image for project: op-deployer with tag: op-deployer:simple-devnet\n2025/03/26 13:20:34 Building contracts bundle: /var/folders/31/rhx71v1d52n7j67f__hl2qj40000gn/T/simple-devnet1651256524/contracts-bundle-simple-devnet.tar.gz\n2025/03/26 13:29:11 l1: contract artifacts available at: http://fileserver/contracts-bundle-simple-devnet.tar.gz\n2025/03/26 13:29:11 l2: contract artifacts available at: http://fileserver/contracts-bundle-simple-devnet.tar.gz\n2025/03/26 13:29:11 Building prestate: /var/folders/31/rhx71v1d52n7j67f__hl2qj40000gn/T/simple-devnet1651256524/proofs/op-program/cannon\n2025/03/26 13:29:44 prestate-proof-interop.json available at: http://fileserver/proofs/op-program/cannon/0x03486bc1d921bc0a44b088a450737e9d6a7da49dd8d35dad361c767aabba024d.json\n2025/03/26 13:29:44 prestate-interop.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x03486bc1d921bc0a44b088a450737e9d6a7da49dd8d35dad361c767aabba024d.bin.gz\n2025/03/26 13:29:44 prestate-proof-mt64.json available at: http://fileserver/proofs/op-program/cannon/0x03d70692978d10aabdbfa7c62729b778c44dd10f5b874b0e07e4bf7a637a1cec.json\n2025/03/26 13:29:44 prestate-mt64.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x03d70692978d10aabdbfa7c62729b778c44dd10f5b874b0e07e4bf7a637a1cec.bin.gz\n2025/03/26 13:29:44 prestate-proof.json available at: http://fileserver/proofs/op-program/cannon/0x03054be52183f128f6fa6f461440688b538a72a359c66e79b4866aa966e7e003.json\n2025/03/26 13:29:44 prestate.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x03054be52183f128f6fa6f461440688b538a72a359c66e79b4866aa966e7e003.bin.gz\nCreating a new enclave for Starlark to run inside...\nEnclave 'simple-devnet' created successfully\n\nINFO[0273] Compressing package 'github.com/ethereum-optimism/optimism/kurtosis-devnet/fileserver' at 'fileserver' for upload\nINFO[0281] Uploading and executing package 'github.com/ethereum-optimism/optimism/kurtosis-devnet/fileserver'\nContainer images used in this run:\n> nginx:latest - remotely downloaded\nUploading file 'upload-content' to files artifact 'fileserver-content'\nFiles with artifact name 'fileserver-content' uploaded with artifact UUID '37b44165a2d247e3aafc3a3e65e157ac'\n\nUploading file 'static_files/nginx' to files artifact 'fileserver-nginx-conf'\nFiles with artifact name 'fileserver-nginx-conf' uploaded with artifact UUID '00e90478d34144909450ec51139401d4'\n\nAdding service with name 'fileserver' and image 'nginx:latest'\nService 'fileserver' added with service UUID '5d3708c61b0242e6b36c0972fca874e6'\n\n\u2b50 us on GitHub - https://github.com/kurtosis-tech/kurtosis\n2025/03/26 13:30:51 Deployment input:\nethereum_package:\n  network_params:\n    additional_preloaded_contracts: |\n      {\n        \"0x4e59b44847b379578588920cA78FbF26c0B4956C\": {\n          \"balance\": \"0ETH\",\n          \"code\": \"0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf3\",\n          \"storage\": {},\n          \"nonce\": \"1\"\n        }\n      }\n    genesis_delay: 5\n    preset: minimal\n  participants:\n    - cl_type: teku\n      el_type: geth\noptimism_package:\n  chains:\n    - additional_services: []\n      batcher_params:\n        extra_params: []\n        image: op-batcher:551aa84b67a0\n      challenger_params:\n        cannon_prestate_path: \"\"\n        cannon_prestates_url: http://fileserver/proofs/op-program/cannon\n        extra_params: []\n        image: op-challenger:b865b632af32\n      mev_params:\n        builder_host: \"\"\n        builder_port: \"\"\n        rollup_boost_image: \"\"\n      network_params:\n        fjord_time_offset: 0\n        fund_dev_accounts: true\n        granite_time_offset: 0\n        holocene_time_offset: 0\n        name: op-kurtosis\n        network: kurtosis\n        network_id: \"2151908\"\n        seconds_per_slot: 2\n      participants:\n        - cl_extra_env_vars: {}\n          cl_extra_labels: {}\n          cl_extra_params: []\n          cl_image: op-node:3b0c9ecb0a87\n          cl_log_level: \"\"\n          cl_max_cpu: 0\n          cl_max_mem: 0\n          cl_min_cpu: 0\n          cl_min_mem: 0\n          cl_tolerations: []\n          cl_type: op-node\n          cl_volume_size: 0\n          count: 1\n          el_extra_env_vars: {}\n          el_extra_labels: {}\n          el_extra_params: []\n          el_image: \"\"\n          el_log_level: \"\"\n          el_max_cpu: 0\n          el_max_mem: 0\n          el_min_cpu: 0\n          el_min_mem: 0\n          el_tolerations: []\n          el_type: op-geth\n          el_volume_size: 0\n          node_selectors: {}\n          tolerations: []\n      proposer_params:\n        extra_params: []\n        game_type: 1\n        image: op-proposer:fc2a2a27ae24\n        proposal_interval: 10m\n  global_log_level: info\n  global_node_selectors: {}\n  global_tolerations: []\n  op_contract_deployer_params:\n    global_deploy_overrides:\n      faultGameAbsolutePrestate: 0x03054be52183f128f6fa6f461440688b538a72a359c66e79b4866aa966e7e003\n    image: op-deployer:a0e5c44110a9\n    l1_artifacts_locator: http://fileserver/contracts-bundle-simple-devnet.tar.gz\n    l2_artifacts_locator: http://fileserver/contracts-bundle-simple-devnet.tar.gz\n  persistent: false\nUsing existing enclave 'simple-devnet'\n\nINFO[0334] Compressing package 'github.com/ethereum-optimism/optimism/kurtosis-devnet/optimism-package-trampoline' at './optimism-package-trampoline/' for upload\nINFO[0334] Uploading and executing package 'github.com/ethereum-optimism/optimism/kurtosis-devnet/optimism-package-trampoline'\nContainer images used in this run:\n> mslipper/deployment-utils:latest - remotely downloaded\n> badouralix/curl-jq - remotely downloaded\n> grafana/grizzly:main-0b88d01 - remotely downloaded\n> consensys/teku:latest - remotely downloaded\n> op-deployer:a0e5c44110a9 - remotely downloaded\n> op-proposer:fc2a2a27ae24 - remotely downloaded\n> op-challenger:b865b632af32 - remotely downloaded\n> op-node:3b0c9ecb0a87 - remotely downloaded\n> prom/prometheus:v3.1.0 - remotely downloaded\n> ethpandaops/ethereum-genesis-generator:3.7.0 - remotely downloaded\n> op-batcher:551aa84b67a0 - remotely downloaded\n> python:3.11-alpine - remotely downloaded\n> ethereum/client-go:latest - remotely downloaded\n> us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest - remotely downloaded\n> grafana/grafana:11.5.0 - remotely downloaded\n> protolambda/eth2-val-tools:latest - remotely downloaded\nPrinting a message\nParsing the L1 input args\n\nPrinting a message\nSanity check for OP package passed\n\nPrinting a message\nDeploying a local L1\n\nPrinting a message\nSanity check passed\n\nPrinting a message\nDocker cache is disabled\n\nUploading file '/static_files/jwt/jwtsecret' to files artifact 'jwt_file'\nFiles with artifact name 'jwt_file' uploaded with artifact UUID 'c9da9cc0f7f04a71b41010823ab5f78d'\n\nUploading file '/static_files/keymanager/keymanager.txt' to files artifact 'keymanager_file'\nFiles with artifact name 'keymanager_file' uploaded with artifact UUID '68506da40f1845499da87ee96fdae163'\n\nPrinting a message\nRead the prometheus, grafana templates\n\nPrinting a message\nLaunching participant network with 1 participants and the following network params struct(additional_preloaded_contracts = \"{\\n  \\\"0x4e59b44847b379578588920cA78FbF26c0B4956C\\\": {\\n    \\\"balance\\\": \\\"0ETH\\\",\\n    \\\"code\\\": \\\"0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf3\\\",\\n    \\\"storage\\\": {},\\n    \\\"nonce\\\": \\\"1\\\"\\n  }\\n}\\n\", altair_fork_epoch = 0, bellatrix_fork_epoch = 0, capella_fork_epoch = 0, churn_limit_quotient = 32, custody_requirement = 4, data_column_sidecar_subnet_count = 128, deneb_fork_epoch = 0, deposit_contract_address = \"0x4242424242424242424242424242424242424242\", devnet_repo = \"ethpandaops\", eip7732_fork_epoch = 100000002, eip7805_fork_epoch = 100000003, ejection_balance = 16000000000, electra_fork_epoch = 100000000, eth1_follow_distance = 16, fulu_fork_epoch = 100000001, genesis_delay = 5, genesis_gaslimit = 30000000, gossip_max_size = 10485760, max_blobs_per_block_electra = 9, max_blobs_per_block_fulu = 12, max_per_epoch_activation_churn_limit = 4, min_validator_withdrawability_delay = 256, network = \"kurtosis\", network_id = \"3151908\", network_sync_base_url = \"https://snapshots.ethpandaops.io/\", num_validator_keys_per_node = 64, prefunded_accounts = {}, preregistered_validator_count = 0, preregistered_validator_keys_mnemonic = \"giant issue aisle success illegal bike spike question tent bar rely arctic volcano long crawl hungry vocal artwork sniff fantasy very lucky have athlete\", preset = \"minimal\", samples_per_slot = 8, seconds_per_slot = 6, shard_committee_period = 64, target_blobs_per_block_electra = 6, target_blobs_per_block_fulu = 9)\n\nPrinting a message\nGenerating cl validator key stores\n\nAdding service with name 'validator-key-generation-cl-validator-keystore' and image 'protolambda/eth2-val-tools:latest'\nService 'validator-key-generation-cl-validator-keystore' added with service UUID 'afd80e0340fb4f65b8d64f8ae7cb12a4'\n\nGenerating keystores\nCommand returned with exit code '0' with no output\n\nVerifying whether two values meet a certain condition '=='\nVerification succeeded. Value is '0'.\n\nStoring files from service 'validator-key-generation-cl-validator-keystore' at path '/node-0-keystores/' to files artifact with name '1-teku-geth-0-63'\nFiles with artifact name '1-teku-geth-0-63' uploaded with artifact UUID 'f22ef016d159418a9fc45b5648026ef2'\n\nStoring prysm password in a file\nCommand returned with exit code '0' with no output\n\nVerifying whether two values meet a certain condition '=='\nVerification succeeded. Value is '0'.\n\nStoring files from service 'validator-key-generation-cl-validator-keystore' at path '/tmp/prysm-password.txt' to files artifact with name 'prysm-password'\nFiles with artifact name 'prysm-password' uploaded with artifact UUID '2f36bffee55349d7a98c7fdbd51012c9'\n\nPrinting a message\n{\n\t\"per_node_keystores\": [\n\t\t{\n\t\t\t\"files_artifact_uuid\": \"1-teku-geth-0-63\",\n\t\t\t\"nimbus_keys_relative_dirpath\": \"/nimbus-keys\",\n\t\t\t\"prysm_relative_dirpath\": \"/prysm\",\n\t\t\t\"raw_keys_relative_dirpath\": \"/keys\",\n\t\t\t\"raw_root_dirpath\": \"\",\n\t\t\t\"raw_secrets_relative_dirpath\": \"/secrets\",\n\t\t\t\"teku_keys_relative_dirpath\": \"/teku-keys\",\n\t\t\t\"teku_secrets_relative_dirpath\": \"/teku-secrets\"\n\t\t}\n\t],\n\t\"prysm_password_artifact_uuid\": \"prysm-password\",\n\t\"prysm_password_relative_filepath\": \"prysm-password.txt\"\n}\n\nGetting final genesis timestamp\nCommand returned with exit code '0' and the following output: 1742995886\n\nPrinting a message\nGenerating EL CL data\n\nRendering a template to a files artifact with name 'genesis-el-cl-env-file'\nTemplates artifact name 'genesis-el-cl-env-file' rendered with artifact UUID '6028d2cfc7b74c399f479e635764c554'\n\nCreating genesis\nCommand returned with exit code '0' and the following output:\n--------------------\n+ '[' -f /data/metadata/genesis.json ']'\n++ mktemp -d -t ci-XXXXXXXXXX\n+ tmp_dir=/tmp/ci-ez3J7plbal\n+ mkdir -p /data/metadata\n+ python3 /apps/envsubst.py\n+ cat /tmp/ci-ez3J7plbal/genesis-config.yaml\npreset_base: minimal\nchain_id: 3151908\ndeposit_contract_address: \"0x4242424242424242424242424242424242424242\"\nmnemonic: giant issue aisle success illegal bike spike question tent bar rely arctic volcano long crawl hungry vocal artwork sniff fantasy very lucky have athlete\nel_premine:\n  \"m/44'/60'/0'/0/0\": 1000000000ETH\n  \"m/44'/60'/0'/0/1\": 1000000000ETH\n  \"m/44'/60'/0'/0/2\": 1000000000ETH\n  \"m/44'/60'/0'/0/3\": 1000000000ETH\n  \"m/44'/60'/0'/0/4\": 1000000000ETH\n  \"m/44'/60'/0'/0/5\": 1000000000ETH\n  \"m/44'/60'/0'/0/6\": 1000000000ETH\n  \"m/44'/60'/0'/0/7\": 1000000000ETH\n  \"m/44'/60'/0'/0/8\": 1000000000ETH\n  \"m/44'/60'/0'/0/9\": 1000000000ETH\n  \"m/44'/60'/0'/0/10\": 1000000000ETH\n  \"m/44'/60'/0'/0/11\": 1000000000ETH\n  \"m/44'/60'/0'/0/12\": 1000000000ETH\n  \"m/44'/60'/0'/0/13\": 1000000000ETH\n  \"m/44'/60'/0'/0/14\": 1000000000ETH\n  \"m/44'/60'/0'/0/15\": 1000000000ETH\n  \"m/44'/60'/0'/0/16\": 1000000000ETH\n  \"m/44'/60'/0'/0/17\": 1000000000ETH\n  \"m/44'/60'/0'/0/18\": 1000000000ETH\n  \"m/44'/60'/0'/0/19\": 1000000000ETH\n  \"m/44'/60'/0'/0/20\": 1000000000ETH\nel_premine_addrs: {}\nadditional_preloaded_contracts: {\n  \"0x4e59b44847b379578588920cA78FbF26c0B4956C\": {\n    \"balance\": \"0ETH\",\n    \"code\": \"0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf3\",\n    \"storage\": {},\n    \"nonce\": \"1\"\n  }\n}\ngenesis_timestamp: 1742995886\ngenesis_delay: 0\ngenesis_gaslimit: 30000000\nslot_duration_in_seconds: 6\nterminal_total_difficulty: 0\naltair_fork_epoch: 0\nbellatrix_fork_epoch: 0\ncapella_fork_epoch: 0\ndeneb_fork_epoch: 0\nelectra_fork_epoch: 100000000\nfulu_fork_epoch: 100000001\ntarget_blobs_per_block_cancun: 3\nmax_blobs_per_block_cancun: 6\nbasefee_update_fraction_cancun: 3338477\ntarget_blobs_per_block_prague: 6\nmax_blobs_per_block_prague: 9\nbasefee_update_fraction_prague: 5007716\ntarget_blobs_per_block_osaka: 9\nmax_blobs_per_block_osaka: 12\nbasefee_update_fraction_osaka: 5007716\n+ python3 /apps/el-gen/genesis_geth.py /tmp/ci-ez3J7plbal/genesis-config.yaml\n+ python3 /apps/el-gen/genesis_chainspec.py /tmp/ci-ez3J7plbal/genesis-config.yaml\n+ python3 /apps/el-gen/genesis_besu.py /tmp/ci-ez3J7plbal/genesis-config.yaml\n+ gen_cl_config\n+ . /apps/el-gen/.venv/bin/activate\n++ deactivate nondestructive\n++ '[' -n /root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin ']'\n++ PATH=/root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ unset _OLD_VIRTUAL_PATH\n++ '[' -n '' ']'\n++ '[' -n /bin/bash -o -n '' ']'\n++ hash -r\n++ '[' -n '' ']'\n++ unset VIRTUAL_ENV\n++ unset VIRTUAL_ENV_PROMPT\n++ '[' '!' nondestructive = nondestructive ']'\n++ VIRTUAL_ENV=/apps/el-gen/.venv\n++ export VIRTUAL_ENV\n++ _OLD_VIRTUAL_PATH=/root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ PATH=/apps/el-gen/.venv/bin:/root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ '[' -n '' ']'\n++ '[' -z '' ']'\n++ _OLD_VIRTUAL_PS1='(.venv) '\n++ PS1='(.venv) (.venv) '\n++ export PS1\n++ VIRTUAL_ENV_PROMPT='(.venv) '\n++ export VIRTUAL_ENV_PROMPT\n++ '[' -n /bin/bash -o -n '' ']'\n++ hash -r\n+ set -x\n+ '[' -f /data/metadata/genesis.ssz ']'\n++ mktemp -d -t ci-XXXXXXXXXX\n+ tmp_dir=/tmp/ci-gkne0jMQ7s\n+ mkdir -p /data/metadata\n+ mkdir -p /data/parsed\n++ date -u -d @1742995886 '+%Y-%b-%d %I:%M:%S %p %Z'\n+ HUMAN_READABLE_TIMESTAMP='2025-Mar-26 01:31:26 PM UTC'\n+ COMMENT='# 2025-Mar-26 01:31:26 PM UTC'\n+ export MAX_REQUEST_BLOB_SIDECARS_ELECTRA=1152\n+ MAX_REQUEST_BLOB_SIDECARS_ELECTRA=1152\n+ export MAX_REQUEST_BLOB_SIDECARS_FULU=1536\n+ MAX_REQUEST_BLOB_SIDECARS_FULU=1536\n+ python3 /apps/envsubst.py\n+ sed -i 's/#HUMAN_TIME_PLACEHOLDER/# 2025-Mar-26 01:31:26 PM UTC/' /data/metadata/config.yaml\n+ python3 /apps/envsubst.py\n+ [[ minimal == \\m\\i\\n\\i\\m\\a\\l ]]\n+ gen_minimal_config\n+ replacements=(['MIN_PER_EPOCH_CHURN_LIMIT']='2' ['MIN_EPOCHS_FOR_BLOCK_REQUESTS']='272' ['WHISK_EPOCHS_PER_SHUFFLING_PHASE']='4' ['WHISK_PROPOSER_SELECTION_GAP']='1' ['MIN_PER_EPOCH_CHURN_LIMIT_ELECTRA']='64000000000' ['MAX_PER_EPOCH_ACTIVATION_EXIT_CHURN_LIMIT']='128000000000')\n+ declare -A replacements\n+ for key in \"${!replacements[@]}\"\n+ sed -i 's/WHISK_PROPOSER_SELECTION_GAP:.*/WHISK_PROPOSER_SELECTION_GAP: 1/' /data/metadata/config.yaml\n+ for key in \"${!replacements[@]}\"\n+ sed -i 's/MAX_PER_EPOCH_ACTIVATION_EXIT_CHURN_LIMIT:.*/MAX_PER_EPOCH_ACTIVATION_EXIT_CHURN_LIMIT: 128000000000/' /data/metadata/config.yaml\n+ for key in \"${!replacements[@]}\"\n+ sed -i 's/WHISK_EPOCHS_PER_SHUFFLING_PHASE:.*/WHISK_EPOCHS_PER_SHUFFLING_PHASE: 4/' /data/metadata/config.yaml\n+ for key in \"${!replacements[@]}\"\n+ sed -i 's/MIN_PER_EPOCH_CHURN_LIMIT:.*/MIN_PER_EPOCH_CHURN_LIMIT: 2/' /data/metadata/config.yaml\n+ for key in \"${!replacements[@]}\"\n+ sed -i 's/MIN_PER_EPOCH_CHURN_LIMIT_ELECTRA:.*/MIN_PER_EPOCH_CHURN_LIMIT_ELECTRA: 64000000000/' /data/metadata/config.yaml\n+ for key in \"${!replacements[@]}\"\n+ sed -i 's/MIN_EPOCHS_FOR_BLOCK_REQUESTS:.*/MIN_EPOCHS_FOR_BLOCK_REQUESTS: 272/' /data/metadata/config.yaml\n+ cp /tmp/ci-gkne0jMQ7s/mnemonics.yaml /data/metadata/mnemonics.yaml\n+ grep DEPOSIT_CONTRACT_ADDRESS /data/metadata/config.yaml\n+ cut -d ' ' -f2\n+ echo 0\n+ echo enr:-Iq4QJk4WqRkjsX5c2CXtOra6HnxN-BMXnWhmhEQO9Bn9iABTJGdjUOurM7Btj1ouKaFkvTRoju5vz2GPmVON2dffQKGAX53x8JigmlkgnY0gmlwhLKAlv6Jc2VjcDI1NmsxoQK6S-Cii_KmfFdUJL2TANL3ksaKUnNXvTCv1tLwXs0QgIN1ZHCCIyk\n+ python3 /apps/envsubst.py\n+ [[ 0 != 0 ]]\n+ [[ 0 != 0 ]]\n+ [[ 0 != 0 ]]\n+ [[ 0 != 0 ]]\n+ genesis_args+=(deneb --config /data/metadata/config.yaml --mnemonics $tmp_dir/mnemonics.yaml --tranches-dir /data/metadata/tranches --state-output /data/metadata/genesis.ssz --preset-phase0 $PRESET_BASE --preset-altair $PRESET_BASE --preset-bellatrix $PRESET_BASE --preset-capella $PRESET_BASE --preset-deneb $PRESET_BASE)\n+ [[ 0x00 == \\0\\x\\0\\1 ]]\n+ [[ '' != '' ]]\n+ [[ '' != '' ]]\n+ [[ 0 == 0 ]]\n+ [[ 0 == 0 ]]\n+ genesis_args+=(--eth1-config /data/metadata/genesis.json)\n+ '[' -z '' ']'\n+ [[ 0 != 0 ]]\n+ [[ 0 != 0 ]]\n+ [[ 0 != 0 ]]\n+ [[ 0 != 0 ]]\n+ zcli_args=(pretty deneb BeaconState --preset-phase0 $PRESET_BASE --preset-altair $PRESET_BASE --preset-bellatrix $PRESET_BASE --preset-capella $PRESET_BASE /data/metadata/genesis.ssz)\n+ /usr/local/bin/eth2-testnet-genesis deneb --config /data/metadata/config.yaml --mnemonics /tmp/ci-gkne0jMQ7s/mnemonics.yaml --tranches-dir /data/metadata/tranches --state-output /data/metadata/genesis.ssz --preset-phase0 minimal --preset-altair minimal --preset-bellatrix minimal --preset-capella minimal --preset-deneb minimal --eth1-config /data/metadata/genesis.json\nzrnt version: v0.33.1\nUsing CL MIN_GENESIS_TIME for genesis timestamp\nprocessing mnemonic 0, for 64 validators\nWriting pubkeys list file...\ngenerated 64 validators from mnemonic yaml (/tmp/ci-gkne0jMQ7s/mnemonics.yaml)\ngenesis at 1742995886 + 0 = 1742995886  (2025-03-26 13:31:26 +0000 UTC)\ndone preparing state, serializing SSZ now...\ndone!\n+ /usr/local/bin/zcli pretty deneb BeaconState --preset-phase0 minimal --preset-altair minimal --preset-bellatrix minimal --preset-capella minimal /data/metadata/genesis.ssz\n+ echo 'Genesis args: deneb' --config /data/metadata/config.yaml --mnemonics /tmp/ci-gkne0jMQ7s/mnemonics.yaml --tranches-dir /data/metadata/tranches --state-output /data/metadata/genesis.ssz --preset-phase0 minimal --preset-altair minimal --preset-bellatrix minimal --preset-capella minimal --preset-deneb minimal --eth1-config /data/metadata/genesis.json\n++ jq -r .latest_execution_payload_header.block_number /data/parsed/parsedConsensusGenesis.json\nGenesis args: deneb --config /data/metadata/config.yaml --mnemonics /tmp/ci-gkne0jMQ7s/mnemonics.yaml --tranches-dir /data/metadata/tranches --state-output /data/metadata/genesis.ssz --preset-phase0 minimal --preset-altair minimal --preset-bellatrix minimal --preset-capella minimal --preset-deneb minimal --eth1-config /data/metadata/genesis.json\nGenesis block number: 0\n+ echo 'Genesis block number: 0'\n++ jq -r .latest_execution_payload_header.block_hash /data/parsed/parsedConsensusGenesis.json\nGenesis block hash: 0xe658dfd7f3b1584bbe5ec0fb030584c220977bbc7ea5c56246827b478f522c61\n+ echo 'Genesis block hash: 0xe658dfd7f3b1584bbe5ec0fb030584c220977bbc7ea5c56246827b478f522c61'\n+ jq -r .eth1_data.block_hash /data/parsed/parsedConsensusGenesis.json\n+ tr -d '\\n'\n+ jq -r .genesis_validators_root /data/parsed/parsedConsensusGenesis.json\n+ tr -d '\\n'\n+ gen_shared_files\n+ . /apps/el-gen/.venv/bin/activate\n++ deactivate nondestructive\n++ '[' -n /root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin ']'\n++ PATH=/root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ unset _OLD_VIRTUAL_PATH\n++ '[' -n '' ']'\n++ '[' -n /bin/bash -o -n '' ']'\n++ hash -r\n++ '[' -n '(.venv) ' ']'\n++ PS1='(.venv) '\n++ export PS1\n++ unset _OLD_VIRTUAL_PS1\n++ unset VIRTUAL_ENV\n++ unset VIRTUAL_ENV_PROMPT\n++ '[' '!' nondestructive = nondestructive ']'\n++ VIRTUAL_ENV=/apps/el-gen/.venv\n++ export VIRTUAL_ENV\n++ _OLD_VIRTUAL_PATH=/root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ PATH=/apps/el-gen/.venv/bin:/root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ '[' -n '' ']'\n++ '[' -z '' ']'\n++ _OLD_VIRTUAL_PS1='(.venv) '\n++ PS1='(.venv) (.venv) '\n++ export PS1\n++ VIRTUAL_ENV_PROMPT='(.venv) '\n++ export VIRTUAL_ENV_PROMPT\n++ '[' -n /bin/bash -o -n '' ']'\n++ hash -r\n+ set -x\n+ mkdir -p /data/metadata\n+ '[' -f /data/jwt/jwtsecret ']'\n+ mkdir -p /data/jwt\n++ openssl rand -hex 32\n++ tr -d '\\n'\n+ echo -n 0x1b34a786d04cca6751805de6d530da70b573df7ec4c2d876c39596ef18ac42be\n+ '[' -f /data/metadata/genesis.json ']'\n++ cat /data/metadata/genesis.json\n++ jq -r '.config.terminalTotalDifficulty | tostring'\n+ terminalTotalDifficulty=0\n+ sed -i 's/TERMINAL_TOTAL_DIFFICULTY:.*/TERMINAL_TOTAL_DIFFICULTY: 0/' /data/metadata/config.yaml\n+ '[' false = true ']'\n\n--------------------\n\nReading genesis validators root\nCommand returned with exit code '0' and the following output: 0xd61ea484febacfae5298d52a2b581f3e305a51f3112a9241b968dccf019f7b11\n\nReading prague time from genesis\nCommand returned with exit code '0' and the following output: 6542995886\n\nAdding service with name 'el-1-geth-teku' and image 'ethereum/client-go:latest'\nService 'el-1-geth-teku' added with service UUID '0307132dec5441f6835481e3245ae1b3'\n\nWaiting for at most '15m0s' for service 'el-1-geth-teku' to reach a certain state\nWait took 1 tries (17.385708ms in total). Assertion passed with following:\nRequest had response code '200' and body \"{\\\"jsonrpc\\\":\\\"2.0\\\",\\\"id\\\":1,\\\"result\\\":{\\\"id\\\":\\\"f63b011fd919fdbab1879edf90a2a0cc5ec001520c8596858e8adbac0e2ef6f1\\\",\\\"name\\\":\\\"Geth/v1.15.7-unstable-21d36f7c-20250325/linux-arm64/go1.24.1\\\",\\\"enode\\\":\\\"enode://a7fddba216f934f27a1e11b99503516876e9a7610f1d39c54be3b02570fb7f08b5cd3ae75b447153b6a3ba29bca71f4c01f868440f89cbd1cfb7e90cd9f21522@172.16.4.11:30303\\\",\\\"enr\\\":\\\"enr:-Ki4QFAhFyVWW7HTnK62DwckuxltNP3v9VW7_Gr0Hah8gK4BISxtF9mlIT9A95bDJc9eR_R0RYdUSNIV8E_oB4SWo6GGAZXSpm1tg2V0aMzLhNcRw72FAYX-Ma6CaWSCdjSCaXCErBAEC4lzZWNwMjU2azGhAqf926IW-TTyeh4RuZUDUWh26adhDx05xUvjsCVw-38IhHNuYXDAg3RjcIJ2X4N1ZHCCdl8\\\",\\\"ip\\\":\\\"172.16.4.11\\\",\\\"ports\\\":{\\\"discovery\\\":30303,\\\"listener\\\":30303},\\\"listenAddr\\\":\\\"[::]:30303\\\",\\\"protocols\\\":{\\\"eth\\\":{\\\"network\\\":3151908,\\\"genesis\\\":\\\"0xe658dfd7f3b1584bbe5ec0fb030584c220977bbc7ea5c56246827b478f522c61\\\",\\\"config\\\":{\\\"chainId\\\":3151908,\\\"homesteadBlock\\\":0,\\\"eip150Block\\\":0,\\\"eip155Block\\\":0,\\\"eip158Block\\\":0,\\\"byzantiumBlock\\\":0,\\\"constantinopleBlock\\\":0,\\\"petersburgBlock\\\":0,\\\"istanbulBlock\\\":0,\\\"berlinBlock\\\":0,\\\"londonBlock\\\":0,\\\"mergeNetsplitBlock\\\":0,\\\"shanghaiTime\\\":0,\\\"cancunTime\\\":0,\\\"pragueTime\\\":6542995886,\\\"osakaTime\\\":6542995934,\\\"terminalTotalDifficulty\\\":0,\\\"depositContractAddress\\\":\\\"0x4242424242424242424242424242424242424242\\\",\\\"blobSchedule\\\":{\\\"cancun\\\":{\\\"target\\\":3,\\\"max\\\":6,\\\"baseFeeUpdateFraction\\\":3338477},\\\"prague\\\":{\\\"target\\\":6,\\\"max\\\":9,\\\"baseFeeUpdateFraction\\\":5007716},\\\"osaka\\\":{\\\"target\\\":9,\\\"max\\\":12,\\\"baseFeeUpdateFraction\\\":5007716}}},\\\"head\\\":\\\"0xe658dfd7f3b1584bbe5ec0fb030584c220977bbc7ea5c56246827b478f522c61\\\"},\\\"snap\\\":{}}}}\\n\", with extracted fields:\n'extract.enr': \"enr:-Ki4QFAhFyVWW7HTnK62DwckuxltNP3v9VW7_Gr0Hah8gK4BISxtF9mlIT9A95bDJc9eR_R0RYdUSNIV8E_oB4SWo6GGAZXSpm1tg2V0aMzLhNcRw72FAYX-Ma6CaWSCdjSCaXCErBAEC4lzZWNwMjU2azGhAqf926IW-TTyeh4RuZUDUWh26adhDx05xUvjsCVw-38IhHNuYXDAg3RjcIJ2X4N1ZHCCdl8\"\n'extract.enode': \"enode://a7fddba216f934f27a1e11b99503516876e9a7610f1d39c54be3b02570fb7f08b5cd3ae75b447153b6a3ba29bca71f4c01f868440f89cbd1cfb7e90cd9f21522@172.16.4.11:30303\"\n\nPrinting a message\nSuccessfully added 1 EL participants\n\nPrinting a message\nLaunching CL network\n\nAdding service with name 'cl-1-teku-geth' and image 'consensys/teku:latest'\nService 'cl-1-teku-geth' added with service UUID '7286f57dac44480fa29c624e834814df'\n\nRunning 'GET' request on service 'cl-1-teku-geth'\nRequest had response code '200' and body \"{\\\"data\\\":{\\\"peer_id\\\":\\\"16Uiu2HAm3cDsT18u6JsEyWChNNSzgvqV3RoXmVjXz7P81oyMcpPt\\\",\\\"enr\\\":\\\"enr:-LK4QHST7O3tz5BxlMPUwkOmCgCIx8okqQYAreZZRYDA3hm0by55plJAmhdJ4f1DImZgOlbsAUGCFDSuHjeMbgZCoQsEh2F0dG5ldHOIAAAAAAADAACEZXRoMpBPtyXhYAAAOADh9QUAAAAAgmlkgnY0gmlwhKwQBAyJc2VjcDI1NmsxoQJ5kVzB6T_5uipanu3VhcjzOKc2CXFbp1MJRQRkDXiFA4N0Y3CCIyiDdWRwgiMo\\\",\\\"p2p_addresses\\\":[\\\"/ip4/172.16.4.12/tcp/9000/p2p/16Uiu2HAm3cDsT18u6JsEyWChNNSzgvqV3RoXmVjXz7P81oyMcpPt\\\"],\\\"discovery_addresses\\\":[\\\"/ip4/172.16.4.12/udp/9000/p2p/16Uiu2HAm3cDsT18u6JsEyWChNNSzgvqV3RoXmVjXz7P81oyMcpPt\\\"],\\\"metadata\\\":{\\\"seq_number\\\":\\\"1\\\",\\\"attnets\\\":\\\"0x0000000000030000\\\",\\\"syncnets\\\":\\\"0x00\\\"}}}\", with extracted fields:\n'extract.multiaddr': \"/ip4/172.16.4.12/tcp/9000/p2p/16Uiu2HAm3cDsT18u6JsEyWChNNSzgvqV3RoXmVjXz7P81oyMcpPt\"\n'extract.peer_id': \"16Uiu2HAm3cDsT18u6JsEyWChNNSzgvqV3RoXmVjXz7P81oyMcpPt\"\n'extract.enr': \"enr:-LK4QHST7O3tz5BxlMPUwkOmCgCIx8okqQYAreZZRYDA3hm0by55plJAmhdJ4f1DImZgOlbsAUGCFDSuHjeMbgZCoQsEh2F0dG5ldHOIAAAAAAADAACEZXRoMpBPtyXhYAAAOADh9QUAAAAAgmlkgnY0gmlwhKwQBAyJc2VjcDI1NmsxoQJ5kVzB6T_5uipanu3VhcjzOKc2CXFbp1MJRQRkDXiFA4N0Y3CCIyiDdWRwgiMo\"\n\nPrinting a message\nSuccessfully added 1 CL participants\n\nPrinting a message\nStart adding validators for participant #1\n\nPrinting a message\nNODE JSON RPC URI: '172.16.4.11:8545'\n\nRendering a template to a files artifact with name 'validator-ranges'\nTemplates artifact name 'validator-ranges' rendered with artifact UUID 'f482ab342f0c4defbdd1370be655974a'\n\nPrinting a message\nstruct(additional_preloaded_contracts = \"{\\n  \\\"0x4e59b44847b379578588920cA78FbF26c0B4956C\\\": {\\n    \\\"balance\\\": \\\"0ETH\\\",\\n    \\\"code\\\": \\\"0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf3\\\",\\n    \\\"storage\\\": {},\\n    \\\"nonce\\\": \\\"1\\\"\\n  }\\n}\\n\", altair_fork_epoch = 0, bellatrix_fork_epoch = 0, capella_fork_epoch = 0, churn_limit_quotient = 32, custody_requirement = 4, data_column_sidecar_subnet_count = 128, deneb_fork_epoch = 0, deposit_contract_address = \"0x4242424242424242424242424242424242424242\", devnet_repo = \"ethpandaops\", eip7732_fork_epoch = 100000002, eip7805_fork_epoch = 100000003, ejection_balance = 16000000000, electra_fork_epoch = 100000000, eth1_follow_distance = 16, fulu_fork_epoch = 100000001, genesis_delay = 5, genesis_gaslimit = 30000000, gossip_max_size = 10485760, max_blobs_per_block_electra = 9, max_blobs_per_block_fulu = 12, max_per_epoch_activation_churn_limit = 4, min_validator_withdrawability_delay = 256, network = \"kurtosis\", network_id = \"3151908\", network_sync_base_url = \"https://snapshots.ethpandaops.io/\", num_validator_keys_per_node = 64, prefunded_accounts = {}, preregistered_validator_count = 0, preregistered_validator_keys_mnemonic = \"giant issue aisle success illegal bike spike question tent bar rely arctic volcano long crawl hungry vocal artwork sniff fantasy very lucky have athlete\", preset = \"minimal\", samples_per_slot = 8, seconds_per_slot = 6, shard_committee_period = 64, target_blobs_per_block_electra = 6, target_blobs_per_block_fulu = 9)\n\nPrinting a message\nWaiting for L1 to start up\n\nWait for L1 to start up - can take up to 2 minutes\nCommand returned with exit code '0' and the following output:\n--------------------\nL1 Chain is starting up\nL1 Chain is starting up\nL1 Chain is starting up\nL1 Chain is starting up\nL1 Chain is starting up\nL1 Chain is starting up\nL1 Chain has started!\n\n--------------------\n\nWait for L1 execution to start up - can take up to 2 minutes\nCommand returned with exit code '0' and the following output:\n--------------------\nL1 Execution is starting up\nL1 Execution is starting up\nL1 Execution has started!\n\n--------------------\n\nInitialize L2 contract deployments\nCommand returned with exit code '0' and the following output:\n--------------------\nSuccessfully initialized op-deployer intent in directory: /network-data\n\n--------------------\n\nUploading file '../../static_files/scripts' to files artifact 'op-deployer-fund-script'\nFiles with artifact name 'op-deployer-fund-script' uploaded with artifact UUID '36c507eba470467daa92382f0c42c63f'\n\nCollect keys, and fund addresses\nCommand returned with exit code '0' and the following output:\n--------------------\nWallet private key and addresses\n{\n  \"2151908\": {\n    \"proposerPrivateKey\": \"0x9d77c0558ed6f31e7be16c2f45fd1f3ec3f66d20526b7fe3961872d38ac98fe7\",\n    \"proposerAddress\": \"0xb0994E702b603df7191cd68E6544f99126135e34\",\n    \"batcherPrivateKey\": \"0xb3d2d558e3491a3709b7c451100a0366b5872520c7aa020c17a0e7fa35b6a8df\",\n    \"batcherAddress\": \"0xD3F2c5AFb2D76f5579F326b0cD7DA5F5a4126c35\",\n    \"sequencerPrivateKey\": \"0x459f58fd8ef3d9123333514d0f2445153fc29ddc43be239356dad70f506475d9\",\n    \"sequencerAddress\": \"0xbb900Cf56918A2639dAA90c3f7DC5DCD2f5B9935\",\n    \"challengerPrivateKey\": \"0x717c53f6d6c266889465d78a885cd0a2e22d41f73e21fa1f07ba5849c82d79c3\",\n    \"challengerAddress\": \"0xf08f610d1956CAAAb34f18e9e0A122E389496529\",\n    \"l2ProxyAdminPrivateKey\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\",\n    \"l2ProxyAdminAddress\": \"0x589A698b7b7dA0Bec545177D3963A2741105C7C9\",\n    \"l1ProxyAdminPrivateKey\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\",\n    \"l1ProxyAdminAddress\": \"0x589A698b7b7dA0Bec545177D3963A2741105C7C9\",\n    \"baseFeeVaultRecipientPrivateKey\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\",\n    \"baseFeeVaultRecipientAddress\": \"0x589A698b7b7dA0Bec545177D3963A2741105C7C9\",\n    \"l1FeeVaultRecipientPrivateKey\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\",\n    \"l1FeeVaultRecipientAddress\": \"0x589A698b7b7dA0Bec545177D3963A2741105C7C9\",\n    \"sequencerFeeVaultRecipientPrivateKey\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\",\n    \"sequencerFeeVaultRecipientAddress\": \"0x589A698b7b7dA0Bec545177D3963A2741105C7C9\",\n    \"systemConfigOwnerPrivateKey\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\",\n    \"systemConfigOwnerAddress\": \"0x589A698b7b7dA0Bec545177D3963A2741105C7C9\",\n    \"l1FaucetPrivateKey\": \"0x04b9f63ecf84210c5366c66d68fa1f5da1fa4f634fad6dfc86178e4d79ff9e59\",\n    \"l1FaucetAddress\": \"0xafF0CA253b97e54440965855cec0A8a2E2399896\",\n    \"l2FaucetPrivateKey\": \"0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\",\n    \"l2FaucetAddress\": \"0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266\"\n  }\n}\n\nblockHash               0x326465fc9b168b3b605d6b38fb00302c99ba65ac742f7ff9d860e98fa1ed22c5\nblockNumber             5\ncontractAddress\ncumulativeGasUsed       21000\neffectiveGasPrice       1339843751\nfrom                    0xD8F3183DEF51A987222D845be228e0Bbb932C222\ngasUsed                 21000\nlogs                    []\nlogsBloom               0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\nroot\nstatus                  1 (success)\ntransactionHash         0x5bf327d2719fb67fe959b4c132bab0841a93592f47630427940440534bef44a7\ntransactionIndex        0\ntype                    2\nblobGasPrice\nblobGasUsed\nauthorizationList\nto                      0xb0994E702b603df7191cd68E6544f99126135e34\n\nblockHash               0x326465fc9b168b3b605d6b38fb00302c99ba65ac742f7ff9d860e98fa1ed22c5\nblockNumber             5\ncontractAddress\ncumulativeGasUsed       42000\neffectiveGasPrice       1339843751\nfrom                    0xD8F3183DEF51A987222D845be228e0Bbb932C222\ngasUsed                 21000\nlogs                    []\nlogsBloom               0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\nroot\nstatus                  1 (success)\ntransactionHash         0x1a2ca0b2a59bee7102487aa5e0c2d2c3b2a683a07028a248856d4b48252f64fc\ntransactionIndex        1\ntype                    2\nblobGasPrice\nblobGasUsed\nauthorizationList\nto                      0xD3F2c5AFb2D76f5579F326b0cD7DA5F5a4126c35\n\nblockHash               0x326465fc9b168b3b605d6b38fb00302c99ba65ac742f7ff9d860e98fa1ed22c5\nblockNumber             5\ncontractAddress\ncumulativeGasUsed       63000\neffectiveGasPrice       1339843751\nfrom                    0xD8F3183DEF51A987222D845be228e0Bbb932C222\ngasUsed                 21000\nlogs                    []\nlogsBloom               0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\nroot\nstatus                  1 (success)\ntransactionHash         0xbf29d97e6e86a4cac3ce4e36f0d0bcc4f184175ebcfbf4459d97aa699164063b\ntransactionIndex        2\ntype                    2\nblobGasPrice\nblobGasUsed\nauthorizationList\nto                      0xbb900Cf56918A2639dAA90c3f7DC5DCD2f5B9935\n\nblockHash               0x326465fc9b168b3b605d6b38fb00302c99ba65ac742f7ff9d860e98fa1ed22c5\nblockNumber             5\ncontractAddress\ncumulativeGasUsed       84000\neffectiveGasPrice       1339843751\nfrom                    0xD8F3183DEF51A987222D845be228e0Bbb932C222\ngasUsed                 21000\nlogs                    []\nlogsBloom               0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\nroot\nstatus                  1 (success)\ntransactionHash         0x1badddb3e829431737ce6ae7e98bbf78ed5c7dc4bfd0ad0198fe64699b4ce7dd\ntransactionIndex        3\ntype                    2\nblobGasPrice\nblobGasUsed\nauthorizationList\nto                      0xf08f610d1956CAAAb34f18e9e0A122E389496529\n\n--------------------\n\nWrite value to a file artifact\nCommand returned with exit code '0' with no output\n\nConfigure L2 contract deployments\nCommand returned with exit code '0' with no output\n\nApply L2 contract deployments\nCommand returned with exit code '0' and the following output:\n--------------------\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (242/242 MB, 1.5 GB/s)\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Initialized path database\" readonly=true cache=\"0.00 B\" buffer=\"0.00 B\" history=0\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"initializing pipeline\" stage=init strategy=live\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"deploying superchain\" stage=deploy-superchain\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"transaction broadcasted\" id=0xed74a0d442bb0ac5dd982a9def75785d7dd1861ece32e73c3cbe68ca2f26f5e4 nonce=0\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x85350b5c80489c56c1b8775085cd9976aa1ed836235c44d771e99ebf1e4790a0 nonce=0 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=2967074\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x85350b5c80489c56c1b8775085cd9976aa1ed836235c44d771e99ebf1e4790a0 nonce=0 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=2967074 tx=0x85350b5c80489c56c1b8775085cd9976aa1ed836235c44d771e99ebf1e4790a0\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"transaction broadcasted\" id=0x1f1bab7319e17c5fb5618beaaa67d5796657ed9811dea33bdb098d9941d1cde4 nonce=0\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x5509298ed72ffe32d1883dacab15e3115435ca46da1f5cd540220ef80c3c39cc nonce=1 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=1181566\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x5509298ed72ffe32d1883dacab15e3115435ca46da1f5cd540220ef80c3c39cc nonce=1 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=1181566 tx=0x5509298ed72ffe32d1883dacab15e3115435ca46da1f5cd540220ef80c3c39cc\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"transaction broadcasted\" id=0xad0144f668fd46e06f254098cdf2afd96eaceccb6b7acfe31185fef9ef8a434b nonce=0\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x0217aaec67099cf7fbfd493d2c2d611d6778f7dc2dd3fdcca65206b9d3ab062f nonce=2 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=1259570\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x0217aaec67099cf7fbfd493d2c2d611d6778f7dc2dd3fdcca65206b9d3ab062f nonce=2 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=1259570 tx=0x0217aaec67099cf7fbfd493d2c2d611d6778f7dc2dd3fdcca65206b9d3ab062f\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"transaction broadcasted\" id=0x492bf79b83a2841e251a5eda7b98ae86b8dbe2e1de4b4245f891f67183cd1b8d nonce=3\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xc599f97ebeca272fce57d44336b77b1ec37222bd0fae600edc552b62ab4c7fbd nonce=3 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=1049464\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xc599f97ebeca272fce57d44336b77b1ec37222bd0fae600edc552b62ab4c7fbd nonce=3 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=1049464 tx=0xc599f97ebeca272fce57d44336b77b1ec37222bd0fae600edc552b62ab4c7fbd\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"transaction broadcasted\" id=0xdd17b37b84da57100000d066d4ebfb4467c68b7bdbce20c1fae6b0133d1f60a7 nonce=4\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x1aec90760fb8b6f8c2702fd46ea725fc7b34028b074d92b8c56da1fea4421c65 nonce=4 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=219234\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x1aec90760fb8b6f8c2702fd46ea725fc7b34028b074d92b8c56da1fea4421c65 nonce=4 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=219234 tx=0x1aec90760fb8b6f8c2702fd46ea725fc7b34028b074d92b8c56da1fea4421c65\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"transaction broadcasted\" id=0x6c8459eb0fcab36279d3a2e0023b81434d36240198bf59d64a8ed4055bed4c5b nonce=5\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xaf725105b364168e5a6a359253d9f330d706fb0117484e3fc458d0c16c161498 nonce=5 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=1049464\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xaf725105b364168e5a6a359253d9f330d706fb0117484e3fc458d0c16c161498 nonce=5 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=1049464 tx=0xaf725105b364168e5a6a359253d9f330d706fb0117484e3fc458d0c16c161498\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"transaction broadcasted\" id=0x2fd6462817d1460ddec7cd429304b9ae96eeeedd41f07c5d3dab07bf814d1857 nonce=6\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x4bb1a2bb906807805bf4ce98eb68c671757712422bf519456d5f72481adb8176 nonce=6 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=322468\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x4bb1a2bb906807805bf4ce98eb68c671757712422bf519456d5f72481adb8176 nonce=6 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=322468 tx=0x4bb1a2bb906807805bf4ce98eb68c671757712422bf519456d5f72481adb8176\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"transaction broadcasted\" id=0xca438bc1de095f49a22ee576e4b852271d22c028b29030d5c3db092b1425678a nonce=7\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x415edfa296271080366a9df56d2e08ebde3c8e68f96c7f33ac88af71b828035a nonce=7 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=51524\nt=2025-03-26T13:32:27+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x415edfa296271080366a9df56d2e08ebde3c8e68f96c7f33ac88af71b828035a nonce=7 gasTipCap=1000000000 gasFeeCap=3000000000 gasLimit=51524 tx=0x415edfa296271080366a9df56d2e08ebde3c8e68f96c7f33ac88af71b828035a\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x85350b5c80489c56c1b8775085cd9976aa1ed836235c44d771e99ebf1e4790a0 block=0x1677268ec921980ec6b96e5e52f5fd00b912f3105c0b3523dd86d7b49a7934fa:7 effectiveGasPrice=1393010062\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction confirmed\" id=0xed74a0d442bb0ac5dd982a9def75785d7dd1861ece32e73c3cbe68ca2f26f5e4 completed=1 total=8 hash=0x85350b5c80489c56c1b8775085cd9976aa1ed836235c44d771e99ebf1e4790a0 nonce=0 creation=0x4bF8D2E79E33cfd5a8348737CA91bE5F65Ea7dd9\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x5509298ed72ffe32d1883dacab15e3115435ca46da1f5cd540220ef80c3c39cc block=0x1677268ec921980ec6b96e5e52f5fd00b912f3105c0b3523dd86d7b49a7934fa:7 effectiveGasPrice=1393010062\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction confirmed\" id=0x1f1bab7319e17c5fb5618beaaa67d5796657ed9811dea33bdb098d9941d1cde4 completed=2 total=8 hash=0x5509298ed72ffe32d1883dacab15e3115435ca46da1f5cd540220ef80c3c39cc nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x0217aaec67099cf7fbfd493d2c2d611d6778f7dc2dd3fdcca65206b9d3ab062f block=0x1677268ec921980ec6b96e5e52f5fd00b912f3105c0b3523dd86d7b49a7934fa:7 effectiveGasPrice=1393010062\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction confirmed\" id=0xad0144f668fd46e06f254098cdf2afd96eaceccb6b7acfe31185fef9ef8a434b completed=3 total=8 hash=0x0217aaec67099cf7fbfd493d2c2d611d6778f7dc2dd3fdcca65206b9d3ab062f nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xc599f97ebeca272fce57d44336b77b1ec37222bd0fae600edc552b62ab4c7fbd block=0x1677268ec921980ec6b96e5e52f5fd00b912f3105c0b3523dd86d7b49a7934fa:7 effectiveGasPrice=1393010062\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction confirmed\" id=0x492bf79b83a2841e251a5eda7b98ae86b8dbe2e1de4b4245f891f67183cd1b8d completed=4 total=8 hash=0xc599f97ebeca272fce57d44336b77b1ec37222bd0fae600edc552b62ab4c7fbd nonce=3 creation=0x91BF7398aFc3d2691aA23799fdb9175EE2EB6105\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x1aec90760fb8b6f8c2702fd46ea725fc7b34028b074d92b8c56da1fea4421c65 block=0x1677268ec921980ec6b96e5e52f5fd00b912f3105c0b3523dd86d7b49a7934fa:7 effectiveGasPrice=1393010062\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction confirmed\" id=0xdd17b37b84da57100000d066d4ebfb4467c68b7bdbce20c1fae6b0133d1f60a7 completed=5 total=8 hash=0x1aec90760fb8b6f8c2702fd46ea725fc7b34028b074d92b8c56da1fea4421c65 nonce=4 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xaf725105b364168e5a6a359253d9f330d706fb0117484e3fc458d0c16c161498 block=0x1677268ec921980ec6b96e5e52f5fd00b912f3105c0b3523dd86d7b49a7934fa:7 effectiveGasPrice=1393010062\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction confirmed\" id=0x6c8459eb0fcab36279d3a2e0023b81434d36240198bf59d64a8ed4055bed4c5b completed=6 total=8 hash=0xaf725105b364168e5a6a359253d9f330d706fb0117484e3fc458d0c16c161498 nonce=5 creation=0xB74Bb6AE1A1804D283D17e95620dA9b9b0E6E0DA\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x4bb1a2bb906807805bf4ce98eb68c671757712422bf519456d5f72481adb8176 block=0x1677268ec921980ec6b96e5e52f5fd00b912f3105c0b3523dd86d7b49a7934fa:7 effectiveGasPrice=1393010062\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction confirmed\" id=0x2fd6462817d1460ddec7cd429304b9ae96eeeedd41f07c5d3dab07bf814d1857 completed=7 total=8 hash=0x4bb1a2bb906807805bf4ce98eb68c671757712422bf519456d5f72481adb8176 nonce=6 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x415edfa296271080366a9df56d2e08ebde3c8e68f96c7f33ac88af71b828035a block=0x1677268ec921980ec6b96e5e52f5fd00b912f3105c0b3523dd86d7b49a7934fa:7 effectiveGasPrice=1393010062\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction confirmed\" id=0xca438bc1de095f49a22ee576e4b852271d22c028b29030d5c3db092b1425678a completed=8 total=8 hash=0x415edfa296271080366a9df56d2e08ebde3c8e68f96c7f33ac88af71b828035a nonce=7 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"deploying implementations\" stage=deploy-implementations\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x4312d49f7722613b9538479f74e75b2e0f64c59f2fd850aa558233584e19b06a nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x80833a4a8cc613d31009cff81700c4e6f393ed7189914c8d8fb96cfc8d9798ef nonce=8 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=4367602\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x80833a4a8cc613d31009cff81700c4e6f393ed7189914c8d8fb96cfc8d9798ef nonce=8 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=4367602 tx=0x80833a4a8cc613d31009cff81700c4e6f393ed7189914c8d8fb96cfc8d9798ef\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x65a0c35834b1a19c69ea9d54a7cd4d643e9dd837229e27684e9686ee2fa7880e nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x7b2d05f3a23d3b304a59049f15cc0e7cefd2576eb4f6db96b9eddd30775de1cd nonce=9 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=3811212\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x7b2d05f3a23d3b304a59049f15cc0e7cefd2576eb4f6db96b9eddd30775de1cd nonce=9 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=3811212 tx=0x7b2d05f3a23d3b304a59049f15cc0e7cefd2576eb4f6db96b9eddd30775de1cd\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0xd2f2dd0dfb647b1cbe0998a1179aefcc5e5834f8f9dbd49cd357e6d0409fdd56 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xb3866125493fec24185adc8129ce4c50ecd07c3e3acb8de2c4232eafb86c3bd4 nonce=10 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=2355954\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xb3866125493fec24185adc8129ce4c50ecd07c3e3acb8de2c4232eafb86c3bd4 nonce=10 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=2355954 tx=0xb3866125493fec24185adc8129ce4c50ecd07c3e3acb8de2c4232eafb86c3bd4\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x690b87c7eea10a9aaf653c489e49db488ccca9c6c98a05887c5d1f9f0afad30b nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xfe378f9c566922ba01765777978a26c9e33a458826d936cedc4ccc08dd7b77e6 nonce=11 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=5105894\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xfe378f9c566922ba01765777978a26c9e33a458826d936cedc4ccc08dd7b77e6 nonce=11 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=5105894 tx=0xfe378f9c566922ba01765777978a26c9e33a458826d936cedc4ccc08dd7b77e6\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x93df5641127270f7cee2b337ccf236dcee6a98d44953f0a02342c46ba6621329 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xbfdc98403ff346b338d36a32c60fa218c5f681de3a8b595dbe1a00550cc9ff5b nonce=12 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=4814360\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xbfdc98403ff346b338d36a32c60fa218c5f681de3a8b595dbe1a00550cc9ff5b nonce=12 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=4814360 tx=0xbfdc98403ff346b338d36a32c60fa218c5f681de3a8b595dbe1a00550cc9ff5b\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x91bfd8750e34560bcadeb2fc2bc5e5900eea4836d7f781451a7afc1d82e77b38 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x0246f2c909a38d4655f685b46730ed37c49c22001a051d6f37f85500b103e1a3 nonce=13 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=9552024\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x0246f2c909a38d4655f685b46730ed37c49c22001a051d6f37f85500b103e1a3 nonce=13 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=9552024 tx=0x0246f2c909a38d4655f685b46730ed37c49c22001a051d6f37f85500b103e1a3\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x1eb11a6c8a744b277a8aa38c763bf9b29df2f78241dfbec58fc93e82d11afbb4 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xfdb8791174c9739d1ae42b38d40d7ef3f4c8158d17fd6f0d2478bbb5fd290fad nonce=14 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1952698\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xfdb8791174c9739d1ae42b38d40d7ef3f4c8158d17fd6f0d2478bbb5fd290fad nonce=14 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1952698 tx=0xfdb8791174c9739d1ae42b38d40d7ef3f4c8158d17fd6f0d2478bbb5fd290fad\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x1322b30a3b54380aed07aa4ed59de69ed6ed0cdf3b64cce94d2eb2af464533f0 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x52ddf64d7332848b7342e3155cfb9b3aca9d19dfee093fc39258b606dda64cb0 nonce=15 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=2598674\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x52ddf64d7332848b7342e3155cfb9b3aca9d19dfee093fc39258b606dda64cb0 nonce=15 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=2598674 tx=0x52ddf64d7332848b7342e3155cfb9b3aca9d19dfee093fc39258b606dda64cb0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0xfd8318c9d88597bbbd374519bba7457f2181976552431965f813b938e86471ab nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xe9ea936329e80836b1f760bd8f4b652e4371f29896d086643d96cde2c3e36866 nonce=16 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=6935494\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xe9ea936329e80836b1f760bd8f4b652e4371f29896d086643d96cde2c3e36866 nonce=16 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=6935494 tx=0xe9ea936329e80836b1f760bd8f4b652e4371f29896d086643d96cde2c3e36866\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x9530d670676a6c2b7b4f97eecc4bdf51caba9580ed3b2fa5b0faf9ad03579dea nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x392965d2dfead5a655496d593d1f83ec1f83f68cab303cf0f4f09c2fe72e9704 nonce=17 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=8835848\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x392965d2dfead5a655496d593d1f83ec1f83f68cab303cf0f4f09c2fe72e9704 nonce=17 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=8835848 tx=0x392965d2dfead5a655496d593d1f83ec1f83f68cab303cf0f4f09c2fe72e9704\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x590c79b4aeddbf5f0051d02b61acb6376d23af2c978494b97b2b7b8bf364b538 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xe74cf2e88255942c6e341f2f727e98b5acdb3eb6ea62e043bbacee46ba521456 nonce=18 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=2537398\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xe74cf2e88255942c6e341f2f727e98b5acdb3eb6ea62e043bbacee46ba521456 nonce=18 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=2537398 tx=0xe74cf2e88255942c6e341f2f727e98b5acdb3eb6ea62e043bbacee46ba521456\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x3c126abaa16a379afacb424fe65bfa472c60a792d3e3fc9756270a354e3a1bf6 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xce372b3b10c8be54a11137415b0135371985cc6e26b7b26e10b5fcb50c718437 nonce=19 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=2924928\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xce372b3b10c8be54a11137415b0135371985cc6e26b7b26e10b5fcb50c718437 nonce=19 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=2924928 tx=0xce372b3b10c8be54a11137415b0135371985cc6e26b7b26e10b5fcb50c718437\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0xbf0667587094dcb5f8e827e4ac934901d63c4d3d35dcdbdd995411708f5983a4 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xef76541b6c7e2c1173faf05f64ae0d65cd5d58629b498f2d2ee10a2535911c63 nonce=20 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=813834\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xef76541b6c7e2c1173faf05f64ae0d65cd5d58629b498f2d2ee10a2535911c63 nonce=20 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=813834 tx=0xef76541b6c7e2c1173faf05f64ae0d65cd5d58629b498f2d2ee10a2535911c63\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x7587f03d3baf3591152531f91fa10844363ce98caa3d2cbe208d7a69a89f932d nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xc11cac4d9626bd223a4b332bcc06c0709b1a1ff71fefa5b44849f421fdc87bf4 nonce=21 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1113220\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xc11cac4d9626bd223a4b332bcc06c0709b1a1ff71fefa5b44849f421fdc87bf4 nonce=21 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1113220 tx=0xc11cac4d9626bd223a4b332bcc06c0709b1a1ff71fefa5b44849f421fdc87bf4\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x48ab683a172335b8cec352d1a061d467e7d1ea1abdfb6ba0aa4b7106acec1f00 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x794e8d683cae62e6b8b90f2d3e877b4ef5b2c997ae8a38f174790199be568a98 nonce=22 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=3007286\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x794e8d683cae62e6b8b90f2d3e877b4ef5b2c997ae8a38f174790199be568a98 nonce=22 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=3007286 tx=0x794e8d683cae62e6b8b90f2d3e877b4ef5b2c997ae8a38f174790199be568a98\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0xa9ae77cc529702392b4f338bad66feaae0d51ca8448c1a169c66be5687ed62cd nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x205baef4d1bf63df04c71db8fe3f88d53bf2b4ef12e3da7799d76352c7aeaf99 nonce=23 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1236622\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x205baef4d1bf63df04c71db8fe3f88d53bf2b4ef12e3da7799d76352c7aeaf99 nonce=23 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1236622 tx=0x205baef4d1bf63df04c71db8fe3f88d53bf2b4ef12e3da7799d76352c7aeaf99\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x504864e3d8ffdf7428128760279aa374154b3f4629f66ee0b535cb0307a83bbd nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x5a5e508468efb707d9910505c703135e7e2ffc0d7a5e9760a9013a274cc56612 nonce=24 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=761320\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x5a5e508468efb707d9910505c703135e7e2ffc0d7a5e9760a9013a274cc56612 nonce=24 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=761320 tx=0x5a5e508468efb707d9910505c703135e7e2ffc0d7a5e9760a9013a274cc56612\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0xe27075a902257b3ae4a62a2a14282e779e6b36718b6eebbe341991b153803660 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x2ebc26646f889f9f813ea94f4b3f58899a34a88e3b11aa22b9cbcdc544d64e77 nonce=25 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=10155402\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x715052d7903e6899eeb93163679b39dff07d4d430c3628efec967d3d1ef15a74 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xb2a9684a62487512c0fc1adbe51f1f356684f76d68e2da226a198b67e773ca77 nonce=26 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1055150\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x2ebc26646f889f9f813ea94f4b3f58899a34a88e3b11aa22b9cbcdc544d64e77 nonce=25 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=10155402 tx=0x2ebc26646f889f9f813ea94f4b3f58899a34a88e3b11aa22b9cbcdc544d64e77\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xb2a9684a62487512c0fc1adbe51f1f356684f76d68e2da226a198b67e773ca77 nonce=26 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1055150 tx=0xb2a9684a62487512c0fc1adbe51f1f356684f76d68e2da226a198b67e773ca77\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x00dbc9a3edf4a20887620d4e602fb6fe39d45129a3d141658413c371b33e73da nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x585c4de51a98dd1d7a7fba55ad0107f4655873506f52084d1a7847649e425f8b nonce=27 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=10162482\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x059b0144805180dbf3c149b94ccd06a1aaa4050ea2f39e25042e6cc280e2ced9 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x5fa6f3dd2202e5213c31a6aaa6efa74eb444a00ab144fe8cf2ba88fe8ab0a589 nonce=28 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=756160\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x585c4de51a98dd1d7a7fba55ad0107f4655873506f52084d1a7847649e425f8b nonce=27 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=10162482 tx=0x585c4de51a98dd1d7a7fba55ad0107f4655873506f52084d1a7847649e425f8b\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x5fa6f3dd2202e5213c31a6aaa6efa74eb444a00ab144fe8cf2ba88fe8ab0a589 nonce=28 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=756160 tx=0x5fa6f3dd2202e5213c31a6aaa6efa74eb444a00ab144fe8cf2ba88fe8ab0a589\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0xd4b4a5c189dd2fb81c81c84d43da2c4ca19b87d9f93b2dbde49f3075b0951842 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x76cdbb1a11fee09b291e578d1782d36fcc4b211cbc9ee54ca745c7adb445f577 nonce=29 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=10159962\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x52bb79f61ffd2ae9f1fb72008ec60d2f231f1e5006cacf6979708e918276e1fb nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x6a9ef29d706899d24b6c37685595362ea078f19ef1ae30c0689417b4b7c3a3a7 nonce=30 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=536436\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x6a9ef29d706899d24b6c37685595362ea078f19ef1ae30c0689417b4b7c3a3a7 nonce=30 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=536436 tx=0x6a9ef29d706899d24b6c37685595362ea078f19ef1ae30c0689417b4b7c3a3a7\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x76cdbb1a11fee09b291e578d1782d36fcc4b211cbc9ee54ca745c7adb445f577 nonce=29 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=10159962 tx=0x76cdbb1a11fee09b291e578d1782d36fcc4b211cbc9ee54ca745c7adb445f577\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0xb0cd954f0d22c8fc6ce33a8a62e330f8140b903cf5cef05a1b68106e0171dde5 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x79ee75d5b0e49ce39dbe4184a1b44445d2fd87cbf89286efe4209f904d83275a nonce=31 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=10166490\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x79ee75d5b0e49ce39dbe4184a1b44445d2fd87cbf89286efe4209f904d83275a nonce=31 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=10166490 tx=0x79ee75d5b0e49ce39dbe4184a1b44445d2fd87cbf89286efe4209f904d83275a\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x10dcf60fb94c4b9a2c7ee841bc98293fa65a264d6bf534d145ac9562ac2f1b97 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x6c31c93bec35bffcd56e76e92df8ef5fc218a0ec55201d5ce3b6ab82fb9eb61d nonce=32 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=209336\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x6c31c93bec35bffcd56e76e92df8ef5fc218a0ec55201d5ce3b6ab82fb9eb61d nonce=32 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=209336 tx=0x6c31c93bec35bffcd56e76e92df8ef5fc218a0ec55201d5ce3b6ab82fb9eb61d\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x1eb2fb7452f77ce4a809c125162827fff5154d4ac8357dcffe8b6201d36ba1f7 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xb5b8a07855fd1d8b67dab95f2d31f8336db19412c02c6d8d5683104efeae9e18 nonce=33 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1777504\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xb5b8a07855fd1d8b67dab95f2d31f8336db19412c02c6d8d5683104efeae9e18 nonce=33 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=1777504 tx=0xb5b8a07855fd1d8b67dab95f2d31f8336db19412c02c6d8d5683104efeae9e18\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x986fc58634248cf9fd9e14938b29883e2a0ac3291718e9adb8403a7c341c2e94 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x96585e1dd798cfd8f17e308350a6f452f6440ddb5d3b55f8350de6974af1de5f nonce=34 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=5955100\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x96585e1dd798cfd8f17e308350a6f452f6440ddb5d3b55f8350de6974af1de5f nonce=34 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=5955100 tx=0x96585e1dd798cfd8f17e308350a6f452f6440ddb5d3b55f8350de6974af1de5f\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x837d574a9ebca1a21e60ca89fe0b73b8b3eb81629d96043fae7c2f5ad31b881c nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x0caf34b5165a30eb4bd3685ce2dfe0e542ce01bd1a65db9b674a09b225c77c45 nonce=35 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=6469576\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x0caf34b5165a30eb4bd3685ce2dfe0e542ce01bd1a65db9b674a09b225c77c45 nonce=35 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=6469576 tx=0x0caf34b5165a30eb4bd3685ce2dfe0e542ce01bd1a65db9b674a09b225c77c45\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0xf18a5efe29df991e95a6af3f65085ff59ec2e9d22c007791424d17e12655ca20 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x5f0a8015128f611fb1334b21867c0dbb97a2936b5418df263e1951d941ee6251 nonce=36 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=5075254\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x5f0a8015128f611fb1334b21867c0dbb97a2936b5418df263e1951d941ee6251 nonce=36 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=5075254 tx=0x5f0a8015128f611fb1334b21867c0dbb97a2936b5418df263e1951d941ee6251\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"transaction broadcasted\" id=0x01f960dfa2209c245ac49341d9985550a67d6674773bbfe39f82a4dda8018ad7 nonce=0\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xc754df3cdf2f8d72988ba8a17c04b8f317a763fd725b7c13f92be78dd0bf0170 nonce=37 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=5382568\nt=2025-03-26T13:32:32+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xc754df3cdf2f8d72988ba8a17c04b8f317a763fd725b7c13f92be78dd0bf0170 nonce=37 gasTipCap=4134674075 gasFeeCap=6134674075 gasLimit=5382568 tx=0xc754df3cdf2f8d72988ba8a17c04b8f317a763fd725b7c13f92be78dd0bf0170\nt=2025-03-26T13:32:32+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:34+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:36+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:38+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xfe378f9c566922ba01765777978a26c9e33a458826d936cedc4ccc08dd7b77e6 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x80833a4a8cc613d31009cff81700c4e6f393ed7189914c8d8fb96cfc8d9798ef block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xbfdc98403ff346b338d36a32c60fa218c5f681de3a8b595dbe1a00550cc9ff5b block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x7b2d05f3a23d3b304a59049f15cc0e7cefd2576eb4f6db96b9eddd30775de1cd block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xb3866125493fec24185adc8129ce4c50ecd07c3e3acb8de2c4232eafb86c3bd4 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x0246f2c909a38d4655f685b46730ed37c49c22001a051d6f37f85500b103e1a3 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xfdb8791174c9739d1ae42b38d40d7ef3f4c8158d17fd6f0d2478bbb5fd290fad block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x52ddf64d7332848b7342e3155cfb9b3aca9d19dfee093fc39258b606dda64cb0 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xe9ea936329e80836b1f760bd8f4b652e4371f29896d086643d96cde2c3e36866 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x392965d2dfead5a655496d593d1f83ec1f83f68cab303cf0f4f09c2fe72e9704 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xe74cf2e88255942c6e341f2f727e98b5acdb3eb6ea62e043bbacee46ba521456 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xef76541b6c7e2c1173faf05f64ae0d65cd5d58629b498f2d2ee10a2535911c63 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xce372b3b10c8be54a11137415b0135371985cc6e26b7b26e10b5fcb50c718437 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:39+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xc11cac4d9626bd223a4b332bcc06c0709b1a1ff71fefa5b44849f421fdc87bf4 block=0xbc98ac17ebb2b98ffdaf2142050d79b5dc163de3438f879b7f2433814fedd94b:8 effectiveGasPrice=4491836694\nt=2025-03-26T13:32:40+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:42+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x5a5e508468efb707d9910505c703135e7e2ffc0d7a5e9760a9013a274cc56612 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x794e8d683cae62e6b8b90f2d3e877b4ef5b2c997ae8a38f174790199be568a98 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x205baef4d1bf63df04c71db8fe3f88d53bf2b4ef12e3da7799d76352c7aeaf99 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x2ebc26646f889f9f813ea94f4b3f58899a34a88e3b11aa22b9cbcdc544d64e77 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xb2a9684a62487512c0fc1adbe51f1f356684f76d68e2da226a198b67e773ca77 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x585c4de51a98dd1d7a7fba55ad0107f4655873506f52084d1a7847649e425f8b block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x5fa6f3dd2202e5213c31a6aaa6efa74eb444a00ab144fe8cf2ba88fe8ab0a589 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x76cdbb1a11fee09b291e578d1782d36fcc4b211cbc9ee54ca745c7adb445f577 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x6a9ef29d706899d24b6c37685595362ea078f19ef1ae30c0689417b4b7c3a3a7 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x79ee75d5b0e49ce39dbe4184a1b44445d2fd87cbf89286efe4209f904d83275a block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x6c31c93bec35bffcd56e76e92df8ef5fc218a0ec55201d5ce3b6ab82fb9eb61d block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xb5b8a07855fd1d8b67dab95f2d31f8336db19412c02c6d8d5683104efeae9e18 block=0x742f2cb115ffcbb3b8aa76370f43a07565fb6cb9d99efc384b7cd469686db78d:9 effectiveGasPrice=4533294234\nt=2025-03-26T13:32:44+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:46+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:48+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x5f0a8015128f611fb1334b21867c0dbb97a2936b5418df263e1951d941ee6251 block=0xee8ed75ac00fab2ca0309e0eaeffdf6d66c77a92eac72337e124371a3fa40613:10 effectiveGasPrice=4566688617\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xc754df3cdf2f8d72988ba8a17c04b8f317a763fd725b7c13f92be78dd0bf0170 block=0xee8ed75ac00fab2ca0309e0eaeffdf6d66c77a92eac72337e124371a3fa40613:10 effectiveGasPrice=4566688617\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x0caf34b5165a30eb4bd3685ce2dfe0e542ce01bd1a65db9b674a09b225c77c45 block=0xee8ed75ac00fab2ca0309e0eaeffdf6d66c77a92eac72337e124371a3fa40613:10 effectiveGasPrice=4566688617\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x96585e1dd798cfd8f17e308350a6f452f6440ddb5d3b55f8350de6974af1de5f block=0xee8ed75ac00fab2ca0309e0eaeffdf6d66c77a92eac72337e124371a3fa40613:10 effectiveGasPrice=4566688617\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction broadcasted\" id=0x8a7b85ad56728eab73c01b81db25ec61a2efb3b40a0b15ec6b9e411c5216d4fb nonce=0\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x4312d49f7722613b9538479f74e75b2e0f64c59f2fd850aa558233584e19b06a completed=1 total=31 hash=0x80833a4a8cc613d31009cff81700c4e6f393ed7189914c8d8fb96cfc8d9798ef nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x4ff76e4138efea7af1cc0ef1ebf8985518364c5713e78c3b086675f3c9dabd01 nonce=38 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=4003886\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x65a0c35834b1a19c69ea9d54a7cd4d643e9dd837229e27684e9686ee2fa7880e completed=2 total=31 hash=0x7b2d05f3a23d3b304a59049f15cc0e7cefd2576eb4f6db96b9eddd30775de1cd nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0xd2f2dd0dfb647b1cbe0998a1179aefcc5e5834f8f9dbd49cd357e6d0409fdd56 completed=3 total=31 hash=0xb3866125493fec24185adc8129ce4c50ecd07c3e3acb8de2c4232eafb86c3bd4 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x690b87c7eea10a9aaf653c489e49db488ccca9c6c98a05887c5d1f9f0afad30b completed=4 total=31 hash=0xfe378f9c566922ba01765777978a26c9e33a458826d936cedc4ccc08dd7b77e6 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x93df5641127270f7cee2b337ccf236dcee6a98d44953f0a02342c46ba6621329 completed=5 total=31 hash=0xbfdc98403ff346b338d36a32c60fa218c5f681de3a8b595dbe1a00550cc9ff5b nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x91bfd8750e34560bcadeb2fc2bc5e5900eea4836d7f781451a7afc1d82e77b38 completed=6 total=31 hash=0x0246f2c909a38d4655f685b46730ed37c49c22001a051d6f37f85500b103e1a3 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x1eb11a6c8a744b277a8aa38c763bf9b29df2f78241dfbec58fc93e82d11afbb4 completed=7 total=31 hash=0xfdb8791174c9739d1ae42b38d40d7ef3f4c8158d17fd6f0d2478bbb5fd290fad nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x1322b30a3b54380aed07aa4ed59de69ed6ed0cdf3b64cce94d2eb2af464533f0 completed=8 total=31 hash=0x52ddf64d7332848b7342e3155cfb9b3aca9d19dfee093fc39258b606dda64cb0 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0xfd8318c9d88597bbbd374519bba7457f2181976552431965f813b938e86471ab completed=9 total=31 hash=0xe9ea936329e80836b1f760bd8f4b652e4371f29896d086643d96cde2c3e36866 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x9530d670676a6c2b7b4f97eecc4bdf51caba9580ed3b2fa5b0faf9ad03579dea completed=10 total=31 hash=0x392965d2dfead5a655496d593d1f83ec1f83f68cab303cf0f4f09c2fe72e9704 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x590c79b4aeddbf5f0051d02b61acb6376d23af2c978494b97b2b7b8bf364b538 completed=11 total=31 hash=0xe74cf2e88255942c6e341f2f727e98b5acdb3eb6ea62e043bbacee46ba521456 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x3c126abaa16a379afacb424fe65bfa472c60a792d3e3fc9756270a354e3a1bf6 completed=12 total=31 hash=0xce372b3b10c8be54a11137415b0135371985cc6e26b7b26e10b5fcb50c718437 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0xbf0667587094dcb5f8e827e4ac934901d63c4d3d35dcdbdd995411708f5983a4 completed=13 total=31 hash=0xef76541b6c7e2c1173faf05f64ae0d65cd5d58629b498f2d2ee10a2535911c63 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x7587f03d3baf3591152531f91fa10844363ce98caa3d2cbe208d7a69a89f932d completed=14 total=31 hash=0xc11cac4d9626bd223a4b332bcc06c0709b1a1ff71fefa5b44849f421fdc87bf4 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x48ab683a172335b8cec352d1a061d467e7d1ea1abdfb6ba0aa4b7106acec1f00 completed=15 total=31 hash=0x794e8d683cae62e6b8b90f2d3e877b4ef5b2c997ae8a38f174790199be568a98 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0xa9ae77cc529702392b4f338bad66feaae0d51ca8448c1a169c66be5687ed62cd completed=16 total=31 hash=0x205baef4d1bf63df04c71db8fe3f88d53bf2b4ef12e3da7799d76352c7aeaf99 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x504864e3d8ffdf7428128760279aa374154b3f4629f66ee0b535cb0307a83bbd completed=17 total=31 hash=0x5a5e508468efb707d9910505c703135e7e2ffc0d7a5e9760a9013a274cc56612 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0xe27075a902257b3ae4a62a2a14282e779e6b36718b6eebbe341991b153803660 completed=18 total=31 hash=0x2ebc26646f889f9f813ea94f4b3f58899a34a88e3b11aa22b9cbcdc544d64e77 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x715052d7903e6899eeb93163679b39dff07d4d430c3628efec967d3d1ef15a74 completed=19 total=31 hash=0xb2a9684a62487512c0fc1adbe51f1f356684f76d68e2da226a198b67e773ca77 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x00dbc9a3edf4a20887620d4e602fb6fe39d45129a3d141658413c371b33e73da completed=20 total=31 hash=0x585c4de51a98dd1d7a7fba55ad0107f4655873506f52084d1a7847649e425f8b nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x059b0144805180dbf3c149b94ccd06a1aaa4050ea2f39e25042e6cc280e2ced9 completed=21 total=31 hash=0x5fa6f3dd2202e5213c31a6aaa6efa74eb444a00ab144fe8cf2ba88fe8ab0a589 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0xd4b4a5c189dd2fb81c81c84d43da2c4ca19b87d9f93b2dbde49f3075b0951842 completed=22 total=31 hash=0x76cdbb1a11fee09b291e578d1782d36fcc4b211cbc9ee54ca745c7adb445f577 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x52bb79f61ffd2ae9f1fb72008ec60d2f231f1e5006cacf6979708e918276e1fb completed=23 total=31 hash=0x6a9ef29d706899d24b6c37685595362ea078f19ef1ae30c0689417b4b7c3a3a7 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0xb0cd954f0d22c8fc6ce33a8a62e330f8140b903cf5cef05a1b68106e0171dde5 completed=24 total=31 hash=0x79ee75d5b0e49ce39dbe4184a1b44445d2fd87cbf89286efe4209f904d83275a nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x10dcf60fb94c4b9a2c7ee841bc98293fa65a264d6bf534d145ac9562ac2f1b97 completed=25 total=31 hash=0x6c31c93bec35bffcd56e76e92df8ef5fc218a0ec55201d5ce3b6ab82fb9eb61d nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x1eb2fb7452f77ce4a809c125162827fff5154d4ac8357dcffe8b6201d36ba1f7 completed=26 total=31 hash=0xb5b8a07855fd1d8b67dab95f2d31f8336db19412c02c6d8d5683104efeae9e18 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x986fc58634248cf9fd9e14938b29883e2a0ac3291718e9adb8403a7c341c2e94 completed=27 total=31 hash=0x96585e1dd798cfd8f17e308350a6f452f6440ddb5d3b55f8350de6974af1de5f nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x837d574a9ebca1a21e60ca89fe0b73b8b3eb81629d96043fae7c2f5ad31b881c completed=28 total=31 hash=0x0caf34b5165a30eb4bd3685ce2dfe0e542ce01bd1a65db9b674a09b225c77c45 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0xf18a5efe29df991e95a6af3f65085ff59ec2e9d22c007791424d17e12655ca20 completed=29 total=31 hash=0x5f0a8015128f611fb1334b21867c0dbb97a2936b5418df263e1951d941ee6251 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"transaction confirmed\" id=0x01f960dfa2209c245ac49341d9985550a67d6674773bbfe39f82a4dda8018ad7 completed=30 total=31 hash=0xc754df3cdf2f8d72988ba8a17c04b8f317a763fd725b7c13f92be78dd0bf0170 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:50+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x4ff76e4138efea7af1cc0ef1ebf8985518364c5713e78c3b086675f3c9dabd01 nonce=38 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=4003886 tx=0x4ff76e4138efea7af1cc0ef1ebf8985518364c5713e78c3b086675f3c9dabd01\nt=2025-03-26T13:32:56+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x4ff76e4138efea7af1cc0ef1ebf8985518364c5713e78c3b086675f3c9dabd01 block=0x80241e3726bef176e89324bf157d308504be38f136fd474ebe21545d63595e66:11 effectiveGasPrice=21092670125\nt=2025-03-26T13:32:56+0000 lvl=info msg=\"transaction confirmed\" id=0x8a7b85ad56728eab73c01b81db25ec61a2efb3b40a0b15ec6b9e411c5216d4fb completed=31 total=31 hash=0x4ff76e4138efea7af1cc0ef1ebf8985518364c5713e78c3b086675f3c9dabd01 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:32:56+0000 lvl=info msg=\"deploying OP chain using local allocs\" stage=deploy-opchain id=0x000000000000000000000000000000000000000000000000000000000020d5e4\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x3B4deCEf0660f96E0202C10B5c6c0d9a035E42FD label=anchorStateRegistryProxy err=\"execution reverted\" depth=3\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x3B4deCEf0660f96E0202C10B5c6c0d9a035E42FD callsite=\"\" label=anchorStateRegistryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=3 byte4=0x38d38c97 addr=0x3B4deCEf0660f96E0202C10B5c6c0d9a035E42FD callsite=\"\" label=anchorStateRegistryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x3B4deCEf0660f96E0202C10B5c6c0d9a035E42FD label=anchorStateRegistryProxy err=\"execution reverted\" revertData=0x depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x3B4deCEf0660f96E0202C10B5c6c0d9a035E42FD label=anchorStateRegistryProxy err=\"execution reverted\" depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x3B4deCEf0660f96E0202C10B5c6c0d9a035E42FD callsite=\"\" label=anchorStateRegistryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x3B4deCEf0660f96E0202C10B5c6c0d9a035E42FD label=anchorStateRegistryProxy err=\"execution reverted\" revertData=0x depth=1\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x3Dd0224A041e63374Bb6B233b87915077c465029 label=disputeGameFactoryProxy err=\"execution reverted\" depth=3\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x3Dd0224A041e63374Bb6B233b87915077c465029 callsite=\"\" label=disputeGameFactoryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=3 byte4=0x38d38c97 addr=0x3Dd0224A041e63374Bb6B233b87915077c465029 callsite=\"\" label=disputeGameFactoryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x3Dd0224A041e63374Bb6B233b87915077c465029 label=disputeGameFactoryProxy err=\"execution reverted\" revertData=0x depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x3Dd0224A041e63374Bb6B233b87915077c465029 label=disputeGameFactoryProxy err=\"execution reverted\" depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x3Dd0224A041e63374Bb6B233b87915077c465029 callsite=\"\" label=disputeGameFactoryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x3Dd0224A041e63374Bb6B233b87915077c465029 label=disputeGameFactoryProxy err=\"execution reverted\" revertData=0x depth=1\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x3C5fd59c5B3907dcfa44959479eF0ACf5CA56c68 label=l1CrossDomainMessengerProxy err=\"execution reverted\" depth=3\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x3C5fd59c5B3907dcfa44959479eF0ACf5CA56c68 callsite=\"\" label=l1CrossDomainMessengerProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=3 byte4=0x38d38c97 addr=0x3C5fd59c5B3907dcfa44959479eF0ACf5CA56c68 callsite=\"\" label=l1CrossDomainMessengerProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x3C5fd59c5B3907dcfa44959479eF0ACf5CA56c68 label=l1CrossDomainMessengerProxy err=\"execution reverted\" revertData=0x depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x3C5fd59c5B3907dcfa44959479eF0ACf5CA56c68 label=l1CrossDomainMessengerProxy err=\"execution reverted\" depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x3C5fd59c5B3907dcfa44959479eF0ACf5CA56c68 callsite=\"\" label=l1CrossDomainMessengerProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x3C5fd59c5B3907dcfa44959479eF0ACf5CA56c68 label=l1CrossDomainMessengerProxy err=\"execution reverted\" revertData=0x depth=1\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x65A76386b510e6ADF5255cA061B71b9148A3FA2e label=l1ERC721BridgeProxy err=\"execution reverted\" depth=3\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x65A76386b510e6ADF5255cA061B71b9148A3FA2e callsite=\"\" label=l1ERC721BridgeProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=3 byte4=0x38d38c97 addr=0x65A76386b510e6ADF5255cA061B71b9148A3FA2e callsite=\"\" label=l1ERC721BridgeProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x65A76386b510e6ADF5255cA061B71b9148A3FA2e label=l1ERC721BridgeProxy err=\"execution reverted\" revertData=0x depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x65A76386b510e6ADF5255cA061B71b9148A3FA2e label=l1ERC721BridgeProxy err=\"execution reverted\" depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x65A76386b510e6ADF5255cA061B71b9148A3FA2e callsite=\"\" label=l1ERC721BridgeProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x65A76386b510e6ADF5255cA061B71b9148A3FA2e label=l1ERC721BridgeProxy err=\"execution reverted\" revertData=0x depth=1\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x6EB3398055b85916711D593f7AF6B89460aBd585 label=l1StandardBridgeProxy err=\"execution reverted\" depth=3\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x6EB3398055b85916711D593f7AF6B89460aBd585 callsite=\"\" label=l1StandardBridgeProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=3 byte4=0x38d38c97 addr=0x6EB3398055b85916711D593f7AF6B89460aBd585 callsite=\"\" label=l1StandardBridgeProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x6EB3398055b85916711D593f7AF6B89460aBd585 label=l1StandardBridgeProxy err=\"execution reverted\" revertData=0x depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x6EB3398055b85916711D593f7AF6B89460aBd585 label=l1StandardBridgeProxy err=\"execution reverted\" depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x6EB3398055b85916711D593f7AF6B89460aBd585 callsite=\"\" label=l1StandardBridgeProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x6EB3398055b85916711D593f7AF6B89460aBd585 label=l1StandardBridgeProxy err=\"execution reverted\" revertData=0x depth=1\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x3Abb4282edE8fF5F3cdB3DCC2341051e2dC47a44 label=optimismMintableERC20FactoryProxy err=\"execution reverted\" depth=3\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x3Abb4282edE8fF5F3cdB3DCC2341051e2dC47a44 callsite=\"\" label=optimismMintableERC20FactoryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=3 byte4=0x38d38c97 addr=0x3Abb4282edE8fF5F3cdB3DCC2341051e2dC47a44 callsite=\"\" label=optimismMintableERC20FactoryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x3Abb4282edE8fF5F3cdB3DCC2341051e2dC47a44 label=optimismMintableERC20FactoryProxy err=\"execution reverted\" revertData=0x depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Fault addr=0x3Abb4282edE8fF5F3cdB3DCC2341051e2dC47a44 label=optimismMintableERC20FactoryProxy err=\"execution reverted\" depth=2\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=1 byte4=0xfc4dcacb addr=0x44476a72FaF563c0e2438C6263184d35EC2CA186 callsite=\"\" label=DeployOPChain\nt=2025-03-26T13:32:56+0000 lvl=warn msg=callframe depth=2 byte4=0x38d38c97 addr=0x3Abb4282edE8fF5F3cdB3DCC2341051e2dC47a44 callsite=\"\" label=optimismMintableERC20FactoryProxy\nt=2025-03-26T13:32:56+0000 lvl=warn msg=Revert addr=0x3Abb4282edE8fF5F3cdB3DCC2341051e2dC47a44 label=optimismMintableERC20FactoryProxy err=\"execution reverted\" revertData=0x depth=1\nt=2025-03-26T13:32:56+0000 lvl=info msg=\"transaction broadcasted\" id=0xc0c18e1ef4c4c32be878031aa595112a9f0b9d3078f67771b0a76c7bb4623b44 nonce=39\nt=2025-03-26T13:32:56+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x59a82eff14b1f37e05164d5018c458dcc4cb2418ebc7622b66d1c0e92dd1946e nonce=39 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=26163082\nt=2025-03-26T13:32:56+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x59a82eff14b1f37e05164d5018c458dcc4cb2418ebc7622b66d1c0e92dd1946e nonce=39 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=26163082 tx=0x59a82eff14b1f37e05164d5018c458dcc4cb2418ebc7622b66d1c0e92dd1946e\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x59a82eff14b1f37e05164d5018c458dcc4cb2418ebc7622b66d1c0e92dd1946e block=0xadb6ab8da137800be33e48e72566c3a8fc58d1332ff8707f7529b1780b05a9af:12 effectiveGasPrice=21047272052\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"transaction confirmed\" id=0xc0c18e1ef4c4c32be878031aa595112a9f0b9d3078f67771b0a76c7bb4623b44 completed=1 total=1 hash=0x59a82eff14b1f37e05164d5018c458dcc4cb2418ebc7622b66d1c0e92dd1946e nonce=39 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"alt-da deployment not needed\" stage=deploy-alt-da\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"using existing preimage oracle\" gameType=0\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"deploying VM\" gameType=0 vmType=CANNON1\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"vm deployed\" gameType=0 vmAddr=0xaA59A0777648BC75cd10364083e878c1cCd6112a\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"deploying dispute game\" gameType=0\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"dispute game deployed\" gameType=0 impl=0xb16d5c55130ea373b6512BF61350A2E15472348F\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"setting dispute game impl on factory\" gameType=0 respected=false\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"transaction broadcasted\" id=0x63755eb2cda83880d51d6b6c3a3a2371f2c2b76d4ec38f4345c2f62ac559302a nonce=0\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0xd3f9dad9c02c153221aa9c94382f3e8bfc69a74e663934371b1fb65f7a4cd4c4 nonce=40 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=4650248\nt=2025-03-26T13:33:02+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0xd3f9dad9c02c153221aa9c94382f3e8bfc69a74e663934371b1fb65f7a4cd4c4 nonce=40 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=4650248 tx=0xd3f9dad9c02c153221aa9c94382f3e8bfc69a74e663934371b1fb65f7a4cd4c4\nt=2025-03-26T13:33:02+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:33:04+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:33:06+0000 lvl=warn msg=\"Failed to create a transaction, will retry\" service=transactor err=\"failed to call: execution reverted, reason: 0x\"\nt=2025-03-26T13:33:08+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0xd3f9dad9c02c153221aa9c94382f3e8bfc69a74e663934371b1fb65f7a4cd4c4 block=0xcde66a0c973f45504aee10d45a1e98c63c7c5322102bada957b1a7d08dd83bb7:13 effectiveGasPrice=21041356433\nt=2025-03-26T13:33:08+0000 lvl=info msg=\"transaction broadcasted\" id=0x89d5f5f09be9f46f4466d5a5aa43654aa920feaf32364caaebe9eabc0ededa7f nonce=0\nt=2025-03-26T13:33:08+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x47dab7b7fb92847707272bbcb66f9c46913f4c4331ae9ddc02c756982406204e nonce=41 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=9992854\nt=2025-03-26T13:33:08+0000 lvl=info msg=\"transaction broadcasted\" id=0xd1b5f5dc0a1d36f59c2332281e01371862dd5614d43750646bbda42ed2839f3f nonce=42\nt=2025-03-26T13:33:08+0000 lvl=info msg=\"transaction confirmed\" id=0x63755eb2cda83880d51d6b6c3a3a2371f2c2b76d4ec38f4345c2f62ac559302a completed=1 total=3 hash=0xd3f9dad9c02c153221aa9c94382f3e8bfc69a74e663934371b1fb65f7a4cd4c4 nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:33:08+0000 lvl=info msg=\"Publishing transaction\" service=transactor tx=0x54f44c908ec8450667c4c030fc59e0a59ab12918f82de6c68d8137961b2ffe72 nonce=42 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=105632\nt=2025-03-26T13:33:08+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x47dab7b7fb92847707272bbcb66f9c46913f4c4331ae9ddc02c756982406204e nonce=41 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=9992854 tx=0x47dab7b7fb92847707272bbcb66f9c46913f4c4331ae9ddc02c756982406204e\nt=2025-03-26T13:33:08+0000 lvl=info msg=\"Transaction successfully published\" service=transactor tx=0x54f44c908ec8450667c4c030fc59e0a59ab12918f82de6c68d8137961b2ffe72 nonce=42 gasTipCap=20673370375 gasFeeCap=22673370375 gasLimit=105632 tx=0x54f44c908ec8450667c4c030fc59e0a59ab12918f82de6c68d8137961b2ffe72\nt=2025-03-26T13:33:14+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x54f44c908ec8450667c4c030fc59e0a59ab12918f82de6c68d8137961b2ffe72 block=0xe82a1bc8db72c6da38bf8244d62d5e33cff4f57a05053f1c249375a804e47eac:14 effectiveGasPrice=21002505205\nt=2025-03-26T13:33:14+0000 lvl=info msg=\"Transaction confirmed\" service=transactor tx=0x47dab7b7fb92847707272bbcb66f9c46913f4c4331ae9ddc02c756982406204e block=0xe82a1bc8db72c6da38bf8244d62d5e33cff4f57a05053f1c249375a804e47eac:14 effectiveGasPrice=21002505205\nt=2025-03-26T13:33:14+0000 lvl=info msg=\"transaction confirmed\" id=0x89d5f5f09be9f46f4466d5a5aa43654aa920feaf32364caaebe9eabc0ededa7f completed=2 total=3 hash=0x47dab7b7fb92847707272bbcb66f9c46913f4c4331ae9ddc02c756982406204e nonce=0 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:33:14+0000 lvl=info msg=\"transaction confirmed\" id=0xd1b5f5dc0a1d36f59c2332281e01371862dd5614d43750646bbda42ed2839f3f completed=3 total=3 hash=0x54f44c908ec8450667c4c030fc59e0a59ab12918f82de6c68d8137961b2ffe72 nonce=42 creation=0x0000000000000000000000000000000000000000\nt=2025-03-26T13:33:14+0000 lvl=info msg=\"generating L2 genesis\" stage=generate-l2-genesis id=0x000000000000000000000000000000000000000000000000000000000020d5e4\nt=2025-03-26T13:33:14+0000 lvl=warn msg=\"RequiredProtocolVersion is empty\" !BADKEY=&{} config=SuperchainL1DeployConfig\nt=2025-03-26T13:33:14+0000 lvl=warn msg=\"RecommendedProtocolVersion is empty\" !BADKEY=&{} config=SuperchainL1DeployConfig\nt=2025-03-26T13:33:14+0000 lvl=warn msg=\"L2OutputOracleStartingBlockNumber is 0, should only be 0 for fresh chains\" !BADKEY=&{} ...\n\nGenerate chainspec\nCommand returned with exit code '0' with no output\n\nUploading file '/static_files/jwt/jwtsecret' to files artifact 'op_jwt_file'\nFiles with artifact name 'op_jwt_file' uploaded with artifact UUID 'b7bcad2e2865425bab38d7dd04fd9f4c'\n\nPrinting a message\nDeploying L2 with name op-kurtosis\n\nus-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest\n>>>\n\nus-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest\n>>>\n\nAdding service with name 'op-el-1-op-geth-op-node-op-kurtosis' and image 'us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest'\nService 'op-el-1-op-geth-op-node-op-kurtosis' added with service UUID 'ce5be3d9b75e4e789c31048a65c00285'\n\nWaiting for at most '15m0s' for service 'op-el-1-op-geth-op-node-op-kurtosis' to reach a certain state\nWait took 1 tries (35.328375ms in total). Assertion passed with following:\nRequest had response code '200' and body \"{\\\"jsonrpc\\\":\\\"2.0\\\",\\\"id\\\":1,\\\"result\\\":{\\\"id\\\":\\\"d9fd0a43e04ed9482e4ef5af77f1cbc2f57cf08a06aa5dc04ecf5c91c74bf0de\\\",\\\"name\\\":\\\"Geth/v1.101503.1-stable-fbc739c3/linux-arm64/go1.24.1\\\",\\\"enode\\\":\\\"enode://a58a6633823e5d2f057466dd8d92f7e1e373e8d70ed327ee12b44f20274b5c34541b0545412929ef10bdbe8dc9112829f8d0c1cf68a6f92b208d5f29cbbd6a19@172.16.4.21:30303\\\",\\\"enr\\\":\\\"enr:-KO4QGJtvLt9l5d9a_2eYyPN-EkbzzIhvhFrbwJmt6TkE1y9OXFNEWfVeqBdgW7WRNTnv0UjqT9ZYurtDtia7vRRTiiGAZXSqEgag2V0aMfGhNYQgTKAgmlkgnY0gmlwhKwQBBWJc2VjcDI1NmsxoQOlimYzgj5dLwV0Zt2Nkvfh43Po1w7TJ-4StE8gJ0tcNIRzbmFwwIN0Y3CCdl-DdWRwgnZf\\\",\\\"ip\\\":\\\"172.16.4.21\\\",\\\"ports\\\":{\\\"discovery\\\":30303,\\\"listener\\\":30303},\\\"listenAddr\\\":\\\"[::]:30303\\\",\\\"protocols\\\":{\\\"eth\\\":{\\\"network\\\":2151908,\\\"genesis\\\":\\\"0x1e3ade6c940bc93484aa43f3c966cf17968c0560edc9a84439ca85e0746666e2\\\",\\\"config\\\":{\\\"chainId\\\":2151908,\\\"homesteadBlock\\\":0,\\\"eip150Block\\\":0,\\\"eip155Block\\\":0,\\\"eip158Block\\\":0,\\\"byzantiumBlock\\\":0,\\\"constantinopleBlock\\\":0,\\\"petersburgBlock\\\":0,\\\"istanbulBlock\\\":0,\\\"muirGlacierBlock\\\":0,\\\"berlinBlock\\\":0,\\\"londonBlock\\\":0,\\\"arrowGlacierBlock\\\":0,\\\"grayGlacierBlock\\\":0,\\\"mergeNetsplitBlock\\\":0,\\\"shanghaiTime\\\":0,\\\"cancunTime\\\":0,\\\"pragueTime\\\":0,\\\"bedrockBlock\\\":0,\\\"regolithTime\\\":0,\\\"canyonTime\\\":0,\\\"ecotoneTime\\\":0,\\\"fjordTime\\\":0,\\\"graniteTime\\\":0,\\\"holoceneTime\\\":0,\\\"isthmusTime\\\":0,\\\"terminalTotalDifficulty\\\":0,\\\"depositContractAddress\\\":\\\"0x0000000000000000000000000000000000000000\\\",\\\"optimism\\\":{\\\"eip1559Elasticity\\\":6,\\\"eip1559Denominator\\\":50,\\\"eip1559DenominatorCanyon\\\":250}},\\\"head\\\":\\\"0x1e3ade6c940bc93484aa43f3c966cf17968c0560edc9a84439ca85e0746666e2\\\"},\\\"snap\\\":{}}}}\\n\", with extracted fields:\n'extract.enode': \"enode://a58a6633823e5d2f057466dd8d92f7e1e373e8d70ed327ee12b44f20274b5c34541b0545412929ef10bdbe8dc9112829f8d0c1cf68a6f92b208d5f29cbbd6a19@172.16.4.21:30303\"\n'extract.enr': \"enr:-KO4QGJtvLt9l5d9a_2eYyPN-EkbzzIhvhFrbwJmt6TkE1y9OXFNEWfVeqBdgW7WRNTnv0UjqT9ZYurtDtia7vRRTiiGAZXSqEgag2V0aMfGhNYQgTKAgmlkgnY0gmlwhKwQBBWJc2VjcDI1NmsxoQOlimYzgj5dLwV0Zt2Nkvfh43Po1w7TJ-4StE8gJ0tcNIRzbmFwwIN0Y3CCdl-DdWRwgnZf\"\n\nRead JSON value\nCommand returned with exit code '0' and the following output: 0x459f58fd8ef3d9123333514d0f2445153fc29ddc43be239356dad70f506475d9\n\nAdding service with name 'op-cl-1-op-node-op-geth-op-kurtosis' and image 'op-node:3b0c9ecb0a87'\nService 'op-cl-1-op-node-op-geth-op-kurtosis' added with service UUID 'fa33f78214be4bd683f0b10026732bdf'\n\nRunning 'POST' request on service 'op-cl-1-op-node-op-geth-op-kurtosis'\nRequest had response code '200' and body \"{\\\"jsonrpc\\\":\\\"2.0\\\",\\\"id\\\":1,\\\"result\\\":{\\\"peerID\\\":\\\"16Uiu2HAkxW46aKSxA7grjj8uksVEweFFwrM1kggWkvjTPa73weMZ\\\",\\\"nodeID\\\":\\\"c18e8e3cefa49c3fbc7dceded396d871a966ad499ce10215384ce6ba81cc65ba\\\",\\\"userAgent\\\":\\\"\\\",\\\"protocolVersion\\\":\\\"\\\",\\\"ENR\\\":\\\"enr:-J-4QIULLcp1zD-2pRE6lm9GkfoxC0tlQADTVWOkItJKimVQHi1130zqEUZpSmKQzzcQjOk6S7i2Mb_gszJdRYKRPqKGAZXSqE6BgmlkgnY0gmlwhKwQBBeHb3BzdGFja4Xkq4MBAIlzZWNwMjU2azGhAi2yZEa4XOxzoN5ot9K7UNZdoo7UD6AE1qQC8DaGdVpsg3RjcIIjK4N1ZHCCIys\\\",\\\"addresses\\\":[\\\"/ip4/127.0.0.1/tcp/9003/p2p/16Uiu2HAkxW46aKSxA7grjj8uksVEweFFwrM1kggWkvjTPa73weMZ\\\",\\\"/ip4/172.16.4.23/tcp/9003/p2p/16Uiu2HAkxW46aKSxA7grjj8uksVEweFFwrM1kggWkvjTPa73weMZ\\\"],\\\"protocols\\\":null,\\\"connectedness\\\":0,\\\"direction\\\":0,\\\"protected\\\":false,\\\"chainID\\\":0,\\\"latency\\\":0,\\\"gossipBlocks\\\":true,\\\"scores\\\":{\\\"gossip\\\":{\\\"total\\\":0,\\\"blocks\\\":{\\\"timeInMesh\\\":0,\\\"firstMessageDeliveries\\\":0,\\\"meshMessageDeliveries\\\":0,\\\"invalidMessageDeliveries\\\":0},\\\"IPColocationFactor\\\":0,\\\"behavioralPenalty\\\":0},\\\"reqResp\\\":{\\\"validResponses\\\":0,\\\"errorResponses\\\":0,\\\"rejectedPayloads\\\":0}}}}\\n\", with extracted fields:\n'extract.enr': \"enr:-J-4QIULLcp1zD-2pRE6lm9GkfoxC0tlQADTVWOkItJKimVQHi1130zqEUZpSmKQzzcQjOk6S7i2Mb_gszJdRYKRPqKGAZXSqE6BgmlkgnY0gmlwhKwQBBeHb3BzdGFja4Xkq4MBAIlzZWNwMjU2azGhAi2yZEa4XOxzoN5ot9K7UNZdoo7UD6AE1qQC8DaGdVpsg3RjcIIjK4N1ZHCCIys\"\n'extract.multiaddr': \"/ip4/127.0.0.1/tcp/9003/p2p/16Uiu2HAkxW46aKSxA7grjj8uksVEweFFwrM1kggWkvjTPa73weMZ\"\n'extract.peer_id': \"16Uiu2HAkxW46aKSxA7grjj8uksVEweFFwrM1kggWkvjTPa73weMZ\"\n\nPrinting a message\nSuccessfully added 1 EL/CL participants\n\nRead JSON value\nCommand returned with exit code '0' and the following output: 0xb3d2d558e3491a3709b7c451100a0366b5872520c7aa020c17a0e7fa35b6a8df\n\nAdding service with name 'op-batcher-op-kurtosis' and image 'op-batcher:551aa84b67a0'\nService 'op-batcher-op-kurtosis' added with service UUID 'e8bf4e6b923c4efd9a554869480f9efd'\n\nRead JSON value\nCommand returned with exit code '0' and the following output: 0x3dd0224a041e63374bb6b233b87915077c465029\n\nRead JSON value\nCommand returned with exit code '0' and the following output: 0x9d77c0558ed6f31e7be16c2f45fd1f3ec3f66d20526b7fe3961872d38ac98fe7\n\nAdding service with name 'op-proposer-op-kurtosis' and image 'op-proposer:fc2a2a27ae24'\nService 'op-proposer-op-kurtosis' added with service UUID '7314272ce90947d0aa9f7e570fa38443'\n\nRead JSON value\nCommand returned with exit code '0' and the following output: 0x6eb3398055b85916711d593f7af6b89460abd585\n\nPrinting a message\n[struct(cl_context = struct(beacon_grpc_url = \"\", beacon_http_url = \"http://172.16.4.23:8547\", beacon_service_name = \"op-cl-1-op-node-op-geth-op-kurtosis\", cl_nodes_metrics_info = [{\"name\": \"op-cl-1-op-node-op-geth-op-kurtosis\", \"path\": \"/debug/metrics/prometheus\", \"url\": \"172.16.4.23:9001\", \"config\": None}], client_name = \"op-node\", enr = \"enr:-J-4QIULLcp1zD-2pRE6lm9GkfoxC0tlQADTVWOkItJKimVQHi1130zqEUZpSmKQzzcQjOk6S7i2Mb_gszJdRYKRPqKGAZXSqE6BgmlkgnY0gmlwhKwQBBeHb3BzdGFja4Xkq4MBAIlzZWNwMjU2azGhAi2yZEa4XOxzoN5ot9K7UNZdoo7UD6AE1qQC8DaGdVpsg3RjcIIjK4N1ZHCCIys\", http_port = 8547, ip_addr = \"172.16.4.23\", multiaddr = \"/ip4/127.0.0.1/tcp/9003/p2p/16Uiu2HAkxW46aKSxA7grjj8uksVEweFFwrM1kggWkvjTPa73weMZ\", peer_id = \"16Uiu2HAkxW46aKSxA7grjj8uksVEweFFwrM1kggWkvjTPa73weMZ\", snooper_enabled = False, snooper_engine_context = None, supernode = False, validator_keystore_files_artifact_uuid = \"\"), cl_type = \"op-node\", el_context = struct(client_name = \"op-geth\", el_metrics_info = [{\"name\": \"op-el-1-op-geth-op-node-op-kurtosis\", \"path\": \"/debug/metrics/prometheus\", \"url\": \"172.16.4.21:9001\", \"config\": None}], engine_rpc_port_num = 8551, enode = \"enode://a58a6633823e5d2f057466dd8d92f7e1e373e8d70ed327ee12b44f20274b5c34541b0545412929ef10bdbe8dc9112829f8d0c1cf68a6f92b208d5f29cbbd6a19@172.16.4.21:30303\", enr = \"enr:-KO4QGJtvLt9l5d9a_2eYyPN-EkbzzIhvhFrbwJmt6TkE1y9OXFNEWfVeqBdgW7WRNTnv0UjqT9ZYurtDtia7vRRTiiGAZXSqEgag2V0aMfGhNYQgTKAgmlkgnY0gmlwhKwQBBWJc2VjcDI1NmsxoQOlimYzgj5dLwV0Zt2Nkvfh43Po1w7TJ-4StE8gJ0tcNIRzbmFwwIN0Y3CCdl-DdWRwgnZf\", ip_addr = \"172.16.4.21\", rpc_http_url = \"http://172.16.4.21:8545\", rpc_port_num = 8545, service_name = \"op-el-1-op-geth-op-node-op-kurtosis\", ws_port_num = 8546, ws_url = \"\"), el_type = \"op-geth\")]\n\nPrinting a message\nBegin your L2 adventures by depositing some L1 Kurtosis ETH to: 0x6eb3398055b85916711d593f7af6b89460abd585\n\nRead JSON value\nCommand returned with exit code '0' and the following output: 0x3dd0224a041e63374bb6b233b87915077c465029\n\nRead JSON value\nCommand returned with exit code '0' and the following output: 0x717c53f6d6c266889465d78a885cd0a2e22d41f73e21fa1f07ba5849c82d79c3\n\nAdding service with name 'op-challenger-op-kurtosis' and image 'op-challenger:b865b632af32'\nService 'op-challenger-op-kurtosis' added with service UUID '353ba4876c0746bcba7b42665c06dc58'\n\nPrinting a message\nLaunching prometheus...\n\nRendering a template to a files artifact with name 'prometheus-config'\nTemplates artifact name 'prometheus-config' rendered with artifact UUID '84ad586910704041b5f8614e2d90fb3e'\n\nAdding service with name 'prometheus' and image 'prom/prometheus:v3.1.0'\nService 'prometheus' added with service UUID '7cf8c54e8579433c88ed213eac04117a'\n\nPrinting a message\nLaunching grafana...\n\nRendering a template to a files artifact with name 'grafana-config'\nTemplates artifact name 'grafana-config' rendered with artifact UUID '864aacb927d0499aa1eeffff69d94687'\n\nAdding service with name 'grafana' and image 'grafana/grafana:11.5.0'\nService 'grafana' added with service UUID '3f2f3e7a6e714191801d8d97faf850bf'\n\nUploading file 'github.com/ethereum-optimism/grafana-dashboards-public/resources' to files artifact 'dashboards-0'\nFiles with artifact name 'dashboards-0' uploaded with artifact UUID '6a0e7847acbe4c4fb9cba401cc85ed6d'\n\nupload dashboards\nCommand returned with exit code '0' and the following output:\n--------------------\nApplying 2 resources\nDashboardFolder.Mq1we2M4k added\nDashboardFolder.adxpiz9jgfapsd added\n2 resources added\nApplying 12 resources\nDashboard.GKBLJyh4k added\nDashboard.SF0r6OBVz added\nDashboard.c543e3f0-4d2e-4b49-ac73-99a9363633ad added\nDashboard.cdpde3152v4sgc added\nDashboard.d1dc51c1-2ec5-4b85-8d3c-65eefaeab240 added\nDashboard.ddpymkhr8csn4e added\nDashboard.edc89b93vev40c added\nDashboard.fdryvktnci4n4d added\nDashboard.nUSlc3d4k added\nDashboard.se3IcqO4k added\nDashboard.cdlayzixtjb40a added\nDashboard.cdlf92t90dxq8d added\n12 resources added\n\n--------------------\n\n\u2b50 us on GitHub - https://github.com/kurtosis-tech/kurtosis\n{\n  \"name\": \"\",\n  \"l1\": {\n    \"name\": \"Ethereum\",\n    \"id\": \"3151908\",\n    \"nodes\": [\n      {\n        \"services\": {\n          \"cl\": {\n            \"name\": \"cl-1-teku-geth\",\n            \"endpoints\": {\n              \"http\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56345,\n                \"private_port\": 4000\n              },\n              \"metrics\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56343,\n                \"private_port\": 8008\n              },\n              \"tcp-discovery\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56344,\n                \"private_port\": 9000\n              },\n              \"udp-discovery\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 58088,\n                \"private_port\": 9000\n              }\n            }\n          },\n          \"el\": {\n            \"name\": \"el-1-geth-teku\",\n            \"endpoints\": {\n              \"engine-rpc\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56337,\n                \"private_port\": 8551\n              },\n              \"metrics\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56338,\n                \"private_port\": 9001\n              },\n              \"rpc\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56340,\n                \"private_port\": 8545\n              },\n              \"tcp-discovery\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56339,\n                \"private_port\": 30303\n              },\n              \"udp-discovery\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 58962,\n                \"private_port\": 30303\n              },\n              \"ws\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56341,\n                \"private_port\": 8546\n              }\n            }\n          }\n        }\n      }\n    ],\n    \"wallets\": {\n      \"user-key-0\": {\n        \"address\": \"0x8943545177806ed17b9f23f0a21ee5948ecaa776\",\n        \"private_key\": \"0xbcdf20249abf0ed6d944c0288fad489e33f66b3960d9e6229c1cd214ed3bbe31\"\n      },\n      \"user-key-1\": {\n        \"address\": \"0xe25583099ba105d9ec0a67f5ae86d90e50036425\",\n        \"private_key\": \"0x39725efee3fb28614de3bacaffe4cc4bd8c436257e2c8bb887c4b5c4be45e76d\"\n      },\n      \"user-key-10\": {\n        \"address\": \"0x3e95dfbbaf6b348396e6674c7871546dcc568e56\",\n        \"private_key\": \"0x94eb3102993b41ec55c241060f47daa0f6372e2e3ad7e91612ae36c364042e44\"\n      },\n      \"user-key-11\": {\n        \"address\": \"0x5918b2e647464d4743601a865753e64c8059dc4f\",\n        \"private_key\": \"0xdaf15504c22a352648a71ef2926334fe040ac1d5005019e09f6c979808024dc7\"\n      },\n      \"user-key-12\": {\n        \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n        \"private_key\": \"0xeaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n      },\n      \"user-key-13\": {\n        \"address\": \"0x4d1cb4eb7969f8806e2caac0cbbb71f88c8ec413\",\n        \"private_key\": \"0x3fd98b5187bf6526734efaa644ffbb4e3670d66f5d0268ce0323ec09124bff61\"\n      },\n      \"user-key-14\": {\n        \"address\": \"0xf5504ce2bcc52614f121aff9b93b2001d92715ca\",\n        \"private_key\": \"0x5288e2f440c7f0cb61a9be8afdeb4295f786383f96f5e35eb0c94ef103996b64\"\n      },\n      \"user-key-15\": {\n        \"address\": \"0xf61e98e7d47ab884c244e39e031978e33162ff4b\",\n        \"private_key\": \"0xf296c7802555da2a5a662be70e078cbd38b44f96f8615ae529da41122ce8db05\"\n      },\n      \"user-key-16\": {\n        \"address\": \"0xf1424826861ffbbd25405f5145b5e50d0f1bfc90\",\n        \"private_key\": \"0xbf3beef3bd999ba9f2451e06936f0423cd62b815c9233dd3bc90f7e02a1e8673\"\n      },\n      \"user-key-17\": {\n        \"address\": \"0xfdce42116f541fc8f7b0776e2b30832bd5621c85\",\n        \"private_key\": \"0x6ecadc396415970e91293726c3f5775225440ea0844ae5616135fd10d66b5954\"\n      },\n      \"user-key-18\": {\n        \"address\": \"0xd9211042f35968820a3407ac3d80c725f8f75c14\",\n        \"private_key\": \"0xa492823c3e193d6c595f37a18e3c06650cf4c74558cc818b16130b293716106f\"\n      },\n      \"user-key-19\": {\n        \"address\": \"0xd8f3183def51a987222d845be228e0bbb932c222\",\n        \"private_key\": \"0xc5114526e042343c6d1899cad05e1c00ba588314de9b96929914ee0df18d46b2\"\n      },\n      \"user-key-2\": {\n        \"address\": \"0x614561d2d143621e126e87831aef287678b442b8\",\n        \"private_key\": \"0x53321db7c1e331d93a11a41d16f004d7ff63972ec8ec7c25db329728ceeb1710\"\n      },\n      \"user-key-20\": {\n        \"address\": \"0xaff0ca253b97e54440965855cec0a8a2e2399896\",\n        \"private_key\": \"0x04b9f63ecf84210c5366c66d68fa1f5da1fa4f634fad6dfc86178e4d79ff9e59\"\n      },\n      \"user-key-3\": {\n        \"address\": \"0xf93ee4cf8c6c40b329b0c0626f28333c132cf241\",\n        \"private_key\": \"0xab63b23eb7941c1251757e24b3d2350d2bc05c3c388d06f8fe6feafefb1e8c70\"\n      },\n      \"user-key-4\": {\n        \"address\": \"0x802dcbe1b1a97554b4f50db5119e37e8e7336417\",\n        \"private_key\": \"0x5d2344259f42259f82d2c140aa66102ba89b57b4883ee441a8b312622bd42491\"\n      },\n      \"user-key-5\": {\n        \"address\": \"0xae95d8da9244c37cac0a3e16ba966a8e852bb6d6\",\n        \"private_key\": \"0x27515f805127bebad2fb9b183508bdacb8c763da16f54e0678b16e8f28ef3fff\"\n      },\n      \"user-key-6\": {\n        \"address\": \"0x2c57d1cfc6d5f8e4182a56b4cf75421472ebaea4\",\n        \"private_key\": \"0x7ff1a4c1d57e5e784d327c4c7651e952350bc271f156afb3d00d20f5ef924856\"\n      },\n      \"user-key-7\": {\n        \"address\": \"0x741bfe4802ce1c4b5b00f9df2f5f179a1c89171a\",\n        \"private_key\": \"0x3a91003acaf4c21b3953d94fa4a6db694fa69e5242b2e37be05dd82761058899\"\n      },\n      \"user-key-8\": {\n        \"address\": \"0xc3913d4d8bab4914328651c2eae817c8b78e1f4c\",\n        \"private_key\": \"0xbb1d0f125b4fb2bb173c318cdead45468474ca71474e2247776b2b4c0fa2d3f5\"\n      },\n      \"user-key-9\": {\n        \"address\": \"0x65d08a056c17ae13370565b04cf77d2afa1cb9fa\",\n        \"private_key\": \"0x850643a0224065ecce3882673c21f56bcf6eef86274cc21cadff15930b59fc8c\"\n      }\n    },\n    \"jwt\": \"0xdc49981516e8e72b401a63e6405495a32dafc3939b5d6d83cc319ac0388bca1b\",\n    \"addresses\": {\n      \"anchorStateRegistryImpl\": \"0x5d57c7cf63db98654aa25572da178d4fa1c6c2dc\",\n      \"delayedWETHImpl\": \"0x5e40b9231b86984b5150507046e354dbfbed3d9e\",\n      \"disputeGameFactoryImpl\": \"0x4bba758f006ef09402ef31724203f316ab74e4a0\",\n      \"ethLockboxImpl\": \"0xe40dd264f8b1dc8ec8a95e9c310dbd3f061053d0\",\n      \"l1CrossDomainMessengerImpl\": \"0x5d5a095665886119693f0b41d8dfee78da033e8b\",\n      \"l1ERC721BridgeImpl\": \"0x7ae1d3bd877a4c5ca257404ce26be93a02c98013\",\n      \"l1StandardBridgeImpl\": \"0x0b09ba359a106c9ea3b181cbc5f394570c7d2a7a\",\n      \"mipsSingleton\": \"0xf027f4a985560fb13324e943edf55ad6f1d15dc1\",\n      \"opcm\": \"0xd4932a3b3ac9931a261f4687fbaed2bf759a2ab4\",\n      \"opcmDeployer\": \"0x0ca2a8e076f4a9b749f6eb03f3b09bc8bdaddabb\",\n      \"opcmGameTypeAdder\": \"0xc1640acc4d54e58a10699868e8cb6a6ad681b1a8\",\n      \"opcmInteropMigrator\": \"0x847353e7eaa87c9fa9156f4051ff2b1b4b41a718\",\n      \"opcmUpgrader\": \"0x54e3b43f50f3e1e28eb83915ba4782ed61f3f362\",\n      \"optimismMintableERC20FactoryImpl\": \"0x5493f4677a186f64805fe7317d6993ba4863988f\",\n      \"optimismPortalImpl\": \"0xb1dfde4e7c3018b97fa68b12f7d5648c96e4674e\",\n      \"preimageOracleSingleton\": \"0x1fb8cdfc6831fc866ed9c51af8817da5c287add3\",\n      \"protocolVersionsImpl\": \"0x37e15e4d6dffa9e5e320ee1ec036922e563cb76c\",\n      \"protocolVersionsProxy\": \"0xb74bb6ae1a1804d283d17e95620da9b9b0e6e0da\",\n      \"proxyAdmin\": \"0x4bf8d2e79e33cfd5a8348737ca91be5f65ea7dd9\",\n      \"superchainConfigImpl\": \"0x4da82a327773965b8d4d85fa3db8249b387458e7\",\n      \"superchainConfigProxy\": \"0x91bf7398afc3d2691aa23799fdb9175ee2eb6105\",\n      \"systemConfigImpl\": \"0xec6c6d47ec88f474bffa4defd38930fb2e79084c\"\n    }\n  },\n  \"l2\": [\n    {\n      \"name\": \"op-kurtosis\",\n      \"id\": \"2151908\",\n      \"services\": {\n        \"batcher\": {\n          \"name\": \"op-batcher-op-kurtosis\",\n          \"endpoints\": {\n            \"http\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 57676,\n              \"private_port\": 8548\n            },\n            \"metrics\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 57677,\n              \"private_port\": 9001\n            }\n          }\n        },\n        \"challenger\": {\n          \"name\": \"op-challenger-op-kurtosis\",\n          \"endpoints\": {\n            \"metrics\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 57739,\n              \"private_port\": 9001\n            }\n          }\n        },\n        \"proposer\": {\n          \"name\": \"op-proposer-op-kurtosis\",\n          \"endpoints\": {\n            \"http\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 57685,\n              \"private_port\": 8560\n            },\n            \"metrics\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 57686,\n              \"private_port\": 9001\n            }\n          }\n        }\n      },\n      \"nodes\": [\n        {\n          \"services\": {\n            \"cl\": {\n              \"name\": \"op-cl-1-op-node-op-geth-op-kurtosis\",\n              \"endpoints\": {\n                \"http\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57668,\n                  \"private_port\": 8547\n                },\n                \"metrics\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57669,\n                  \"private_port\": 9001\n                },\n                \"tcp-discovery\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57670,\n                  \"private_port\": 9003\n                },\n                \"udp-discovery\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 60059,\n                  \"private_port\": 9003\n                }\n              }\n            },\n            \"el\": {\n              \"name\": \"op-el-1-op-geth-op-node-op-kurtosis\",\n              \"endpoints\": {\n                \"engine-rpc\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57657,\n                  \"private_port\": 8551\n                },\n                \"metrics\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57658,\n                  \"private_port\": 9001\n                },\n                \"rpc\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57655,\n                  \"private_port\": 8545\n                },\n                \"tcp-discovery\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57654,\n                  \"private_port\": 30303\n                },\n                \"udp-discovery\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 65009,\n                  \"private_port\": 30303\n                },\n                \"ws\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57656,\n                  \"private_port\": 8546\n                }\n              }\n            }\n          }\n        }\n      ],\n      \"wallets\": {\n        \"dev-account-0\": {\n          \"address\": \"0xf39fd6e51aad88f6f4ce6ab8827279cfffb92266\",\n          \"private_key\": \"0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\"\n        },\n        \"dev-account-1\": {\n          \"address\": \"0x70997970c51812dc3a010c7d01b50e0d17dc79c8\",\n          \"private_key\": \"0x59c6995e998f97a5a0044966f0945389dc9e86dae88c7a8412f4603b6b78690d\"\n        },\n        \"dev-account-10\": {\n          \"address\": \"0xbcd4042de499d14e55001ccbb24a551f3b954096\",\n          \"private_key\": \"0xf214f2b2cd398c806f84e317254e0f0b801d0643303237d97a22a48e01628897\"\n        },\n        \"dev-account-11\": {\n          \"address\": \"0x71be63f3384f5fb98995898a86b02fb2426c5788\",\n          \"private_key\": \"0x701b615bbdfb9de65240bc28bd21bbc0d996645a3dd57e7b12bc2bdf6f192c82\"\n        },\n        \"dev-account-12\": {\n          \"address\": \"0xfabb0ac9d68b0b445fb7357272ff202c5651694a\",\n          \"private_key\": \"0xa267530f49f8280200edf313ee7af6b827f2a8bce2897751d06a843f644967b1\"\n        },\n        \"dev-account-13\": {\n          \"address\": \"0x1cbd3b2770909d4e10f157cabc84c7264073c9ec\",\n          \"private_key\": \"0x47c99abed3324a2707c28affff1267e45918ec8c3f20b8aa892e8b065d2942dd\"\n        },\n        \"dev-account-14\": {\n          \"address\": \"0xdf3e18d64bc6a983f673ab319ccae4f1a57c7097\",\n          \"private_key\": \"0xc526ee95bf44d8fc405a158bb884d9d1238d99f0612e9f33d006bb0789009aaa\"\n        },\n        \"dev-account-15\": {\n          \"address\": \"0xcd3b766ccdd6ae721141f452c550ca635964ce71\",\n          \"private_key\": \"0x8166f546bab6da521a8369cab06c5d2b9e46670292d85c875ee9ec20e84ffb61\"\n        },\n        \"dev-account-16\": {\n          \"address\": \"0x2546bcd3c84621e976d8185a91a922ae77ecec30\",\n          \"private_key\": \"0xea6c44ac03bff858b476bba40716402b03e41b8e97e276d1baec7c37d42484a0\"\n        },\n        \"dev-account-17\": {\n          \"address\": \"0xbda5747bfd65f08deb54cb465eb87d40e51b197e\",\n          \"private_key\": \"0x689af8efa8c651a91ad287602527f3af2fe9f6501a7ac4b061667b5a93e037fd\"\n        },\n        \"dev-account-18\": {\n          \"address\": \"0xdd2fd4581271e230360230f9337d5c0430bf44c0\",\n          \"private_key\": \"0xde9be858da4a475276426320d5e9262ecfc3ba460bfac56360bfa6c4c28b4ee0\"\n        },\n        \"dev-account-19\": {\n          \"address\": \"0x8626f6940e2eb28930efb4cef49b2d1f2c9c1199\",\n          \"private_key\": \"0xdf57089febbacf7ba0bc227dafbffa9fc08a93fdc68e1e42411a14efcf23656e\"\n        },\n        \"dev-account-2\": {\n          \"address\": \"0x3c44cdddb6a900fa2b585dd299e03d12fa4293bc\",\n          \"private_key\": \"0x5de4111afa1a4b94908f83103eb1f1706367c2e68ca870fc3fb9a804cdab365a\"\n        },\n        \"dev-account-20\": {\n          \"address\": \"0x09db0a93b389bef724429898f539aeb7ac2dd55f\",\n          \"private_key\": \"0xeaa861a9a01391ed3d587d8a5a84ca56ee277629a8b02c22093a419bf240e65d\"\n        },\n        \"dev-account-21\": {\n          \"address\": \"0x02484cb50aac86eae85610d6f4bf026f30f6627d\",\n          \"private_key\": \"0xc511b2aa70776d4ff1d376e8537903dae36896132c90b91d52c1dfbae267cd8b\"\n        },\n        \"dev-account-22\": {\n          \"address\": \"0x08135da0a343e492fa2d4282f2ae34c6c5cc1bbe\",\n          \"private_key\": \"0x224b7eb7449992aac96d631d9677f7bf5888245eef6d6eeda31e62d2f29a83e4\"\n        },\n        \"dev-account-23\": {\n          \"address\": \"0x5e661b79fe2d3f6ce70f5aac07d8cd9abb2743f1\",\n          \"private_key\": \"0x4624e0802698b9769f5bdb260a3777fbd4941ad2901f5966b854f953497eec1b\"\n        },\n        \"dev-account-24\": {\n          \"address\": \"0x61097ba76cd906d2ba4fd106e757f7eb455fc295\",\n          \"private_key\": \"0x375ad145df13ed97f8ca8e27bb21ebf2a3819e9e0a06509a812db377e533def7\"\n        },\n        \"dev-account-25\": {\n          \"address\": \"0xdf37f81daad2b0327a0a50003740e1c935c70913\",\n          \"private_key\": \"0x18743e59419b01d1d846d97ea070b5a3368a3e7f6f0242cf497e1baac6972427\"\n        },\n        \"dev-account-26\": {\n          \"address\": \"0x553bc17a05702530097c3677091c5bb47a3a7931\",\n          \"private_key\": \"0xe383b226df7c8282489889170b0f68f66af6459261f4833a781acd0804fafe7a\"\n        },\n        \"dev-account-27\": {\n          \"address\": \"0x87bdce72c06c21cd96219bd8521bdf1f42c78b5e\",\n          \"private_key\": \"0xf3a6b71b94f5cd909fb2dbb287da47badaa6d8bcdc45d595e2884835d8749001\"\n        },\n        \"dev-account-28\": {\n          \"address\": \"0x40fc963a729c542424cd800349a7e4ecc4896624\",\n          \"private_key\": \"0x4e249d317253b9641e477aba8dd5d8f1f7cf5250a5acadd1229693e262720a19\"\n        },\n        \"dev-account-29\": {\n          \"address\": \"0x9dcce783b6464611f38631e6c851bf441907c710\",\n          \"private_key\": \"0x233c86e887ac435d7f7dc64979d7758d69320906a0d340d2b6518b0fd20aa998\"\n        },\n        \"dev-account-3\": {\n          \"address\": \"0x90f79bf6eb2c4f870365e785982e1f101e93b906\",\n          \"private_key\": \"0x7c852118294e51e653712a81e05800f419141751be58f605c371e15141b007a6\"\n        },\n        \"dev-account-4\": {\n          \"address\": \"0x15d34aaf54267db7d7c367839aaf71a00a2c6a65\",\n          \"private_key\": \"0x47e179ec197488593b187f80a00eb0da91f1b9d0b13f8733639f19c30a34926a\"\n        },\n        \"dev-account-5\": {\n          \"address\": \"0x9965507d1a55bcc2695c58ba16fb37d819b0a4dc\",\n          \"private_key\": \"0x8b3a350cf5c34c9194ca85829a2df0ec3153be0318b5e2d3348e872092edffba\"\n        },\n        \"dev-account-6\": {\n          \"address\": \"0x976ea74026e726554db657fa54763abd0c3a0aa9\",\n          \"private_key\": \"0x92db14e403b83dfe3df233f83dfa3a0d7096f21ca9b0d6d6b8d88b2b4ec1564e\"\n        },\n        \"dev-account-7\": {\n          \"address\": \"0x14dc79964da2c08b23698b3d3cc7ca32193d9955\",\n          \"private_key\": \"0x4bbbf85ce3377467afe5d46f804f221813b2bb87f24d81f60f1fcdbf7cbf4356\"\n        },\n        \"dev-account-8\": {\n          \"address\": \"0x23618e81e3f5cdf7f54c3d65f7fbc0abf5b21e8f\",\n          \"private_key\": \"0xdbda1821b80551c9d65939329250298aa3472ba22feea921c0cf5d620ea67b97\"\n        },\n        \"dev-account-9\": {\n          \"address\": \"0xa0ee7a142d267c1f36714e4a8f75612f20a79720\",\n          \"private_key\": \"0x2a871d0798f97d79848a013d4936a73bf4cc922c825d33c1cf7073dff6d409c6\"\n        }\n      },\n      \"jwt\": \"0xdc49981516e8e72b401a63e6405495a32dafc3939b5d6d83cc319ac0388bca1b\",\n      \"config\": {\n        \"chainId\": 2151908,\n        \"homesteadBlock\": 0,\n        \"eip150Block\": 0,\n        \"eip155Block\": 0,\n        \"eip158Block\": 0,\n        \"byzantiumBlock\": 0,\n        \"constantinopleBlock\": 0,\n        \"petersburgBlock\": 0,\n        \"istanbulBlock\": 0,\n        \"muirGlacierBlock\": 0,\n        \"berlinBlock\": 0,\n        \"londonBlock\": 0,\n        \"arrowGlacierBlock\": 0,\n        \"grayGlacierBlock\": 0,\n        \"mergeNetsplitBlock\": 0,\n        \"shanghaiTime\": 0,\n        \"cancunTime\": 0,\n        \"pragueTime\": 0,\n        \"bedrockBlock\": 0,\n        \"regolithTime\": 0,\n        \"canyonTime\": 0,\n        \"ecotoneTime\": 0,\n        \"fjordTime\": 0,\n        \"graniteTime\": 0,\n        \"holoceneTime\": 0,\n        \"isthmusTime\": 0,\n        \"terminalTotalDifficulty\": 0,\n        \"depositContractAddress\": \"0x0000000000000000000000000000000000000000\",\n        \"optimism\": {\n          \"eip1559Elasticity\": 6,\n          \"eip1559Denominator\": 50,\n          \"eip1559DenominatorCanyon\": 250\n        }\n      },\n      \"addresses\": {\n        \"optimismMintableERC20FactoryProxy\": \"0x3abb4282ede8ff5f3cdb3dcc2341051e2dc47a44\"\n      },\n      \"l1_addresses\": {\n        \"addressManager\": \"0xd9f419936821a8757dbe8b86dc11c3f5c70043ab\",\n        \"anchorStateRegistryProxy\": \"0x3b4decef0660f96e0202c10b5c6c0d9a035e42fd\",\n        \"dataAvailabilityChallengeImpl\": \"0x0000000000000000000000000000000000000000\",\n        \"dataAvailabilityChallengeProxy\": \"0x0000000000000000000000000000000000000000\",\n        \"delayedWETHPermissionedGameProxy\": \"0x3c061fd7ebfe0a49e4862579b009424e9a77ff5f\",\n        \"delayedWETHPermissionlessGameProxy\": \"0x0000000000000000000000000000000000000000\",\n        \"disputeGameFactoryProxy\": \"0x3dd0224a041e63374bb6b233b87915077c465029\",\n        \"ethLockboxProxy\": \"0x43a303b991697620e89936162514bf54e708afbb\",\n        \"faultDisputeGame\": \"0x0000000000000000000000000000000000000000\",\n        \"l1CrossDomainMessengerProxy\": \"0x3c5fd59c5b3907dcfa44959479ef0acf5ca56c68\",\n        \"l1ERC721BridgeProxy\": \"0x65a76386b510e6adf5255ca061b71b9148a3fa2e\",\n        \"l1StandardBridgeProxy\": \"0x6eb3398055b85916711d593f7af6b89460abd585\",\n        \"optimismPortalProxy\": \"0x4477c16acbb8cd92b7c889c5b421273dd472d985\",\n        \"permissionedDisputeGame\": \"0x1a8444c9195844ed4daf7b18621f1ec3c32052e4\",\n        \"proxyAdmin\": \"0x2228ff6abbdccff2ae1a4c3116717bbb759e60cb\",\n        \"systemConfigProxy\": \"0x72c5075d484240b4045b5866f9007a298272d262\"\n      },\n      \"l1_wallets\": {\n        \"baseFeeVaultRecipient\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"batcher\": {\n          \"address\": \"0xd3f2c5afb2d76f5579f326b0cd7da5f5a4126c35\",\n          \"private_key\": \"0xb3d2d558e3491a3709b7c451100a0366b5872520c7aa020c17a0e7fa35b6a8df\"\n        },\n        \"challenger\": {\n          \"address\": \"0xf08f610d1956caaab34f18e9e0a122e389496529\",\n          \"private_key\": \"0x717c53f6d6c266889465d78a885cd0a2e22d41f73e21fa1f07ba5849c82d79c3\"\n        },\n        \"l1Faucet\": {\n          \"address\": \"0xaff0ca253b97e54440965855cec0a8a2e2399896\",\n          \"private_key\": \"0x04b9f63ecf84210c5366c66d68fa1f5da1fa4f634fad6dfc86178e4d79ff9e59\"\n        },\n        \"l1FeeVaultRecipient\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"l1ProxyAdmin\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"l2Faucet\": {\n          \"address\": \"0xf39fd6e51aad88f6f4ce6ab8827279cfffb92266\",\n          \"private_key\": \"0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\"\n        },\n        \"l2ProxyAdmin\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"proposer\": {\n          \"address\": \"0xb0994e702b603df7191cd68e6544f99126135e34\",\n          \"private_key\": \"0x9d77c0558ed6f31e7be16c2f45fd1f3ec3f66d20526b7fe3961872d38ac98fe7\"\n        },\n        \"sequencer\": {\n          \"address\": \"0xbb900cf56918a2639daa90c3f7dc5dcd2f5b9935\",\n          \"private_key\": \"0x459f58fd8ef3d9123333514d0f2445153fc29ddc43be239356dad70f506475d9\"\n        },\n        \"sequencerFeeVaultRecipient\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"systemConfigOwner\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        }\n      }\n    }\n  ]\n}\njust simple-devnet  213.68s user 16.60s system 27% cpu 13:55.01 total\n```\n</details> \nand\n<details>\n<summary>second invocation logs</summary>\n\n```\n\u279c  kurtosis-devnet git:(develop) \u2717 time just simple-devnet\n2025/03/26 13:49:19 Building docker image for project: op-node with tag: op-node:simple-devnet\n2025/03/26 13:49:23 Building docker image for project: op-batcher with tag: op-batcher:simple-devnet\n2025/03/26 13:49:24 Building docker image for project: op-challenger with tag: op-challenger:simple-devnet\n2025/03/26 13:49:26 Building docker image for project: op-proposer with tag: op-proposer:simple-devnet\n2025/03/26 13:49:27 Building docker image for project: op-deployer with tag: op-deployer:simple-devnet\n2025/03/26 13:49:28 Building contracts bundle: /var/folders/31/rhx71v1d52n7j67f__hl2qj40000gn/T/simple-devnet3180853229/contracts-bundle-simple-devnet.tar.gz\n2025/03/26 13:49:51 l1: contract artifacts available at: http://fileserver/contracts-bundle-simple-devnet.tar.gz\n2025/03/26 13:49:51 l2: contract artifacts available at: http://fileserver/contracts-bundle-simple-devnet.tar.gz\n2025/03/26 13:49:51 Building prestate: /var/folders/31/rhx71v1d52n7j67f__hl2qj40000gn/T/simple-devnet3180853229/proofs/op-program/cannon\n2025/03/26 13:50:51 prestate-proof-interop.json available at: http://fileserver/proofs/op-program/cannon/0x03486bc1d921bc0a44b088a450737e9d6a7da49dd8d35dad361c767aabba024d.json\n2025/03/26 13:50:51 prestate-interop.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x03486bc1d921bc0a44b088a450737e9d6a7da49dd8d35dad361c767aabba024d.bin.gz\n2025/03/26 13:50:51 prestate-proof-mt64.json available at: http://fileserver/proofs/op-program/cannon/0x03d70692978d10aabdbfa7c62729b778c44dd10f5b874b0e07e4bf7a637a1cec.json\n2025/03/26 13:50:51 prestate-mt64.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x03d70692978d10aabdbfa7c62729b778c44dd10f5b874b0e07e4bf7a637a1cec.bin.gz\n2025/03/26 13:50:51 prestate-proof.json available at: http://fileserver/proofs/op-program/cannon/0x03054be52183f128f6fa6f461440688b538a72a359c66e79b4866aa966e7e003.json\n2025/03/26 13:50:51 prestate.bin.gz available at: http://fileserver/proofs/op-program/cannon/0x03054be52183f128f6fa6f461440688b538a72a359c66e79b4866aa966e7e003.bin.gz\n2025/03/26 13:50:51 No changes to fileserver, skipping deployment\n2025/03/26 13:50:51 Deployment input:\nethereum_package:\n  network_params:\n    additional_preloaded_contracts: |\n      {\n        \"0x4e59b44847b379578588920cA78FbF26c0B4956C\": {\n          \"balance\": \"0ETH\",\n          \"code\": \"0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffe03601600081602082378035828234f58015156039578182fd5b8082525050506014600cf3\",\n          \"storage\": {},\n          \"nonce\": \"1\"\n        }\n      }\n    genesis_delay: 5\n    preset: minimal\n  participants:\n    - cl_type: teku\n      el_type: geth\noptimism_package:\n  chains:\n    - additional_services: []\n      batcher_params:\n        extra_params: []\n        image: op-batcher:551aa84b67a0\n      challenger_params:\n        cannon_prestate_path: \"\"\n        cannon_prestates_url: http://fileserver/proofs/op-program/cannon\n        extra_params: []\n        image: op-challenger:b865b632af32\n      mev_params:\n        builder_host: \"\"\n        builder_port: \"\"\n        rollup_boost_image: \"\"\n      network_params:\n        fjord_time_offset: 0\n        fund_dev_accounts: true\n        granite_time_offset: 0\n        holocene_time_offset: 0\n        name: op-kurtosis\n        network: kurtosis\n        network_id: \"2151908\"\n        seconds_per_slot: 2\n      participants:\n        - cl_extra_env_vars: {}\n          cl_extra_labels: {}\n          cl_extra_params: []\n          cl_image: op-node:3b0c9ecb0a87\n          cl_log_level: \"\"\n          cl_max_cpu: 0\n          cl_max_mem: 0\n          cl_min_cpu: 0\n          cl_min_mem: 0\n          cl_tolerations: []\n          cl_type: op-node\n          cl_volume_size: 0\n          count: 1\n          el_extra_env_vars: {}\n          el_extra_labels: {}\n          el_extra_params: []\n          el_image: \"\"\n          el_log_level: debug\n          el_max_cpu: 0\n          el_max_mem: 0\n          el_min_cpu: 0\n          el_min_mem: 0\n          el_tolerations: []\n          el_type: op-geth\n          el_volume_size: 0\n          node_selectors: {}\n          tolerations: []\n      proposer_params:\n        extra_params: []\n        game_type: 1\n        image: op-proposer:fc2a2a27ae24\n        proposal_interval: 10m\n  global_log_level: info\n  global_node_selectors: {}\n  global_tolerations: []\n  op_contract_deployer_params:\n    global_deploy_overrides:\n      faultGameAbsolutePrestate: 0x03054be52183f128f6fa6f461440688b538a72a359c66e79b4866aa966e7e003\n    image: op-deployer:a0e5c44110a9\n    l1_artifacts_locator: http://fileserver/contracts-bundle-simple-devnet.tar.gz\n    l2_artifacts_locator: http://fileserver/contracts-bundle-simple-devnet.tar.gz\n  persistent: false\nUsing existing enclave 'simple-devnet'\n\nINFO[0093] Compressing package 'github.com/ethereum-optimism/optimism/kurtosis-devnet/optimism-package-trampoline' at './optimism-package-trampoline/' for upload\nINFO[0093] Uploading and executing package 'github.com/ethereum-optimism/optimism/kurtosis-devnet/optimism-package-trampoline'\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nUploading file '/static_files/jwt/jwtsecret' to files artifact 'jwt_file'\nSKIPPED - This instruction has already been run in this enclave\n\nUploading file '/static_files/keymanager/keymanager.txt' to files artifact 'keymanager_file'\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'validator-key-generation-cl-validator-keystore' and image 'protolambda/eth2-val-tools:latest'\nSKIPPED - This instruction has already been run in this enclave\n\nGenerating keystores\nSKIPPED - This instruction has already been run in this enclave\n\nVerifying whether two values meet a certain condition '=='\nSKIPPED - This instruction has already been run in this enclave\n\nStoring files from service 'validator-key-generation-cl-validator-keystore' at path '/node-0-keystores/' to files artifact with name '1-teku-geth-0-63'\nSKIPPED - This instruction has already been run in this enclave\n\nStoring prysm password in a file\nSKIPPED - This instruction has already been run in this enclave\n\nVerifying whether two values meet a certain condition '=='\nSKIPPED - This instruction has already been run in this enclave\n\nStoring files from service 'validator-key-generation-cl-validator-keystore' at path '/tmp/prysm-password.txt' to files artifact with name 'prysm-password'\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nGetting final genesis timestamp\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nRendering a template to a files artifact with name 'genesis-el-cl-env-file'\nSKIPPED - This instruction has already been run in this enclave\n\nCreating genesis\nSKIPPED - This instruction has already been run in this enclave\n\nReading genesis validators root\nSKIPPED - This instruction has already been run in this enclave\n\nReading prague time from genesis\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'el-1-geth-teku' and image 'ethereum/client-go:latest'\nSKIPPED - This instruction has already been run in this enclave\n\nWaiting for at most '15m0s' for service 'el-1-geth-teku' to reach a certain state\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'cl-1-teku-geth' and image 'consensys/teku:latest'\nSKIPPED - This instruction has already been run in this enclave\n\nRunning 'GET' request on service 'cl-1-teku-geth'\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nRendering a template to a files artifact with name 'validator-ranges'\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nWait for L1 to start up - can take up to 2 minutes\nSKIPPED - This instruction has already been run in this enclave\n\nWait for L1 execution to start up - can take up to 2 minutes\nSKIPPED - This instruction has already been run in this enclave\n\nInitialize L2 contract deployments\nSKIPPED - This instruction has already been run in this enclave\n\nUploading file '../../static_files/scripts' to files artifact 'op-deployer-fund-script'\nSKIPPED - This instruction has already been run in this enclave\n\nCollect keys, and fund addresses\nSKIPPED - This instruction has already been run in this enclave\n\nWrite value to a file artifact\nSKIPPED - This instruction has already been run in this enclave\n\nConfigure L2 contract deployments\nSKIPPED - This instruction has already been run in this enclave\n\nApply L2 contract deployments\nSKIPPED - This instruction has already been run in this enclave\n\nGenerate chainspec\nSKIPPED - This instruction has already been run in this enclave\n\nUploading file '/static_files/jwt/jwtsecret' to files artifact 'op_jwt_file'\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nus-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest\nSKIPPED - This instruction has already been run in this enclave\n\nus-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'op-el-1-op-geth-op-node-op-kurtosis' and image 'us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:latest'\nSKIPPED - This instruction has already been run in this enclave\n\nWaiting for at most '15m0s' for service 'op-el-1-op-geth-op-node-op-kurtosis' to reach a certain state\nSKIPPED - This instruction has already been run in this enclave\n\nRead JSON value\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'op-cl-1-op-node-op-geth-op-kurtosis' and image 'op-node:3b0c9ecb0a87'\nSKIPPED - This instruction has already been run in this enclave\n\nRunning 'POST' request on service 'op-cl-1-op-node-op-geth-op-kurtosis'\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nRead JSON value\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'op-batcher-op-kurtosis' and image 'op-batcher:551aa84b67a0'\nSKIPPED - This instruction has already been run in this enclave\n\nRead JSON value\nSKIPPED - This instruction has already been run in this enclave\n\nRead JSON value\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'op-proposer-op-kurtosis' and image 'op-proposer:fc2a2a27ae24'\nSKIPPED - This instruction has already been run in this enclave\n\nRead JSON value\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nRead JSON value\nSKIPPED - This instruction has already been run in this enclave\n\nRead JSON value\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'op-challenger-op-kurtosis' and image 'op-challenger:b865b632af32'\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nRendering a template to a files artifact with name 'prometheus-config'\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'prometheus' and image 'prom/prometheus:v3.1.0'\nSKIPPED - This instruction has already been run in this enclave\n\nPrinting a message\nSKIPPED - This instruction has already been run in this enclave\n\nRendering a template to a files artifact with name 'grafana-config'\nSKIPPED - This instruction has already been run in this enclave\n\nAdding service with name 'grafana' and image 'grafana/grafana:11.5.0'\nSKIPPED - This instruction has already been run in this enclave\n\nUploading file 'github.com/ethereum-optimism/grafana-dashboards-public/resources' to files artifact 'dashboards-0'\nSKIPPED - This instruction has already been run in this enclave\n\nupload dashboards\nSKIPPED - This instruction has already been run in this enclave\n\n\u2b50 us on GitHub - https://github.com/kurtosis-tech/kurtosis\n{\n  \"name\": \"\",\n  \"l1\": {\n    \"name\": \"Ethereum\",\n    \"id\": \"3151908\",\n    \"nodes\": [\n      {\n        \"services\": {\n          \"cl\": {\n            \"name\": \"cl-1-teku-geth\",\n            \"endpoints\": {\n              \"http\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56345,\n                \"private_port\": 4000\n              },\n              \"metrics\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56343,\n                \"private_port\": 8008\n              },\n              \"tcp-discovery\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56344,\n                \"private_port\": 9000\n              },\n              \"udp-discovery\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 58088,\n                \"private_port\": 9000\n              }\n            }\n          },\n          \"el\": {\n            \"name\": \"el-1-geth-teku\",\n            \"endpoints\": {\n              \"engine-rpc\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56337,\n                \"private_port\": 8551\n              },\n              \"metrics\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56338,\n                \"private_port\": 9001\n              },\n              \"rpc\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56340,\n                \"private_port\": 8545\n              },\n              \"tcp-discovery\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56339,\n                \"private_port\": 30303\n              },\n              \"udp-discovery\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 58962,\n                \"private_port\": 30303\n              },\n              \"ws\": {\n                \"host\": \"127.0.0.1\",\n                \"port\": 56341,\n                \"private_port\": 8546\n              }\n            }\n          }\n        }\n      }\n    ],\n    \"wallets\": {\n      \"user-key-0\": {\n        \"address\": \"0x8943545177806ed17b9f23f0a21ee5948ecaa776\",\n        \"private_key\": \"0xbcdf20249abf0ed6d944c0288fad489e33f66b3960d9e6229c1cd214ed3bbe31\"\n      },\n      \"user-key-1\": {\n        \"address\": \"0xe25583099ba105d9ec0a67f5ae86d90e50036425\",\n        \"private_key\": \"0x39725efee3fb28614de3bacaffe4cc4bd8c436257e2c8bb887c4b5c4be45e76d\"\n      },\n      \"user-key-10\": {\n        \"address\": \"0x3e95dfbbaf6b348396e6674c7871546dcc568e56\",\n        \"private_key\": \"0x94eb3102993b41ec55c241060f47daa0f6372e2e3ad7e91612ae36c364042e44\"\n      },\n      \"user-key-11\": {\n        \"address\": \"0x5918b2e647464d4743601a865753e64c8059dc4f\",\n        \"private_key\": \"0xdaf15504c22a352648a71ef2926334fe040ac1d5005019e09f6c979808024dc7\"\n      },\n      \"user-key-12\": {\n        \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n        \"private_key\": \"0xeaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n      },\n      \"user-key-13\": {\n        \"address\": \"0x4d1cb4eb7969f8806e2caac0cbbb71f88c8ec413\",\n        \"private_key\": \"0x3fd98b5187bf6526734efaa644ffbb4e3670d66f5d0268ce0323ec09124bff61\"\n      },\n      \"user-key-14\": {\n        \"address\": \"0xf5504ce2bcc52614f121aff9b93b2001d92715ca\",\n        \"private_key\": \"0x5288e2f440c7f0cb61a9be8afdeb4295f786383f96f5e35eb0c94ef103996b64\"\n      },\n      \"user-key-15\": {\n        \"address\": \"0xf61e98e7d47ab884c244e39e031978e33162ff4b\",\n        \"private_key\": \"0xf296c7802555da2a5a662be70e078cbd38b44f96f8615ae529da41122ce8db05\"\n      },\n      \"user-key-16\": {\n        \"address\": \"0xf1424826861ffbbd25405f5145b5e50d0f1bfc90\",\n        \"private_key\": \"0xbf3beef3bd999ba9f2451e06936f0423cd62b815c9233dd3bc90f7e02a1e8673\"\n      },\n      \"user-key-17\": {\n        \"address\": \"0xfdce42116f541fc8f7b0776e2b30832bd5621c85\",\n        \"private_key\": \"0x6ecadc396415970e91293726c3f5775225440ea0844ae5616135fd10d66b5954\"\n      },\n      \"user-key-18\": {\n        \"address\": \"0xd9211042f35968820a3407ac3d80c725f8f75c14\",\n        \"private_key\": \"0xa492823c3e193d6c595f37a18e3c06650cf4c74558cc818b16130b293716106f\"\n      },\n      \"user-key-19\": {\n        \"address\": \"0xd8f3183def51a987222d845be228e0bbb932c222\",\n        \"private_key\": \"0xc5114526e042343c6d1899cad05e1c00ba588314de9b96929914ee0df18d46b2\"\n      },\n      \"user-key-2\": {\n        \"address\": \"0x614561d2d143621e126e87831aef287678b442b8\",\n        \"private_key\": \"0x53321db7c1e331d93a11a41d16f004d7ff63972ec8ec7c25db329728ceeb1710\"\n      },\n      \"user-key-20\": {\n        \"address\": \"0xaff0ca253b97e54440965855cec0a8a2e2399896\",\n        \"private_key\": \"0x04b9f63ecf84210c5366c66d68fa1f5da1fa4f634fad6dfc86178e4d79ff9e59\"\n      },\n      \"user-key-3\": {\n        \"address\": \"0xf93ee4cf8c6c40b329b0c0626f28333c132cf241\",\n        \"private_key\": \"0xab63b23eb7941c1251757e24b3d2350d2bc05c3c388d06f8fe6feafefb1e8c70\"\n      },\n      \"user-key-4\": {\n        \"address\": \"0x802dcbe1b1a97554b4f50db5119e37e8e7336417\",\n        \"private_key\": \"0x5d2344259f42259f82d2c140aa66102ba89b57b4883ee441a8b312622bd42491\"\n      },\n      \"user-key-5\": {\n        \"address\": \"0xae95d8da9244c37cac0a3e16ba966a8e852bb6d6\",\n        \"private_key\": \"0x27515f805127bebad2fb9b183508bdacb8c763da16f54e0678b16e8f28ef3fff\"\n      },\n      \"user-key-6\": {\n        \"address\": \"0x2c57d1cfc6d5f8e4182a56b4cf75421472ebaea4\",\n        \"private_key\": \"0x7ff1a4c1d57e5e784d327c4c7651e952350bc271f156afb3d00d20f5ef924856\"\n      },\n      \"user-key-7\": {\n        \"address\": \"0x741bfe4802ce1c4b5b00f9df2f5f179a1c89171a\",\n        \"private_key\": \"0x3a91003acaf4c21b3953d94fa4a6db694fa69e5242b2e37be05dd82761058899\"\n      },\n      \"user-key-8\": {\n        \"address\": \"0xc3913d4d8bab4914328651c2eae817c8b78e1f4c\",\n        \"private_key\": \"0xbb1d0f125b4fb2bb173c318cdead45468474ca71474e2247776b2b4c0fa2d3f5\"\n      },\n      \"user-key-9\": {\n        \"address\": \"0x65d08a056c17ae13370565b04cf77d2afa1cb9fa\",\n        \"private_key\": \"0x850643a0224065ecce3882673c21f56bcf6eef86274cc21cadff15930b59fc8c\"\n      }\n    },\n    \"jwt\": \"0xdc49981516e8e72b401a63e6405495a32dafc3939b5d6d83cc319ac0388bca1b\",\n    \"addresses\": {\n      \"anchorStateRegistryImpl\": \"0x5d57c7cf63db98654aa25572da178d4fa1c6c2dc\",\n      \"delayedWETHImpl\": \"0x5e40b9231b86984b5150507046e354dbfbed3d9e\",\n      \"disputeGameFactoryImpl\": \"0x4bba758f006ef09402ef31724203f316ab74e4a0\",\n      \"ethLockboxImpl\": \"0xe40dd264f8b1dc8ec8a95e9c310dbd3f061053d0\",\n      \"l1CrossDomainMessengerImpl\": \"0x5d5a095665886119693f0b41d8dfee78da033e8b\",\n      \"l1ERC721BridgeImpl\": \"0x7ae1d3bd877a4c5ca257404ce26be93a02c98013\",\n      \"l1StandardBridgeImpl\": \"0x0b09ba359a106c9ea3b181cbc5f394570c7d2a7a\",\n      \"mipsSingleton\": \"0xf027f4a985560fb13324e943edf55ad6f1d15dc1\",\n      \"opcm\": \"0xd4932a3b3ac9931a261f4687fbaed2bf759a2ab4\",\n      \"opcmDeployer\": \"0x0ca2a8e076f4a9b749f6eb03f3b09bc8bdaddabb\",\n      \"opcmGameTypeAdder\": \"0xc1640acc4d54e58a10699868e8cb6a6ad681b1a8\",\n      \"opcmInteropMigrator\": \"0x847353e7eaa87c9fa9156f4051ff2b1b4b41a718\",\n      \"opcmUpgrader\": \"0x54e3b43f50f3e1e28eb83915ba4782ed61f3f362\",\n      \"optimismMintableERC20FactoryImpl\": \"0x5493f4677a186f64805fe7317d6993ba4863988f\",\n      \"optimismPortalImpl\": \"0xb1dfde4e7c3018b97fa68b12f7d5648c96e4674e\",\n      \"preimageOracleSingleton\": \"0x1fb8cdfc6831fc866ed9c51af8817da5c287add3\",\n      \"protocolVersionsImpl\": \"0x37e15e4d6dffa9e5e320ee1ec036922e563cb76c\",\n      \"protocolVersionsProxy\": \"0xb74bb6ae1a1804d283d17e95620da9b9b0e6e0da\",\n      \"proxyAdmin\": \"0x4bf8d2e79e33cfd5a8348737ca91be5f65ea7dd9\",\n      \"superchainConfigImpl\": \"0x4da82a327773965b8d4d85fa3db8249b387458e7\",\n      \"superchainConfigProxy\": \"0x91bf7398afc3d2691aa23799fdb9175ee2eb6105\",\n      \"systemConfigImpl\": \"0xec6c6d47ec88f474bffa4defd38930fb2e79084c\"\n    }\n  },\n  \"l2\": [\n    {\n      \"name\": \"op-kurtosis\",\n      \"id\": \"2151908\",\n      \"services\": {\n        \"batcher\": {\n          \"name\": \"op-batcher-op-kurtosis\",\n          \"endpoints\": {\n            \"http\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 49501,\n              \"private_port\": 8548\n            },\n            \"metrics\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 49502,\n              \"private_port\": 9001\n            }\n          }\n        },\n        \"challenger\": {\n          \"name\": \"op-challenger-op-kurtosis\",\n          \"endpoints\": {\n            \"metrics\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 57739,\n              \"private_port\": 9001\n            }\n          }\n        },\n        \"proposer\": {\n          \"name\": \"op-proposer-op-kurtosis\",\n          \"endpoints\": {\n            \"http\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 57685,\n              \"private_port\": 8560\n            },\n            \"metrics\": {\n              \"host\": \"127.0.0.1\",\n              \"port\": 57686,\n              \"private_port\": 9001\n            }\n          }\n        }\n      },\n      \"nodes\": [\n        {\n          \"services\": {\n            \"cl\": {\n              \"name\": \"op-cl-1-op-node-op-geth-op-kurtosis\",\n              \"endpoints\": {\n                \"http\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57668,\n                  \"private_port\": 8547\n                },\n                \"metrics\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57669,\n                  \"private_port\": 9001\n                },\n                \"tcp-discovery\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57670,\n                  \"private_port\": 9003\n                },\n                \"udp-discovery\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 60059,\n                  \"private_port\": 9003\n                }\n              }\n            },\n            \"el\": {\n              \"name\": \"op-el-1-op-geth-op-node-op-kurtosis\",\n              \"endpoints\": {\n                \"engine-rpc\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57657,\n                  \"private_port\": 8551\n                },\n                \"metrics\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57658,\n                  \"private_port\": 9001\n                },\n                \"rpc\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57655,\n                  \"private_port\": 8545\n                },\n                \"tcp-discovery\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57654,\n                  \"private_port\": 30303\n                },\n                \"udp-discovery\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 65009,\n                  \"private_port\": 30303\n                },\n                \"ws\": {\n                  \"host\": \"127.0.0.1\",\n                  \"port\": 57656,\n                  \"private_port\": 8546\n                }\n              }\n            }\n          }\n        }\n      ],\n      \"wallets\": {\n        \"dev-account-0\": {\n          \"address\": \"0xf39fd6e51aad88f6f4ce6ab8827279cfffb92266\",\n          \"private_key\": \"0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\"\n        },\n        \"dev-account-1\": {\n          \"address\": \"0x70997970c51812dc3a010c7d01b50e0d17dc79c8\",\n          \"private_key\": \"0x59c6995e998f97a5a0044966f0945389dc9e86dae88c7a8412f4603b6b78690d\"\n        },\n        \"dev-account-10\": {\n          \"address\": \"0xbcd4042de499d14e55001ccbb24a551f3b954096\",\n          \"private_key\": \"0xf214f2b2cd398c806f84e317254e0f0b801d0643303237d97a22a48e01628897\"\n        },\n        \"dev-account-11\": {\n          \"address\": \"0x71be63f3384f5fb98995898a86b02fb2426c5788\",\n          \"private_key\": \"0x701b615bbdfb9de65240bc28bd21bbc0d996645a3dd57e7b12bc2bdf6f192c82\"\n        },\n        \"dev-account-12\": {\n          \"address\": \"0xfabb0ac9d68b0b445fb7357272ff202c5651694a\",\n          \"private_key\": \"0xa267530f49f8280200edf313ee7af6b827f2a8bce2897751d06a843f644967b1\"\n        },\n        \"dev-account-13\": {\n          \"address\": \"0x1cbd3b2770909d4e10f157cabc84c7264073c9ec\",\n          \"private_key\": \"0x47c99abed3324a2707c28affff1267e45918ec8c3f20b8aa892e8b065d2942dd\"\n        },\n        \"dev-account-14\": {\n          \"address\": \"0xdf3e18d64bc6a983f673ab319ccae4f1a57c7097\",\n          \"private_key\": \"0xc526ee95bf44d8fc405a158bb884d9d1238d99f0612e9f33d006bb0789009aaa\"\n        },\n        \"dev-account-15\": {\n          \"address\": \"0xcd3b766ccdd6ae721141f452c550ca635964ce71\",\n          \"private_key\": \"0x8166f546bab6da521a8369cab06c5d2b9e46670292d85c875ee9ec20e84ffb61\"\n        },\n        \"dev-account-16\": {\n          \"address\": \"0x2546bcd3c84621e976d8185a91a922ae77ecec30\",\n          \"private_key\": \"0xea6c44ac03bff858b476bba40716402b03e41b8e97e276d1baec7c37d42484a0\"\n        },\n        \"dev-account-17\": {\n          \"address\": \"0xbda5747bfd65f08deb54cb465eb87d40e51b197e\",\n          \"private_key\": \"0x689af8efa8c651a91ad287602527f3af2fe9f6501a7ac4b061667b5a93e037fd\"\n        },\n        \"dev-account-18\": {\n          \"address\": \"0xdd2fd4581271e230360230f9337d5c0430bf44c0\",\n          \"private_key\": \"0xde9be858da4a475276426320d5e9262ecfc3ba460bfac56360bfa6c4c28b4ee0\"\n        },\n        \"dev-account-19\": {\n          \"address\": \"0x8626f6940e2eb28930efb4cef49b2d1f2c9c1199\",\n          \"private_key\": \"0xdf57089febbacf7ba0bc227dafbffa9fc08a93fdc68e1e42411a14efcf23656e\"\n        },\n        \"dev-account-2\": {\n          \"address\": \"0x3c44cdddb6a900fa2b585dd299e03d12fa4293bc\",\n          \"private_key\": \"0x5de4111afa1a4b94908f83103eb1f1706367c2e68ca870fc3fb9a804cdab365a\"\n        },\n        \"dev-account-20\": {\n          \"address\": \"0x09db0a93b389bef724429898f539aeb7ac2dd55f\",\n          \"private_key\": \"0xeaa861a9a01391ed3d587d8a5a84ca56ee277629a8b02c22093a419bf240e65d\"\n        },\n        \"dev-account-21\": {\n          \"address\": \"0x02484cb50aac86eae85610d6f4bf026f30f6627d\",\n          \"private_key\": \"0xc511b2aa70776d4ff1d376e8537903dae36896132c90b91d52c1dfbae267cd8b\"\n        },\n        \"dev-account-22\": {\n          \"address\": \"0x08135da0a343e492fa2d4282f2ae34c6c5cc1bbe\",\n          \"private_key\": \"0x224b7eb7449992aac96d631d9677f7bf5888245eef6d6eeda31e62d2f29a83e4\"\n        },\n        \"dev-account-23\": {\n          \"address\": \"0x5e661b79fe2d3f6ce70f5aac07d8cd9abb2743f1\",\n          \"private_key\": \"0x4624e0802698b9769f5bdb260a3777fbd4941ad2901f5966b854f953497eec1b\"\n        },\n        \"dev-account-24\": {\n          \"address\": \"0x61097ba76cd906d2ba4fd106e757f7eb455fc295\",\n          \"private_key\": \"0x375ad145df13ed97f8ca8e27bb21ebf2a3819e9e0a06509a812db377e533def7\"\n        },\n        \"dev-account-25\": {\n          \"address\": \"0xdf37f81daad2b0327a0a50003740e1c935c70913\",\n          \"private_key\": \"0x18743e59419b01d1d846d97ea070b5a3368a3e7f6f0242cf497e1baac6972427\"\n        },\n        \"dev-account-26\": {\n          \"address\": \"0x553bc17a05702530097c3677091c5bb47a3a7931\",\n          \"private_key\": \"0xe383b226df7c8282489889170b0f68f66af6459261f4833a781acd0804fafe7a\"\n        },\n        \"dev-account-27\": {\n          \"address\": \"0x87bdce72c06c21cd96219bd8521bdf1f42c78b5e\",\n          \"private_key\": \"0xf3a6b71b94f5cd909fb2dbb287da47badaa6d8bcdc45d595e2884835d8749001\"\n        },\n        \"dev-account-28\": {\n          \"address\": \"0x40fc963a729c542424cd800349a7e4ecc4896624\",\n          \"private_key\": \"0x4e249d317253b9641e477aba8dd5d8f1f7cf5250a5acadd1229693e262720a19\"\n        },\n        \"dev-account-29\": {\n          \"address\": \"0x9dcce783b6464611f38631e6c851bf441907c710\",\n          \"private_key\": \"0x233c86e887ac435d7f7dc64979d7758d69320906a0d340d2b6518b0fd20aa998\"\n        },\n        \"dev-account-3\": {\n          \"address\": \"0x90f79bf6eb2c4f870365e785982e1f101e93b906\",\n          \"private_key\": \"0x7c852118294e51e653712a81e05800f419141751be58f605c371e15141b007a6\"\n        },\n        \"dev-account-4\": {\n          \"address\": \"0x15d34aaf54267db7d7c367839aaf71a00a2c6a65\",\n          \"private_key\": \"0x47e179ec197488593b187f80a00eb0da91f1b9d0b13f8733639f19c30a34926a\"\n        },\n        \"dev-account-5\": {\n          \"address\": \"0x9965507d1a55bcc2695c58ba16fb37d819b0a4dc\",\n          \"private_key\": \"0x8b3a350cf5c34c9194ca85829a2df0ec3153be0318b5e2d3348e872092edffba\"\n        },\n        \"dev-account-6\": {\n          \"address\": \"0x976ea74026e726554db657fa54763abd0c3a0aa9\",\n          \"private_key\": \"0x92db14e403b83dfe3df233f83dfa3a0d7096f21ca9b0d6d6b8d88b2b4ec1564e\"\n        },\n        \"dev-account-7\": {\n          \"address\": \"0x14dc79964da2c08b23698b3d3cc7ca32193d9955\",\n          \"private_key\": \"0x4bbbf85ce3377467afe5d46f804f221813b2bb87f24d81f60f1fcdbf7cbf4356\"\n        },\n        \"dev-account-8\": {\n          \"address\": \"0x23618e81e3f5cdf7f54c3d65f7fbc0abf5b21e8f\",\n          \"private_key\": \"0xdbda1821b80551c9d65939329250298aa3472ba22feea921c0cf5d620ea67b97\"\n        },\n        \"dev-account-9\": {\n          \"address\": \"0xa0ee7a142d267c1f36714e4a8f75612f20a79720\",\n          \"private_key\": \"0x2a871d0798f97d79848a013d4936a73bf4cc922c825d33c1cf7073dff6d409c6\"\n        }\n      },\n      \"jwt\": \"0xdc49981516e8e72b401a63e6405495a32dafc3939b5d6d83cc319ac0388bca1b\",\n      \"config\": {\n        \"chainId\": 2151908,\n        \"homesteadBlock\": 0,\n        \"eip150Block\": 0,\n        \"eip155Block\": 0,\n        \"eip158Block\": 0,\n        \"byzantiumBlock\": 0,\n        \"constantinopleBlock\": 0,\n        \"petersburgBlock\": 0,\n        \"istanbulBlock\": 0,\n        \"muirGlacierBlock\": 0,\n        \"berlinBlock\": 0,\n        \"londonBlock\": 0,\n        \"arrowGlacierBlock\": 0,\n        \"grayGlacierBlock\": 0,\n        \"mergeNetsplitBlock\": 0,\n        \"shanghaiTime\": 0,\n        \"cancunTime\": 0,\n        \"pragueTime\": 0,\n        \"bedrockBlock\": 0,\n        \"regolithTime\": 0,\n        \"canyonTime\": 0,\n        \"ecotoneTime\": 0,\n        \"fjordTime\": 0,\n        \"graniteTime\": 0,\n        \"holoceneTime\": 0,\n        \"isthmusTime\": 0,\n        \"terminalTotalDifficulty\": 0,\n        \"depositContractAddress\": \"0x0000000000000000000000000000000000000000\",\n        \"optimism\": {\n          \"eip1559Elasticity\": 6,\n          \"eip1559Denominator\": 50,\n          \"eip1559DenominatorCanyon\": 250\n        }\n      },\n      \"addresses\": {\n        \"optimismMintableERC20FactoryProxy\": \"0x3abb4282ede8ff5f3cdb3dcc2341051e2dc47a44\"\n      },\n      \"l1_addresses\": {\n        \"addressManager\": \"0xd9f419936821a8757dbe8b86dc11c3f5c70043ab\",\n        \"anchorStateRegistryProxy\": \"0x3b4decef0660f96e0202c10b5c6c0d9a035e42fd\",\n        \"dataAvailabilityChallengeImpl\": \"0x0000000000000000000000000000000000000000\",\n        \"dataAvailabilityChallengeProxy\": \"0x0000000000000000000000000000000000000000\",\n        \"delayedWETHPermissionedGameProxy\": \"0x3c061fd7ebfe0a49e4862579b009424e9a77ff5f\",\n        \"delayedWETHPermissionlessGameProxy\": \"0x0000000000000000000000000000000000000000\",\n        \"disputeGameFactoryProxy\": \"0x3dd0224a041e63374bb6b233b87915077c465029\",\n        \"ethLockboxProxy\": \"0x43a303b991697620e89936162514bf54e708afbb\",\n        \"faultDisputeGame\": \"0x0000000000000000000000000000000000000000\",\n        \"l1CrossDomainMessengerProxy\": \"0x3c5fd59c5b3907dcfa44959479ef0acf5ca56c68\",\n        \"l1ERC721BridgeProxy\": \"0x65a76386b510e6adf5255ca061b71b9148a3fa2e\",\n        \"l1StandardBridgeProxy\": \"0x6eb3398055b85916711d593f7af6b89460abd585\",\n        \"optimismPortalProxy\": \"0x4477c16acbb8cd92b7c889c5b421273dd472d985\",\n        \"permissionedDisputeGame\": \"0x1a8444c9195844ed4daf7b18621f1ec3c32052e4\",\n        \"proxyAdmin\": \"0x2228ff6abbdccff2ae1a4c3116717bbb759e60cb\",\n        \"systemConfigProxy\": \"0x72c5075d484240b4045b5866f9007a298272d262\"\n      },\n      \"l1_wallets\": {\n        \"baseFeeVaultRecipient\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"batcher\": {\n          \"address\": \"0xd3f2c5afb2d76f5579f326b0cd7da5f5a4126c35\",\n          \"private_key\": \"0xb3d2d558e3491a3709b7c451100a0366b5872520c7aa020c17a0e7fa35b6a8df\"\n        },\n        \"challenger\": {\n          \"address\": \"0xf08f610d1956caaab34f18e9e0a122e389496529\",\n          \"private_key\": \"0x717c53f6d6c266889465d78a885cd0a2e22d41f73e21fa1f07ba5849c82d79c3\"\n        },\n        \"l1Faucet\": {\n          \"address\": \"0xaff0ca253b97e54440965855cec0a8a2e2399896\",\n          \"private_key\": \"0x04b9f63ecf84210c5366c66d68fa1f5da1fa4f634fad6dfc86178e4d79ff9e59\"\n        },\n        \"l1FeeVaultRecipient\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"l1ProxyAdmin\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"l2Faucet\": {\n          \"address\": \"0xf39fd6e51aad88f6f4ce6ab8827279cfffb92266\",\n          \"private_key\": \"0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\"\n        },\n        \"l2ProxyAdmin\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"proposer\": {\n          \"address\": \"0xb0994e702b603df7191cd68e6544f99126135e34\",\n          \"private_key\": \"0x9d77c0558ed6f31e7be16c2f45fd1f3ec3f66d20526b7fe3961872d38ac98fe7\"\n        },\n        \"sequencer\": {\n          \"address\": \"0xbb900cf56918a2639daa90c3f7dc5dcd2f5b9935\",\n          \"private_key\": \"0x459f58fd8ef3d9123333514d0f2445153fc29ddc43be239356dad70f506475d9\"\n        },\n        \"sequencerFeeVaultRecipient\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        },\n        \"systemConfigOwner\": {\n          \"address\": \"0x589a698b7b7da0bec545177d3963a2741105c7c9\",\n          \"private_key\": \"eaba42282ad33c8ef2524f07277c03a776d98ae19f581990ce75becb7cfa1c23\"\n        }\n      }\n    }\n  ]\n}\njust simple-devnet  31.66s user 17.44s system 45% cpu 1:47.83 total\n```\n</details>\n\nThis experience is already way better than it used to be, so thanks for making all the improvements so far! Might be worth broadcasting more about the switch to `mise` in case others have that problem", "2025-03-26T14:02:54Z", "2025-03-26T14:02:54Z", "geoknee", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM6-HEO4", "I_kwDODjvEJM6qPqN_", "@janjakubnanista to confirm whether there is any additional work left before closing out.", "2025-08-14T18:46:55Z", "2025-08-14T18:46:55Z", "alfonso-op", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7DSV5O", "I_kwDODjvEJM6qPqN_", "Largely (if not fully) completed, also closing since Kurtosis is not currently a priority.", "2025-09-10T20:14:10Z", "2025-09-10T20:14:10Z", "alfonso-op", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7EDtOV", "I_kwDODjvEJM6mxmwB", "Resolved!", "2025-09-14T07:25:28Z", "2025-09-14T07:25:28Z", "pengin7384", "2025-10-13 20:35:28"]
["IC_kwDODjvEJM7GiCdV", "I_kwDODjvEJM7NsGCH", "Related: https://github.com/ethereum-optimism/optimism/pull/17412#discussion_r2373746848", "2025-09-24T21:45:41Z", "2025-09-24T21:45:41Z", "hexshire", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7Gg3vl", "I_kwDODjvEJM7Nrd8q", "Related: https://github.com/ethereum-optimism/optimism/pull/17412#discussion_r2373741172", "2025-09-24T20:08:23Z", "2025-09-24T20:08:23Z", "hexshire", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7GhMg5", "I_kwDODjvEJM7NrXJx", "Related: https://github.com/ethereum-optimism/optimism/pull/17412#discussion_r2373742751", "2025-09-24T20:25:31Z", "2025-09-24T20:25:31Z", "hexshire", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7GiFes", "I_kwDODjvEJM7NrXJx", "Related: https://github.com/ethereum-optimism/optimism/pull/17412#discussion_r2373760488", "2025-09-24T21:50:58Z", "2025-09-24T21:50:58Z", "hexshire", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7Gy3LK", "I_kwDODjvEJM7MAxxE", "See this recipe for manual testing against a Fusaka L1 https://www.notion.so/oplabs/Pointing-a-batcher-at-a-Fusaka-devnet-279f153ee1628013b176f28cc5b8c5c0", "2025-09-25T17:41:21Z", "2025-09-25T17:41:21Z", "geoknee", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7C8aBC", "I_kwDODjvEJM7Ifq1Z", "~Enabling mainnet sync testing is blocked by https://github.com/ethereum-optimism/infra/issues/454~", "2025-09-09T12:54:26Z", "2025-09-12T11:18:15Z", "pcw109550", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7B8xeq", "I_kwDODjvEJM7BpC1f", "Latest updates:\n- Both of the sunnyside sequencers have op-rbuilder deployed and synced.\n\nNext steps:\n- Deploy rollup-boosts on their end with disabled mode\n- Deploy a backup sequencer on their end with no flashblocks.\n- Based on flashblocks-enablement's health, enable execution_mode on their rollup-boost", "2025-09-04T14:11:44Z", "2025-09-04T14:11:44Z", "yashvardhan-kukreja", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7GIeYS", "I_kwDODjvEJM7BpC1f", "Done here - https://github.com/ethereum-optimism/k8s/pull/7341", "2025-09-23T13:50:15Z", "2025-09-23T13:50:15Z", "yashvardhan-kukreja", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7F7Zpv", "I_kwDODjvEJM6xau5F", "Closing since we are moving away from `kurtosis`", "2025-09-22T18:25:26Z", "2025-09-22T18:25:26Z", "janjakubnanista", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7KHtly", "I_kwDODjvEJM7QrYWo", "#17819 will at least reduce the problem to situations where we have a real output root not found error. \n\nBut for a general solution we could use syncStatus. Use the highest unsafe block across all nodes as the expected current L2 head. If the requested L2 block is greater then it's a future proposal, and treat it like we do now. If it's not future, then for nodes that are behind ignore the result (and log). ", "2025-10-10T16:18:14Z", "2025-10-10T16:18:14Z", "pauldowman", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7KYJo9", "I_kwDODjvEJM7QrYWo", "We can't necessarily trust that the highest L2 block is correct though - that may be the node that followed an incorrect fork. I think what we'd want to look at is the CurrentL1 from the sync status to see whether the node has processed the L1 block the dispute game was created in or not.  If it has processed it, then it is in sync enough, if not it is behind and may give invalid results.", "2025-10-12T20:33:19Z", "2025-10-12T20:33:19Z", "ajsutton", "2025-10-13 20:35:39"]
["IC_kwDODjvEJM7KgIoF", "I_kwDODjvEJM7PDBrt", "Hi @yashvardhan-kukreja!\n\nMade a pr with an intention to solve this issue, please let me know what you think about it", "2025-10-13T12:55:34Z", "2025-10-13T12:55:34Z", "CreeptoGengar", "2025-10-13 20:35:40"]
["IC_kwDODjvEJM7KgP-s", "I_kwDODjvEJM7PDBrt", "Hi @CreeptoGengar , thanks for making the effort to make the contribution but this issue is out of the scope of op-conductor.\nIt's implementation is meant to be made on the infra end of things on our side. Hence, it has a status \"blocked\" and its association to our Platforms Team Tracker.\n\nWe'd also appreciate firstly getting the issue assigned to yourself by discussing with a maintainer over the issue's discussion.\n\nFor this reason, I'll proceed to close your PR but looking forwards to any future contributions.", "2025-10-13T13:04:03Z", "2025-10-13T13:04:03Z", "yashvardhan-kukreja", "2025-10-13 20:35:40"]
["IC_kwDOKIsnqM7GgvcA", "I_kwDOKIsnqM7NrX0r", "7. I couldn't find an explanation of what the \"normalized state diff hash\" is. Be nice to include that.", "2025-09-24T19:59:34Z", "2025-09-24T19:59:34Z", "geoknee", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7Gyfbk", "I_kwDOKIsnqM7NrX0r", "Thanks for the feedback @geoknee. I've raised a PR here that addresses some of the snags you raised above: https://github.com/ethereum-optimism/superchain-ops/pull/1230\n\nI'd like to step through each item to provide more colour:\n\n1. During task simulation we intentionally print the markdown with 'TODOs'. During signing this is not printed. The reason we print the markdown is so that the task developer, if required, can easily create the VALIDATION.md files state diff section. I think there are some conversations about only adding state diffs for non-OPCM upgrades. I'd suggest until we have a clear decision here, we should keep printing this markdown during the simulation step. The 'TODOs' are meant to guide the task developer to insert missing details that the signer will want to see when reviewing VALIDATION.md.\n\n2. True, the addition of audit reports to our workflow was experimental and not something we've fully committed to. The language here should be changed. I've added a fix in the PR mentioned above.\n\n3. op-txverify is required for some signers but not for others, I still think we need to land on a solid decision here. cc: @smartcontracts \n\n4. Good call out, added this in the PR above.\n\n5. This is due to the task developer not writing the correct markdown. I've made some helpful changes to the boilerplate markdown file in the PR above.\n\n6. Again, we still do validate state diffs for non-opcm upgrades. I think for the time being we should keep the tenderly link.\n\n7. Added in the PR.", "2025-09-25T17:10:45Z", "2025-09-25T17:10:45Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7Gy_dc", "I_kwDOKIsnqM7NrX0r", "1. You say the TODOs are only printing during signing, but don't we ask signers to perform the task simulation? If so, then it is part of the UX for signers? \n\nThanks for the clarification on the other points and fixing some things on that PR.", "2025-09-25T17:52:51Z", "2025-09-25T17:52:51Z", "geoknee", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GzDaV", "I_kwDOKIsnqM7NrX0r", "@geoknee true, we do expect the signers to simulate too. In practice I think most signers run the `sign` command and check the tenderly link before actually signing on their ledger. Not ideal. So on second thoughts, this is a valid point and we should look to clean up the simulation command for when signers are running it. ", "2025-09-25T17:58:10Z", "2025-09-25T17:58:10Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgeyD", "I_kwDOKIsnqM7GPl0h", "@JosepBove built this template here: https://github.com/ethereum-optimism/superchain-ops/pull/1219", "2025-09-24T19:41:35Z", "2025-09-24T19:41:35Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgvdT", "I_kwDOKIsnqM6vMNvV", "I think superchain-op's architecture has changed meaningfully since this issue was created. For that reason, I'm going to close this issue. Please let me know if I am missing something. ", "2025-09-24T19:59:35Z", "2025-09-24T19:59:35Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GguZ8", "I_kwDOKIsnqM6vAKpi", "Closing this ticket based on the de-prioritization of this work. ", "2025-09-24T19:58:25Z", "2025-09-24T19:58:25Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgfTa", "I_kwDOKIsnqM6slOrv", "Closing as this is not longer needed. ", "2025-09-24T19:42:10Z", "2025-09-24T19:42:10Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgxtS", "I_kwDOKIsnqM6sLSQl", "@alcueca can you add a little more detail to this ticket please? ", "2025-09-24T20:02:02Z", "2025-09-24T20:02:02Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7Ggt5V", "I_kwDOKIsnqM6sLSJZ", "Closing this as I believe this is covered by doing \n```\ncd src/\njust new task # follow the instructions\n```\nLet me know if I am missing something. ", "2025-09-24T19:57:52Z", "2025-09-24T19:57:52Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6fuOlF", "I_kwDOKIsnqM6rb0di", "This issue should not block mainnet as this is a small refactor/cleanup.", "2025-02-24T21:31:08Z", "2025-02-24T21:31:08Z", "ElliotFriedman", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GggpX", "I_kwDOKIsnqM6rb0di", "Closing as AccountAccessParser has been refactored since this. ", "2025-09-24T19:43:49Z", "2025-09-24T19:43:49Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7Ggf9A", "I_kwDOKIsnqM6q_sC4", "I think this can be closed as we do not use sim-seq anymore. Please let me know if I'm missing something.", "2025-09-24T19:42:58Z", "2025-09-24T19:42:58Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgfyH", "I_kwDOKIsnqM6q_q8u", "I think this can be closed as we do not use sim-seq anymore. Please let me know if I'm missing something.", "2025-09-24T19:42:44Z", "2025-09-24T19:42:44Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GglH6", "I_kwDOKIsnqM6qrfR4", "This CI 429 error has been meaningfully reduced so closing this issue. ", "2025-09-24T19:48:59Z", "2025-09-24T19:48:59Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7Gghwo", "I_kwDOKIsnqM6qf2kT", "Completed. ", "2025-09-24T19:45:06Z", "2025-09-24T19:45:06Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GghHs", "I_kwDOKIsnqM6qJRqQ", "Architecture has meaningfully changed since this issue was opened so closing. ", "2025-09-24T19:44:22Z", "2025-09-24T19:44:22Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GghhG", "I_kwDOKIsnqM6p1e7I", "Architecture has meaningfully changed since this issue was opened so closing.", "2025-09-24T19:44:50Z", "2025-09-24T19:44:50Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgiCW", "I_kwDOKIsnqM6pg8DL", "Architecture has meaningfully changed since this issue was opened so closing.", "2025-09-24T19:45:28Z", "2025-09-24T19:45:28Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgicS", "I_kwDOKIsnqM6pZAyx", "This is no longer an issue, closing. Let me know if I'm missing something.", "2025-09-24T19:45:56Z", "2025-09-24T19:45:56Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgjQH", "I_kwDOKIsnqM6oXGVC", "Completed: https://github.com/ethereum-optimism/superchain-ops/pull/1121 https://github.com/ethereum-optimism/superchain-ops/pull/1128 https://github.com/ethereum-optimism/superchain-ops/pull/1132", "2025-09-24T19:46:50Z", "2025-09-24T19:46:50Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7Ggpju", "I_kwDOKIsnqM6n30-p", "Closing this issue as superchain-ops has documentation that goes over all the above. ", "2025-09-24T19:53:28Z", "2025-09-24T19:53:28Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgrcM", "I_kwDOKIsnqM6kRbAT", "Closing this issue as it's no longer a problem. ", "2025-09-24T19:55:21Z", "2025-09-24T19:55:27Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6YetKa", "I_kwDOKIsnqM6kIDMK", "I\u2019m curious if you see a way to ensure in CI that this toml file us kept up to date?", "2024-12-21T17:48:31Z", "2024-12-21T17:48:31Z", "maurelian", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgpAV", "I_kwDOKIsnqM6kIDMK", "We have this feature via stacked simulations. We can tell which tasks are pending by running `just list-stack <network>`. Closing this issue. ", "2025-09-24T19:52:56Z", "2025-09-24T19:52:56Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgzFN", "I_kwDOKIsnqM6j5kpu", "Closing this issue as superchain-ops no longer relies on `JsonTxBuilderBase`. ", "2025-09-24T20:03:23Z", "2025-09-24T20:03:23Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6B9_wJ", "I_kwDOKIsnqM6Jnybe", "May i work on this issue?", "2024-06-20T12:08:13Z", "2024-06-20T12:08:13Z", "ShubhSensei", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6N34AF", "I_kwDOKIsnqM6Jnybe", "Hey @ShubhSensei, are you still interested in working on this? If so, here is my thinking for what we should do:\r\n- Add a new `script/utils/check-task-numbering.sh`\r\n- This is a shell script that loops through every `tasks/{network}` folder. These folders contain a bunch of tasks of the form `{number}-{optionalOtherNumber-}{taskName}`\r\n- We call `{number}-{optionalOtherNumber-}` the task prefix\r\n- For each folder, ensure that every task prefix is unique\r\n- Run this new shell script in CI\r\n\r\nI think we should only check for uniqueness, and not for any type of ordering or incrementing pattern. This is because:\r\n1. Uniqueness is simpler\r\n2. We do not always add tasks in order, i.e. sometimes for a bundle of upcoming tasks we'll add task 018 before task 017 is ready, but we don't want to block task 018 from being merged\r\n\r\n@maurelian what do you think of this spec?", "2024-09-27T22:33:01Z", "2024-09-27T22:33:01Z", "mds1", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6N63Kw", "I_kwDOKIsnqM6Jnybe", "Yes @mds1 of course... I'm on it \ud83e\udee1 \nBTW, is their any other medium where I can solve my queries otherwise it'll take forever like this. ", "2024-09-28T23:01:23Z", "2024-09-28T23:01:23Z", "ShubhSensei", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6OD_6u", "I_kwDOKIsnqM6Jnybe", "Thanks, assigned! And apologies for the slow response here, I had meant to come back to this after #152 was addressed but never did \ud83d\ude05", "2024-09-30T14:44:21Z", "2024-09-30T14:44:21Z", "mds1", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6OF02S", "I_kwDOKIsnqM6Jnybe", "The spec LGTM yes.", "2024-09-30T18:29:28Z", "2024-09-30T18:29:28Z", "maurelian", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6O2I8G", "I_kwDOKIsnqM6Jnybe", "How do I run this script in CI? @mds1 ", "2024-10-07T10:39:15Z", "2024-10-07T10:39:15Z", "ShubhSensei", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6O7dKR", "I_kwDOKIsnqM6Jnybe", "> How do I run this script in CI?\r\n\r\nI left a comment on the PR with this info", "2024-10-07T21:40:21Z", "2024-10-07T21:40:21Z", "mds1", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM6gZQHC", "I_kwDOKIsnqM6Jnybe", "@ShubhSensei, thanks for your initial work on this, I gave it another go.", "2025-02-28T15:42:05Z", "2025-02-28T15:42:05Z", "alcueca", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgnRG", "I_kwDOKIsnqM6Jnybe", "This issue has been implemented and can now be closed. ", "2025-09-24T19:51:12Z", "2025-09-24T19:51:12Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7Ggkpw", "I_kwDOKIsnqM6F3SOR", "Superchain-ops has implemented a new architecture based on the conversation above. We can now close this issue. ", "2025-09-24T19:48:26Z", "2025-09-24T19:48:26Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7GgkFn", "I_kwDOKIsnqM59MydP", "Architecture has meaningfully changed since this issue was opened so closing. Please let me know if this issue should be reopened. ", "2025-09-24T19:47:47Z", "2025-09-24T19:47:47Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOKIsnqM7Ggmh3", "I_kwDOKIsnqM5zb2xU", "Closing this issue based on superchain-ops's new architecture. We no longer use `JsonTxBuilderBase.sol`. ", "2025-09-24T19:50:25Z", "2025-09-24T19:50:25Z", "blmalone", "2025-10-13 20:35:49"]
["IC_kwDOJ_r-bs7IokYQ", "I_kwDOJ_r-bs6eo8EE", "Spoke with @bitwiseguy, closing this as this is not relevant given that we have switched to a report-based approach from a validation one for some time now.", "2025-10-03T15:01:52Z", "2025-10-03T15:01:52Z", "alfonso-op", "2025-10-13 20:36:00"]
["IC_kwDODjvEJM7B8jCn", "I_kwDODjvEJM6_Vvda", "Archived considering op-node-v2 work revising interop.", "2025-09-04T13:59:47Z", "2025-09-04T13:59:47Z", "yashvardhan-kukreja", "2025-10-13 20:36:05"]
["IC_kwDOJ_r-bs7CPvub", "I_kwDOJ_r-bs6ix3vn", "@bitwiseguy I thought we already made chain configs self-descriptive - is that the case? Can we close this as done?", "2025-09-05T16:09:08Z", "2025-09-05T16:09:08Z", "alfonso-op", "2025-10-13 20:36:11"]
["IC_kwDODjvEJM7ENf1R", "I_kwDODjvEJM7LIS2N", "@pauldowman is this a duplicate of https://github.com/ethereum-optimism/optimism/issues/15784 or is the intent to go more in-depth on how the constructor and implArgs work?", "2025-09-15T12:16:22Z", "2025-09-15T12:16:22Z", "stevennevins", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7E4DCg", "I_kwDODjvEJM7LIS2N", "Duplicate", "2025-09-17T13:30:10Z", "2025-09-17T13:30:10Z", "pauldowman", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7DTh6S", "I_kwDODjvEJM7K4hBz", "Draft PR that adds a hint for this: https://github.com/ethereum-optimism/optimism/pull/16151\n\nWe should validate that it actually has the right info etc but it's at least a good starting point.", "2025-09-10T21:59:32Z", "2025-09-10T21:59:32Z", "ajsutton", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7DfqFl", "I_kwDODjvEJM7K4hBz", "@meyer9 for vis", "2025-09-11T10:41:48Z", "2025-09-11T10:41:48Z", "emhane", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7EhH59", "I_kwDODjvEJM7JLEnS", "Another flake: https://app.circleci.com/pipelines/github/ethereum-optimism/optimism/100984/workflows/9f4f5833-66c8-418a-99a6-e9726a696bde/jobs/3869287/tests", "2025-09-16T09:25:36Z", "2025-09-16T09:25:36Z", "scharissis", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7E5Oqn", "I_kwDODjvEJM7JLEnS", "https://github.com/ethereum-optimism/optimism/pull/17495 did not fix the issue.", "2025-09-17T14:40:49Z", "2025-09-17T14:40:49Z", "pcw109550", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7EvnrI", "I_kwDODjvEJM7H4a3c", "Closed via https://github.com/ethereum-optimism/optimism/pull/17231", "2025-09-17T00:57:23Z", "2025-09-17T00:57:23Z", "bitwiseguy", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7DSbnN", "I_kwDODjvEJM7Hu8Er", "Won't implement, closing.", "2025-09-10T20:23:07Z", "2025-09-10T20:23:07Z", "alfonso-op", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7DThB5", "I_kwDODjvEJM7Hu8Er", "@alfonso-op Reopening this one because there's a TODO still in the code that references it:\n\n```\nRepository & Issue                       | Title                                                             | Location                                \n-----------------------------------------+-------------------------------------------------------------------+---------------------------------------------------\nethereum-optimism/optimism #17194        | Implement Websockets support in the Kurtosis Reverse Proxy        | op-devstack/sysext/helpers.go:88                  \n```\n\nIf we don't plan to do this anymore we need to remove the TODO.", "2025-09-10T21:57:35Z", "2025-09-10T21:57:35Z", "ajsutton", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7Db5ug", "I_kwDODjvEJM7Hu8Er", "@serpixel mind removing the TODO in code and then closing this out? We won't be implementing this feature and I'm working on grooming our backlog.", "2025-09-11T07:30:45Z", "2025-09-11T07:31:18Z", "alfonso-op", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7EQ5U-", "I_kwDODjvEJM7BpI-T", "Done here - https://www.notion.so/oplabs/eris-betanet-Acceptance-test-results-218f153ee16280b1ab5fd6cd5f8248cb?source=copy_link", "2025-09-15T15:27:26Z", "2025-09-15T15:27:26Z", "yashvardhan-kukreja", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7EP_3j", "I_kwDODjvEJM7BouYk", "Completed here - https://www.notion.so/oplabs/Flashblocks-Load-Testing-265f153ee16280d4bac0ffde54ce60bf?source=copy_link\n\n@zhwrd if you have anything additional to add, feel free to go ahead, otherwise, we can proceed to close this issue.", "2025-09-15T14:39:43Z", "2025-09-15T14:39:43Z", "yashvardhan-kukreja", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM6rg1ZO", "I_kwDODjvEJM62Qq_h", "@protolambda - can you expand on what you mean for the checklist item `op-supervisor loaded from registry`?", "2025-05-13T18:10:45Z", "2025-05-13T18:10:45Z", "bitwiseguy", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM6sIRNp", "I_kwDODjvEJM62Qq_h", "FYI I added https://github.com/ethereum-optimism/optimism/issues/13917 as a sub-issue. It's also tracked on the proofs team project board. cc @Inphi ", "2025-05-16T23:41:15Z", "2025-05-16T23:41:15Z", "pauldowman", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM6kY8bJ", "I_kwDODjvEJM6vCqCt", "I am running into this a lot with devnet-sdk work too. Can I help fix it? Or is this already scheduled into current cycle?\n", "2025-03-27T13:13:53Z", "2025-03-27T13:13:53Z", "protolambda", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM6kY-wQ", "I_kwDODjvEJM6vCqCt", "Also, some interfaces + composition of scope of addresses would be nice. I tried something in the legacy op-chain-ops inputs/outputs like that, but with structs instead of interfaces, but it's not consistently used yet.\n\nHaving a read-only definition of addresses, by using interfaces, would also be really nice for ease of verifying what is happening.\n\n", "2025-03-27T13:17:00Z", "2025-03-27T13:17:00Z", "protolambda", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM6ka9YO", "I_kwDODjvEJM6vCqCt", "@protolambda I am planning to start working on this next week. I can explore your idea to use interfaces. Here is a [notion doc](https://www.notion.so/oplabs/redundant-address-structs-1c3f153ee16280e1af1afe2bda94f752) to collect some thoughts. Let's move the conversation there.\n\nPlease let me know what is the biggest pain point related to devnet-sdk that you are currently running into. If possible, we can  address that initially, then work on the broader refactor as a follow-up", "2025-03-27T15:45:19Z", "2025-03-27T15:45:19Z", "bitwiseguy", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7FdcXw", "I_kwDODjvEJM6vCqCt", "Intended scope was completed here, closing.", "2025-09-19T16:19:41Z", "2025-09-19T16:19:41Z", "alfonso-op", "2025-10-13 20:36:28"]
["IC_kwDODjvEJM7H0UzS", "I_kwDODjvEJM7Ozwpr", "Completed here - https://www.optimism.io/blog/flashblocks-deep-dive-250ms-preconfirmations-on-op-mainnet", "2025-09-30T14:08:20Z", "2025-09-30T14:08:20Z", "yashvardhan-kukreja", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7HVNFT", "I_kwDODjvEJM7OGsLC", "Hi @pcw109550 \n\nMade an implementation of this, please check it out when you have free time", "2025-09-28T20:25:49Z", "2025-09-28T20:25:49Z", "avorylli", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7HIRvA", "I_kwDODjvEJM7OGKXC", "op-geth won't trigger snap sync if it already has a finalised sync but it *will* still sync. We don't actually care how the EL syncs - we just need it to get itself to the new chain head. ", "2025-09-27T00:04:57Z", "2025-09-27T00:04:57Z", "ajsutton", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7HcQgJ", "I_kwDODjvEJM7OGKXC", "Agree. It doesn't need to be snap-sync that backfills blocks after a FCU call, and it probably won't be most of the time, only when a node hasn't been synced yet. EL clients will then do full block execution via their p2p network afaik. So what they need as prerequisites to backfill should just be 1. p2p connections to other nodes that have the block range they want to backfill, and 2. receive a FCU call to set the new sync targets.", "2025-09-29T10:02:23Z", "2025-09-29T12:08:48Z", "sebastianst", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7Hdsfp", "I_kwDODjvEJM7OGKXC", "@ajsutton @sebastianst Thanks for the comments! I generally agree with these comments, we may refine the ticket and emphasize for avoiding leaking abstractions of the implementation detail from EL.\n\nThe confusion came from my misunderstanding that we may trigger snap-sync twice(not possible unless manual db wipe), by properly setting the conditions of op-geth. But yes I agree that EL _should_ just sync. \n\n", "2025-09-29T11:32:36Z", "2025-09-29T11:32:52Z", "pcw109550", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7IPuHn", "I_kwDODjvEJM7OGKXC", "We may close this ticket. Since for removing ReqRes sync, we may focus EL Syncs after the initial EL Sync run. So this may be already handled at `engineController.insertUnsafePayload`.", "2025-10-02T07:24:27Z", "2025-10-02T07:24:27Z", "pcw109550", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7H0mz1", "I_kwDODjvEJM7NSfcU", "Closing this, https://github.com/ethereum-optimism/optimism/pull/17648 is merged, and tested after the proper locking is applied with race window amplified. The PR seems to fix the issue. We may need race detector enabled for the future impls.", "2025-09-30T14:23:21Z", "2025-09-30T14:23:34Z", "pcw109550", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7ItQtV", "I_kwDODjvEJM7MPF1U", "via #17666 ", "2025-10-03T21:48:06Z", "2025-10-03T21:48:06Z", "geoknee", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7FKZVn", "I_kwDODjvEJM7MNTa-", "Note that if we tightly couple to upstream geth code, it's possible that every upstream merge + op-geth / op-node release requires an absolute prestate update.", "2025-09-18T14:37:04Z", "2025-09-18T14:37:04Z", "geoknee", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7ITe76", "I_kwDODjvEJM7MBr-U", "Since kurtosis devnet is deprecated, we should instead ensure op-reth is integrated into the devstack / sysgo system instead. ", "2025-10-02T10:59:25Z", "2025-10-02T10:59:25Z", "geoknee", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7Hfs4q", "I_kwDODjvEJM7Lyy8h", "Performed and augmented [this doc](https://www.notion.so/oplabs/Flashblocks-Load-Testing-eris-0-265f153ee16280d4bac0ffde54ce60bf) to include their results.\n\nNote:\n- The additional scenario `Flashblocks-enabled with rollup-boost forwarding flashblocks` per load test covers this case's load tests.\n- The inflated p90/p99 latency is due to the fact that these tests were performed from India as opposed to Canada causing speed-of-light problems.", "2025-09-29T13:31:44Z", "2025-09-29T13:31:44Z", "yashvardhan-kukreja", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7IJKVh", "I_kwDODjvEJM7K9VvR", "Closed by https://github.com/ethereum-optimism/k8s/pull/7612", "2025-10-01T19:31:01Z", "2025-10-01T19:31:01Z", "yashvardhan-kukreja", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7Hs8pr", "I_kwDODjvEJM7GwbQK", "Done [here](https://www.notion.so/oplabs/Flashblocks-onboarding-27df153ee162802e956dd8187659e4f1) with a presented this in the last 10-12 mins. of [this recording](https://drive.google.com/file/d/1HPY89efVXCuEysRVi3eS3vUFqESkoNjF/view)", "2025-09-30T07:45:40Z", "2025-09-30T07:45:40Z", "yashvardhan-kukreja", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7Ibx1Y", "I_kwDODjvEJM7F4Hds", "Link to the blocking issue: https://github.com/golang/go/issues/74998", "2025-10-02T19:56:58Z", "2025-10-02T19:56:58Z", "pauldowman", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7Ii3zx", "I_kwDODjvEJM7F4Hds", "@pauldowman Are we affected by this issue? Seems like it affects some emulated MIPS hardware, but not others.", "2025-10-03T07:24:57Z", "2025-10-03T07:24:57Z", "sebastianst", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7IuNx9", "I_kwDODjvEJM7F4Hds", "I believe it does affect us. @ajsutton has more context.", "2025-10-03T23:42:56Z", "2025-10-03T23:42:56Z", "pauldowman", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7I1EB9", "I_kwDODjvEJM7F4Hds", "Yes, it affects us.  All keccak hashes are incorrect when running in cannon with go1.25. I assume other crypto is likely affected as well, but we hit the keccak problem first as we can't calculate block hashes correctly.\n\nhttps://www.notion.so/oplabs/Cannon-Go-1-25-Compatibility-250f153ee1628080844cd79c93fa991b?source=copy_link has a long diagnosis but the linked issue is exactly the underlying issue.", "2025-10-05T21:48:22Z", "2025-10-05T21:48:22Z", "ajsutton", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7B8oQF", "I_kwDODjvEJM7BpGUX", "Need to confirm its need.", "2025-09-04T14:03:48Z", "2025-09-04T14:03:48Z", "yashvardhan-kukreja", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7HgZh4", "I_kwDODjvEJM7BpGUX", "Spun up with the identifier - op-opn-reth-f-fb-0", "2025-09-29T14:03:00Z", "2025-09-29T14:03:00Z", "yashvardhan-kukreja", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7IE0f9", "I_kwDODjvEJM7Ab1Xl", "This was fully done in the spike, including a PR that split out the first step, but that was not supported. Closing this as not planned.\n", "2025-10-01T14:35:52Z", "2025-10-01T14:35:52Z", "protolambda", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7Hs-vw", "I_kwDODjvEJM68L4nd", "Flashblocks were enabled live on mainnet on September 30, 2025.\n\n```\n\u276f ./op-conductor-ops status op-mainnet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Sequencer ID                           \u2503 Active \u2503 Healthy \u2503 Leader \u2503 Sequencing \u2503 Voting \u2503 Unsafe #  \u2503 Unsafe Hash                                                        \u2503 Bldr Synced \u2503 RB Mode \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 prod-mainnet-sequencer-0               \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u274c     \u2502 141809829 \u2502 0xed64e202804be236459b902b7bd64a8c992c8ca620e04a0f690f3ed1253a2b18 \u2502 \u2753          \u2502 \u2753      \u2502\n\u2502 prod-mainnet-op-opn-geth-a-sequencer-0 \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u274c     \u2502 141809829 \u2502 0xed64e202804be236459b902b7bd64a8c992c8ca620e04a0f690f3ed1253a2b18 \u2502 \u2753          \u2502 \u2753      \u2502\n\u2502 prod-mainnet-op-opn-geth-f-sequencer-0 \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u2705     \u2502 141809829 \u2502 0xed64e202804be236459b902b7bd64a8c992c8ca620e04a0f690f3ed1253a2b18 \u2502 \u2705          \u2502 enabled \u2502\n\u2502 prod-mainnet-op-opn-geth-f-sequencer-1 \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u2705     \u2502 141809829 \u2502 0xed64e202804be236459b902b7bd64a8c992c8ca620e04a0f690f3ed1253a2b18 \u2502 \u2705          \u2502 enabled \u2502\n\u2502 prod-mainnet-op-opn-reth-f-sequencer-2 \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u274c     \u2502 141809829 \u2502 0xed64e202804be236459b902b7bd64a8c992c8ca620e04a0f690f3ed1253a2b18 \u2502 \u2753          \u2502 \u2753      \u2502\n\u2502 prod-mainnet-op-opn-geth-f-sequencer-3 \u2502 \u2705     \u2502 \u2705      \u2502 \u2705     \u2502 \u2705         \u2502 \u2705     \u2502 141809829 \u2502 0xed64e202804be236459b902b7bd64a8c992c8ca620e04a0f690f3ed1253a2b18 \u2502 \u2705          \u2502 enabled \u2502\n\u2502 prod-mainnet-op-opn-geth-f-sunnyseq-1  \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u2705     \u2502 141809830 \u2502 0xc7b072383dec041f7779c7b977cc26c440d28dc94fa7570d9b4c0454ab49c422 \u2502 \u2705          \u2502 enabled \u2502\n\u2502 prod-mainnet-op-opn-geth-f-sunnyseq-2  \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u2705     \u2502 141809830 \u2502 0xc7b072383dec041f7779c7b977cc26c440d28dc94fa7570d9b4c0454ab49c422 \u2502 \u2705          \u2502 enabled \u2502\n\u2502 prod-mainnet-op-opn-geth-f-sunnyseq-3  \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u274c     \u2502 141809830 \u2502 0xc7b072383dec041f7779c7b977cc26c440d28dc94fa7570d9b4c0454ab49c422 \u2502 \u2753          \u2502 \u2753      \u2502\n\u2502 prod-mainnet-op-opn-reth-f-sunnyseq-1  \u2502 \u2705     \u2502 \u2705      \u2502 \u274c     \u2502 \u274c         \u2502 \u274c     \u2502 141809830 \u2502 0xc7b072383dec041f7779c7b977cc26c440d28dc94fa7570d9b4c0454ab49c422 \u2502 \u2753          \u2502 \u2753      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```", "2025-09-30T07:47:40Z", "2025-09-30T07:47:40Z", "yashvardhan-kukreja", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM6WJzRa", "I_kwDODjvEJM6iC5rV", "Hi @Inphi I would like to take this issue", "2024-12-05T05:10:45Z", "2024-12-05T05:10:45Z", "Dhir0808", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM6Wbup3", "I_kwDODjvEJM6iC5rV", "@Dhir0808 Thanks! Assigning to you.", "2024-12-06T17:51:53Z", "2024-12-06T17:51:53Z", "Inphi", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM6XxLDi", "I_kwDODjvEJM6iC5rV", "Do you have an example artifact that fails to load? And which `forge` version do you use? I can help fix this issue.", "2024-12-16T17:39:17Z", "2024-12-16T17:39:38Z", "protolambda", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM6RteEN", "I_kwDODjvEJM6cK7hD", "One note is that `expectRevert` in forge has a giant footgun in that it's intended to only work on the next CALL, but by accident it also works on code at the same call depth as the test (e.g. JUMPs to a function), but only for the first one in a test, and the rest of the test silently does not execute. Example + more info in this issue: https://github.com/foundry-rs/foundry/issues/3437#issuecomment-2394472184\r\n\r\nEveryone has come to rely on this bug in `expectRevert` so it has not been removed yet. Regardless, let's take the stricter approach here and only support `expectRevert` on CALLs, which is the ideal foundry v1 fix since that is how all other `expect*` cheats work: https://github.com/foundry-rs/foundry/issues/7238", "2024-10-29T15:30:03Z", "2024-10-29T15:30:03Z", "mds1", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM6XxRFQ", "I_kwDODjvEJM6cK7hD", "Planning on doing a larger op-chain-ops test-cheatcodes support push now. I am keen to close the gap between Go e2e testing and smart-contracts.", "2024-12-16T17:51:28Z", "2024-12-16T17:51:28Z", "protolambda", "2025-10-13 20:36:45"]
["IC_kwDODjvEJM7IZIlT", "I_kwDODjvEJM6cGQMK", "Isthmus is done", "2025-10-02T16:12:49Z", "2025-10-02T16:12:49Z", "sebastianst", "2025-10-13 20:36:45"]
["IC_kwDOLB-lzc7LKUw8", "I_kwDOLB-lzc7P5y7K", "Via #800 ", "2025-10-15T22:05:19Z", "2025-10-15T22:05:19Z", "geoknee", "2025-10-17 03:24:09"]
["IC_kwDODjvEJM7LVooa", "I_kwDODjvEJM7R8dwD", "@zhwrd  Could you help me? Thank you!", "2025-10-16T15:25:07Z", "2025-10-16T15:25:07Z", "wluisw", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7LVFHp", "I_kwDODjvEJM7RykJM", "This analysis may imply no need for any saturating math https://github.com/ethereum-optimism/design-docs/pull/349#discussion_r2433848227", "2025-10-16T14:51:32Z", "2025-10-16T14:51:32Z", "geoknee", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7KxxIA", "I_kwDODjvEJM7RdNJ2", "Done but waiting on their confirmation.", "2025-10-14T14:01:26Z", "2025-10-14T14:01:26Z", "yashvardhan-kukreja", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7LB-E0", "I_kwDODjvEJM7RdNJ2", "Everyone apart from Blockscout went ahead.\nBlockscout ack'd to complete it later this week.", "2025-10-15T12:53:38Z", "2025-10-15T12:53:38Z", "yashvardhan-kukreja", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7KxwCK", "I_kwDODjvEJM7RdLrl", "Flashblocks-websocket-proxy's connectivity issues and subscriber pong timeouts caused occurrences of lost Flashblocks.\n\nThe new upgrades of it with rollup-boost fixed this.", "2025-10-14T14:00:19Z", "2025-10-14T14:00:19Z", "yashvardhan-kukreja", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7K23Ev", "I_kwDODjvEJM7QrYWo", "Your solution is better. (But noting that if the node that followed an incorrect fork had a higher block I think it would just affect not found handling, not agreement.)\n\nClosing this.", "2025-10-14T19:50:18Z", "2025-10-14T19:50:18Z", "pauldowman", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7K23Zx", "I_kwDODjvEJM7QrYWo", "Oops I meant to close the PR, not the issue. Reopening until the PR is merged.", "2025-10-14T19:50:44Z", "2025-10-14T19:51:53Z", "pauldowman", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7Lab9l", "I_kwDODjvEJM7PC6bc", "Did not actually lighten op-opn-geth-a-sequencer-1. Rather kept in in the sepolia voter set and instead removed the prod-sepolia-op-opn-geth-f-sunnyseq-1\n\nhttps://github.com/ethereum-optimism/k8s/commit/9dc733b11aacc1fe8775890cd77cca85e42283e2", "2025-10-16T20:20:58Z", "2025-10-16T20:20:58Z", "jelias2", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7LWStt", "I_kwDODjvEJM7Kk6sg", "Via https://github.com/ethereum-optimism/op-geth/pull/655", "2025-10-16T16:07:24Z", "2025-10-16T16:07:24Z", "geoknee", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7LWQze", "I_kwDODjvEJM7IsZ5y", "Via https://github.com/ethereum-optimism/optimism/pull/17466", "2025-10-16T16:05:59Z", "2025-10-16T16:05:59Z", "geoknee", "2025-10-17 03:24:29"]
["IC_kwDODjvEJM7K5vcF", "I_kwDODjvEJM7IgEOU", "@mbaxter Reopening this one because there's still a TODO in the code: \n\n```\nRepository & Issue                       | Title                                                             | Location                                \n-----------------------------------------+-------------------------------------------------------------------+---------------------------------------------------\nethereum-optimism/optimism #17257        | Use v2 dispute game implementation in OPCM.deploy                 | op-acceptance-tests/tests/base/disputegame_v2/smoke_test.go:12\n\n```", "2025-10-15T00:54:44Z", "2025-10-15T00:54:44Z", "ajsutton", "2025-10-17 03:24:29"]
